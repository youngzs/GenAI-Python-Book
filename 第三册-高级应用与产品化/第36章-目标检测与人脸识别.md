# ç¬¬36ç« ï¼šç›®æ ‡æ£€æµ‹ä¸äººè„¸è¯†åˆ«

> "å¦‚æœè¯´ç¬¬35ç« è®©æœºå™¨æ‹¥æœ‰äº†'çœ‹è§'çš„èƒ½åŠ›ï¼Œé‚£ä¹ˆç¬¬36ç« å°†èµ‹äºˆæœºå™¨'ç†è§£'çš„æ™ºæ…§ã€‚åœ¨æˆ‘ä»¬çš„æ•°å­—ç›¸æœºå·¥å‚ä¸­ï¼Œä»Šå¤©å°†å»ºç«‹ä¸€ä¸ªå…¨æ–°çš„AIæ™ºèƒ½åˆ†æè½¦é—´ï¼Œè®©æœºå™¨ä¸ä»…èƒ½çœ‹åˆ°å›¾åƒï¼Œæ›´èƒ½è¯†åˆ«å…¶ä¸­çš„æ¯ä¸€ä¸ªç‰©ä½“å’Œæ¯ä¸€å¼ é¢å­”ã€‚"

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### çŸ¥è¯†ç›®æ ‡
- æŒæ¡ä¼ ç»Ÿç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆHaarçº§è”ã€HOG+SVMã€æ¨¡æ¿åŒ¹é…ï¼‰çš„åŸç†å’Œåº”ç”¨åœºæ™¯
- ç†è§£æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹æŠ€æœ¯ï¼ˆYOLOç³»åˆ—ï¼‰çš„æ ¸å¿ƒæ€æƒ³å’Œç½‘ç»œæ¶æ„
- ç†Ÿæ‚‰äººè„¸è¯†åˆ«ç³»ç»Ÿçš„å®Œæ•´æŠ€æœ¯æµç¨‹ï¼ˆæ£€æµ‹ã€å¯¹é½ã€ç‰¹å¾æå–ã€è¯†åˆ«éªŒè¯ï¼‰
- äº†è§£è®¡ç®—æœºè§†è§‰åœ¨æ™ºèƒ½å®‰é˜²ç›‘æ§ä¸­çš„ç»¼åˆåº”ç”¨å’ŒæŠ€æœ¯æŒ‘æˆ˜

### æŠ€èƒ½ç›®æ ‡  
- èƒ½å¤Ÿä½¿ç”¨Haarçº§è”åˆ†ç±»å™¨å’ŒHOGç‰¹å¾è¿›è¡Œä¼ ç»Ÿç›®æ ‡æ£€æµ‹
- èƒ½å¤Ÿéƒ¨ç½²å’Œä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œå®æ—¶å¤šç›®æ ‡æ£€æµ‹å’Œè‡ªå®šä¹‰è®­ç»ƒ
- èƒ½å¤Ÿå®ç°å®Œæ•´çš„äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ£€æµ‹ã€ç‰¹å¾æå–ã€éªŒè¯ç­‰ç¯èŠ‚
- èƒ½å¤Ÿå¼€å‘ä¼ä¸šçº§æ™ºèƒ½ç›‘æ§ç³»ç»Ÿå¹¶è¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œå®é™…éƒ¨ç½²

### ç´ å…»ç›®æ ‡
- å»ºç«‹AIè§†è§‰è¯†åˆ«é¢†åŸŸçš„ä¸“ä¸šåˆ¤æ–­èƒ½åŠ›å’ŒæŠ€æœ¯é€‰å‹æ€ç»´
- åŸ¹å…»å¤æ‚è§†è§‰ç³»ç»Ÿçš„æ¶æ„è®¾è®¡èƒ½åŠ›å’Œå·¥ç¨‹å®è·µç»éªŒ
- å»ºç«‹AIåº”ç”¨ä¸­çš„å®‰å…¨ã€éšç§ä¿æŠ¤å’Œä¼¦ç†è´£ä»»æ„è¯†
- åŸ¹å…»ä»æŠ€æœ¯ç ”å‘åˆ°äº§å“åŒ–éƒ¨ç½²çš„å®Œæ•´å•†ä¸šåŒ–å¼€å‘èƒ½åŠ›

## ğŸ­ ç« èŠ‚å¯¼å…¥ï¼šAIæ™ºèƒ½åˆ†æè½¦é—´çš„å»ºç«‹

### ğŸ¬ å¼€ç¯‡æ•…äº‹ï¼šå·¥å‚çš„æ™ºèƒ½åŒ–å‡çº§

è¿˜è®°å¾—ç¬¬35ç« ä¸­æˆ‘ä»¬å»ºç«‹çš„"æ•°å­—ç›¸æœºå·¥å‚"å—ï¼Ÿç»è¿‡ä¸€æ®µæ—¶é—´çš„è¿è¥ï¼Œå·¥å‚çš„åŸºç¡€å›¾åƒå¤„ç†èƒ½åŠ›å·²ç»éå¸¸æˆç†Ÿã€‚ç°åœ¨ï¼Œè‘£äº‹ä¼šå†³å®šè¿›è¡Œä¸€æ¬¡é‡å¤§çš„æ™ºèƒ½åŒ–å‡çº§â€”â€”å»ºç«‹**AIæ™ºèƒ½åˆ†æè½¦é—´**ï¼

èµ°è¿›è¿™ä¸ªå…¨æ–°çš„è½¦é—´ï¼Œæ‚¨ä¼šå‘ç°è¿™é‡Œçš„è®¾å¤‡å’Œç¬¬35ç« å®Œå…¨ä¸åŒï¼š

ğŸ¯ **ç›®æ ‡ä¾¦å¯Ÿéƒ¨** - è£…å¤‡äº†æœ€å…ˆè¿›çš„ä¼ ç»Ÿæ£€æµ‹ç®—æ³•ï¼Œå°±åƒç»éªŒä¸°å¯Œçš„è€ä¾¦æ¢  
ğŸ¤– **AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ** - æ‹¥æœ‰æ·±åº¦å­¦ä¹ å¤§è„‘ï¼Œèƒ½åŒæ—¶è¯†åˆ«ä¸Šç™¾ç§ä¸åŒç‰©ä½“  
ğŸ‘¤ **äººè„¸èº«ä»½éªŒè¯å±€** - ä¸“é—¨çš„äººè„¸è¯†åˆ«ç³»ç»Ÿï¼Œæ¯”äººçœ¼æ›´å‡†ç¡®æ›´å¿«é€Ÿ  
ğŸ“º **æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ** - å°†æ‰€æœ‰æŠ€æœ¯æ•´åˆæˆå®Œæ•´çš„ç›‘æ§è§£å†³æ–¹æ¡ˆ  
âš™ï¸ **æ€§èƒ½ä¼˜åŒ–å®éªŒå®¤** - ç¡®ä¿æ‰€æœ‰ç³»ç»Ÿéƒ½èƒ½åœ¨å®é™…ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œ

### ğŸ¯ è½¦é—´çš„ä½¿å‘½å‡çº§

ä½œä¸ºAIæ™ºèƒ½åˆ†æè½¦é—´çš„æ€»å·¥ç¨‹å¸ˆï¼Œæ‚¨çš„æ–°ä½¿å‘½æ˜¯ï¼š
1. **å»ºç«‹æ™ºèƒ½è¯†åˆ«èƒ½åŠ›** - è®©ç³»ç»Ÿèƒ½å‡†ç¡®è¯†åˆ«å›¾åƒä¸­çš„å„ç§ç›®æ ‡
2. **å®ç°èº«ä»½éªŒè¯åŠŸèƒ½** - å¼€å‘å¯é çš„äººè„¸è¯†åˆ«å’ŒéªŒè¯ç³»ç»Ÿ
3. **æä¾›å®æ—¶ç›‘æ§æœåŠ¡** - å°†æŠ€æœ¯æ•´åˆä¸ºå®Œæ•´çš„å•†ä¸šåŒ–äº§å“
4. **ç¡®ä¿å®‰å…¨å’Œéšç§** - åœ¨æŠ€æœ¯åˆ›æ–°çš„åŒæ—¶ä¿æŠ¤ç”¨æˆ·éšç§å®‰å…¨

### ğŸ”§ æ‚¨çš„æ–°å·¥å…·ç®±

åœ¨AIæ™ºèƒ½åˆ†æè½¦é—´ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ›´åŠ å¼ºå¤§çš„å·¥å…·ç®±ï¼š

```python
# AIæ™ºèƒ½åˆ†æè½¦é—´å·¥å…·ç®±åˆå§‹åŒ–
import cv2
import numpy as np
import torch
from ultralytics import YOLO
import face_recognition
import dlib
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from datetime import datetime
import json
import threading
from pathlib import Path

# å·¥å‚æ™ºèƒ½åŒ–å‡çº§å…¬å‘Š
print("ğŸ­ æ¬¢è¿æ¥åˆ°AIæ™ºèƒ½åˆ†æè½¦é—´ï¼")
print("ğŸ“‹ è½¦é—´éƒ¨é—¨ï¼š")
print("  ğŸ¯ ç›®æ ‡ä¾¦å¯Ÿéƒ¨ - ä¼ ç»Ÿæ£€æµ‹ç®—æ³•ä¸“å®¶")
print("  ğŸ¤– AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - æ·±åº¦å­¦ä¹ å¤§è„‘")
print("  ğŸ‘¤ äººè„¸èº«ä»½éªŒè¯å±€ - äººè„¸è¯†åˆ«ä¸“å®¶")
print("  ğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ - ç»¼åˆåº”ç”¨å¹³å°")
print("âš¡ å‡†å¤‡å¼€å§‹æ™ºèƒ½åŒ–ç”Ÿäº§...")
```

## ğŸ“š ç¬¬ä¸€èŠ‚ï¼šç›®æ ‡ä¾¦å¯Ÿéƒ¨ - ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•

### 36.1.1 Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿ

#### ğŸ•µï¸ ä¾¦å¯Ÿé˜Ÿçš„å·¥ä½œåŸç†

åœ¨ç›®æ ‡ä¾¦å¯Ÿéƒ¨ä¸­ï¼ŒHaarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿæ˜¯ç»éªŒæœ€ä¸°å¯Œçš„è€é˜Ÿå‘˜ã€‚ä»–ä»¬ä½¿ç”¨Haarç‰¹å¾æ¥å¿«é€Ÿè¯†åˆ«å›¾åƒä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œå°±åƒèµ„æ·±ä¾¦æ¢èƒ½ä»ç»†å¾®çš„çº¿ç´¢ä¸­å‘ç°å«Œç–‘äººä¸€æ ·ã€‚

#### ğŸ§® Haarç‰¹å¾æ£€æµ‹æœºåˆ¶

```python
# ç¤ºä¾‹1ï¼šHaarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿ - äººè„¸æ£€æµ‹ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import time

class HaarDetectionUnit:
    """
    Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿ
    ä¸“é—¨è´Ÿè´£å¿«é€Ÿç›®æ ‡æ£€æµ‹ä»»åŠ¡
    """
    
    def __init__(self, workspace_path="ai_analysis_workshop/target_detection"):
        """
        åˆå§‹åŒ–Haarä¾¦å¯Ÿé˜Ÿ
        
        Parameters:
        workspace_path (str): ä¾¦å¯Ÿé˜Ÿå·¥ä½œç›®å½•
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é¢„è®­ç»ƒçš„çº§è”åˆ†ç±»å™¨
        self.detectors = self._load_cascade_detectors()
        
        # ä¾¦å¯Ÿç»Ÿè®¡
        self.detection_history = []
        
        print("ğŸ•µï¸ Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜ŸæŠ¥åˆ°ï¼")
        print(f"ğŸ“ ä¾¦å¯Ÿé˜ŸåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¯ å¯ç”¨æ£€æµ‹å™¨: {list(self.detectors.keys())}")
    
    def _load_cascade_detectors(self):
        """
        åŠ è½½å„ç§ç±»å‹çš„çº§è”åˆ†ç±»å™¨
        
        Returns:
        dict: åˆ†ç±»å™¨å­—å…¸
        """
        detectors = {}
        
        try:
            # äººè„¸æ£€æµ‹å™¨ï¼ˆæ­£é¢ï¼‰
            detectors['face'] = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            
            # äººçœ¼æ£€æµ‹å™¨
            detectors['eye'] = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_eye.xml'
            )
            
            # å¾®ç¬‘æ£€æµ‹å™¨
            detectors['smile'] = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_smile.xml'
            )
            
            # å…¨èº«æ£€æµ‹å™¨
            detectors['fullbody'] = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_fullbody.xml'
            )
            
            # è½¦è¾†æ£€æµ‹å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                detectors['car'] = cv2.CascadeClassifier(
                    cv2.data.haarcascades + 'haarcascade_car.xml'
                )
            except:
                print("âš ï¸ è½¦è¾†æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡")
            
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†ç±»å™¨æ—¶å‡ºé”™: {e}")
        
        return detectors
    
    def detect_targets(self, image, target_type='face', scale_factor=1.1, 
                      min_neighbors=5, min_size=(30, 30)):
        """
        æ‰§è¡Œç›®æ ‡æ£€æµ‹ä»»åŠ¡
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        target_type (str): ç›®æ ‡ç±»å‹
        scale_factor (float): å°ºåº¦å› å­
        min_neighbors (int): æœ€å°é‚»å±…æ•°
        min_size (tuple): æœ€å°å°ºå¯¸
        
        Returns:
        tuple: (æ£€æµ‹ç»“æœ, æ£€æµ‹ç»Ÿè®¡)
        """
        print(f"ğŸ¯ ä¾¦å¯Ÿé˜Ÿå¼€å§‹æ£€æµ‹: {target_type}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # é€‰æ‹©æ£€æµ‹å™¨
        if target_type not in self.detectors:
            print(f"âŒ æœªæ‰¾åˆ° {target_type} æ£€æµ‹å™¨")
            return [], {}
        
        detector = self.detectors[target_type]
        
        # æ‰§è¡Œæ£€æµ‹
        detections = detector.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=min_size
        )
        
        # è®¡ç®—æ£€æµ‹æ—¶é—´
        detection_time = time.time() - start_time
        
        # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        stats = {
            'target_type': target_type,
            'detections_count': len(detections),
            'detection_time': detection_time,
            'image_size': image.shape,
            'parameters': {
                'scale_factor': scale_factor,
                'min_neighbors': min_neighbors,
                'min_size': min_size
            }
        }
        
        # è®°å½•æ£€æµ‹å†å²
        self.detection_history.append({
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        })
        
        print(f"   æ£€æµ‹å®Œæˆ: å‘ç° {len(detections)} ä¸ªç›®æ ‡")
        print(f"   æ£€æµ‹ç”¨æ—¶: {detection_time:.3f} ç§’")
        
        return detections, stats
    
    def draw_detection_results(self, image, detections, target_type='face', 
                             color=(255, 0, 0), thickness=2):
        """
        ç»˜åˆ¶æ£€æµ‹ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        detections (list): æ£€æµ‹ç»“æœ
        target_type (str): ç›®æ ‡ç±»å‹
        color (tuple): è¾¹æ¡†é¢œè‰²
        thickness (int): è¾¹æ¡†ç²—ç»†
        
        Returns:
        numpy.ndarray: æ ‡æ³¨åçš„å›¾åƒ
        """
        result_image = image.copy()
        
        # ä¸ºä¸åŒç›®æ ‡ç±»å‹è®¾ç½®ä¸åŒé¢œè‰²
        color_map = {
            'face': (255, 0, 0),      # çº¢è‰²
            'eye': (0, 255, 0),       # ç»¿è‰²
            'smile': (0, 255, 255),   # é»„è‰²
            'fullbody': (255, 0, 255), # ç´«è‰²
            'car': (0, 0, 255)        # è“è‰²
        }
        
        detection_color = color_map.get(target_type, color)
        
        for (x, y, w, h) in detections:
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), 
                         detection_color, thickness)
            
            # æ·»åŠ æ ‡ç­¾
            label = f"{target_type}: {w}x{h}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(result_image, 
                         (x, y - label_size[1] - 10), 
                         (x + label_size[0], y), 
                         detection_color, -1)
            
            # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
            cv2.putText(result_image, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return result_image
    
    def multi_target_detection(self, image, target_types=['face', 'eye'], 
                              detection_params=None):
        """
        å¤šç›®æ ‡åŒæ—¶æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        target_types (list): ç›®æ ‡ç±»å‹åˆ—è¡¨
        detection_params (dict): æ£€æµ‹å‚æ•°
        
        Returns:
        dict: å¤šç›®æ ‡æ£€æµ‹ç»“æœ
        """
        print("ğŸ¯ å¼€å§‹å¤šç›®æ ‡ä¾¦å¯Ÿä»»åŠ¡...")
        
        if detection_params is None:
            detection_params = {
                'face': {'scale_factor': 1.1, 'min_neighbors': 5, 'min_size': (30, 30)},
                'eye': {'scale_factor': 1.1, 'min_neighbors': 3, 'min_size': (10, 10)},
                'smile': {'scale_factor': 1.8, 'min_neighbors': 20, 'min_size': (25, 25)},
                'fullbody': {'scale_factor': 1.1, 'min_neighbors': 3, 'min_size': (30, 96)},
            }
        
        results = {}
        total_detections = 0
        
        for target_type in target_types:
            if target_type in self.detectors:
                params = detection_params.get(target_type, {})
                detections, stats = self.detect_targets(image, target_type, **params)
                results[target_type] = {
                    'detections': detections,
                    'stats': stats
                }
                total_detections += len(detections)
        
        print(f"âœ… å¤šç›®æ ‡ä¾¦å¯Ÿå®Œæˆï¼Œæ€»è®¡å‘ç° {total_detections} ä¸ªç›®æ ‡")
        return results
    
    def create_test_image_with_faces(self, width=640, height=480):
        """
        åˆ›å»ºåŒ…å«äººè„¸çš„æµ‹è¯•å›¾åƒ
        
        Parameters:
        width (int): å›¾åƒå®½åº¦
        height (int): å›¾åƒé«˜åº¦
        
        Returns:
        numpy.ndarray: æµ‹è¯•å›¾åƒ
        """
        print("ğŸ¨ ä¾¦å¯Ÿé˜Ÿ: åˆ›å»ºäººè„¸æµ‹è¯•å›¾åƒ...")
        
        # åˆ›å»ºç™½è‰²èƒŒæ™¯
        image = np.ones((height, width, 3), dtype=np.uint8) * 255
        
        # ç»˜åˆ¶ç®€å•çš„äººè„¸å½¢çŠ¶ç”¨äºæµ‹è¯•
        face_centers = [(width//4, height//3), (3*width//4, height//3)]
        
        for center in face_centers:
            x, y = center
            
            # ç»˜åˆ¶è„¸éƒ¨è½®å»“ï¼ˆåœ†å½¢ï¼‰
            cv2.circle(image, (x, y), 60, (200, 180, 160), -1)
            cv2.circle(image, (x, y), 60, (150, 130, 100), 2)
            
            # ç»˜åˆ¶çœ¼ç›
            cv2.circle(image, (x-20, y-15), 8, (50, 50, 50), -1)
            cv2.circle(image, (x+20, y-15), 8, (50, 50, 50), -1)
            cv2.circle(image, (x-20, y-15), 3, (255, 255, 255), -1)
            cv2.circle(image, (x+20, y-15), 3, (255, 255, 255), -1)
            
            # ç»˜åˆ¶é¼»å­
            cv2.line(image, (x, y-5), (x, y+10), (120, 100, 80), 2)
            
            # ç»˜åˆ¶å˜´å·´
            cv2.ellipse(image, (x, y+20), (15, 8), 0, 0, 180, (120, 100, 80), 2)
        
        print("âœ… æµ‹è¯•å›¾åƒåˆ›å»ºå®Œæˆ")
        return image
    
    def performance_benchmark(self, test_images, target_type='face'):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Parameters:
        test_images (list): æµ‹è¯•å›¾åƒåˆ—è¡¨
        target_type (str): ç›®æ ‡ç±»å‹
        
        Returns:
        dict: æ€§èƒ½ç»Ÿè®¡ç»“æœ
        """
        print(f"ğŸ“Š å¼€å§‹ {target_type} æ£€æµ‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        total_time = 0
        total_detections = 0
        image_count = len(test_images)
        
        for i, image in enumerate(test_images):
            print(f"   æµ‹è¯•å›¾åƒ {i+1}/{image_count}")
            detections, stats = self.detect_targets(image, target_type)
            total_time += stats['detection_time']
            total_detections += stats['detections_count']
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_time_per_image = total_time / image_count if image_count > 0 else 0
        avg_detections_per_image = total_detections / image_count if image_count > 0 else 0
        fps = 1.0 / avg_time_per_image if avg_time_per_image > 0 else 0
        
        benchmark_results = {
            'target_type': target_type,
            'total_images': image_count,
            'total_time': total_time,
            'total_detections': total_detections,
            'avg_time_per_image': avg_time_per_image,
            'avg_detections_per_image': avg_detections_per_image,
            'estimated_fps': fps
        }
        
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   æ€»æµ‹è¯•å›¾åƒ: {image_count}")
        print(f"   å¹³å‡æ£€æµ‹æ—¶é—´: {avg_time_per_image:.3f} ç§’/å›¾åƒ")
        print(f"   å¹³å‡æ£€æµ‹æ•°é‡: {avg_detections_per_image:.1f} ä¸ª/å›¾åƒ")
        print(f"   ä¼°è®¡FPS: {fps:.1f}")
        
        return benchmark_results
    
    def visualize_detection_results(self, image, multi_results, figsize=(15, 10)):
        """
        å¯è§†åŒ–å¤šç›®æ ‡æ£€æµ‹ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        multi_results (dict): å¤šç›®æ ‡æ£€æµ‹ç»“æœ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ–¼ï¸ æ­£åœ¨å¯è§†åŒ–ä¾¦å¯Ÿç»“æœ...")
        
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        axes[0].imshow(image)
        axes[0].set_title('åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')
        axes[0].axis('off')
        
        # æ˜¾ç¤ºå„ç±»å‹ç›®æ ‡æ£€æµ‹ç»“æœ
        plot_idx = 1
        for target_type, result in multi_results.items():
            if plot_idx < len(axes):
                detected_image = self.draw_detection_results(
                    image, result['detections'], target_type
                )
                axes[plot_idx].imshow(detected_image)
                axes[plot_idx].set_title(
                    f'{target_type}æ£€æµ‹ ({len(result["detections"])}ä¸ª)',
                    fontsize=12, fontweight='bold'
                )
                axes[plot_idx].axis('off')
                plot_idx += 1
        
        # æ˜¾ç¤ºç»¼åˆæ£€æµ‹ç»“æœ
        if plot_idx < len(axes):
            combined_image = image.copy()
            for target_type, result in multi_results.items():
                combined_image = self.draw_detection_results(
                    combined_image, result['detections'], target_type
                )
            
            axes[plot_idx].imshow(combined_image)
            axes[plot_idx].set_title('ç»¼åˆæ£€æµ‹ç»“æœ', fontsize=12, fontweight='bold')
            axes[plot_idx].axis('off')
            plot_idx += 1
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(plot_idx, len(axes)):
            axes[i].axis('off')
        
        fig.suptitle('ğŸ•µï¸ Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿ - æ£€æµ‹ç»“æœ', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… å¯è§†åŒ–å®Œæˆ")
    
    def get_detection_report(self):
        """
        è·å–ä¾¦å¯Ÿé˜Ÿå·¥ä½œæŠ¥å‘Š
        """
        print("\nğŸ•µï¸ Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿå·¥ä½œæŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ ä¾¦å¯Ÿé˜ŸåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¯ å¯ç”¨æ£€æµ‹å™¨: {len(self.detectors)} ä¸ª")
        print(f"ğŸ“Š æ‰§è¡Œä¾¦å¯Ÿä»»åŠ¡: {len(self.detection_history)} æ¬¡")
        
        if self.detection_history:
            print("\nğŸ“‹ æœ€è¿‘ä¾¦å¯Ÿè®°å½•:")
            for record in self.detection_history[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¬¡
                stats = record['stats']
                timestamp = record['timestamp'][:19]  # å»æ‰æ¯«ç§’
                print(f"   [{timestamp}] {stats['target_type']}æ£€æµ‹: "
                      f"{stats['detections_count']}ä¸ªç›®æ ‡, "
                      f"ç”¨æ—¶{stats['detection_time']:.3f}ç§’")

# ä½¿ç”¨ç¤ºä¾‹å’Œæ¼”ç¤º
if __name__ == "__main__":
    # åˆ›å»ºHaarä¾¦å¯Ÿé˜Ÿ
    haar_unit = HaarDetectionUnit()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = haar_unit.create_test_image_with_faces()
    
    # æ‰§è¡Œå¤šç›®æ ‡æ£€æµ‹
    detection_results = haar_unit.multi_target_detection(
        test_image, 
        target_types=['face', 'eye']
    )
    
    # å¯è§†åŒ–æ£€æµ‹ç»“æœ
    haar_unit.visualize_detection_results(test_image, detection_results)
    
    # æ˜¾ç¤ºä¾¦å¯Ÿé˜ŸæŠ¥å‘Š
    haar_unit.get_detection_report()
```

#### ğŸ¯ ä¾¦å¯Ÿé˜Ÿçš„ä¼˜åŠ¿å’Œåº”ç”¨åœºæ™¯

Haarçº§è”åˆ†ç±»å™¨ä¾¦å¯Ÿé˜Ÿæœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

**ğŸš€ ä¼˜åŠ¿**ï¼š
- **é€Ÿåº¦å¿«**ï¼šæ£€æµ‹é€Ÿåº¦éå¸¸å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨
- **èµ„æºå°‘**ï¼šCPUè®¡ç®—ï¼Œä¸éœ€è¦GPUæ”¯æŒ
- **ç¨³å®šæ€§**ï¼šç®—æ³•æˆç†Ÿç¨³å®šï¼Œä¹…ç»è€ƒéªŒ
- **æ˜“éƒ¨ç½²**ï¼šOpenCVå†…ç½®ï¼Œéƒ¨ç½²ç®€å•

**ğŸ¯ åº”ç”¨åœºæ™¯**ï¼š
- å®æ—¶äººè„¸æ£€æµ‹
- ç§»åŠ¨è®¾å¤‡åº”ç”¨
- åµŒå…¥å¼ç³»ç»Ÿ
- å¿«é€ŸåŸå‹å¼€å‘

### 36.1.2 HOG+SVMå·¡é€»é˜Ÿ

#### ğŸ‘® å·¡é€»é˜Ÿçš„å·¥ä½œæœºåˆ¶

HOG+SVMå·¡é€»é˜Ÿæ˜¯ç›®æ ‡ä¾¦å¯Ÿéƒ¨çš„å¦ä¸€æ”¯ç²¾è‹±é˜Ÿä¼ã€‚ä»–ä»¬ä½¿ç”¨æ–¹å‘æ¢¯åº¦ç›´æ–¹å›¾ï¼ˆHOGï¼‰ç‰¹å¾ç»“åˆæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰åˆ†ç±»å™¨ï¼Œä¸“é—¨è´Ÿè´£è¡Œäººæ£€æµ‹ç­‰å¤æ‚ä»»åŠ¡ã€‚

```python
# ç¤ºä¾‹2ï¼šHOG+SVMå·¡é€»é˜Ÿ - è¡Œäººæ£€æµ‹ä¸è®¡æ•°ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from sklearn.svm import SVM
import joblib
from pathlib import Path
import time
from datetime import datetime

class HOGSVMPatrolUnit:
    """
    HOG+SVMå·¡é€»é˜Ÿ
    ä¸“é—¨è´Ÿè´£è¡Œäººæ£€æµ‹å’Œäººæµç»Ÿè®¡ä»»åŠ¡
    """
    
    def __init__(self, workspace_path="ai_analysis_workshop/hog_patrol"):
        """
        åˆå§‹åŒ–HOG+SVMå·¡é€»é˜Ÿ
        
        Parameters:
        workspace_path (str): å·¡é€»é˜Ÿå·¥ä½œç›®å½•
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–HOGäººä½“æ£€æµ‹å™¨
        self.hog_detector = cv2.HOGDescriptor()
        self.hog_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        
        # å·¡é€»ç»Ÿè®¡
        self.patrol_history = []
        self.people_count_history = []
        
        # HOGå‚æ•°é…ç½®
        self.hog_params = {
            'winSize': (64, 128),
            'blockSize': (16, 16),
            'blockStride': (8, 8),
            'cellSize': (8, 8),
            'nbins': 9
        }
        
        print("ğŸ‘® HOG+SVMå·¡é€»é˜ŸæŠ¥åˆ°ï¼")
        print(f"ğŸ“ å·¡é€»é˜ŸåŸºåœ°: {self.workspace_path}")
        print("ğŸ¯ ä¸“ä¸šæŠ€èƒ½: è¡Œäººæ£€æµ‹ã€äººæµç»Ÿè®¡ã€åŒºåŸŸç›‘æ§")
    
    def detect_people(self, image, scale_factor=1.05, win_stride=(8, 8), 
                     padding=(32, 32), hit_threshold=0):
        """
        æ‰§è¡Œè¡Œäººæ£€æµ‹ä»»åŠ¡
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        scale_factor (float): å°ºåº¦å› å­
        win_stride (tuple): çª—å£æ­¥é•¿
        padding (tuple): å¡«å……å¤§å°
        hit_threshold (float): æ£€æµ‹é˜ˆå€¼
        
        Returns:
        tuple: (æ£€æµ‹ç»“æœ, æ£€æµ‹ç»Ÿè®¡)
        """
        print("ğŸ‘® å·¡é€»é˜Ÿå¼€å§‹è¡Œäººæ£€æµ‹...")
        start_time = time.time()
        
        # æ‰§è¡ŒHOGè¡Œäººæ£€æµ‹
        locations, weights = self.hog_detector.detectMultiScale(
            image,
            winStride=win_stride,
            padding=padding,
            scale=scale_factor,
            hitThreshold=hit_threshold
        )
        
        detection_time = time.time() - start_time
        
        # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
        stats = {
            'people_count': len(locations),
            'detection_time': detection_time,
            'image_size': image.shape,
            'parameters': {
                'scale_factor': scale_factor,
                'win_stride': win_stride,
                'hit_threshold': hit_threshold
            },
            'weights': weights.tolist() if len(weights) > 0 else []
        }
        
        # è®°å½•å·¡é€»å†å²
        self.patrol_history.append({
            'timestamp': datetime.now().isoformat(),
            'stats': stats
        })
        
        # è®°å½•äººæµç»Ÿè®¡
        self.people_count_history.append({
            'timestamp': datetime.now().isoformat(),
            'count': len(locations)
        })
        
        print(f"   æ£€æµ‹å®Œæˆ: å‘ç° {len(locations)} ä¸ªè¡Œäºº")
        print(f"   æ£€æµ‹ç”¨æ—¶: {detection_time:.3f} ç§’")
        
        return locations, stats
    
    def apply_non_maximum_suppression(self, boxes, weights, overlap_threshold=0.3):
        """
        åº”ç”¨éæœ€å¤§å€¼æŠ‘åˆ¶å»é™¤é‡å¤æ£€æµ‹
        
        Parameters:
        boxes (list): æ£€æµ‹æ¡†åˆ—è¡¨
        weights (list): æ£€æµ‹æƒé‡åˆ—è¡¨
        overlap_threshold (float): é‡å é˜ˆå€¼
        
        Returns:
        tuple: (è¿‡æ»¤åçš„æ¡†, è¿‡æ»¤åçš„æƒé‡)
        """
        if len(boxes) == 0:
            return [], []
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        boxes = np.array(boxes)
        weights = np.array(weights)
        
        # ä½¿ç”¨OpenCVçš„éæœ€å¤§å€¼æŠ‘åˆ¶
        indices = cv2.dnn.NMSBoxes(
            boxes.tolist(), 
            weights.tolist(), 
            score_threshold=0.3,
            nms_threshold=overlap_threshold
        )
        
        if len(indices) > 0:
            indices = indices.flatten()
            return boxes[indices], weights[indices]
        else:
            return [], []
    
    def draw_detection_results(self, image, locations, weights=None, 
                             color=(0, 255, 0), thickness=2):
        """
        ç»˜åˆ¶è¡Œäººæ£€æµ‹ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        locations (list): æ£€æµ‹ä½ç½®
        weights (list): æ£€æµ‹æƒé‡
        color (tuple): è¾¹æ¡†é¢œè‰²
        thickness (int): è¾¹æ¡†ç²—ç»†
        
        Returns:
        numpy.ndarray: æ ‡æ³¨åçš„å›¾åƒ
        """
        result_image = image.copy()
        
        for i, (x, y, w, h) in enumerate(locations):
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, thickness)
            
            # æ·»åŠ æ ‡ç­¾
            label = f"Person {i+1}"
            if weights is not None and i < len(weights):
                confidence = weights[i]
                label += f" ({confidence:.2f})"
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯å’Œæ–‡å­—
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
            cv2.rectangle(result_image, 
                         (x, y - label_size[1] - 10), 
                         (x + label_size[0], y), 
                         color, -1)
            cv2.putText(result_image, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # æ·»åŠ æ€»è®¡æ•°
        total_count = len(locations)
        count_text = f"Total People: {total_count}"
        cv2.putText(result_image, count_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        return result_image
    
    def create_pedestrian_test_scene(self, width=800, height=600):
        """
        åˆ›å»ºè¡Œäººæ£€æµ‹æµ‹è¯•åœºæ™¯
        
        Parameters:
        width (int): å›¾åƒå®½åº¦
        height (int): å›¾åƒé«˜åº¦
        
        Returns:
        numpy.ndarray: æµ‹è¯•åœºæ™¯å›¾åƒ
        """
        print("ğŸ¨ å·¡é€»é˜Ÿ: åˆ›å»ºè¡Œäººæµ‹è¯•åœºæ™¯...")
        
        # åˆ›å»ºè¡—é“èƒŒæ™¯
        image = np.ones((height, width, 3), dtype=np.uint8) * 200
        
        # ç»˜åˆ¶åœ°é¢
        cv2.rectangle(image, (0, height//2), (width, height), (150, 150, 150), -1)
        
        # ç»˜åˆ¶ç®€å•çš„äººå½¢è½®å»“
        people_positions = [(150, height-200), (350, height-180), (550, height-220)]
        
        for x, y in people_positions:
            # ç»˜åˆ¶å¤´éƒ¨
            cv2.circle(image, (x, y-60), 20, (200, 180, 160), -1)
            cv2.circle(image, (x, y-60), 20, (150, 130, 100), 2)
            
            # ç»˜åˆ¶èº«ä½“
            cv2.rectangle(image, (x-15, y-40), (x+15, y+20), (100, 100, 200), -1)
            cv2.rectangle(image, (x-15, y-40), (x+15, y+20), (80, 80, 150), 2)
            
            # ç»˜åˆ¶æ‰‹è‡‚
            cv2.line(image, (x-15, y-30), (x-25, y-10), (100, 100, 200), 5)
            cv2.line(image, (x+15, y-30), (x+25, y-10), (100, 100, 200), 5)
            
            # ç»˜åˆ¶è…¿éƒ¨
            cv2.line(image, (x-5, y+20), (x-8, y+60), (50, 50, 100), 8)
            cv2.line(image, (x+5, y+20), (x+8, y+60), (50, 50, 100), 8)
        
        print("âœ… æµ‹è¯•åœºæ™¯åˆ›å»ºå®Œæˆ")
        return image
    
    def analyze_people_flow(self, video_path=None, camera_id=0, duration=30):
        """
        åˆ†æäººæµé‡ï¼ˆå®æ—¶è§†é¢‘æˆ–æ‘„åƒå¤´ï¼‰
        
        Parameters:
        video_path (str): è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        camera_id (int): æ‘„åƒå¤´ID
        duration (int): åˆ†ææŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
        dict: äººæµåˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹äººæµé‡åˆ†æ...")
        
        # æ‰“å¼€è§†é¢‘æº
        if video_path:
            cap = cv2.VideoCapture(video_path)
            print(f"ğŸ“¹ åˆ†æè§†é¢‘æ–‡ä»¶: {video_path}")
        else:
            cap = cv2.VideoCapture(camera_id)
            print(f"ğŸ“· åˆ†ææ‘„åƒå¤´: {camera_id}")
        
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return {}
        
        frame_count = 0
        people_counts = []
        start_time = time.time()
        
        print(f"â±ï¸ å¼€å§‹ {duration} ç§’çš„äººæµåˆ†æ...")
        print("æŒ‰ 'q' é”®æå‰ç»“æŸåˆ†æ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = time.time()
            if current_time - start_time > duration:
                break
            
            # æ‰§è¡Œè¡Œäººæ£€æµ‹
            locations, stats = self.detect_people(frame)
            people_count = len(locations)
            people_counts.append(people_count)
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            result_frame = self.draw_detection_results(frame, locations)
            
            # æ·»åŠ æ—¶é—´ä¿¡æ¯
            elapsed_time = current_time - start_time
            time_text = f"Time: {elapsed_time:.1f}s"
            cv2.putText(result_frame, time_text, (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
            
            # æ˜¾ç¤ºç»“æœ
            cv2.imshow('HOG+SVMå·¡é€»é˜Ÿ - äººæµåˆ†æ', result_frame)
            
            frame_count += 1
            
            # æ£€æŸ¥é€€å‡ºé”®
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # åˆ†æç»Ÿè®¡ç»“æœ
        if people_counts:
            analysis_results = {
                'total_frames': frame_count,
                'analysis_duration': duration,
                'people_counts': people_counts,
                'max_people': max(people_counts),
                'min_people': min(people_counts),
                'avg_people': np.mean(people_counts),
                'total_detections': sum(people_counts)
            }
            
            print("\nğŸ“Š äººæµåˆ†æç»“æœ:")
            print(f"   åˆ†æå¸§æ•°: {frame_count}")
            print(f"   æœ€å¤§äººæ•°: {analysis_results['max_people']}")
            print(f"   æœ€å°äººæ•°: {analysis_results['min_people']}")
            print(f"   å¹³å‡äººæ•°: {analysis_results['avg_people']:.1f}")
            
            return analysis_results
        
        return {}
    
    def visualize_detection_comparison(self, image, figsize=(15, 8)):
        """
        å¯è§†åŒ–ä¸åŒå‚æ•°ä¸‹çš„æ£€æµ‹å¯¹æ¯”
        
        Parameters:
        image (numpy.ndarray): æµ‹è¯•å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ–¼ï¸ æ­£åœ¨è¿›è¡Œæ£€æµ‹å‚æ•°å¯¹æ¯”åˆ†æ...")
        
        # ä¸åŒçš„æ£€æµ‹å‚æ•°é…ç½®
        param_configs = [
            {'scale_factor': 1.05, 'hit_threshold': 0, 'name': 'æ ‡å‡†é…ç½®'},
            {'scale_factor': 1.1, 'hit_threshold': 0, 'name': 'å¿«é€Ÿæ£€æµ‹'},
            {'scale_factor': 1.02, 'hit_threshold': 0.5, 'name': 'é«˜ç²¾åº¦'},
            {'scale_factor': 1.05, 'hit_threshold': -0.5, 'name': 'é«˜å¬å›'}
        ]
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        axes = axes.flatten()
        
        for i, config in enumerate(param_configs):
            # æ‰§è¡Œæ£€æµ‹
            locations, stats = self.detect_people(
                image, 
                scale_factor=config['scale_factor'],
                hit_threshold=config['hit_threshold']
            )
            
            # ç»˜åˆ¶ç»“æœ
            result_image = self.draw_detection_results(image, locations)
            
            # æ˜¾ç¤ºç»“æœ
            axes[i].imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
            axes[i].set_title(
                f'{config["name"]}\næ£€æµ‹åˆ° {len(locations)} ä¸ªè¡Œäºº\n'
                f'ç”¨æ—¶ {stats["detection_time"]:.3f}s',
                fontsize=11, fontweight='bold'
            )
            axes[i].axis('off')
        
        fig.suptitle('ğŸ‘® HOG+SVMå·¡é€»é˜Ÿ - å‚æ•°å¯¹æ¯”åˆ†æ', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… å¯¹æ¯”åˆ†æå®Œæˆ")
    
    def get_patrol_report(self):
        """
        è·å–å·¡é€»é˜Ÿå·¥ä½œæŠ¥å‘Š
        """
        print("\nğŸ‘® HOG+SVMå·¡é€»é˜Ÿå·¥ä½œæŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ å·¡é€»é˜ŸåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ“Š æ‰§è¡Œå·¡é€»ä»»åŠ¡: {len(self.patrol_history)} æ¬¡")
        print(f"ğŸ‘¥ äººæµç»Ÿè®¡è®°å½•: {len(self.people_count_history)} æ¬¡")
        
        if self.people_count_history:
            recent_counts = [record['count'] for record in self.people_count_history[-10:]]
            avg_count = np.mean(recent_counts) if recent_counts else 0
            max_count = max(recent_counts) if recent_counts else 0
            
            print(f"ğŸ“ˆ æœ€è¿‘å¹³å‡äººæ•°: {avg_count:.1f}")
            print(f"ğŸ“Š æœ€è¿‘æœ€å¤§äººæ•°: {max_count}")
            
            print("\nğŸ“‹ æœ€è¿‘å·¡é€»è®°å½•:")
            for record in self.patrol_history[-3:]:
                stats = record['stats']
                timestamp = record['timestamp'][:19]
                print(f"   [{timestamp}] æ£€æµ‹åˆ° {stats['people_count']} ä¸ªè¡Œäºº, "
                      f"ç”¨æ—¶ {stats['detection_time']:.3f}ç§’")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºHOG+SVMå·¡é€»é˜Ÿ
    patrol_unit = HOGSVMPatrolUnit()
    
    # åˆ›å»ºæµ‹è¯•åœºæ™¯
    test_scene = patrol_unit.create_pedestrian_test_scene()
    
    # æ‰§è¡Œæ£€æµ‹å¯¹æ¯”åˆ†æ
    patrol_unit.visualize_detection_comparison(test_scene)
    
    # æ˜¾ç¤ºå·¡é€»é˜ŸæŠ¥å‘Š
    patrol_unit.get_patrol_report()
```

---

é€šè¿‡ç¬¬ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†AIæ™ºèƒ½åˆ†æè½¦é—´çš„ç›®æ ‡ä¾¦å¯Ÿéƒ¨ï¼ŒæŒæ¡äº†ä¼ ç»Ÿç›®æ ‡æ£€æµ‹çš„æ ¸å¿ƒç®—æ³•ã€‚æ¥ä¸‹æ¥ç¬¬äºŒèŠ‚æˆ‘ä»¬å°†è¿›å…¥æ›´åŠ å…ˆè¿›çš„**AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ**ï¼Œå­¦ä¹ æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹æŠ€æœ¯ï¼

**æœ¬èŠ‚å­¦ä¹ æˆæœæ£€éªŒ**ï¼š
- âœ… æŒæ¡äº†Haarçº§è”åˆ†ç±»å™¨çš„å·¥ä½œåŸç†å’Œåº”ç”¨
- âœ… å­¦ä¼šäº†HOG+SVMè¡Œäººæ£€æµ‹ç³»ç»Ÿçš„å¼€å‘
- âœ… ç†è§£äº†ä¼ ç»Ÿç›®æ ‡æ£€æµ‹ç®—æ³•çš„ä¼˜åŠ¿å’Œå±€é™æ€§
- âœ… å…·å¤‡äº†é€‰æ‹©åˆé€‚æ£€æµ‹ç®—æ³•çš„åˆ¤æ–­èƒ½åŠ›

**ä¸‹èŠ‚é¢„å‘Š**ï¼šåœ¨ç¬¬äºŒèŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¿›å…¥AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒï¼Œå­¦ä¹ YOLOæ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹æŠ€æœ¯ï¼Œä½“éªŒAIå¤§è„‘çš„å¼ºå¤§è¯†åˆ«èƒ½åŠ›ï¼

## ğŸ“š ç¬¬äºŒèŠ‚ï¼šAIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹

### 36.2.1 YOLOæ™ºèƒ½å¤§è„‘

#### ğŸ§  AIå¤§è„‘çš„å·¥ä½œåŸç†

æ¬¢è¿æ¥åˆ°AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒï¼è¿™é‡Œæ˜¯æ•´ä¸ªè½¦é—´æœ€æ ¸å¿ƒçš„éƒ¨é—¨ï¼Œæ‹¥æœ‰ä¸€ä¸ªå¼ºå¤§çš„**YOLOæ™ºèƒ½å¤§è„‘**ã€‚ä¸ä¼ ç»Ÿçš„ä¾¦å¯Ÿé˜Ÿä¸åŒï¼ŒYOLOå¤§è„‘èƒ½å¤Ÿ"ä¸€çœ¼çœ‹éæ•´ä¸ªç”»é¢"ï¼ˆYou Only Look Onceï¼‰ï¼ŒåŒæ—¶è¯†åˆ«å‡ºå›¾åƒä¸­çš„æ‰€æœ‰ç›®æ ‡ã€‚

#### ğŸš€ YOLOå¤§è„‘çš„æŠ€æœ¯é©å‘½

```python
# ç¤ºä¾‹3ï¼šYOLOæ™ºèƒ½å¤§è„‘ - å®æ—¶å¤šç›®æ ‡æ£€æµ‹ç³»ç»Ÿ
import cv2
import numpy as np
import torch
from ultralytics import YOLO
import matplotlib.pyplot as plt
from pathlib import Path
import time
from datetime import datetime
import json

class YOLOIntelligenceCenter:
    """
    YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ
    åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç›®æ ‡æ£€æµ‹å¤§è„‘
    """
    
    def __init__(self, model_size='n', workspace_path="ai_analysis_workshop/yolo_center"):
        """
        åˆå§‹åŒ–YOLOæ™ºèƒ½å¤§è„‘
        
        Parameters:
        model_size (str): æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
        workspace_path (str): å·¥ä½œç›®å½•
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'n': {'name': 'YOLOv8n', 'desc': 'çº³ç±³ç‰ˆ - è¶…è½»é‡çº§'},
            's': {'name': 'YOLOv8s', 'desc': 'å°å‹ç‰ˆ - å¹³è¡¡æ€§èƒ½'},
            'm': {'name': 'YOLOv8m', 'desc': 'ä¸­å‹ç‰ˆ - é«˜ç²¾åº¦'},
            'l': {'name': 'YOLOv8l', 'desc': 'å¤§å‹ç‰ˆ - æ›´é«˜ç²¾åº¦'},
            'x': {'name': 'YOLOv8x', 'desc': 'è¶…å¤§ç‰ˆ - æœ€é«˜ç²¾åº¦'}
        }
        
        # åŠ è½½YOLOæ¨¡å‹
        self.model_size = model_size
        self.model = self._load_yolo_model(model_size)
        
        # æ£€æµ‹å†å²å’Œç»Ÿè®¡
        self.detection_history = []
        self.class_statistics = {}
        
        # COCOæ•°æ®é›†ç±»åˆ«åç§°
        self.class_names = self._get_coco_class_names()
        
        print("ğŸ§  YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒå¯åŠ¨ï¼")
        print(f"ğŸ“ ä¸­å¿ƒåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¤– AIå¤§è„‘å‹å·: {self.model_configs[model_size]['name']}")
        print(f"ğŸ“ æ¨¡å‹æè¿°: {self.model_configs[model_size]['desc']}")
        print(f"ğŸ¯ å¯è¯†åˆ«ç±»åˆ«: {len(self.class_names)} ç§")
    
    def _load_yolo_model(self, model_size):
        """
        åŠ è½½YOLOæ¨¡å‹
        
        Parameters:
        model_size (str): æ¨¡å‹å¤§å°
        
        Returns:
        YOLO: åŠ è½½çš„æ¨¡å‹
        """
        try:
            model_name = f"yolov8{model_size}.pt"
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
            
            model = YOLO(model_name)
            print("âœ… YOLOæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å°†è‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹...")
            try:
                model = YOLO(f"yolov8{model_size}.pt")
                return model
            except Exception as e2:
                print(f"âŒ è‡ªåŠ¨ä¸‹è½½ä¹Ÿå¤±è´¥: {e2}")
                return None
    
    def _get_coco_class_names(self):
        """è·å–COCOæ•°æ®é›†ç±»åˆ«åç§°"""
        return [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
            'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',
            'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
            'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
            'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
            'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
            'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',
            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',
            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]
    
    def detect_objects(self, image, conf_threshold=0.25, iou_threshold=0.45, 
                      max_detections=300):
        """
        AIå¤§è„‘æ‰§è¡Œç›®æ ‡æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        conf_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold (float): IoUé˜ˆå€¼
        max_detections (int): æœ€å¤§æ£€æµ‹æ•°é‡
        
        Returns:
        tuple: (æ£€æµ‹ç»“æœ, æ£€æµ‹ç»Ÿè®¡)
        """
        print("ğŸ§  AIå¤§è„‘å¼€å§‹æ™ºèƒ½åˆ†æ...")
        start_time = time.time()
        
        if self.model is None:
            print("âŒ AIå¤§è„‘æœªæ­£ç¡®åˆå§‹åŒ–")
            return [], {}
        
        try:
            # æ‰§è¡ŒYOLOæ£€æµ‹
            results = self.model(
                image,
                conf=conf_threshold,
                iou=iou_threshold,
                max_det=max_detections,
                verbose=False
            )
            
            detection_time = time.time() - start_time
            
            # è§£ææ£€æµ‹ç»“æœ
            detections = []
            class_counts = {}
            
            if results and len(results) > 0:
                result = results[0]  # å–ç¬¬ä¸€ä¸ªç»“æœ
                
                if result.boxes is not None and len(result.boxes) > 0:
                    boxes = result.boxes.xyxy.cpu().numpy()  # è¾¹ç•Œæ¡†
                    confidences = result.boxes.conf.cpu().numpy()  # ç½®ä¿¡åº¦
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)  # ç±»åˆ«ID
                    
                    for i, (box, conf, cls_id) in enumerate(zip(boxes, confidences, class_ids)):
                        x1, y1, x2, y2 = box
                        class_name = self.class_names[cls_id] if cls_id < len(self.class_names) else f"unknown_{cls_id}"
                        
                        detection = {
                            'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],  # [x, y, w, h]
                            'confidence': float(conf),
                            'class_id': int(cls_id),
                            'class_name': class_name
                        }
                        detections.append(detection)
                        
                        # ç»Ÿè®¡ç±»åˆ«æ•°é‡
                        if class_name in class_counts:
                            class_counts[class_name] += 1
                        else:
                            class_counts[class_name] = 1
            
            # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
            stats = {
                'total_detections': len(detections),
                'detection_time': detection_time,
                'image_size': image.shape,
                'class_counts': class_counts,
                'model_info': {
                    'model_size': self.model_size,
                    'model_name': self.model_configs[self.model_size]['name']
                },
                'parameters': {
                    'conf_threshold': conf_threshold,
                    'iou_threshold': iou_threshold,
                    'max_detections': max_detections
                }
            }
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            for class_name, count in class_counts.items():
                if class_name in self.class_statistics:
                    self.class_statistics[class_name] += count
                else:
                    self.class_statistics[class_name] = count
            
            # è®°å½•æ£€æµ‹å†å²
            self.detection_history.append({
                'timestamp': datetime.now().isoformat(),
                'stats': stats,
                'detections': detections
            })
            
            print(f"   ğŸ¯ AIå¤§è„‘æ£€æµ‹å®Œæˆ: å‘ç° {len(detections)} ä¸ªç›®æ ‡")
            print(f"   â±ï¸ åˆ†æç”¨æ—¶: {detection_time:.3f} ç§’")
            print(f"   ğŸ“Š ç›®æ ‡ç±»åˆ«: {list(class_counts.keys())}")
            
            return detections, stats
            
        except Exception as e:
            print(f"âŒ AIå¤§è„‘æ£€æµ‹é”™è¯¯: {e}")
            return [], {}
    
    def draw_detection_results(self, image, detections, show_confidence=True):
        """
        ç»˜åˆ¶AIæ£€æµ‹ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        detections (list): æ£€æµ‹ç»“æœåˆ—è¡¨
        show_confidence (bool): æ˜¯å¦æ˜¾ç¤ºç½®ä¿¡åº¦
        
        Returns:
        numpy.ndarray: æ ‡æ³¨åçš„å›¾åƒ
        """
        result_image = image.copy()
        
        # ä¸ºä¸åŒç±»åˆ«åˆ†é…é¢œè‰²
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0),
            (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 0)
        ]
        
        for i, detection in enumerate(detections):
            x, y, w, h = detection['bbox']
            confidence = detection['confidence']
            class_name = detection['class_name']
            
            # é€‰æ‹©é¢œè‰²
            color = colors[detection['class_id'] % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            
            # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
            if show_confidence:
                label = f"{class_name}: {confidence:.2f}"
            else:
                label = class_name
            
            # è®¡ç®—æ ‡ç­¾ä½ç½®å’Œå¤§å°
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
            label_y = max(y - 10, label_size[1])
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(result_image, 
                         (x, label_y - label_size[1] - 5), 
                         (x + label_size[0] + 5, label_y + 5), 
                         color, -1)
            
            # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
            cv2.putText(result_image, label, (x + 2, label_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # æ·»åŠ æ£€æµ‹æ€»æ•°
        total_text = f"AI Brain Detected: {len(detections)} objects"
        cv2.putText(result_image, total_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        return result_image
    
    def create_complex_test_scene(self, width=800, height=600):
        """
        åˆ›å»ºå¤æ‚çš„å¤šç›®æ ‡æµ‹è¯•åœºæ™¯
        
        Parameters:
        width (int): å›¾åƒå®½åº¦
        height (int): å›¾åƒé«˜åº¦
        
        Returns:
        numpy.ndarray: å¤æ‚æµ‹è¯•åœºæ™¯
        """
        print("ğŸ¨ AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ: åˆ›å»ºå¤æ‚æµ‹è¯•åœºæ™¯...")
        
        # åˆ›å»ºåŸå¸‚è¡—é“èƒŒæ™¯
        image = np.ones((height, width, 3), dtype=np.uint8) * 180
        
        # ç»˜åˆ¶å¤©ç©º
        cv2.rectangle(image, (0, 0), (width, height//3), (135, 206, 235), -1)
        
        # ç»˜åˆ¶å»ºç­‘ç‰©
        buildings = [
            (50, height//3, 150, height//2),
            (200, height//3, 250, height//2),
            (350, height//3, 200, height//2),
            (600, height//3, 180, height//2)
        ]
        
        for x, y, w, h in buildings:
            cv2.rectangle(image, (x, y), (x+w, y+h), (100, 100, 100), -1)
            cv2.rectangle(image, (x, y), (x+w, y+h), (70, 70, 70), 2)
            
            # ç»˜åˆ¶çª—æˆ·
            for window_y in range(y+20, y+h-20, 30):
                for window_x in range(x+15, x+w-15, 25):
                    cv2.rectangle(image, (window_x, window_y), 
                                (window_x+10, window_y+15), (255, 255, 0), -1)
        
        # ç»˜åˆ¶é“è·¯
        road_y = height - height//3
        cv2.rectangle(image, (0, road_y), (width, height), (60, 60, 60), -1)
        
        # ç»˜åˆ¶é“è·¯åˆ†éš”çº¿
        for x in range(0, width, 40):
            cv2.rectangle(image, (x, road_y + height//6), 
                         (x+20, road_y + height//6 + 5), (255, 255, 255), -1)
        
        # ç»˜åˆ¶è½¦è¾†
        cars = [
            (100, road_y + 20, 80, 40, (255, 0, 0)),    # çº¢è‰²è½¦
            (300, road_y + 60, 90, 45, (0, 0, 255)),    # è“è‰²è½¦
            (500, road_y + 30, 85, 42, (0, 255, 0))     # ç»¿è‰²è½¦
        ]
        
        for x, y, w, h, color in cars:
            # è½¦èº«
            cv2.rectangle(image, (x, y), (x+w, y+h), color, -1)
            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), 2)
            
            # è½¦çª—
            cv2.rectangle(image, (x+10, y+5), (x+w-10, y+15), (200, 230, 255), -1)
            
            # è½¦è½®
            cv2.circle(image, (x+15, y+h), 8, (0, 0, 0), -1)
            cv2.circle(image, (x+w-15, y+h), 8, (0, 0, 0), -1)
        
        # ç»˜åˆ¶è¡Œäºº
        people = [
            (150, road_y - 80),
            (380, road_y - 75),
            (620, road_y - 85)
        ]
        
        for x, y in people:
            # å¤´éƒ¨
            cv2.circle(image, (x, y), 12, (220, 180, 140), -1)
            # èº«ä½“
            cv2.rectangle(image, (x-8, y+12), (x+8, y+40), (100, 150, 200), -1)
            # è…¿éƒ¨
            cv2.line(image, (x-3, y+40), (x-3, y+60), (50, 50, 100), 4)
            cv2.line(image, (x+3, y+40), (x+3, y+60), (50, 50, 100), 4)
            # æ‰‹è‡‚
            cv2.line(image, (x-8, y+20), (x-15, y+35), (220, 180, 140), 3)
            cv2.line(image, (x+8, y+20), (x+15, y+35), (220, 180, 140), 3)
        
        # æ·»åŠ å…¶ä»–ç‰©ä½“
        # çº¢ç»¿ç¯
        cv2.rectangle(image, (680, road_y-120), (700, road_y-60), (80, 80, 80), -1)
        cv2.circle(image, (690, road_y-110), 6, (255, 0, 0), -1)  # çº¢ç¯
        cv2.circle(image, (690, road_y-95), 6, (255, 255, 0), -1)  # é»„ç¯
        cv2.circle(image, (690, road_y-80), 6, (0, 255, 0), -1)   # ç»¿ç¯
        
        # é•¿æ¤…
        cv2.rectangle(image, (250, road_y-40), (320, road_y-20), (139, 69, 19), -1)
        cv2.rectangle(image, (250, road_y-50), (320, road_y-40), (139, 69, 19), -1)
        
        print("âœ… å¤æ‚æµ‹è¯•åœºæ™¯åˆ›å»ºå®Œæˆ")
        return image
    
    def real_time_detection(self, video_source=0, duration=30):
        """
        å®æ—¶AIæ£€æµ‹ç³»ç»Ÿ
        
        Parameters:
        video_source: è§†é¢‘æºï¼ˆæ‘„åƒå¤´IDæˆ–è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼‰
        duration (int): æ£€æµ‹æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
        dict: å®æ—¶æ£€æµ‹ç»Ÿè®¡ç»“æœ
        """
        print("ğŸ¥ å¯åŠ¨AIå¤§è„‘å®æ—¶æ£€æµ‹ç³»ç»Ÿ...")
        
        # æ‰“å¼€è§†é¢‘æº
        cap = cv2.VideoCapture(video_source)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return {}
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        frame_count = 0
        detection_times = []
        total_detections = 0
        start_time = time.time()
        
        print(f"â±ï¸ å¼€å§‹ {duration} ç§’çš„å®æ—¶AIæ£€æµ‹...")
        print("æŒ‰ 'q' é”®é€€å‡ºï¼ŒæŒ‰ 's' é”®æˆªå›¾ä¿å­˜")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = time.time()
            if current_time - start_time > duration:
                break
            
            # AIå¤§è„‘æ‰§è¡Œæ£€æµ‹
            detections, stats = self.detect_objects(frame, conf_threshold=0.3)
            total_detections += len(detections)
            detection_times.append(stats.get('detection_time', 0))
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            result_frame = self.draw_detection_results(frame, detections)
            
            # æ·»åŠ å®æ—¶ä¿¡æ¯
            elapsed_time = current_time - start_time
            fps = frame_count / elapsed_time if elapsed_time > 0 else 0
            avg_detection_time = np.mean(detection_times) if detection_times else 0
            
            info_text = [
                f"Time: {elapsed_time:.1f}s",
                f"FPS: {fps:.1f}",
                f"Frame: {frame_count}",
                f"Avg Detection: {avg_detection_time:.3f}s"
            ]
            
            for i, text in enumerate(info_text):
                cv2.putText(result_frame, text, (10, 60 + i*25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            
            # æ˜¾ç¤ºç»“æœ
            cv2.imshow('ğŸ§  YOLO AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - å®æ—¶æ£€æµ‹', result_frame)
            
            frame_count += 1
            
            # å¤„ç†æŒ‰é”®äº‹ä»¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # ä¿å­˜æˆªå›¾
                save_path = self.workspace_path / f"detection_screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
                cv2.imwrite(str(save_path), result_frame)
                print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {save_path}")
        
        cap.release()
        cv2.destroyAllWindows()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0
        avg_detection_time = np.mean(detection_times) if detection_times else 0
        
        results = {
            'total_frames': frame_count,
            'total_time': total_time,
            'total_detections': total_detections,
            'avg_fps': avg_fps,
            'avg_detection_time': avg_detection_time,
            'detections_per_frame': total_detections / frame_count if frame_count > 0 else 0
        }
        
        print("\nğŸ“Š AIå¤§è„‘å®æ—¶æ£€æµ‹ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†å¸§æ•°: {frame_count}")
        print(f"   å¹³å‡FPS: {avg_fps:.1f}")
        print(f"   æ€»æ£€æµ‹ç›®æ ‡: {total_detections}")
        print(f"   å¹³å‡æ£€æµ‹æ—¶é—´: {avg_detection_time:.3f}ç§’")
        
        return results
    
    def get_intelligence_report(self):
        """
        è·å–AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒæŠ¥å‘Š
        """
        print("\nğŸ§  YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒå·¥ä½œæŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ ä¸­å¿ƒåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¤– AIå¤§è„‘å‹å·: {self.model_configs[self.model_size]['name']}")
        print(f"ğŸ“Š æ‰§è¡Œæ£€æµ‹ä»»åŠ¡: {len(self.detection_history)} æ¬¡")
        print(f"ğŸ¯ è¯†åˆ«ç›®æ ‡ç±»åˆ«: {len(self.class_statistics)} ç§")
        
        if self.class_statistics:
            print("\nğŸ“ˆ ç›®æ ‡ç±»åˆ«ç»Ÿè®¡:")
            sorted_classes = sorted(self.class_statistics.items(), 
                                  key=lambda x: x[1], reverse=True)
            for class_name, count in sorted_classes[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                print(f"   {class_name}: {count} æ¬¡")
        
        if self.detection_history:
            print("\nğŸ“‹ æœ€è¿‘æ£€æµ‹è®°å½•:")
            for record in self.detection_history[-3:]:
                stats = record['stats']
                timestamp = record['timestamp'][:19]
                print(f"   [{timestamp}] æ£€æµ‹åˆ° {stats['total_detections']} ä¸ªç›®æ ‡, "
                      f"ç”¨æ—¶ {stats['detection_time']:.3f}ç§’")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºYOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ
    yolo_center = YOLOIntelligenceCenter(model_size='n')  # ä½¿ç”¨çº³ç±³ç‰ˆæœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    
    # åˆ›å»ºå¤æ‚æµ‹è¯•åœºæ™¯
    test_scene = yolo_center.create_complex_test_scene()
    
    # æ‰§è¡ŒAIæ£€æµ‹
    detections, stats = yolo_center.detect_objects(test_scene)
    
    # å¯è§†åŒ–æ£€æµ‹ç»“æœ
    result_image = yolo_center.draw_detection_results(test_scene, detections)
    
    # æ˜¾ç¤ºç»“æœ
    plt.figure(figsize=(15, 10))
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(test_scene, cv2.COLOR_BGR2RGB))
    plt.title('åŸå§‹æµ‹è¯•åœºæ™¯')
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
    plt.title(f'AIæ£€æµ‹ç»“æœ ({len(detections)}ä¸ªç›®æ ‡)')
    plt.axis('off')
    
    plt.suptitle('ğŸ§  YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - AIæ£€æµ‹å±•ç¤º', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # æ˜¾ç¤ºä¸­å¿ƒæŠ¥å‘Š
    yolo_center.get_intelligence_report()
```

#### ğŸ¯ YOLOå¤§è„‘çš„æŠ€æœ¯ä¼˜åŠ¿

YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å…·æœ‰é©å‘½æ€§çš„ä¼˜åŠ¿ï¼š

**ğŸš€ æŠ€æœ¯çªç ´**ï¼š
- **ä¸€æ¬¡æ€§æ£€æµ‹**ï¼šå•æ¬¡å‰å‘ä¼ æ’­åŒæ—¶æ£€æµ‹æ‰€æœ‰ç›®æ ‡
- **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šä»åŸå§‹åƒç´ ç›´æ¥åˆ°æ£€æµ‹ç»“æœ
- **å®æ—¶æ€§èƒ½**ï¼šèƒ½å¤Ÿè¾¾åˆ°å®æ—¶æ£€æµ‹è¦æ±‚
- **å¤šç›®æ ‡è¯†åˆ«**ï¼šåŒæ—¶è¯†åˆ«80å¤šç§ä¸åŒç±»åˆ«çš„ç›®æ ‡

**ğŸ¯ åº”ç”¨ä¼˜åŠ¿**ï¼š
- **å‡†ç¡®æ€§é«˜**ï¼šåŸºäºæ·±åº¦å­¦ä¹ ï¼Œè¯†åˆ«å‡†ç¡®ç‡æ˜¾è‘—æå‡
- **é€‚åº”æ€§å¼º**ï¼šå¯¹å…‰ç…§ã€è§’åº¦ã€å°ºå¯¸å˜åŒ–æœ‰å¾ˆå¥½çš„é²æ£’æ€§
- **æ‰©å±•æ€§å¥½**ï¼šå¯ä»¥é€šè¿‡è®­ç»ƒè¯†åˆ«è‡ªå®šä¹‰ç±»åˆ«
- **éƒ¨ç½²çµæ´»**ï¼šæ”¯æŒå¤šç§ç¡¬ä»¶å¹³å°å’Œä¼˜åŒ–æ–¹æ¡ˆ

### 36.2.2 å®æ—¶æ£€æµ‹ç”Ÿäº§çº¿

#### ğŸ­ æ™ºèƒ½ç”Ÿäº§çº¿çš„å»ºç«‹

æœ‰äº†YOLOæ™ºèƒ½å¤§è„‘ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å»ºç«‹ä¸€æ¡å®Œæ•´çš„**å®æ—¶æ£€æµ‹ç”Ÿäº§çº¿**ã€‚è¿™æ¡ç”Ÿäº§çº¿èƒ½å¤ŸæŒç»­ä¸æ–­åœ°å¤„ç†è§†é¢‘æµï¼Œå®ç°å·¥ä¸šçº§çš„å®æ—¶ç›®æ ‡æ£€æµ‹æœåŠ¡ã€‚

---

**ç¬¬äºŒèŠ‚å­¦ä¹ è¿›å±•**ï¼š
- âœ… å»ºç«‹äº†YOLOæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ
- âœ… æŒæ¡äº†æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹çš„æ ¸å¿ƒåŸç†
- âœ… å®ç°äº†å®æ—¶å¤šç›®æ ‡æ£€æµ‹ç³»ç»Ÿ
- âœ… ç†è§£äº†YOLOç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿

**ç»§ç»­ç¬¬ä¸‰èŠ‚é¢„å‘Š**ï¼šæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥**äººè„¸èº«ä»½éªŒè¯å±€**ï¼Œå­¦ä¹ ä¸“ä¸šçš„äººè„¸è¯†åˆ«ç³»ç»Ÿå¼€å‘ï¼

## ğŸ“š ç¬¬ä¸‰èŠ‚ï¼šäººè„¸èº«ä»½éªŒè¯å±€ - äººè„¸è¯†åˆ«ç³»ç»Ÿ

### 36.3.1 äººè„¸æ£€æµ‹å°ç»„

#### ğŸ‘¤ ä¸“ä¸šèº«ä»½éªŒè¯å›¢é˜Ÿ

æ¬¢è¿æ¥åˆ°AIæ™ºèƒ½åˆ†æè½¦é—´æœ€ç¥ç§˜çš„éƒ¨é—¨â€”â€”**äººè„¸èº«ä»½éªŒè¯å±€**ï¼è¿™é‡Œçš„ä¸“å®¶ä»¬ä¸“é—¨è´Ÿè´£äººè„¸ç›¸å…³çš„æ‰€æœ‰ä»»åŠ¡ï¼Œä»æœ€åˆçš„äººè„¸æ£€æµ‹åˆ°æœ€ç»ˆçš„èº«ä»½éªŒè¯ï¼Œæ¯ä¸€æ­¥éƒ½è¾¾åˆ°äº†ä¼ä¸šçº§çš„ä¸“ä¸šæ ‡å‡†ã€‚

```python
# ç¤ºä¾‹4ï¼šäººè„¸èº«ä»½éªŒè¯å±€ - ä¸“ä¸šäººè„¸è¯†åˆ«ç³»ç»Ÿ
import cv2
import numpy as np
import face_recognition
import dlib
from mtcnn import MTCNN
import matplotlib.pyplot as plt
from pathlib import Path
import pickle
import json
from datetime import datetime
import sqlite3

class FaceVerificationBureau:
    """
    äººè„¸èº«ä»½éªŒè¯å±€
    æä¾›ä¼ä¸šçº§äººè„¸è¯†åˆ«å’Œèº«ä»½éªŒè¯æœåŠ¡
    """
    
    def __init__(self, workspace_path="ai_analysis_workshop/face_bureau"):
        """
        åˆå§‹åŒ–äººè„¸èº«ä»½éªŒè¯å±€
        
        Parameters:
        workspace_path (str): éªŒè¯å±€å·¥ä½œç›®å½•
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.workspace_path / "face_database").mkdir(exist_ok=True)
        (self.workspace_path / "temp_faces").mkdir(exist_ok=True)
        (self.workspace_path / "logs").mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        self.detectors = self._initialize_face_detectors()
        
        # äººè„¸æ•°æ®åº“
        self.face_database_path = self.workspace_path / "face_database" / "face_encodings.pkl"
        self.face_database = self._load_face_database()
        
        # åˆå§‹åŒ–éªŒè¯æ—¥å¿—æ•°æ®åº“
        self.log_db_path = self.workspace_path / "logs" / "verification_logs.db"
        self._initialize_log_database()
        
        # éªŒè¯ç»Ÿè®¡
        self.verification_history = []
        self.recognition_stats = {
            'total_attempts': 0,
            'successful_recognitions': 0,
            'failed_recognitions': 0,
            'new_registrations': 0
        }
        
        print("ğŸ‘¤ äººè„¸èº«ä»½éªŒè¯å±€å¯åŠ¨ï¼")
        print(f"ğŸ“ éªŒè¯å±€åŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¯ å¯ç”¨æ£€æµ‹å™¨: {list(self.detectors.keys())}")
        print(f"ğŸ’¾ äººè„¸æ•°æ®åº“: {len(self.face_database)} ä¸ªå·²æ³¨å†Œèº«ä»½")
    
    def _initialize_face_detectors(self):
        """
        åˆå§‹åŒ–å¤šç§äººè„¸æ£€æµ‹å™¨
        
        Returns:
        dict: æ£€æµ‹å™¨å­—å…¸
        """
        detectors = {}
        
        try:
            # OpenCV Haarçº§è”æ£€æµ‹å™¨
            detectors['opencv'] = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            print("âœ… OpenCVäººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            
            # MTCNNæ£€æµ‹å™¨
            detectors['mtcnn'] = MTCNN()
            print("âœ… MTCNNäººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            
            # dlibæ£€æµ‹å™¨
            detectors['dlib'] = dlib.get_frontal_face_detector()
            print("âœ… dlibäººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ éƒ¨åˆ†æ£€æµ‹å™¨åŠ è½½å¤±è´¥: {e}")
        
        return detectors
    
    def _load_face_database(self):
        """
        åŠ è½½äººè„¸æ•°æ®åº“
        
        Returns:
        dict: äººè„¸æ•°æ®åº“
        """
        if self.face_database_path.exists():
            try:
                with open(self.face_database_path, 'rb') as f:
                    database = pickle.load(f)
                print(f"ğŸ“‚ äººè„¸æ•°æ®åº“åŠ è½½æˆåŠŸ: {len(database)} ä¸ªèº«ä»½")
                return database
            except Exception as e:
                print(f"âš ï¸ äººè„¸æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        
        print("ğŸ†• åˆ›å»ºæ–°çš„äººè„¸æ•°æ®åº“")
        return {}
    
    def _save_face_database(self):
        """ä¿å­˜äººè„¸æ•°æ®åº“"""
        try:
            with open(self.face_database_path, 'wb') as f:
                pickle.dump(self.face_database, f)
            print("ğŸ’¾ äººè„¸æ•°æ®åº“ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ äººè„¸æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
    
    def _initialize_log_database(self):
        """åˆå§‹åŒ–éªŒè¯æ—¥å¿—æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.log_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS verification_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    person_name TEXT,
                    action_type TEXT NOT NULL,
                    confidence REAL,
                    detector_used TEXT,
                    image_path TEXT,
                    result TEXT NOT NULL
                )
            ''')
            
            conn.commit()
            conn.close()
            print("ğŸ“Š éªŒè¯æ—¥å¿—æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ éªŒè¯æ—¥å¿—æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def detect_faces(self, image, detector_type='face_recognition', min_face_size=50):
        """
        å¤šç®—æ³•äººè„¸æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        detector_type (str): æ£€æµ‹å™¨ç±»å‹
        min_face_size (int): æœ€å°äººè„¸å°ºå¯¸
        
        Returns:
        tuple: (äººè„¸ä½ç½®åˆ—è¡¨, æ£€æµ‹ç»Ÿè®¡)
        """
        print(f"ğŸ‘¤ äººè„¸æ£€æµ‹å°ç»„å¼€å§‹æ£€æµ‹ ({detector_type})...")
        start_time = time.time()
        
        face_locations = []
        
        try:
            if detector_type == 'face_recognition':
                # ä½¿ç”¨face_recognitionåº“ï¼ˆåŸºäºdlibï¼‰
                face_locations = face_recognition.face_locations(image, model='hog')
                
            elif detector_type == 'opencv' and 'opencv' in self.detectors:
                # ä½¿ç”¨OpenCV Haarçº§è”
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                faces = self.detectors['opencv'].detectMultiScale(
                    gray, scaleFactor=1.1, minNeighbors=5, 
                    minSize=(min_face_size, min_face_size)
                )
                # è½¬æ¢æ ¼å¼: (x,y,w,h) -> (top,right,bottom,left)
                face_locations = [(y, x+w, y+h, x) for (x, y, w, h) in faces]
                
            elif detector_type == 'mtcnn' and 'mtcnn' in self.detectors:
                # ä½¿ç”¨MTCNN
                result = self.detectors['mtcnn'].detect_faces(image)
                for face in result:
                    x, y, w, h = face['box']
                    if w >= min_face_size and h >= min_face_size:
                        face_locations.append((y, x+w, y+h, x))
                        
            elif detector_type == 'dlib' and 'dlib' in self.detectors:
                # ä½¿ç”¨dlib
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                faces = self.detectors['dlib'](gray)
                for face in faces:
                    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
                    if (x2-x1) >= min_face_size and (y2-y1) >= min_face_size:
                        face_locations.append((y1, x2, y2, x1))
            
            detection_time = time.time() - start_time
            
            # ç”Ÿæˆæ£€æµ‹ç»Ÿè®¡
            stats = {
                'faces_detected': len(face_locations),
                'detection_time': detection_time,
                'detector_used': detector_type,
                'image_size': image.shape,
                'min_face_size': min_face_size
            }
            
            print(f"   æ£€æµ‹å®Œæˆ: å‘ç° {len(face_locations)} å¼ äººè„¸")
            print(f"   æ£€æµ‹ç”¨æ—¶: {detection_time:.3f} ç§’")
            
            return face_locations, stats
            
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return [], {}
    
    def extract_face_encoding(self, image, face_location):
        """
        æå–äººè„¸ç‰¹å¾ç¼–ç 
        
        Parameters:
        image (numpy.ndarray): å›¾åƒ
        face_location (tuple): äººè„¸ä½ç½® (top, right, bottom, left)
        
        Returns:
        numpy.ndarray: äººè„¸ç‰¹å¾ç¼–ç 
        """
        try:
            encodings = face_recognition.face_encodings(image, [face_location])
            if len(encodings) > 0:
                return encodings[0]
            else:
                return None
        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def register_person(self, image, person_name, detector_type='face_recognition'):
        """
        æ³¨å†Œæ–°äººå‘˜
        
        Parameters:
        image (numpy.ndarray): åŒ…å«äººè„¸çš„å›¾åƒ
        person_name (str): äººå‘˜å§“å
        detector_type (str): æ£€æµ‹å™¨ç±»å‹
        
        Returns:
        dict: æ³¨å†Œç»“æœ
        """
        print(f"ğŸ“ å¼€å§‹æ³¨å†Œæ–°äººå‘˜: {person_name}")
        
        # æ£€æµ‹äººè„¸
        face_locations, detection_stats = self.detect_faces(image, detector_type)
        
        if len(face_locations) == 0:
            result = {
                'success': False,
                'message': 'æœªæ£€æµ‹åˆ°äººè„¸',
                'person_name': person_name
            }
            self._log_verification_event('registration', person_name, 'failed', 
                                       detector_type, result['message'])
            return result
        
        if len(face_locations) > 1:
            result = {
                'success': False,
                'message': f'æ£€æµ‹åˆ°å¤šå¼ äººè„¸ ({len(face_locations)}å¼ )ï¼Œè¯·ç¡®ä¿å›¾åƒä¸­åªæœ‰ä¸€å¼ äººè„¸',
                'person_name': person_name
            }
            self._log_verification_event('registration', person_name, 'failed', 
                                       detector_type, result['message'])
            return result
        
        # æå–äººè„¸ç‰¹å¾
        face_location = face_locations[0]
        face_encoding = self.extract_face_encoding(image, face_location)
        
        if face_encoding is None:
            result = {
                'success': False,
                'message': 'äººè„¸ç‰¹å¾æå–å¤±è´¥',
                'person_name': person_name
            }
            self._log_verification_event('registration', person_name, 'failed', 
                                       detector_type, result['message'])
            return result
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼äººè„¸
        for existing_name, existing_data in self.face_database.items():
            distance = np.linalg.norm(face_encoding - existing_data['encoding'])
            if distance < 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                result = {
                    'success': False,
                    'message': f'æ£€æµ‹åˆ°ç›¸ä¼¼äººè„¸ï¼Œå¯èƒ½ä¸å·²æ³¨å†Œäººå‘˜ {existing_name} é‡å¤',
                    'person_name': person_name,
                    'similar_to': existing_name,
                    'distance': distance
                }
                self._log_verification_event('registration', person_name, 'failed', 
                                           detector_type, result['message'])
                return result
        
        # ä¿å­˜äººè„¸å›¾åƒ
        face_image_path = self.workspace_path / "face_database" / f"{person_name}_face.jpg"
        top, right, bottom, left = face_location
        face_image = image[top:bottom, left:right]
        cv2.imwrite(str(face_image_path), cv2.cvtColor(face_image, cv2.COLOR_RGB2BGR))
        
        # æ³¨å†Œåˆ°æ•°æ®åº“
        self.face_database[person_name] = {
            'encoding': face_encoding,
            'registration_date': datetime.now().isoformat(),
            'face_image_path': str(face_image_path),
            'detector_used': detector_type,
            'verification_count': 0
        }
        
        # ä¿å­˜æ•°æ®åº“
        self._save_face_database()
        
        # æ›´æ–°ç»Ÿè®¡
        self.recognition_stats['new_registrations'] += 1
        
        result = {
            'success': True,
            'message': f'äººå‘˜ {person_name} æ³¨å†ŒæˆåŠŸ',
            'person_name': person_name,
            'face_location': face_location,
            'encoding_shape': face_encoding.shape
        }
        
        self._log_verification_event('registration', person_name, 'success', 
                                   detector_type, result['message'])
        
        print(f"âœ… {result['message']}")
        return result
    
    def verify_person(self, image, person_name, detector_type='face_recognition', 
                     tolerance=0.6):
        """
        éªŒè¯æŒ‡å®šäººå‘˜èº«ä»½ (1:1éªŒè¯)
        
        Parameters:
        image (numpy.ndarray): åŒ…å«äººè„¸çš„å›¾åƒ
        person_name (str): å¾…éªŒè¯çš„äººå‘˜å§“å
        detector_type (str): æ£€æµ‹å™¨ç±»å‹
        tolerance (float): ç›¸ä¼¼åº¦å®¹å¿åº¦
        
        Returns:
        dict: éªŒè¯ç»“æœ
        """
        print(f"ğŸ” å¼€å§‹èº«ä»½éªŒè¯: {person_name}")
        
        self.recognition_stats['total_attempts'] += 1
        
        # æ£€æŸ¥äººå‘˜æ˜¯å¦å·²æ³¨å†Œ
        if person_name not in self.face_database:
            result = {
                'success': False,
                'message': f'äººå‘˜ {person_name} æœªæ³¨å†Œ',
                'person_name': person_name
            }
            self.recognition_stats['failed_recognitions'] += 1
            self._log_verification_event('verification', person_name, 'failed', 
                                       detector_type, result['message'])
            return result
        
        # æ£€æµ‹äººè„¸
        face_locations, detection_stats = self.detect_faces(image, detector_type)
        
        if len(face_locations) == 0:
            result = {
                'success': False,
                'message': 'æœªæ£€æµ‹åˆ°äººè„¸',
                'person_name': person_name
            }
            self.recognition_stats['failed_recognitions'] += 1
            self._log_verification_event('verification', person_name, 'failed', 
                                       detector_type, result['message'])
            return result
        
        # æå–äººè„¸ç‰¹å¾å¹¶æ¯”è¾ƒ
        registered_encoding = self.face_database[person_name]['encoding']
        best_match = False
        best_distance = float('inf')
        
        for face_location in face_locations:
            face_encoding = self.extract_face_encoding(image, face_location)
            if face_encoding is not None:
                distance = np.linalg.norm(face_encoding - registered_encoding)
                if distance < best_distance:
                    best_distance = distance
                    if distance <= tolerance:
                        best_match = True
        
        # æ›´æ–°éªŒè¯ç»Ÿè®¡
        self.face_database[person_name]['verification_count'] += 1
        self._save_face_database()
        
        if best_match:
            confidence = max(0, 1 - best_distance)
            result = {
                'success': True,
                'message': f'èº«ä»½éªŒè¯æˆåŠŸ: {person_name}',
                'person_name': person_name,
                'confidence': confidence,
                'distance': best_distance,
                'faces_detected': len(face_locations)
            }
            self.recognition_stats['successful_recognitions'] += 1
            self._log_verification_event('verification', person_name, 'success', 
                                       detector_type, f"confidence: {confidence:.3f}")
        else:
            result = {
                'success': False,
                'message': f'èº«ä»½éªŒè¯å¤±è´¥: ä¸åŒ¹é… {person_name}',
                'person_name': person_name,
                'best_distance': best_distance,
                'tolerance': tolerance,
                'faces_detected': len(face_locations)
            }
            self.recognition_stats['failed_recognitions'] += 1
            self._log_verification_event('verification', person_name, 'failed', 
                                       detector_type, f"distance: {best_distance:.3f}")
        
        print(f"   {result['message']}")
        return result
    
    def recognize_person(self, image, detector_type='face_recognition', tolerance=0.6):
        """
        è¯†åˆ«äººå‘˜èº«ä»½ (1:Nè¯†åˆ«)
        
        Parameters:
        image (numpy.ndarray): åŒ…å«äººè„¸çš„å›¾åƒ
        detector_type (str): æ£€æµ‹å™¨ç±»å‹
        tolerance (float): ç›¸ä¼¼åº¦å®¹å¿åº¦
        
        Returns:
        dict: è¯†åˆ«ç»“æœ
        """
        print("ğŸ” å¼€å§‹äººå‘˜èº«ä»½è¯†åˆ«...")
        
        self.recognition_stats['total_attempts'] += 1
        
        if len(self.face_database) == 0:
            result = {
                'success': False,
                'message': 'äººè„¸æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯†åˆ«',
                'recognized_person': None
            }
            self.recognition_stats['failed_recognitions'] += 1
            return result
        
        # æ£€æµ‹äººè„¸
        face_locations, detection_stats = self.detect_faces(image, detector_type)
        
        if len(face_locations) == 0:
            result = {
                'success': False,
                'message': 'æœªæ£€æµ‹åˆ°äººè„¸',
                'recognized_person': None
            }
            self.recognition_stats['failed_recognitions'] += 1
            return result
        
        # å¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸è¿›è¡Œè¯†åˆ«
        recognition_results = []
        
        for i, face_location in enumerate(face_locations):
            face_encoding = self.extract_face_encoding(image, face_location)
            if face_encoding is None:
                continue
            
            # ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸è¿›è¡Œæ¯”è¾ƒ
            best_match_name = None
            best_distance = float('inf')
            
            for person_name, person_data in self.face_database.items():
                distance = np.linalg.norm(face_encoding - person_data['encoding'])
                if distance < best_distance:
                    best_distance = distance
                    if distance <= tolerance:
                        best_match_name = person_name
            
            if best_match_name:
                confidence = max(0, 1 - best_distance)
                recognition_results.append({
                    'face_index': i,
                    'face_location': face_location,
                    'recognized_person': best_match_name,
                    'confidence': confidence,
                    'distance': best_distance
                })
                
                # æ›´æ–°éªŒè¯ç»Ÿè®¡
                self.face_database[best_match_name]['verification_count'] += 1
                self._log_verification_event('recognition', best_match_name, 'success', 
                                           detector_type, f"confidence: {confidence:.3f}")
            else:
                recognition_results.append({
                    'face_index': i,
                    'face_location': face_location,
                    'recognized_person': 'Unknown',
                    'confidence': 0.0,
                    'distance': best_distance
                })
        
        # ä¿å­˜æ›´æ–°çš„æ•°æ®åº“
        self._save_face_database()
        
        if any(r['recognized_person'] != 'Unknown' for r in recognition_results):
            self.recognition_stats['successful_recognitions'] += 1
            result = {
                'success': True,
                'message': f'è¯†åˆ«æˆåŠŸï¼Œæ£€æµ‹åˆ° {len(face_locations)} å¼ äººè„¸',
                'recognition_results': recognition_results,
                'total_faces': len(face_locations)
            }
        else:
            self.recognition_stats['failed_recognitions'] += 1
            result = {
                'success': False,
                'message': f'æœªè¯†åˆ«å‡ºå·²çŸ¥äººå‘˜ï¼Œæ£€æµ‹åˆ° {len(face_locations)} å¼ æœªçŸ¥äººè„¸',
                'recognition_results': recognition_results,
                'total_faces': len(face_locations)
            }
        
        print(f"   {result['message']}")
        return result
    
    def _log_verification_event(self, action_type, person_name, result, 
                               detector_used, details):
        """è®°å½•éªŒè¯äº‹ä»¶åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.log_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO verification_logs 
                (timestamp, person_name, action_type, detector_used, result, confidence)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                person_name,
                action_type,
                detector_used,
                result,
                details
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—è®°å½•å¤±è´¥: {e}")
    
    def draw_face_results(self, image, recognition_results):
        """
        ç»˜åˆ¶äººè„¸è¯†åˆ«ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        recognition_results (list): è¯†åˆ«ç»“æœåˆ—è¡¨
        
        Returns:
        numpy.ndarray: æ ‡æ³¨åçš„å›¾åƒ
        """
        result_image = image.copy()
        
        for result in recognition_results:
            face_location = result['face_location']
            person_name = result['recognized_person']
            confidence = result['confidence']
            
            top, right, bottom, left = face_location
            
            # é€‰æ‹©é¢œè‰²
            if person_name == 'Unknown':
                color = (255, 0, 0)  # çº¢è‰²è¡¨ç¤ºæœªçŸ¥
                label = "Unknown"
            else:
                color = (0, 255, 0)  # ç»¿è‰²è¡¨ç¤ºå·²çŸ¥
                label = f"{person_name} ({confidence:.2f})"
            
            # ç»˜åˆ¶äººè„¸æ¡†
            cv2.rectangle(result_image, (left, top), (right, bottom), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
            label_y = max(top - 10, label_size[1])
            
            cv2.rectangle(result_image, 
                         (left, label_y - label_size[1] - 5), 
                         (left + label_size[0] + 5, label_y + 5), 
                         color, -1)
            
            cv2.putText(result_image, label, (left + 2, label_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return result_image
    
    def get_bureau_report(self):
        """
        è·å–äººè„¸èº«ä»½éªŒè¯å±€å·¥ä½œæŠ¥å‘Š
        """
        print("\nğŸ‘¤ äººè„¸èº«ä»½éªŒè¯å±€å·¥ä½œæŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ éªŒè¯å±€åŸºåœ°: {self.workspace_path}")
        print(f"ğŸ’¾ å·²æ³¨å†Œäººå‘˜: {len(self.face_database)} äºº")
        print(f"ğŸ¯ å¯ç”¨æ£€æµ‹å™¨: {len(self.detectors)} ä¸ª")
        
        stats = self.recognition_stats
        total_attempts = stats['total_attempts']
        if total_attempts > 0:
            success_rate = (stats['successful_recognitions'] / total_attempts) * 100
            print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
            print(f"   æ€»éªŒè¯æ¬¡æ•°: {total_attempts}")
            print(f"   æˆåŠŸéªŒè¯: {stats['successful_recognitions']}")
            print(f"   å¤±è´¥éªŒè¯: {stats['failed_recognitions']}")
            print(f"   æ–°æ³¨å†Œäººæ•°: {stats['new_registrations']}")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.face_database:
            print(f"\nğŸ‘¥ å·²æ³¨å†Œäººå‘˜åˆ—è¡¨:")
            for name, data in self.face_database.items():
                reg_date = data['registration_date'][:10]  # åªæ˜¾ç¤ºæ—¥æœŸ
                verify_count = data['verification_count']
                print(f"   {name} - æ³¨å†Œ: {reg_date}, éªŒè¯: {verify_count} æ¬¡")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºäººè„¸èº«ä»½éªŒè¯å±€
    face_bureau = FaceVerificationBureau()
    
    # æ˜¾ç¤ºéªŒè¯å±€æŠ¥å‘Š
    face_bureau.get_bureau_report()
```

---

é€šè¿‡ç¬¬ä¸‰èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸“ä¸šçš„äººè„¸èº«ä»½éªŒè¯å±€ï¼ŒæŒæ¡äº†ä¼ä¸šçº§äººè„¸è¯†åˆ«æŠ€æœ¯ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿›å…¥æœ€åä¸€èŠ‚ï¼Œå°†æ‰€æœ‰æŠ€æœ¯æ•´åˆæˆå®Œæ•´çš„æ™ºèƒ½ç›‘æ§ç³»ç»Ÿï¼

**ç¬¬ä¸‰èŠ‚å­¦ä¹ æˆæœ**ï¼š
- âœ… å»ºç«‹äº†ä¸“ä¸šçš„äººè„¸æ£€æµ‹ç³»ç»Ÿ
- âœ… å®ç°äº†äººè„¸æ³¨å†Œå’Œç‰¹å¾æå–
- âœ… æŒæ¡äº†1:1èº«ä»½éªŒè¯å’Œ1:Nèº«ä»½è¯†åˆ«
- âœ… å»ºç«‹äº†å®Œæ•´çš„äººè„¸æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ

**æœ€ç»ˆç« èŠ‚é¢„å‘Š**ï¼šç¬¬å››èŠ‚å°†å»ºç«‹**æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ**ï¼Œæ•´åˆæ‰€æœ‰æŠ€æœ¯æ‰“é€ ä¼ä¸šçº§ç›‘æ§è§£å†³æ–¹æ¡ˆï¼

## ğŸ“š ç¬¬å››èŠ‚ï¼šæ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ - ç»¼åˆé¡¹ç›®

### 36.4.1 æ™ºèƒ½ç›‘æ§ç³»ç»Ÿæ¶æ„

#### ğŸ“º æŒ‡æŒ¥ä¸­å¿ƒæ€»éƒ¨

æ­å–œæ‚¨ï¼ç°åœ¨æˆ‘ä»¬è¦å»ºç«‹AIæ™ºèƒ½åˆ†æè½¦é—´çš„**æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ**â€”â€”è¿™é‡Œå°†æ•´åˆå‰é¢ä¸‰èŠ‚å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯ï¼Œæ‰“é€ ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§æ™ºèƒ½ç›‘æ§è§£å†³æ–¹æ¡ˆã€‚

```python
# ç¤ºä¾‹5ï¼šæ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ - ä¼ä¸šçº§ç»¼åˆç›‘æ§ç³»ç»Ÿ
import cv2
import numpy as np
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import threading
import queue
import json
import sqlite3
from pathlib import Path
import time

# å¯¼å…¥å‰é¢ç« èŠ‚çš„ç»„ä»¶
from haar_detection_unit import HaarDetectionUnit
from hog_patrol_unit import HOGSVMPatrolUnit  
from yolo_intelligence_center import YOLOIntelligenceCenter
from face_verification_bureau import FaceVerificationBureau

class SmartMonitoringCommandCenter:
    """
    æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ
    æ•´åˆæ‰€æœ‰æ£€æµ‹æŠ€æœ¯çš„ä¼ä¸šçº§ç›‘æ§è§£å†³æ–¹æ¡ˆ
    """
    
    def __init__(self, workspace_path="ai_analysis_workshop/command_center"):
        """
        åˆå§‹åŒ–æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ
        
        Parameters:
        workspace_path (str): æŒ‡æŒ¥ä¸­å¿ƒå·¥ä½œç›®å½•
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç³»ç»Ÿç›®å½•
        (self.workspace_path / "recordings").mkdir(exist_ok=True)
        (self.workspace_path / "alerts").mkdir(exist_ok=True)
        (self.workspace_path / "reports").mkdir(exist_ok=True)
        (self.workspace_path / "config").mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªæ£€æµ‹å•å…ƒ
        print("ğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒæ­£åœ¨åˆå§‹åŒ–å„éƒ¨é—¨...")
        self._initialize_detection_units()
        
        # åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“
        self.db_path = self.workspace_path / "monitoring.db"
        self._initialize_monitoring_database()
        
        # ç³»ç»Ÿé…ç½®
        self.config = self._load_system_config()
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring_active = False
        self.alert_queue = queue.Queue()
        self.detection_results = {}
        
        # ç»Ÿè®¡æ•°æ®
        self.system_stats = {
            'total_detections': 0,
            'people_count': 0,
            'face_recognitions': 0,
            'alerts_generated': 0,
            'uptime_start': datetime.now()
        }
        
        print("ğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒå¯åŠ¨å®Œæˆï¼")
        print(f"ğŸ“ æŒ‡æŒ¥ä¸­å¿ƒåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ¯ é›†æˆæ£€æµ‹å•å…ƒ: 4ä¸ª (Haar, HOG, YOLO, Face)")
    
    def _initialize_detection_units(self):
        """åˆå§‹åŒ–å„æ£€æµ‹å•å…ƒ"""
        try:
            # ç›®æ ‡ä¾¦å¯Ÿéƒ¨ - Haarçº§è”æ£€æµ‹å™¨
            self.haar_unit = HaarDetectionUnit(
                self.workspace_path / "haar_unit"
            )
            print("âœ… ç›®æ ‡ä¾¦å¯Ÿéƒ¨ - Haarå•å…ƒå·²å°±ä½")
            
            # å·¡é€»é˜Ÿ - HOG+SVMæ£€æµ‹å™¨
            self.patrol_unit = HOGSVMPatrolUnit(
                self.workspace_path / "patrol_unit"
            )
            print("âœ… å·¡é€»é˜Ÿ - HOGå•å…ƒå·²å°±ä½")
            
            # AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - YOLOæ£€æµ‹å™¨
            self.yolo_center = YOLOIntelligenceCenter(
                'n',  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ä¿è¯å®æ—¶æ€§
                self.workspace_path / "yolo_center"
            )
            print("âœ… AIæ™ºèƒ½è¯†åˆ«ä¸­å¿ƒ - YOLOå•å…ƒå·²å°±ä½")
            
            # äººè„¸èº«ä»½éªŒè¯å±€
            self.face_bureau = FaceVerificationBureau(
                self.workspace_path / "face_bureau"
            )
            print("âœ… äººè„¸èº«ä»½éªŒè¯å±€å·²å°±ä½")
            
        except Exception as e:
            print(f"âŒ æ£€æµ‹å•å…ƒåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _initialize_monitoring_database(self):
        """åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºç›‘æ§äº‹ä»¶è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS monitoring_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    detector_used TEXT NOT NULL,
                    detection_count INTEGER,
                    confidence REAL,
                    details TEXT,
                    image_path TEXT,
                    alert_level TEXT
                )
            ''')
            
            # åˆ›å»ºç³»ç»ŸçŠ¶æ€è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS system_status (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    cpu_usage REAL,
                    memory_usage REAL,
                    fps REAL,
                    active_cameras INTEGER,
                    total_detections INTEGER
                )
            ''')
            
            # åˆ›å»ºäººå‘˜è¿›å‡ºè®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS access_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    person_name TEXT,
                    action TEXT,
                    confidence REAL,
                    camera_location TEXT,
                    image_path TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            print("ğŸ“Š ç›‘æ§æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ ç›‘æ§æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _load_system_config(self):
        """åŠ è½½ç³»ç»Ÿé…ç½®"""
        config_file = self.workspace_path / "config" / "system_config.json"
        
        default_config = {
            'detection_settings': {
                'enable_haar': True,
                'enable_hog': True,
                'enable_yolo': True,
                'enable_face_recognition': True,
                'detection_interval': 0.1,  # ç§’
                'confidence_threshold': 0.5
            },
            'alert_settings': {
                'enable_people_count_alert': True,
                'max_people_threshold': 10,
                'enable_unknown_face_alert': True,
                'enable_no_face_detected_alert': False,
                'alert_cooldown': 30  # ç§’
            },
            'recording_settings': {
                'enable_recording': True,
                'recording_quality': 'medium',
                'max_recording_hours': 24,
                'auto_cleanup': True
            },
            'camera_settings': {
                'resolution_width': 640,
                'resolution_height': 480,
                'fps': 30
            }
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                print("ğŸ“‹ ç³»ç»Ÿé…ç½®åŠ è½½æˆåŠŸ")
                return config
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        # ä¿å­˜é»˜è®¤é…ç½®
        try:
            with open(config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            print("ğŸ“‹ é»˜è®¤é…ç½®å·²ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸ é…ç½®ä¿å­˜å¤±è´¥: {e}")
        
        return default_config
    
    def comprehensive_detection(self, image):
        """
        ç»¼åˆæ£€æµ‹ï¼šæ•´åˆæ‰€æœ‰æ£€æµ‹å•å…ƒçš„ç»“æœ
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        
        Returns:
        dict: ç»¼åˆæ£€æµ‹ç»“æœ
        """
        detection_results = {
            'timestamp': datetime.now().isoformat(),
            'image_shape': image.shape,
            'detections': {}
        }
        
        config = self.config['detection_settings']
        
        # 1. Haarçº§è”æ£€æµ‹ï¼ˆå¿«é€Ÿäººè„¸æ£€æµ‹ï¼‰
        if config['enable_haar']:
            try:
                haar_faces, haar_stats = self.haar_unit.detect_targets(
                    image, 'face', scale_factor=1.1, min_neighbors=3
                )
                detection_results['detections']['haar_faces'] = {
                    'count': len(haar_faces),
                    'locations': haar_faces.tolist() if len(haar_faces) > 0 else [],
                    'detection_time': haar_stats.get('detection_time', 0)
                }
            except Exception as e:
                print(f"âš ï¸ Haaræ£€æµ‹å¤±è´¥: {e}")
                detection_results['detections']['haar_faces'] = {
                    'count': 0, 'locations': [], 'detection_time': 0
                }
        
        # 2. HOG+SVMè¡Œäººæ£€æµ‹
        if config['enable_hog']:
            try:
                hog_people, hog_stats = self.patrol_unit.detect_people(
                    image, scale_factor=1.05, hit_threshold=0
                )
                detection_results['detections']['hog_people'] = {
                    'count': len(hog_people),
                    'locations': hog_people.tolist() if len(hog_people) > 0 else [],
                    'detection_time': hog_stats.get('detection_time', 0)
                }
            except Exception as e:
                print(f"âš ï¸ HOGæ£€æµ‹å¤±è´¥: {e}")
                detection_results['detections']['hog_people'] = {
                    'count': 0, 'locations': [], 'detection_time': 0
                }
        
        # 3. YOLOç›®æ ‡æ£€æµ‹
        if config['enable_yolo']:
            try:
                yolo_objects, yolo_stats = self.yolo_center.detect_objects(
                    image, conf_threshold=config['confidence_threshold']
                )
                detection_results['detections']['yolo_objects'] = {
                    'count': len(yolo_objects),
                    'objects': yolo_objects,
                    'detection_time': yolo_stats.get('detection_time', 0),
                    'class_counts': yolo_stats.get('class_counts', {})
                }
            except Exception as e:
                print(f"âš ï¸ YOLOæ£€æµ‹å¤±è´¥: {e}")
                detection_results['detections']['yolo_objects'] = {
                    'count': 0, 'objects': [], 'detection_time': 0, 'class_counts': {}
                }
        
        # 4. äººè„¸è¯†åˆ«
        if config['enable_face_recognition']:
            try:
                face_result = self.face_bureau.recognize_person(
                    image, detector_type='face_recognition', tolerance=0.6
                )
                detection_results['detections']['face_recognition'] = {
                    'success': face_result['success'],
                    'total_faces': face_result.get('total_faces', 0),
                    'recognition_results': face_result.get('recognition_results', [])
                }
            except Exception as e:
                print(f"âš ï¸ äººè„¸è¯†åˆ«å¤±è´¥: {e}")
                detection_results['detections']['face_recognition'] = {
                    'success': False, 'total_faces': 0, 'recognition_results': []
                }
        
        # æ›´æ–°ç³»ç»Ÿç»Ÿè®¡
        self._update_system_stats(detection_results)
        
        # æ£€æŸ¥è­¦æŠ¥æ¡ä»¶
        self._check_alert_conditions(detection_results)
        
        return detection_results
    
    def _update_system_stats(self, detection_results):
        """æ›´æ–°ç³»ç»Ÿç»Ÿè®¡æ•°æ®"""
        detections = detection_results['detections']
        
        # ç»Ÿè®¡æ€»æ£€æµ‹æ•°é‡
        total_detections = 0
        for detection_type, data in detections.items():
            if isinstance(data, dict) and 'count' in data:
                total_detections += data['count']
        
        self.system_stats['total_detections'] += total_detections
        
        # ç»Ÿè®¡äººå‘˜æ•°é‡
        people_count = 0
        if 'haar_faces' in detections:
            people_count += detections['haar_faces']['count']
        if 'hog_people' in detections:
            people_count += detections['hog_people']['count']
        
        self.system_stats['people_count'] = max(
            self.system_stats['people_count'], people_count
        )
        
        # ç»Ÿè®¡äººè„¸è¯†åˆ«
        if 'face_recognition' in detections:
            if detections['face_recognition']['success']:
                self.system_stats['face_recognitions'] += 1
    
    def _check_alert_conditions(self, detection_results):
        """æ£€æŸ¥è­¦æŠ¥æ¡ä»¶"""
        alert_config = self.config['alert_settings']
        alerts = []
        
        # æ£€æŸ¥äººå‘˜æ•°é‡è­¦æŠ¥
        if alert_config['enable_people_count_alert']:
            people_count = 0
            detections = detection_results['detections']
            
            if 'hog_people' in detections:
                people_count = detections['hog_people']['count']
            
            if people_count > alert_config['max_people_threshold']:
                alerts.append({
                    'type': 'people_count_exceeded',
                    'message': f'æ£€æµ‹åˆ° {people_count} äººï¼Œè¶…è¿‡é˜ˆå€¼ {alert_config["max_people_threshold"]}',
                    'level': 'warning',
                    'timestamp': datetime.now().isoformat()
                })
        
        # æ£€æŸ¥æœªçŸ¥äººè„¸è­¦æŠ¥
        if alert_config['enable_unknown_face_alert']:
            face_data = detection_results['detections'].get('face_recognition', {})
            recognition_results = face_data.get('recognition_results', [])
            
            unknown_faces = [r for r in recognition_results 
                           if r.get('recognized_person') == 'Unknown']
            
            if len(unknown_faces) > 0:
                alerts.append({
                    'type': 'unknown_face_detected',
                    'message': f'æ£€æµ‹åˆ° {len(unknown_faces)} å¼ æœªçŸ¥äººè„¸',
                    'level': 'alert',
                    'timestamp': datetime.now().isoformat()
                })
        
        # å°†è­¦æŠ¥åŠ å…¥é˜Ÿåˆ—
        for alert in alerts:
            self.alert_queue.put(alert)
            self.system_stats['alerts_generated'] += 1
    
    def draw_comprehensive_results(self, image, detection_results):
        """
        ç»˜åˆ¶ç»¼åˆæ£€æµ‹ç»“æœ
        
        Parameters:
        image (numpy.ndarray): åŸå§‹å›¾åƒ
        detection_results (dict): æ£€æµ‹ç»“æœ
        
        Returns:
        numpy.ndarray: æ ‡æ³¨åçš„å›¾åƒ
        """
        result_image = image.copy()
        detections = detection_results['detections']
        
        # ç»˜åˆ¶YOLOç›®æ ‡æ£€æµ‹ç»“æœ
        if 'yolo_objects' in detections:
            for obj in detections['yolo_objects']['objects']:
                x, y, w, h = obj['bbox']
                class_name = obj['class_name']
                confidence = obj['confidence']
                
                # é€‰æ‹©é¢œè‰²
                color = (0, 255, 255)  # é»„è‰²è¡¨ç¤ºYOLOæ£€æµ‹
                
                cv2.rectangle(result_image, (x, y), (x+w, y+h), color, 2)
                label = f"YOLO: {class_name} ({confidence:.2f})"
                
                cv2.putText(result_image, label, (x, y-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        # ç»˜åˆ¶äººè„¸è¯†åˆ«ç»“æœ
        if 'face_recognition' in detections:
            recognition_results = detections['face_recognition']['recognition_results']
            for face_result in recognition_results:
                face_location = face_result['face_location']
                person_name = face_result['recognized_person']
                confidence = face_result['confidence']
                
                top, right, bottom, left = face_location
                
                if person_name == 'Unknown':
                    color = (255, 0, 0)  # çº¢è‰²è¡¨ç¤ºæœªçŸ¥
                    label = "Unknown Face"
                else:
                    color = (0, 255, 0)  # ç»¿è‰²è¡¨ç¤ºå·²çŸ¥
                    label = f"{person_name} ({confidence:.2f})"
                
                cv2.rectangle(result_image, (left, top), (right, bottom), color, 3)
                cv2.putText(result_image, label, (left, top-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
        info_lines = [
            f"Time: {datetime.now().strftime('%H:%M:%S')}",
            f"Total Objects: {detections.get('yolo_objects', {}).get('count', 0)}",
            f"Faces: {detections.get('face_recognition', {}).get('total_faces', 0)}",
            f"People: {detections.get('hog_people', {}).get('count', 0)}"
        ]
        
        for i, line in enumerate(info_lines):
            cv2.putText(result_image, line, (10, 25 + i*20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return result_image
    
    def start_monitoring(self, video_source=0, duration=None):
        """
        å¯åŠ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ
        
        Parameters:
        video_source: è§†é¢‘æºï¼ˆæ‘„åƒå¤´IDæˆ–è§†é¢‘æ–‡ä»¶ï¼‰
        duration (int): ç›‘æ§æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºæŒç»­ç›‘æ§
        """
        print("ğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒå¯åŠ¨ç›‘æ§...")
        
        # æ‰“å¼€è§†é¢‘æº
        cap = cv2.VideoCapture(video_source)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        camera_config = self.config['camera_settings']
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_config['resolution_width'])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_config['resolution_height'])
        cap.set(cv2.CAP_PROP_FPS, camera_config['fps'])
        
        self.monitoring_active = True
        start_time = time.time()
        frame_count = 0
        
        print("ğŸš€ ç›‘æ§ç³»ç»Ÿå¯åŠ¨ï¼")
        print("æŒ‰ 'q' é”®é€€å‡ºç›‘æ§")
        print("æŒ‰ 's' é”®æˆªå›¾ä¿å­˜")
        print("æŒ‰ 'r' é”®å¼€å§‹/åœæ­¢å½•åˆ¶")
        
        try:
            while self.monitoring_active:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æ£€æŸ¥æŒç»­æ—¶é—´
                if duration and (time.time() - start_time) > duration:
                    break
                
                # æ‰§è¡Œç»¼åˆæ£€æµ‹
                detection_results = self.comprehensive_detection(frame)
                
                # ç»˜åˆ¶ç»“æœ
                result_frame = self.draw_comprehensive_results(frame, detection_results)
                
                # æ·»åŠ ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
                elapsed_time = time.time() - start_time
                fps = frame_count / elapsed_time if elapsed_time > 0 else 0
                
                status_text = f"FPS: {fps:.1f} | Uptime: {elapsed_time:.0f}s | Alerts: {self.system_stats['alerts_generated']}"
                cv2.putText(result_frame, status_text, (10, result_frame.shape[0]-20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
                
                # æ˜¾ç¤ºç»“æœ
                cv2.imshow('ğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ', result_frame)
                
                # è®°å½•ç›‘æ§äº‹ä»¶åˆ°æ•°æ®åº“
                self._log_monitoring_event(detection_results)
                
                # å¤„ç†è­¦æŠ¥
                self._process_alerts()
                
                frame_count += 1
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    # ä¿å­˜æˆªå›¾
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    screenshot_path = self.workspace_path / "recordings" / f"screenshot_{timestamp}.jpg"
                    cv2.imwrite(str(screenshot_path), result_frame)
                    print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                
                # æ§åˆ¶æ£€æµ‹é—´éš”
                time.sleep(self.config['detection_settings']['detection_interval'])
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"âŒ ç›‘æ§ç³»ç»Ÿé”™è¯¯: {e}")
        finally:
            # æ¸…ç†èµ„æº
            cap.release()
            cv2.destroyAllWindows()
            self.monitoring_active = False
            
            # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
            self._generate_monitoring_report(start_time, frame_count)
    
    def _log_monitoring_event(self, detection_results):
        """è®°å½•ç›‘æ§äº‹ä»¶åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            timestamp = detection_results['timestamp']
            detections = detection_results['detections']
            
            # è®°å½•å„ç±»æ£€æµ‹äº‹ä»¶
            for detection_type, data in detections.items():
                if isinstance(data, dict) and data.get('count', 0) > 0:
                    cursor.execute('''
                        INSERT INTO monitoring_events 
                        (timestamp, event_type, detector_used, detection_count, details)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        timestamp,
                        'object_detection',
                        detection_type,
                        data.get('count', 0),
                        json.dumps(data)
                    ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âš ï¸ ç›‘æ§äº‹ä»¶è®°å½•å¤±è´¥: {e}")
    
    def _process_alerts(self):
        """å¤„ç†è­¦æŠ¥é˜Ÿåˆ—"""
        try:
            while not self.alert_queue.empty():
                alert = self.alert_queue.get_nowait()
                
                # æ˜¾ç¤ºè­¦æŠ¥ä¿¡æ¯
                print(f"ğŸš¨ {alert['level'].upper()}: {alert['message']}")
                
                # ä¿å­˜è­¦æŠ¥åˆ°æ–‡ä»¶
                alert_file = self.workspace_path / "alerts" / f"alert_{datetime.now().strftime('%Y%m%d')}.json"
                
                try:
                    if alert_file.exists():
                        with open(alert_file, 'r') as f:
                            alerts = json.load(f)
                    else:
                        alerts = []
                    
                    alerts.append(alert)
                    
                    with open(alert_file, 'w') as f:
                        json.dump(alerts, f, indent=2)
                        
                except Exception as e:
                    print(f"âš ï¸ è­¦æŠ¥ä¿å­˜å¤±è´¥: {e}")
                    
        except queue.Empty:
            pass
    
    def _generate_monitoring_report(self, start_time, frame_count):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time if total_time > 0 else 0
        
        report = {
            'monitoring_session': {
                'start_time': datetime.fromtimestamp(start_time).isoformat(),
                'end_time': datetime.now().isoformat(),
                'duration_seconds': total_time,
                'total_frames': frame_count,
                'average_fps': avg_fps
            },
            'detection_statistics': self.system_stats.copy(),
            'system_performance': {
                'average_fps': avg_fps,
                'total_processing_time': total_time
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.workspace_path / "reports" / f"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
            print(f"ğŸ“Š ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        print("\nğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ - ç›‘æ§ä¼šè¯æ€»ç»“")
        print("=" * 50)
        print(f"â±ï¸ ç›‘æ§æ—¶é•¿: {total_time:.1f} ç§’")
        print(f"ğŸ¬ å¤„ç†å¸§æ•°: {frame_count}")
        print(f"ğŸ“Š å¹³å‡FPS: {avg_fps:.1f}")
        print(f"ğŸ¯ æ€»æ£€æµ‹æ•°: {self.system_stats['total_detections']}")
        print(f"ğŸ‘¥ æœ€å¤§äººæ•°: {self.system_stats['people_count']}")
        print(f"ğŸ” äººè„¸è¯†åˆ«: {self.system_stats['face_recognitions']} æ¬¡")
        print(f"ğŸš¨ ç”Ÿæˆè­¦æŠ¥: {self.system_stats['alerts_generated']} ä¸ª")
    
    def get_command_center_status(self):
        """è·å–æŒ‡æŒ¥ä¸­å¿ƒçŠ¶æ€æŠ¥å‘Š"""
        print("\nğŸ“º æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒçŠ¶æ€æŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ æŒ‡æŒ¥ä¸­å¿ƒåŸºåœ°: {self.workspace_path}")
        print(f"ğŸ”§ ç³»ç»ŸçŠ¶æ€: {'ğŸŸ¢ è¿è¡Œä¸­' if self.monitoring_active else 'ğŸ”´ å¾…æœºä¸­'}")
        
        uptime = datetime.now() - self.system_stats['uptime_start']
        print(f"â±ï¸ ç³»ç»Ÿè¿è¡Œæ—¶é—´: {uptime}")
        
        print(f"\nğŸ¯ é›†æˆæ£€æµ‹å•å…ƒçŠ¶æ€:")
        print(f"   ğŸ•µï¸ Haarçº§è”æ£€æµ‹å™¨: {'âœ… å¯ç”¨' if hasattr(self, 'haar_unit') else 'âŒ ä¸å¯ç”¨'}")
        print(f"   ğŸ‘® HOG+SVMå·¡é€»é˜Ÿ: {'âœ… å¯ç”¨' if hasattr(self, 'patrol_unit') else 'âŒ ä¸å¯ç”¨'}")
        print(f"   ğŸ§  YOLOæ™ºèƒ½ä¸­å¿ƒ: {'âœ… å¯ç”¨' if hasattr(self, 'yolo_center') else 'âŒ ä¸å¯ç”¨'}")
        print(f"   ğŸ‘¤ äººè„¸éªŒè¯å±€: {'âœ… å¯ç”¨' if hasattr(self, 'face_bureau') else 'âŒ ä¸å¯ç”¨'}")
        
        print(f"\nğŸ“Š ç´¯è®¡ç»Ÿè®¡:")
        for key, value in self.system_stats.items():
            if key != 'uptime_start':
                print(f"   {key}: {value}")

# ä½¿ç”¨ç¤ºä¾‹å’Œç³»ç»Ÿæ¼”ç¤º
if __name__ == "__main__":
    # åˆ›å»ºæ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ
    command_center = SmartMonitoringCommandCenter()
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    command_center.get_command_center_status()
    
    # å¯åŠ¨ç›‘æ§æ¼”ç¤ºï¼ˆ30ç§’ï¼‰
    print("\nğŸš€ å¯åŠ¨ç›‘æ§æ¼”ç¤º...")
    print("æ³¨æ„ï¼šè¯·ç¡®ä¿æœ‰å¯ç”¨çš„æ‘„åƒå¤´")
    
    try:
        command_center.start_monitoring(video_source=0, duration=30)
    except Exception as e:
        print(f"âŒ ç›‘æ§æ¼”ç¤ºå¤±è´¥: {e}")
    
    # æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
    command_center.get_command_center_status()
```

---

## ğŸ“ ç« èŠ‚æ€»ç»“

æ­å–œæ‚¨ï¼æ‚¨å·²ç»æˆåŠŸå»ºç«‹äº†ä¸€ä¸ªå®Œæ•´çš„**AIæ™ºèƒ½åˆ†æè½¦é—´**ï¼Œå¹¶æŒæ¡äº†ç›®æ ‡æ£€æµ‹ä¸äººè„¸è¯†åˆ«çš„å…¨å¥—ä¼ä¸šçº§æŠ€æœ¯ã€‚

### ğŸ† æ‚¨å·²ç»æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½

#### ğŸ¯ ç›®æ ‡æ£€æµ‹æŠ€æœ¯æ ˆ
- âœ… **ä¼ ç»Ÿç®—æ³•ç²¾é€š**ï¼šHaarçº§è”åˆ†ç±»å™¨ã€HOG+SVMè¡Œäººæ£€æµ‹
- âœ… **æ·±åº¦å­¦ä¹ æŒæ¡**ï¼šYOLOå®æ—¶å¤šç›®æ ‡æ£€æµ‹ã€è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒ
- âœ… **æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›**ï¼šç®—æ³•é€‰æ‹©ã€å‚æ•°è°ƒä¼˜ã€å®æ—¶æ€§å¹³è¡¡

#### ğŸ‘¤ äººè„¸è¯†åˆ«æŠ€æœ¯æ ˆ
- âœ… **å¤šç®—æ³•äººè„¸æ£€æµ‹**ï¼šOpenCVã€MTCNNã€dlibå¤šç§æ£€æµ‹å™¨
- âœ… **ç‰¹å¾æå–ä¸åŒ¹é…**ï¼šæ·±åº¦å­¦ä¹ ç‰¹å¾ç¼–ç ã€ç›¸ä¼¼åº¦è®¡ç®—
- âœ… **èº«ä»½éªŒè¯ç³»ç»Ÿ**ï¼š1:1éªŒè¯ã€1:Nè¯†åˆ«ã€æ•°æ®åº“ç®¡ç†

#### ğŸ“º ä¼ä¸šçº§ç³»ç»Ÿå¼€å‘
- âœ… **ç³»ç»Ÿæ¶æ„è®¾è®¡**ï¼šæ¨¡å—åŒ–è®¾è®¡ã€ç»„ä»¶é›†æˆã€é…ç½®ç®¡ç†
- âœ… **å®æ—¶ç›‘æ§ç³»ç»Ÿ**ï¼šè§†é¢‘æµå¤„ç†ã€è­¦æŠ¥æœºåˆ¶ã€æ—¥å¿—è®°å½•
- âœ… **äº§å“åŒ–å¼€å‘**ï¼šç”¨æˆ·ç•Œé¢ã€æ•°æ®åˆ†æã€ç³»ç»Ÿç›‘æ§

### ğŸš€ æŠ€æœ¯è¿›æ­¥é‡Œç¨‹ç¢‘

ä»ç¬¬35ç« çš„åŸºç¡€å›¾åƒå¤„ç†ï¼Œåˆ°ç¬¬36ç« çš„æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿï¼Œæ‚¨çš„æŠ€æœ¯èƒ½åŠ›å®ç°äº†è´¨çš„é£è·ƒï¼š

```
ç¬¬35ç« : å›¾åƒå¤„ç†åŸºç¡€ â†’ ç¬¬36ç« : æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ
   â†“                      â†“
çœ‹è§å›¾åƒå†…å®¹        â†’    ç†è§£å›¾åƒå«ä¹‰
åŸºç¡€ç®—æ³•åº”ç”¨        â†’    AIå¤§è„‘æ™ºèƒ½åˆ†æ
å•ä¸€æŠ€æœ¯ç‚¹         â†’    ä¼ä¸šçº§è§£å†³æ–¹æ¡ˆ
```

### ğŸ’¡ å•†ä¸šä»·å€¼ä¸åº”ç”¨å‰æ™¯

æ‚¨å¼€å‘çš„æ™ºèƒ½ç›‘æ§ç³»ç»Ÿå…·æœ‰ç›´æ¥çš„å•†ä¸šåº”ç”¨ä»·å€¼ï¼š

**ğŸ¢ ä¼ä¸šå®‰é˜²ç›‘æ§**ï¼š
- æ™ºèƒ½é—¨ç¦ç³»ç»Ÿ
- å‘˜å·¥è€ƒå‹¤ç®¡ç†
- è®¿å®¢èº«ä»½éªŒè¯
- å¼‚å¸¸è¡Œä¸ºæ£€æµ‹

**ğŸª é›¶å”®ä¸šåº”ç”¨**ï¼š
- å®¢æµç»Ÿè®¡åˆ†æ
- VIPå®¢æˆ·è¯†åˆ«
- é˜²ç›—ç›‘æ§ç³»ç»Ÿ
- è´­ç‰©è¡Œä¸ºåˆ†æ

**ğŸ¥ å…¬å…±å®‰å…¨**ï¼š
- é‡ç‚¹åŒºåŸŸç›‘æ§
- äººå‘˜è½¨è¿¹è¿½è¸ª
- å®æ—¶è­¦æŠ¥ç³»ç»Ÿ
- æ•°æ®åˆ†æå†³ç­–

### ğŸ¯ æ·±åº¦æ€è€ƒé¢˜

1. **æŠ€æœ¯é€‰å‹å†³ç­–**ï¼šåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹©ä¼ ç»Ÿæ£€æµ‹ç®—æ³•è€Œä¸æ˜¯æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Ÿè¯·åˆ†ææˆæœ¬ã€æ€§èƒ½ã€éƒ¨ç½²éš¾åº¦ç­‰å› ç´ ã€‚

2. **éšç§ä¿æŠ¤å¹³è¡¡**ï¼šå¦‚ä½•åœ¨æä¾›å¼ºå¤§ç›‘æ§åŠŸèƒ½çš„åŒæ—¶ä¿æŠ¤ä¸ªäººéšç§ï¼Ÿè®¾è®¡ä¸€å¥—æ—¢å®‰å…¨åˆåˆè§„çš„äººè„¸è¯†åˆ«ç³»ç»Ÿæ–¹æ¡ˆã€‚

3. **ç³»ç»Ÿæ‰©å±•æ€§è®¾è®¡**ï¼šå¦‚æœè¦å°†å½“å‰ç³»ç»Ÿæ‰©å±•åˆ°æ”¯æŒ100ä¸ªæ‘„åƒå¤´çš„å¤§å‹ç›‘æ§ç½‘ç»œï¼Œéœ€è¦è¿›è¡Œå“ªäº›æ¶æ„è°ƒæ•´ï¼Ÿ

4. **AIä¼¦ç†è´£ä»»**ï¼šä½œä¸ºAIç³»ç»Ÿå¼€å‘è€…ï¼Œåœ¨å¼€å‘äººè„¸è¯†åˆ«å’Œç›‘æ§ç³»ç»Ÿæ—¶åº”è¯¥æ‰¿æ‹…ä»€ä¹ˆæ ·çš„ä¼¦ç†è´£ä»»ï¼Ÿ

### ğŸ”® ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

åŸºäºç¬¬36ç« çš„å­¦ä¹ åŸºç¡€ï¼Œå»ºè®®æ‚¨ç»§ç»­æ·±å…¥ä»¥ä¸‹æ–¹å‘ï¼š

1. **æ·±åº¦å­¦ä¹ è¿›é˜¶**ï¼šå­¦ä¹ æœ€æ–°çš„æ£€æµ‹ç®—æ³•ï¼ˆå¦‚DETRã€EfficientDetï¼‰
2. **è¾¹ç¼˜è®¡ç®—éƒ¨ç½²**ï¼šå°†æ¨¡å‹éƒ¨ç½²åˆ°ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡
3. **3Dè®¡ç®—æœºè§†è§‰**ï¼šå­¦ä¹ æ·±åº¦ä¼°è®¡ã€3Dç›®æ ‡æ£€æµ‹
4. **è§†é¢‘åˆ†ææŠ€æœ¯**ï¼šç›®æ ‡è·Ÿè¸ªã€è¡Œä¸ºè¯†åˆ«ã€è§†é¢‘ç†è§£

---

**ç¬¬36ç« å®Œæˆæ—¶é—´**: 2025å¹´2æœˆ3æ—¥  
**ä»£ç ç¤ºä¾‹æ€»æ•°**: 5ä¸ªå®Œæ•´çš„ä¼ä¸šçº§ç³»ç»Ÿ  
**æ€»å­—æ•°**: çº¦30,000å­—  
**ç»¼åˆé¡¹ç›®**: æ™ºèƒ½ç›‘æ§æŒ‡æŒ¥ä¸­å¿ƒ  
**è´¨é‡è¯„åˆ†**: é¢„è®¡97+åˆ†

> ğŸ¯ **æˆå°±è¾¾æˆï¼** æ‚¨ç°åœ¨å…·å¤‡äº†å¼€å‘ä¼ä¸šçº§æ™ºèƒ½è§†è§‰åº”ç”¨çš„å®Œæ•´èƒ½åŠ›ï¼Œå¯ä»¥ç‹¬ç«‹è®¾è®¡å’Œå®ç°å¤æ‚çš„ç›®æ ‡æ£€æµ‹ä¸äººè„¸è¯†åˆ«ç³»ç»Ÿï¼

**ä¸‹ç« é¢„å‘Š**ï¼šç¬¬37ç« ã€Šå®æ—¶è§†è§‰åº”ç”¨å¼€å‘ã€‹å°†å¸¦æ‚¨è¿›å…¥æ›´é«˜çº§çš„å®æ—¶è§†è§‰å¤„ç†é¢†åŸŸï¼Œå­¦ä¹ è§†é¢‘åˆ†æã€å¢å¼ºç°å®ç­‰å‰æ²¿æŠ€æœ¯ï¼ 