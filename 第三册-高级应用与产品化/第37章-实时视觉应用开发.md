# ç¬¬37ç« ï¼šå®æ—¶è§†è§‰åº”ç”¨å¼€å‘

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- **è§†é¢‘å¤„ç†åŸç†**: æ·±å…¥ç†è§£è§†é¢‘æ•°æ®ç»“æ„ã€ç¼–è§£ç æŠ€æœ¯ã€å¸§ç‡æ§åˆ¶ç­‰æ ¸å¿ƒæ¦‚å¿µ
- **å®æ—¶ä¼˜åŒ–æŠ€æœ¯**: æŒæ¡å¤šçº¿ç¨‹å¤„ç†ã€GPUåŠ é€Ÿã€å†…å­˜ä¼˜åŒ–ç­‰æ€§èƒ½æå‡ç­–ç•¥  
- **å¢å¼ºç°å®æŠ€æœ¯**: ç†è§£ARæŠ€æœ¯åŸç†ã€æ‘„åƒå¤´æ ‡å®šã€3Dæ¸²æŸ“ã€è™šå®èåˆç­‰å…³é”®æŠ€æœ¯
- **è®¡ç®—æœºè§†è§‰åº”ç”¨**: ç»¼åˆè¿ç”¨ç›®æ ‡æ£€æµ‹ã€å§¿æ€ä¼°è®¡ã€å®æ—¶æ¸²æŸ“ç­‰æŠ€æœ¯

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡
- **è§†é¢‘å¤„ç†å¼€å‘**: èƒ½å¤Ÿç‹¬ç«‹å¼€å‘è§†é¢‘è¯»å†™ã€å¸§å·®æ£€æµ‹ã€ç›®æ ‡è·Ÿè¸ªç­‰åŠŸèƒ½
- **æ€§èƒ½ä¼˜åŒ–å®è·µ**: å…·å¤‡å¤šçº¿ç¨‹ç¼–ç¨‹ã€GPUç¼–ç¨‹ã€ç³»ç»Ÿè°ƒä¼˜çš„å®æˆ˜èƒ½åŠ›
- **ARåº”ç”¨å¼€å‘**: æŒæ¡æ‘„åƒå¤´æ ‡å®šã€3Dæ¸²æŸ“ã€è™šå®èåˆçš„å®Œæ•´å¼€å‘æµç¨‹
- **ä¼ä¸šçº§é¡¹ç›®**: èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„ARè¯•è¡£ç³»ç»Ÿï¼Œå…·å¤‡å•†ä¸šåŒ–éƒ¨ç½²èƒ½åŠ›

### ğŸ’¡ ç´ å…»ç›®æ ‡  
- **åˆ›æ–°æ€ç»´**: åŸ¹å…»å¯¹æ–°å…´æŠ€æœ¯çš„æ•æ„Ÿåº¦å’Œåˆ›æ–°åº”ç”¨èƒ½åŠ›
- **å·¥ç¨‹æ€ç»´**: å»ºç«‹ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–å’Œæ¶æ„è®¾è®¡æ€ç»´
- **ç”¨æˆ·ä½“éªŒæ„è¯†**: æ³¨é‡å®æ—¶äº¤äº’å’Œç”¨æˆ·ä½“éªŒçš„è®¾è®¡ç†å¿µ
- **æŠ€æœ¯å‰ç»**: äº†è§£AR/VRæŠ€æœ¯å‘å±•è¶‹åŠ¿ï¼Œå…·å¤‡æŠ€æœ¯å‰ç»æ€§

---

## ğŸŒŸ ç« èŠ‚å¯¼å…¥ï¼šèµ°è¿›å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒ

äº²çˆ±çš„æœ‹å‹ä»¬ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„**å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒ**ï¼è¿™æ˜¯ä¸€ä¸ªå……æ»¡æœªæ¥ç§‘æŠ€æ„Ÿçš„æ™ºèƒ½åŒ–å·¥å‚ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§è¯è§†é¢‘æ•°æ®å¦‚ä½•åœ¨æ¯«ç§’çº§çš„æ—¶é—´å†…å®Œæˆä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´å¤„ç†æµç¨‹ã€‚

### ğŸ¬ å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒå…¨æ™¯

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£ç«™åœ¨ä¸€ä¸ªç°ä»£åŒ–çš„ç§‘æŠ€å›­åŒºé—¨å£ï¼Œçœ¼å‰æ˜¯å››åº§é£æ ¼è¿¥å¼‚ä½†åˆç´§å¯†ç›¸è¿çš„å»ºç­‘ç¾¤ï¼š

#### ğŸ¥ è§†é¢‘æµåª’ä½“å·¥å‚
è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€ç«™ï¼Œä¸€åº§ç°ä»£åŒ–çš„**è§†é¢‘å¤„ç†æµæ°´çº¿å·¥å‚**ã€‚åœ¨è¿™é‡Œï¼š
- **ç”Ÿäº§è½¦é—´**é‡Œï¼Œå·¥ç¨‹å¸ˆä»¬æ­£åœ¨è°ƒè¯•è§†é¢‘è¯»å–ä¸å†™å…¥çš„æ ‡å‡†åŒ–ä½œä¸šæµç¨‹
- **è´¨æ£€éƒ¨é—¨**çš„ä¸“å®¶ä»¬è¿ç”¨å¸§é—´å·®åˆ†ç®—æ³•ï¼Œç²¾ç¡®ç›‘æµ‹æ¯ä¸€å¸§ç”»é¢çš„åŠ¨æ€å˜åŒ–  
- **è¿½è¸ªå°ç»„**å¦‚åŒä¸“ä¸šçš„ä¾¦æ¢å›¢é˜Ÿï¼Œè¿ç”¨å…ˆè¿›çš„ç®—æ³•è¿½è¸ªç”»é¢ä¸­æ¯ä¸€ä¸ªè¿åŠ¨ç›®æ ‡

#### âš¡ é«˜é€Ÿå¤„ç†åŠ å·¥å‚  
è¿™åº§å»ºç­‘é—ªçƒç€è“è‰²çš„å…‰èŠ’ï¼Œè±¡å¾ç€**é«˜æ•ˆèƒ½çš„å¹¶è¡Œå¤„ç†åˆ¶é€ å·¥å‚**ï¼š
- **å¤šçº¿ç¨‹è½¦é—´**é‡Œï¼Œæ•°åä¸ªå·¥ä½œç«™åŒæ—¶è¿è½¬ï¼Œå±•ç¤ºç€å¹¶è¡Œè§†é¢‘å¤„ç†çš„ååŒä½œä¸šæ¨¡å¼
- **GPUåŠ é€Ÿå¼•æ“**å¦‚åŒä¸€å°å·¨å¤§çš„è¶…çº§è®¡ç®—æœºï¼Œä¸“é—¨è´Ÿè´£å›¾å½¢å¤„ç†çš„ç¡¬ä»¶åŠ é€Ÿ
- **æ€§èƒ½è°ƒä¼˜ä¸­å¿ƒ**æ±‡èšäº†ç³»ç»Ÿä¼˜åŒ–çš„ä¸“ä¸šæŠ€æœ¯å›¢é˜Ÿï¼ŒæŒç»­ç›‘æ§å’Œæ”¹è¿›ç³»ç»Ÿæ€§èƒ½

#### ğŸŒŸ è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠ
è¿™æ˜¯ä¸€åº§å……æ»¡è‰ºæœ¯æ°”æ¯çš„**è™šå®èåˆåˆ›æ„è®¾è®¡å·¥ä½œå®¤**ï¼š
- **æ‘„åƒå¤´æ ¡å‡†å®¤**å¦‚åŒç²¾å¯†çš„å®éªŒå®¤ï¼Œå·¥ç¨‹å¸ˆä»¬åœ¨è¿™é‡Œè¿›è¡Œè®¾å¤‡æ ‡å®š
- **3Då»ºæ¨¡éƒ¨**é‡Œï¼Œè®¾è®¡å¸ˆä»¬å°†ç°å®ä¸–ç•Œçš„ç‰©ä½“è½¬åŒ–ä¸ºç²¾ç¾çš„æ•°å­—æ¨¡å‹
- **è™šå®èåˆä¸­å¿ƒ**æ˜¯æ•´ä¸ªå·¥åŠçš„æ ¸å¿ƒï¼Œåœ¨è¿™é‡Œç°å®ä¸è™šæ‹Ÿå®ç°äº†å®Œç¾çš„æ— ç¼å¯¹æ¥

#### ğŸ‘— ARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†
æœ€ä»¤äººå…´å¥‹çš„æ˜¯è¿™åº§**æœªæ¥æ„Ÿåè¶³çš„æ™ºèƒ½è¯•è¡£ä½“éªŒä¸­å¿ƒ**ï¼š
- **å§¿æ€è¯†åˆ«ç³»ç»Ÿ**å¦‚åŒæ™ºèƒ½æœºå™¨äººï¼Œèƒ½å¤Ÿç²¾ç¡®åˆ†æäººä½“çš„æ¯ä¸€ä¸ªåŠ¨ä½œ
- **è™šæ‹Ÿæœè£…åº“**å±•ç¤ºç€æ•°ä»¥åƒè®¡çš„æ•°å­—åŒ–æœè£…ï¼Œç­‰å¾…ç”¨æˆ·çš„é€‰æ‹©å’Œè¯•ç©¿
- **å®æ—¶æ¸²æŸ“å¼•æ“**æ˜¯æ•´ä¸ªä½“éªŒé¦†çš„å¤§è„‘ï¼Œè´Ÿè´£ç”Ÿæˆé«˜è´¨é‡çš„è§†è§‰æ•ˆæœ

### ğŸš€ æŠ€æœ¯é©å‘½çš„è§è¯è€…

åœ¨è¿™ä¸ªå®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒï¼Œæˆ‘ä»¬å°†è§è¯è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„ä¸‰å¤§é©å‘½ï¼š

#### ğŸ“¹ è§†é¢‘å¤„ç†é©å‘½
ä»ä¼ ç»Ÿçš„é™æ€å›¾åƒå¤„ç†ï¼Œåˆ°åŠ¨æ€çš„è§†é¢‘æµå¤„ç†ï¼Œæˆ‘ä»¬å°†æŒæ¡ï¼š
- è§†é¢‘æ•°æ®çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
- å®æ—¶çš„è¿åŠ¨æ£€æµ‹å’Œç›®æ ‡è·Ÿè¸ªæŠ€æœ¯
- é«˜æ•ˆçš„è§†é¢‘ç¼–è§£ç å’Œæ ¼å¼è½¬æ¢æ–¹æ³•

#### âš¡ æ€§èƒ½ä¼˜åŒ–é©å‘½  
ä»å•çº¿ç¨‹çš„ä¸²è¡Œå¤„ç†ï¼Œåˆ°å¤šçº¿ç¨‹+GPUçš„å¹¶è¡Œå¤„ç†ï¼Œæˆ‘ä»¬å°†å®ç°ï¼š
- å¤„ç†é€Ÿåº¦æå‡10-100å€çš„æ€§èƒ½é£è·ƒ
- èµ„æºåˆ©ç”¨ç‡çš„æ˜¾è‘—æé«˜
- å®æ—¶å¤„ç†èƒ½åŠ›çš„è´¨çš„çªç ´

#### ğŸŒŸ è™šå®èåˆé©å‘½
ä»ç°å®ä¸–ç•Œåˆ°è™šæ‹Ÿä¸–ç•Œï¼Œå†åˆ°è™šå®èåˆçš„å¢å¼ºç°å®ï¼Œæˆ‘ä»¬å°†åˆ›é€ ï¼š
- æ²‰æµ¸å¼çš„ç”¨æˆ·ä½“éªŒ
- è¶…è¶Šç°å®çš„äº¤äº’å¯èƒ½
- å•†ä¸šåŒ–çš„ARåº”ç”¨è§£å†³æ–¹æ¡ˆ

### ğŸ¯ å­¦ä»¥è‡´ç”¨çš„ä¼ä¸šçº§é¡¹ç›®

åœ¨æœ¬ç« çš„æœ€åï¼Œæˆ‘ä»¬å°†ç»¼åˆè¿ç”¨æ‰€å­¦çš„æ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„**ARè¯•è¡£æ™ºèƒ½ä½“éªŒç³»ç»Ÿ**ã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå­¦ä¹ é¡¹ç›®ï¼Œæ›´æ˜¯ä¸€ä¸ªå…·å¤‡å®é™…å•†ä¸šéƒ¨ç½²ä»·å€¼çš„ä¼ä¸šçº§åº”ç”¨ï¼š

- **ç”µå•†å¹³å°**å¯ä»¥é›†æˆè¿™ä¸ªç³»ç»Ÿï¼Œè®©ç”¨æˆ·åœ¨çº¿è¯•è¡£ï¼Œæ˜¾è‘—æå‡è´­ç‰©ä½“éªŒ
- **æœè£…åº—é“º**å¯ä»¥éƒ¨ç½²è¿™ä¸ªç³»ç»Ÿï¼Œæ‰“é€ æ•°å­—åŒ–çš„è¯•è¡£ä½“éªŒï¼Œå¸å¼•æ›´å¤šé¡¾å®¢
- **å“ç‰Œæ–¹**å¯ä»¥åˆ©ç”¨è¿™ä¸ªç³»ç»Ÿè¿›è¡Œåˆ›æ–°çš„è¥é”€å’Œå“ç‰Œå±•ç¤º
- **æŠ€æœ¯æœåŠ¡å•†**å¯ä»¥åŸºäºè¿™ä¸ªç³»ç»Ÿä¸ºå®¢æˆ·æä¾›å®šåˆ¶åŒ–çš„ARè§£å†³æ–¹æ¡ˆ

### ğŸ”¥ å‡†å¤‡å¥½äº†å—ï¼Ÿ

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æˆ´ä¸Šå®‰å…¨å¸½ï¼Œç©¿ä¸Šå·¥ä½œæœï¼Œä¸€èµ·èµ°è¿›è¿™ä¸ªå……æ»¡ç§‘æŠ€é­…åŠ›çš„**å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒ**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ä»…è¦å­¦ä¹ æœ€å‰æ²¿çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œæ›´è¦å°†è¿™äº›æŠ€æœ¯è½¬åŒ–ä¸ºçœŸæ­£æœ‰ä»·å€¼çš„å•†ä¸šåº”ç”¨ï¼

å‡†å¤‡å¥½è¿æ¥è¿™åœºæŠ€æœ¯é©å‘½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹è¿™æ¿€åŠ¨äººå¿ƒçš„å­¦ä¹ ä¹‹æ—…ï¼

---

## 37.1 è§†é¢‘æµåª’ä½“å·¥å‚ï¼šè§†é¢‘å¤„ç†æŠ€æœ¯

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒçš„ç¬¬ä¸€ç«™â€”â€”**è§†é¢‘æµåª’ä½“å·¥å‚**ï¼è¿™åº§ç°ä»£åŒ–çš„å·¥å‚ä¸“é—¨è´Ÿè´£è§†é¢‘æ•°æ®çš„é‡‡é›†ã€å¤„ç†å’Œè¾“å‡ºã€‚åœ¨è¿™é‡Œï¼Œæ¯ä¸€å¸§å›¾åƒéƒ½åƒæ˜¯æµæ°´çº¿ä¸Šçš„äº§å“ï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„å·¥åºï¼Œæœ€ç»ˆå˜æˆé«˜è´¨é‡çš„è§†é¢‘è¾“å‡ºã€‚

### ğŸ­ å·¥å‚è½¦é—´å¸ƒå±€

#### ğŸ¥ ç”Ÿäº§è½¦é—´ï¼šè§†é¢‘æ•°æ®çš„ç”Ÿäº§çº¿
åœ¨è¿™ä¸ªè½¦é—´é‡Œï¼Œæˆ‘ä»¬çš„**è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ**è´Ÿè´£ï¼š
- **åŸæ–™é‡‡è´­**ï¼šä»å„ç§è§†é¢‘æºè·å–åŸå§‹æ•°æ®ï¼ˆæ‘„åƒå¤´ã€æ–‡ä»¶ã€ç½‘ç»œæµï¼‰
- **è´¨é‡æ£€éªŒ**ï¼šæ£€æŸ¥è§†é¢‘æ ¼å¼ã€åˆ†è¾¨ç‡ã€å¸§ç‡ç­‰å…³é”®å‚æ•°
- **æ ‡å‡†åŒ–å¤„ç†**ï¼šç»Ÿä¸€è§†é¢‘æ ¼å¼ï¼Œç¡®ä¿åç»­å¤„ç†çš„å…¼å®¹æ€§
- **æˆå“åŒ…è£…**ï¼šå°†å¤„ç†åçš„è§†é¢‘è¾“å‡ºä¸ºå„ç§æ ‡å‡†æ ¼å¼

#### ğŸ” è´¨æ£€éƒ¨é—¨ï¼šå¸§é—´å˜åŒ–çš„ç²¾å¯†ç›‘æµ‹
è¿™é‡Œçš„**å¸§é—´å·®åˆ†æ£€æµ‹å™¨**å¦‚åŒæœ€æ•é”çš„è´¨æ£€ä¸“å®¶ï¼š
- **åŸºå‡†å»ºç«‹**ï¼šå»ºç«‹èƒŒæ™¯æ¨¡å‹ï¼Œä½œä¸ºå˜åŒ–æ£€æµ‹çš„åŸºå‡†
- **å˜åŒ–ç›‘æµ‹**ï¼šå®æ—¶æ£€æµ‹æ¯ä¸€å¸§çš„å˜åŒ–ï¼Œè¯†åˆ«è¿åŠ¨åŒºåŸŸ
- **å™ªå£°è¿‡æ»¤**ï¼šè¿‡æ»¤ç¯å¢ƒå™ªå£°å’Œå¾®å°æŠ–åŠ¨ï¼Œç¡®ä¿æ£€æµ‹ç²¾åº¦
- **æŠ¥å‘Šç”Ÿæˆ**ï¼šç”Ÿæˆè¯¦ç»†çš„å˜åŒ–åˆ†ææŠ¥å‘Š

#### ğŸ¯ è¿½è¸ªå°ç»„ï¼šç›®æ ‡çš„ä¸“ä¸šè¿½è¸ªå›¢é˜Ÿ
è¿™æ”¯ç²¾è‹±å›¢é˜Ÿè¿ç”¨**å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ**ï¼š
- **ç›®æ ‡è¯†åˆ«**ï¼šåœ¨å¤æ‚åœºæ™¯ä¸­å‡†ç¡®è¯†åˆ«éœ€è¦è¿½è¸ªçš„ç›®æ ‡
- **è½¨è¿¹é¢„æµ‹**ï¼šåŸºäºå¡å°”æ›¼æ»¤æ³¢ç®—æ³•é¢„æµ‹ç›®æ ‡çš„è¿åŠ¨è½¨è¿¹
- **èº«ä»½å…³è”**ï¼šé€šè¿‡åŒˆç‰™åˆ©ç®—æ³•è§£å†³ç›®æ ‡å…³è”é—®é¢˜
- **æŒç»­è·Ÿè¸ª**ï¼šåœ¨ç›®æ ‡é®æŒ¡ã€å˜å½¢ç­‰æƒ…å†µä¸‹ä¿æŒç¨³å®šè·Ÿè¸ª

#### ğŸ”§ å·¥ç¨‹éƒ¨é—¨ï¼šæ™ºèƒ½åŒ–æµæ°´çº¿è®¾è®¡
**è§†é¢‘æµå¤„ç†ç®¡é“**æ˜¯æ•´ä¸ªå·¥å‚çš„ç¥ç»ç³»ç»Ÿï¼š
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªå¤„ç†ç¯èŠ‚éƒ½æ˜¯ç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—
- **é…ç½®ç®¡ç†**ï¼šæ”¯æŒçµæ´»çš„å‚æ•°é…ç½®å’Œæµç¨‹å®šåˆ¶
- **æ’ä»¶æ¶æ„**ï¼šå¯ä»¥è½»æ¾æ·»åŠ æ–°çš„å¤„ç†åŠŸèƒ½
- **ç›‘æ§é¢„è­¦**ï¼šå®æ—¶ç›‘æ§å¤„ç†çŠ¶æ€ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜

### ğŸ’» æŠ€æœ¯æ·±åº¦è§£æ

#### è§†é¢‘æ•°æ®ç»“æ„ç†è§£
è§†é¢‘æœ¬è´¨ä¸Šæ˜¯ä¸€ç³»åˆ—è¿ç»­çš„å›¾åƒå¸§ï¼Œæ¯ä¸ªè§†é¢‘æ–‡ä»¶åŒ…å«ï¼š
```python
video_structure = {
    "å®¹å™¨æ ¼å¼": ["MP4", "AVI", "MOV", "MKV"],  # è§†é¢‘æ–‡ä»¶çš„å°è£…æ ¼å¼
    "è§†é¢‘ç¼–ç ": ["H.264", "H.265", "VP9", "AV1"],  # è§†é¢‘å‹ç¼©ç®—æ³•
    "éŸ³é¢‘ç¼–ç ": ["AAC", "MP3", "OGG"],  # éŸ³é¢‘å‹ç¼©ç®—æ³•
    "å…ƒæ•°æ®": {
        "åˆ†è¾¨ç‡": "1920x1080",  # è§†é¢‘åˆ†è¾¨ç‡
        "å¸§ç‡": "30fps",        # æ¯ç§’å¸§æ•°
        "æ¯”ç‰¹ç‡": "5Mbps",      # æ•°æ®ä¼ è¾“é€Ÿç‡
        "è‰²å½©ç©ºé—´": "YUV420",   # é¢œè‰²ç¼–ç æ–¹å¼
        "æ—¶é•¿": "120ç§’"         # è§†é¢‘æ€»æ—¶é•¿
    }
}
```

#### å¸§é—´å·®åˆ†çš„æ•°å­¦åŸç†
å¸§é—´å·®åˆ†æ£€æµ‹åŸºäºä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å‡è®¾ï¼šé™æ­¢çš„èƒŒæ™¯åœ¨è¿ç»­å¸§ä¹‹é—´å·®å¼‚å¾ˆå°ï¼Œè€Œè¿åŠ¨çš„ç›®æ ‡ä¼šäº§ç”Ÿæ˜æ˜¾çš„åƒç´ å€¼å˜åŒ–ã€‚

```python
# åŸºæœ¬çš„å¸§é—´å·®åˆ†å…¬å¼
diff = |I(t) - I(t-1)|  # å½“å‰å¸§ä¸å‰ä¸€å¸§çš„ç»å¯¹å·®å€¼
motion_mask = diff > threshold  # è®¾å®šé˜ˆå€¼åˆ†ç¦»è¿åŠ¨åŒºåŸŸ
```

### ğŸ¯ ç¤ºä¾‹1ï¼šè§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ

è®©æˆ‘ä»¬ä»æœ€åŸºç¡€ä½†æœ€é‡è¦çš„åŠŸèƒ½å¼€å§‹â€”â€”æ„å»ºä¸€ä¸ªä¸“ä¸šçº§çš„è§†é¢‘è¯»å†™å¤„ç†ç³»ç»Ÿã€‚è¿™ä¸ªç³»ç»Ÿå°†æˆä¸ºæˆ‘ä»¬æ•´ä¸ªè§†é¢‘å¤„ç†å·¥å‚çš„åŸºç¡€è®¾æ–½ã€‚

```python
"""
è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ - ä¸“ä¸šçº§è§†é¢‘å¤„ç†ç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šç§è§†é¢‘æ ¼å¼çš„è¯»å–å’Œå†™å…¥
2. æä¾›è¯¦ç»†çš„è§†é¢‘ä¿¡æ¯åˆ†æ
3. å®ç°è§†é¢‘æ ¼å¼è½¬æ¢å’Œå‚æ•°è°ƒæ•´
4. æ”¯æŒå®æ—¶è§†é¢‘æµå¤„ç†
"""

import cv2
import numpy as np
import threading
import queue
import time
from datetime import datetime
import os
import json
from pathlib import Path
import logging

class VideoIOCenter:
    """è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ - ä¼ä¸šçº§è§†é¢‘å¤„ç†æ ¸å¿ƒç±»"""
    
    def __init__(self, config_file=None):
        """
        åˆå§‹åŒ–è§†é¢‘å¤„ç†ä¸­å¿ƒ
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«å„ç§å¤„ç†å‚æ•°
        """
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self.setup_logging()
        
        # åŠ è½½é…ç½®
        self.config = self.load_config(config_file)
        
        # åˆå§‹åŒ–è§†é¢‘å¤„ç†å‚æ•°
        self.supported_formats = {
            'input': ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm'],
            'output': ['.mp4', '.avi', '.mov', '.mkv']
        }
        
        # ç¼–ç å™¨è®¾ç½®
        self.encoders = {
            '.mp4': cv2.VideoWriter_fourcc(*'mp4v'),
            '.avi': cv2.VideoWriter_fourcc(*'XVID'),
            '.mov': cv2.VideoWriter_fourcc(*'mp4v'),
            '.mkv': cv2.VideoWriter_fourcc(*'XVID')
        }
        
        # å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_frames': 0,
            'start_time': None,
            'processing_time': 0,
            'average_fps': 0
        }
        
        self.logger.info("è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒåˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('video_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('VideoIOCenter')
    
    def load_config(self, config_file):
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: é…ç½®å‚æ•°å­—å…¸
        """
        default_config = {
            'quality': {
                'output_resolution': None,  # Noneè¡¨ç¤ºä¿æŒåŸåˆ†è¾¨ç‡
                'output_fps': None,         # Noneè¡¨ç¤ºä¿æŒåŸå¸§ç‡
                'compression_level': 0.8    # å‹ç¼©è´¨é‡ 0-1
            },
            'processing': {
                'buffer_size': 30,          # ç¼“å†²åŒºå¤§å°
                'timeout': 30,              # å¤„ç†è¶…æ—¶æ—¶é—´(ç§’)
                'multi_threading': True     # æ˜¯å¦å¯ç”¨å¤šçº¿ç¨‹
            }
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
                    self.logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            except Exception as e:
                self.logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def get_video_info(self, video_path):
        """
        è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        try:
            # è·å–åŸºæœ¬ä¿¡æ¯
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(video_path)
            file_ext = Path(video_path).suffix.lower()
            
            video_info = {
                'file_path': video_path,
                'file_name': Path(video_path).name,
                'file_size_mb': round(file_size / (1024 * 1024), 2),
                'format': file_ext,
                'resolution': f"{width}x{height}",
                'width': width,
                'height': height,
                'fps': round(fps, 2),
                'frame_count': frame_count,
                'duration_seconds': round(duration, 2),
                'duration_formatted': self.format_duration(duration),
                'bitrate_kbps': round((file_size * 8) / (duration * 1000), 2) if duration > 0 else 0
            }
            
            self.logger.info(f"è§†é¢‘ä¿¡æ¯è·å–æˆåŠŸ: {video_info['file_name']}")
            return video_info
            
        finally:
            cap.release()
    
    def format_duration(self, seconds):
        """
        æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º
        
        Args:
            seconds: æ—¶é•¿(ç§’)
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ—¶é•¿å­—ç¬¦ä¸²
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"
    
    def convert_video(self, input_path, output_path, **kwargs):
        """
        è§†é¢‘æ ¼å¼è½¬æ¢å’Œå‚æ•°è°ƒæ•´
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            **kwargs: è½¬æ¢å‚æ•°
                - target_fps: ç›®æ ‡å¸§ç‡
                - target_resolution: ç›®æ ‡åˆ†è¾¨ç‡ (width, height)
                - quality: è¾“å‡ºè´¨é‡(0-1)
                - start_time: å¼€å§‹æ—¶é—´(ç§’)
                - end_time: ç»“æŸæ—¶é—´(ç§’)
        """
        # è·å–è¾“å…¥è§†é¢‘ä¿¡æ¯
        input_info = self.get_video_info(input_path)
        
        # è®¾ç½®è¾“å‡ºå‚æ•°
        target_fps = kwargs.get('target_fps', input_info['fps'])
        target_resolution = kwargs.get('target_resolution', (input_info['width'], input_info['height']))
        quality = kwargs.get('quality', self.config['quality']['compression_level'])
        start_time = kwargs.get('start_time', 0)
        end_time = kwargs.get('end_time', input_info['duration_seconds'])
        
        # æ£€æŸ¥è¾“å‡ºæ ¼å¼æ”¯æŒ
        output_ext = Path(output_path).suffix.lower()
        if output_ext not in self.supported_formats['output']:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_ext}")
        
        # æ‰“å¼€è¾“å…¥è§†é¢‘
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è¾“å…¥è§†é¢‘: {input_path}")
        
        try:
            # è®¾ç½®å¼€å§‹æ—¶é—´
            if start_time > 0:
                cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
            
            # è®¾ç½®è§†é¢‘å†™å…¥å™¨
            fourcc = self.encoders[output_ext]
            out = cv2.VideoWriter(
                output_path, 
                fourcc, 
                target_fps, 
                target_resolution
            )
            
            if not out.isOpened():
                raise ValueError(f"æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘: {output_path}")
            
            # åˆå§‹åŒ–å¤„ç†ç»Ÿè®¡
            self.stats['start_time'] = time.time()
            self.stats['processed_frames'] = 0
            
            self.logger.info(f"å¼€å§‹è§†é¢‘è½¬æ¢: {input_info['file_name']} -> {Path(output_path).name}")
            
            try:
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç»“æŸæ—¶é—´
                    current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
                    if current_time > end_time:
                        break
                    
                    # è°ƒæ•´å¸§å¤§å°
                    if (frame.shape[1], frame.shape[0]) != target_resolution:
                        frame = cv2.resize(frame, target_resolution, interpolation=cv2.INTER_AREA)
                    
                    # å†™å…¥å¸§
                    out.write(frame)
                    self.stats['processed_frames'] += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if self.stats['processed_frames'] % 30 == 0:
                        progress = (current_time - start_time) / (end_time - start_time) * 100
                        self.logger.info(f"è½¬æ¢è¿›åº¦: {progress:.1f}%")
                
                # è®¡ç®—å¤„ç†ç»Ÿè®¡
                self.stats['processing_time'] = time.time() - self.stats['start_time']
                self.stats['average_fps'] = self.stats['processed_frames'] / self.stats['processing_time']
                
                self.logger.info(f"è§†é¢‘è½¬æ¢å®Œæˆ!")
                self.logger.info(f"å¤„ç†å¸§æ•°: {self.stats['processed_frames']}")
                self.logger.info(f"å¤„ç†æ—¶é—´: {self.stats['processing_time']:.2f}ç§’")
                self.logger.info(f"å¹³å‡å¤„ç†å¸§ç‡: {self.stats['average_fps']:.2f} fps")
                
                # è·å–è¾“å‡ºè§†é¢‘ä¿¡æ¯
                output_info = self.get_video_info(output_path)
                return {
                    'input_info': input_info,
                    'output_info': output_info,
                    'processing_stats': self.stats.copy()
                }
                
            finally:
                out.release()
                
        finally:
            cap.release()
    
    def extract_frames(self, video_path, output_dir, frame_interval=1, max_frames=None):
        """
        ä»è§†é¢‘ä¸­æå–å¸§å›¾åƒ
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            frame_interval: å¸§é—´éš”ï¼ˆæ¯éš”å¤šå°‘å¸§æå–ä¸€å¸§ï¼‰
            max_frames: æœ€å¤§æå–å¸§æ•°
            
        Returns:
            list: æå–çš„å¸§æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = self.get_video_info(video_path)
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        extracted_frames = []
        frame_count = 0
        extracted_count = 0
        
        try:
            self.logger.info(f"å¼€å§‹æå–å¸§: {video_info['file_name']}")
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æŒ‰é—´éš”æå–å¸§
                if frame_count % frame_interval == 0:
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
                    frame_filename = f"frame_{extracted_count:06d}_t{timestamp:.3f}.jpg"
                    frame_path = output_dir / frame_filename
                    
                    # ä¿å­˜å¸§
                    cv2.imwrite(str(frame_path), frame)
                    extracted_frames.append(str(frame_path))
                    extracted_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æå–æ•°é‡
                    if max_frames and extracted_count >= max_frames:
                        break
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if extracted_count % 10 == 0:
                        progress = frame_count / video_info['frame_count'] * 100
                        self.logger.info(f"æå–è¿›åº¦: {progress:.1f}% ({extracted_count} å¸§)")
                
                frame_count += 1
            
            self.logger.info(f"å¸§æå–å®Œæˆ! å…±æå– {extracted_count} å¸§")
            return extracted_frames
            
        finally:
            cap.release()
    
    def create_video_from_frames(self, frame_dir, output_path, fps=30, frame_pattern="*.jpg"):
        """
        ä»å¸§å›¾åƒåˆ›å»ºè§†é¢‘
        
        Args:
            frame_dir: å¸§å›¾åƒç›®å½•
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: è¾“å‡ºè§†é¢‘å¸§ç‡
            frame_pattern: å¸§æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            dict: è§†é¢‘åˆ›å»ºä¿¡æ¯
        """
        frame_dir = Path(frame_dir)
        
        # è·å–æ‰€æœ‰å¸§æ–‡ä»¶
        frame_files = sorted(list(frame_dir.glob(frame_pattern)))
        
        if not frame_files:
            raise ValueError(f"åœ¨ç›®å½• {frame_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„å¸§æ–‡ä»¶: {frame_pattern}")
        
        # è¯»å–ç¬¬ä¸€å¸§è·å–å°ºå¯¸ä¿¡æ¯
        first_frame = cv2.imread(str(frame_files[0]))
        if first_frame is None:
            raise ValueError(f"æ— æ³•è¯»å–ç¬¬ä¸€å¸§: {frame_files[0]}")
        
        height, width = first_frame.shape[:2]
        
        # è®¾ç½®è¾“å‡ºæ ¼å¼
        output_ext = Path(output_path).suffix.lower()
        fourcc = self.encoders.get(output_ext, cv2.VideoWriter_fourcc(*'mp4v'))
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            raise ValueError(f"æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘: {output_path}")
        
        try:
            self.logger.info(f"å¼€å§‹ä» {len(frame_files)} å¸§åˆ›å»ºè§†é¢‘")
            
            for i, frame_file in enumerate(frame_files):
                frame = cv2.imread(str(frame_file))
                if frame is None:
                    self.logger.warning(f"è·³è¿‡æ— æ³•è¯»å–çš„å¸§: {frame_file}")
                    continue
                
                # ç¡®ä¿å¸§å°ºå¯¸ä¸€è‡´
                if frame.shape[:2] != (height, width):
                    frame = cv2.resize(frame, (width, height))
                
                out.write(frame)
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 30 == 0:
                    progress = (i + 1) / len(frame_files) * 100
                    self.logger.info(f"åˆ›å»ºè¿›åº¦: {progress:.1f}%")
            
            self.logger.info("è§†é¢‘åˆ›å»ºå®Œæˆ!")
            
            # è·å–è¾“å‡ºè§†é¢‘ä¿¡æ¯
            output_info = self.get_video_info(output_path)
            return {
                'frame_count': len(frame_files),
                'output_info': output_info
            }
            
        finally:
            out.release()

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def demo_video_io_center():
    """è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒæ¼”ç¤º"""
    print("ğŸ¥ è§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºè§†é¢‘å¤„ç†ä¸­å¿ƒ
    video_center = VideoIOCenter()
    
    # æ¼”ç¤º1: è§†é¢‘ä¿¡æ¯è·å–
    print("\nğŸ“Š æ¼”ç¤º1: è§†é¢‘ä¿¡æ¯åˆ†æ")
    try:
        # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„è§†é¢‘æ–‡ä»¶è·¯å¾„
        test_video = "test_video.mp4"  # è¯·æ›¿æ¢ä¸ºå®é™…è§†é¢‘æ–‡ä»¶
        
        # å¦‚æœæ²¡æœ‰æµ‹è¯•è§†é¢‘ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è§†é¢‘
        if not os.path.exists(test_video):
            print("åˆ›å»ºæµ‹è¯•è§†é¢‘...")
            create_test_video(test_video)
        
        video_info = video_center.get_video_info(test_video)
        
        print(f"æ–‡ä»¶å: {video_info['file_name']}")
        print(f"æ–‡ä»¶å¤§å°: {video_info['file_size_mb']} MB")
        print(f"åˆ†è¾¨ç‡: {video_info['resolution']}")
        print(f"å¸§ç‡: {video_info['fps']} fps")
        print(f"æ€»å¸§æ•°: {video_info['frame_count']}")
        print(f"æ—¶é•¿: {video_info['duration_formatted']}")
        print(f"æ¯”ç‰¹ç‡: {video_info['bitrate_kbps']} kbps")
        
    except Exception as e:
        print(f"è§†é¢‘ä¿¡æ¯è·å–å¤±è´¥: {e}")
    
    # æ¼”ç¤º2: è§†é¢‘æ ¼å¼è½¬æ¢
    print("\nğŸ”„ æ¼”ç¤º2: è§†é¢‘æ ¼å¼è½¬æ¢")
    try:
        if os.path.exists(test_video):
            output_video = "converted_video.avi"
            
            conversion_result = video_center.convert_video(
                test_video,
                output_video,
                target_fps=25,
                target_resolution=(640, 480),
                quality=0.8
            )
            
            print("è½¬æ¢å®Œæˆ!")
            print(f"è¾“å…¥è§†é¢‘: {conversion_result['input_info']['resolution']} @ {conversion_result['input_info']['fps']}fps")
            print(f"è¾“å‡ºè§†é¢‘: {conversion_result['output_info']['resolution']} @ {conversion_result['output_info']['fps']}fps")
            print(f"å¤„ç†ç»Ÿè®¡: {conversion_result['processing_stats']['processed_frames']} å¸§, "
                  f"{conversion_result['processing_stats']['average_fps']:.2f} fps")
            
    except Exception as e:
        print(f"è§†é¢‘è½¬æ¢å¤±è´¥: {e}")
    
    # æ¼”ç¤º3: å¸§æå–
    print("\nğŸ–¼ï¸  æ¼”ç¤º3: è§†é¢‘å¸§æå–")
    try:
        if os.path.exists(test_video):
            frame_output_dir = "extracted_frames"
            
            extracted_frames = video_center.extract_frames(
                test_video,
                frame_output_dir,
                frame_interval=10,  # æ¯10å¸§æå–ä¸€å¸§
                max_frames=20       # æœ€å¤šæå–20å¸§
            )
            
            print(f"æˆåŠŸæå– {len(extracted_frames)} å¸§")
            print(f"è¾“å‡ºç›®å½•: {frame_output_dir}")
            
    except Exception as e:
        print(f"å¸§æå–å¤±è´¥: {e}")

def create_test_video(output_path, duration=5, fps=30):
    """
    åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è§†é¢‘
    
    Args:
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        duration: è§†é¢‘æ—¶é•¿(ç§’)
        fps: å¸§ç‡
    """
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (640, 480))
    
    total_frames = duration * fps
    
    for i in range(total_frames):
        # åˆ›å»ºå½©è‰²èƒŒæ™¯
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        
        # æ·»åŠ æ¸å˜èƒŒæ™¯
        frame[:, :, 0] = (i * 255 // total_frames) % 255  # è“è‰²é€šé“
        frame[:, :, 1] = 100  # ç»¿è‰²é€šé“
        frame[:, :, 2] = (255 - i * 255 // total_frames) % 255  # çº¢è‰²é€šé“
        
        # æ·»åŠ ç§»åŠ¨çš„åœ†åœˆ
        center_x = int(50 + (540 * i / total_frames))
        center_y = 240
        cv2.circle(frame, (center_x, center_y), 30, (255, 255, 255), -1)
        
        # æ·»åŠ å¸§è®¡æ•°æ–‡æœ¬
        cv2.putText(frame, f"Frame: {i+1}/{total_frames}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = i / fps
        cv2.putText(frame, f"Time: {timestamp:.2f}s", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        out.write(frame)
    
    out.release()
    print(f"æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")

if __name__ == "__main__":
    demo_video_io_center()
```
---

```mermaid
flowchart TD
    A[è§†é¢‘è¾“å…¥] --> B[æ ¼å¼æ£€æŸ¥]
    B --> C{æ”¯æŒçš„æ ¼å¼?}
    C -->|æ˜¯| D[å‚æ•°è§£æ]
    C -->|å¦| E[æ ¼å¼è½¬æ¢]
    E --> D
    D --> F[è§†é¢‘è¯»å–]
    F --> G[å¸§å¤„ç†]
    G --> H[è´¨é‡è°ƒæ•´]
    H --> I[ç¼–ç è¾“å‡º]
    I --> J[è§†é¢‘æ–‡ä»¶]
    
    G --> K[å¸§æå–]
    K --> L[å›¾åƒæ–‡ä»¶]
    
    style A fill:#e3f2fd
    style J fill:#e8f5e8
    style L fill:#fff3e0
```

---

è¿™ä¸ªè§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒä¸ºæˆ‘ä»¬çš„æ•´ä¸ªè§†é¢‘å¤„ç†å·¥å‚å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚åœ¨ä¸‹ä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ„å»ºæ™ºèƒ½çš„å¸§é—´å·®åˆ†æ£€æµ‹å™¨ï¼Œå®ç°è¿åŠ¨ç›®æ ‡çš„ç²¾ç¡®è¯†åˆ«ï¼

---

```mermaid
graph TB
    A[å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒ] --> B[ğŸ¬ è§†é¢‘æµåª’ä½“å·¥å‚<br/>è§†é¢‘å¤„ç†æŠ€æœ¯]
    A --> C[âš¡ é«˜é€Ÿå¤„ç†åŠ å·¥å‚<br/>æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯] 
    A --> D[ğŸŒŸ è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠ<br/>å¢å¼ºç°å®åŸºç¡€]
    A --> E[ğŸ‘— ARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†<br/>ç»¼åˆé¡¹ç›®åº”ç”¨]
    
    B --> B1[è§†é¢‘è¯»å†™æ§åˆ¶]
    B --> B2[å¸§é—´å·®åˆ†æ£€æµ‹]
    B --> B3[å¤šç›®æ ‡è¿½è¸ª]
    B --> B4[æµå¤„ç†ç®¡é“]
    
    C --> C1[å¤šçº¿ç¨‹å¤„ç†]
    C --> C2[GPUç¡¬ä»¶åŠ é€Ÿ]
    C --> C3[æ€§èƒ½ç›‘æ§ä¼˜åŒ–]
    
    D --> D1[æ‘„åƒå¤´æ ‡å®š]
    D --> D2[3Dç‰©ä½“æ¸²æŸ“]
    D --> D3[è™šå®èåˆ]
    
    E --> E1[å§¿æ€æ£€æµ‹ç³»ç»Ÿ]
    E --> E2[ARè¯•è¡£å®Œæ•´ç³»ç»Ÿ]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
```

--- 

### ğŸ¯ ç¤ºä¾‹2ï¼šå¸§é—´å·®åˆ†æ£€æµ‹å™¨

ç°åœ¨è®©æˆ‘ä»¬è¿›å…¥è´¨æ£€éƒ¨é—¨ï¼Œæ„å»ºä¸€ä¸ªæ™ºèƒ½çš„**å¸§é—´å·®åˆ†æ£€æµ‹å™¨**ã€‚è¿™ä¸ªç³»ç»Ÿå°±åƒæ˜¯æœ€æ•é”çš„è´¨æ£€ä¸“å®¶ï¼Œèƒ½å¤Ÿç²¾ç¡®åœ°æ£€æµ‹å‡ºè§†é¢‘ä¸­çš„è¿åŠ¨å˜åŒ–ï¼Œä¸ºåç»­çš„ç›®æ ‡è·Ÿè¸ªå’Œè¡Œä¸ºåˆ†ææä¾›åŸºç¡€æ•°æ®ã€‚

```python
"""
å¸§é—´å·®åˆ†æ£€æµ‹å™¨ - æ™ºèƒ½è¿åŠ¨æ£€æµ‹ç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. å¤šç§èƒŒæ™¯å»ºæ¨¡ç®—æ³•æ”¯æŒ
2. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
3. å™ªå£°è¿‡æ»¤å’Œå½¢æ€å­¦å¤„ç†
4. è¿åŠ¨åŒºåŸŸåˆ†æå’Œç»Ÿè®¡
5. å®æ—¶æ€§èƒ½ç›‘æ§
"""

import cv2
import numpy as np
import time
from collections import deque
from dataclasses import dataclass
from typing import List, Tuple, Optional
import matplotlib.pyplot as plt
from datetime import datetime
import json

@dataclass
class MotionRegion:
    """è¿åŠ¨åŒºåŸŸæ•°æ®ç»“æ„"""
    x: int          # åŒºåŸŸå·¦ä¸Šè§’xåæ ‡
    y: int          # åŒºåŸŸå·¦ä¸Šè§’yåæ ‡
    width: int      # åŒºåŸŸå®½åº¦
    height: int     # åŒºåŸŸé«˜åº¦
    area: int       # åŒºåŸŸé¢ç§¯
    confidence: float  # ç½®ä¿¡åº¦
    timestamp: float   # æ—¶é—´æˆ³

class FrameDifferenceDetector:
    """å¸§é—´å·®åˆ†æ£€æµ‹å™¨ - ä¸“ä¸šè¿åŠ¨æ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–å¸§é—´å·®åˆ†æ£€æµ‹å™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        # é»˜è®¤é…ç½®
        self.config = {
            'detection': {
                'method': 'adaptive',      # æ£€æµ‹æ–¹æ³•: simple, adaptive, mog2, knn
                'threshold': 30,           # å·®åˆ†é˜ˆå€¼
                'min_area': 500,          # æœ€å°è¿åŠ¨åŒºåŸŸé¢ç§¯
                'max_area': 50000,        # æœ€å¤§è¿åŠ¨åŒºåŸŸé¢ç§¯
                'learning_rate': 0.01,    # èƒŒæ™¯å­¦ä¹ ç‡
                'adaptive_threshold': True # è‡ªé€‚åº”é˜ˆå€¼
            },
            'morphology': {
                'enable': True,           # æ˜¯å¦å¯ç”¨å½¢æ€å­¦å¤„ç†
                'kernel_size': (5, 5),    # å½¢æ€å­¦æ ¸å¤§å°
                'opening_iterations': 1,   # å¼€è¿ç®—è¿­ä»£æ¬¡æ•°
                'closing_iterations': 2    # é—­è¿ç®—è¿­ä»£æ¬¡æ•°
            },
            'noise_filter': {
                'enable': True,           # æ˜¯å¦å¯ç”¨å™ªå£°è¿‡æ»¤
                'gaussian_blur': (5, 5),  # é«˜æ–¯æ¨¡ç³Šæ ¸å¤§å°
                'bilateral_filter': True   # åŒè¾¹æ»¤æ³¢
            },
            'tracking': {
                'history_size': 10,       # å†å²å¸§æ•°é‡
                'motion_threshold': 0.02  # è¿åŠ¨é˜ˆå€¼ç™¾åˆ†æ¯”
            }
        }
        
        # æ›´æ–°é…ç½®
        if config:
            self._update_config(config)
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨ç»„ä»¶
        self.background_subtractor = None
        self.frame_history = deque(maxlen=self.config['tracking']['history_size'])
        self.motion_history = deque(maxlen=100)  # è¿åŠ¨å†å²è®°å½•
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_frames': 0,
            'motion_frames': 0,
            'total_motion_area': 0,
            'processing_times': deque(maxlen=100),
            'start_time': time.time()
        }
        
        # åˆå§‹åŒ–èƒŒæ™¯å»ºæ¨¡å™¨
        self._init_background_subtractor()
        
        print("ğŸ” å¸§é—´å·®åˆ†æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _update_config(self, new_config):
        """é€’å½’æ›´æ–°é…ç½®"""
        def update_dict(base_dict, update_dict):
            for key, value in update_dict.items():
                if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                    update_dict(base_dict[key], value)
                else:
                    base_dict[key] = value
        
        update_dict(self.config, new_config)
    
    def _init_background_subtractor(self):
        """åˆå§‹åŒ–èƒŒæ™¯å»ºæ¨¡å™¨"""
        method = self.config['detection']['method']
        
        if method == 'mog2':
            self.background_subtractor = cv2.createBackgroundSubtractorMOG2(
                detectShadows=True,
                varThreshold=16,
                history=500
            )
        elif method == 'knn':
            self.background_subtractor = cv2.createBackgroundSubtractorKNN(
                detectShadows=True,
                dist2Threshold=400,
                history=500
            )
        else:
            # ä½¿ç”¨ç®€å•çš„å¸§é—´å·®åˆ†æˆ–è‡ªé€‚åº”æ–¹æ³•
            self.background_subtractor = None
    
    def preprocess_frame(self, frame):
        """
        å¸§é¢„å¤„ç†
        
        Args:
            frame: è¾“å…¥å¸§
            
        Returns:
            processed_frame: é¢„å¤„ç†åçš„å¸§
        """
        processed = frame.copy()
        
        # å™ªå£°è¿‡æ»¤
        if self.config['noise_filter']['enable']:
            # é«˜æ–¯æ¨¡ç³Š
            if self.config['noise_filter']['gaussian_blur']:
                processed = cv2.GaussianBlur(
                    processed, 
                    self.config['noise_filter']['gaussian_blur'], 
                    0
                )
            
            # åŒè¾¹æ»¤æ³¢ï¼ˆä¿è¾¹å»å™ªï¼‰
            if self.config['noise_filter']['bilateral_filter']:
                processed = cv2.bilateralFilter(processed, 9, 75, 75)
        
        return processed
    
    def detect_motion(self, frame):
        """
        æ£€æµ‹è¿åŠ¨åŒºåŸŸ
        
        Args:
            frame: è¾“å…¥å¸§
            
        Returns:
            motion_mask: è¿åŠ¨æ©ç 
            motion_regions: è¿åŠ¨åŒºåŸŸåˆ—è¡¨
        """
        start_time = time.time()
        
        # é¢„å¤„ç†
        processed_frame = self.preprocess_frame(frame)
        gray_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)
        
        # æ·»åŠ åˆ°å†å²å¸§é˜Ÿåˆ—
        self.frame_history.append(gray_frame)
        
        # æ ¹æ®æ–¹æ³•è¿›è¡Œè¿åŠ¨æ£€æµ‹
        method = self.config['detection']['method']
        
        if method in ['mog2', 'knn'] and self.background_subtractor:
            # ä½¿ç”¨èƒŒæ™¯å»ºæ¨¡æ–¹æ³•
            motion_mask = self.background_subtractor.apply(
                gray_frame, 
                learningRate=self.config['detection']['learning_rate']
            )
        else:
            # ä½¿ç”¨å¸§é—´å·®åˆ†æ–¹æ³•
            motion_mask = self._frame_difference_detection(gray_frame)
        
        # å½¢æ€å­¦å¤„ç†
        if self.config['morphology']['enable']:
            motion_mask = self._morphological_processing(motion_mask)
        
        # æå–è¿åŠ¨åŒºåŸŸ
        motion_regions = self._extract_motion_regions(motion_mask, frame)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self._update_statistics(motion_mask, motion_regions, processing_time)
        
        return motion_mask, motion_regions
    
    def _frame_difference_detection(self, gray_frame):
        """
        å¸§é—´å·®åˆ†æ£€æµ‹
        
        Args:
            gray_frame: ç°åº¦å¸§
            
        Returns:
            motion_mask: è¿åŠ¨æ©ç 
        """
        if len(self.frame_history) < 2:
            return np.zeros(gray_frame.shape, dtype=np.uint8)
        
        method = self.config['detection']['method']
        
        if method == 'simple':
            # ç®€å•å¸§é—´å·®åˆ†
            prev_frame = self.frame_history[-2]
            diff = cv2.absdiff(gray_frame, prev_frame)
            
        elif method == 'adaptive':
            # è‡ªé€‚åº”å¸§é—´å·®åˆ†
            if len(self.frame_history) >= 3:
                # ä½¿ç”¨ä¸‰å¸§å·®åˆ†æé«˜ç¨³å®šæ€§
                diff1 = cv2.absdiff(self.frame_history[-1], self.frame_history[-2])
                diff2 = cv2.absdiff(self.frame_history[-2], self.frame_history[-3])
                diff = cv2.bitwise_and(diff1, diff2)
            else:
                prev_frame = self.frame_history[-2]
                diff = cv2.absdiff(gray_frame, prev_frame)
        
        # é˜ˆå€¼åŒ–
        threshold = self.config['detection']['threshold']
        
        if self.config['detection']['adaptive_threshold']:
            # è‡ªé€‚åº”é˜ˆå€¼
            motion_mask = cv2.adaptiveThreshold(
                diff, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 11, 2
            )
        else:
            # å›ºå®šé˜ˆå€¼
            _, motion_mask = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)
        
        return motion_mask
    
    def _morphological_processing(self, motion_mask):
        """
        å½¢æ€å­¦å¤„ç†
        
        Args:
            motion_mask: åŸå§‹è¿åŠ¨æ©ç 
            
        Returns:
            processed_mask: å¤„ç†åçš„æ©ç 
        """
        kernel_size = self.config['morphology']['kernel_size']
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, kernel_size)
        
        # å¼€è¿ç®—ï¼ˆå»é™¤å™ªå£°ï¼‰
        processed_mask = cv2.morphologyEx(
            motion_mask, cv2.MORPH_OPEN, kernel,
            iterations=self.config['morphology']['opening_iterations']
        )
        
        # é—­è¿ç®—ï¼ˆå¡«å……ç©ºæ´ï¼‰
        processed_mask = cv2.morphologyEx(
            processed_mask, cv2.MORPH_CLOSE, kernel,
            iterations=self.config['morphology']['closing_iterations']
        )
        
        return processed_mask
    
    def _extract_motion_regions(self, motion_mask, original_frame):
        """
        æå–è¿åŠ¨åŒºåŸŸ
        
        Args:
            motion_mask: è¿åŠ¨æ©ç 
            original_frame: åŸå§‹å¸§
            
        Returns:
            motion_regions: è¿åŠ¨åŒºåŸŸåˆ—è¡¨
        """
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(
            motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        
        motion_regions = []
        current_time = time.time()
        
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # é¢ç§¯è¿‡æ»¤
            if (area < self.config['detection']['min_area'] or 
                area > self.config['detection']['max_area']):
                continue
            
            # è·å–è¾¹ç•Œæ¡†
            x, y, w, h = cv2.boundingRect(contour)
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºé¢ç§¯å’Œå½¢çŠ¶ï¼‰
            confidence = min(1.0, area / self.config['detection']['max_area'])
            
            # åˆ›å»ºè¿åŠ¨åŒºåŸŸå¯¹è±¡
            motion_region = MotionRegion(
                x=x, y=y, width=w, height=h,
                area=int(area), confidence=confidence,
                timestamp=current_time
            )
            
            motion_regions.append(motion_region)
        
        return motion_regions
    
    def _update_statistics(self, motion_mask, motion_regions, processing_time):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_frames'] += 1
        self.stats['processing_times'].append(processing_time)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿åŠ¨
        motion_pixel_count = np.sum(motion_mask > 0)
        total_pixels = motion_mask.shape[0] * motion_mask.shape[1]
        motion_ratio = motion_pixel_count / total_pixels
        
        if motion_ratio > self.config['tracking']['motion_threshold']:
            self.stats['motion_frames'] += 1
        
        # è®°å½•è¿åŠ¨åŒºåŸŸé¢ç§¯
        total_area = sum(region.area for region in motion_regions)
        self.stats['total_motion_area'] += total_area
        
        # æ·»åŠ åˆ°è¿åŠ¨å†å²
        self.motion_history.append({
            'timestamp': time.time(),
            'motion_ratio': motion_ratio,
            'region_count': len(motion_regions),
            'total_area': total_area
        })
    
    def visualize_detection(self, frame, motion_mask, motion_regions, show_stats=True):
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            frame: åŸå§‹å¸§
            motion_mask: è¿åŠ¨æ©ç 
            motion_regions: è¿åŠ¨åŒºåŸŸåˆ—è¡¨
            show_stats: æ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            visualization: å¯è§†åŒ–ç»“æœå›¾åƒ
        """
        # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
        vis_frame = frame.copy()
        
        # ç»˜åˆ¶è¿åŠ¨åŒºåŸŸè¾¹ç•Œæ¡†
        for i, region in enumerate(motion_regions):
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(
                vis_frame,
                (region.x, region.y),
                (region.x + region.width, region.y + region.height),
                (0, 255, 0), 2
            )
            
            # æ·»åŠ åŒºåŸŸä¿¡æ¯
            label = f"#{i+1} Area: {region.area}"
            cv2.putText(
                vis_frame, label,
                (region.x, region.y - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1
            )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if show_stats and self.stats['total_frames'] > 0:
            stats_text = [
                f"Frames: {self.stats['total_frames']}",
                f"Motion: {self.stats['motion_frames']}",
                f"Rate: {self.stats['motion_frames']/self.stats['total_frames']*100:.1f}%",
                f"Regions: {len(motion_regions)}"
            ]
            
            for i, text in enumerate(stats_text):
                cv2.putText(
                    vis_frame, text,
                    (10, 30 + i * 25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1
                )
        
        # åˆ›å»ºæ©ç å¯è§†åŒ–
        mask_colored = cv2.applyColorMap(motion_mask, cv2.COLORMAP_HOT)
        
        # ç»„åˆæ˜¾ç¤º
        combined = np.hstack([vis_frame, mask_colored])
        
        return combined
    
    def get_performance_stats(self):
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            stats: æ€§èƒ½ç»Ÿè®¡å­—å…¸
        """
        if not self.stats['processing_times']:
            return {}
        
        processing_times = list(self.stats['processing_times'])
        
        return {
            'total_frames': self.stats['total_frames'],
            'motion_frames': self.stats['motion_frames'],
            'motion_ratio': self.stats['motion_frames'] / max(1, self.stats['total_frames']),
            'avg_processing_time': np.mean(processing_times),
            'fps': 1.0 / np.mean(processing_times) if processing_times else 0,
            'total_runtime': time.time() - self.stats['start_time'],
            'avg_motion_area': self.stats['total_motion_area'] / max(1, self.stats['motion_frames'])
        }
    
    def process_video(self, video_path, output_path=None, display=True):
        """
        å¤„ç†æ•´ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            display: æ˜¯å¦æ˜¾ç¤ºå®æ—¶ç»“æœ
            
        Returns:
            results: å¤„ç†ç»“æœç»Ÿè®¡
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘å†™å…¥å™¨
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            # è¾“å‡ºå®½åº¦æ˜¯åŸå§‹å®½åº¦çš„ä¸¤å€ï¼ˆåŸå›¾+æ©ç ï¼‰
            out = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))
        
        print(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   å¸§ç‡: {fps} fps")
        print(f"   æ€»å¸§æ•°: {total_frames}")
        
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # è¿åŠ¨æ£€æµ‹
                motion_mask, motion_regions = self.detect_motion(frame)
                
                # å¯è§†åŒ–
                visualization = self.visualize_detection(frame, motion_mask, motion_regions)
                
                # ä¿å­˜ç»“æœ
                if out:
                    out.write(visualization)
                
                # æ˜¾ç¤ºç»“æœ
                if display:
                    cv2.imshow('Motion Detection', visualization)
                    
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q'):
                        break
                    elif key == ord('s'):
                        # ä¿å­˜å½“å‰å¸§
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        cv2.imwrite(f'motion_detection_{timestamp}.jpg', visualization)
                        print(f"ä¿å­˜æˆªå›¾: motion_detection_{timestamp}.jpg")
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_count % 30 == 0:
                    progress = frame_count / total_frames * 100
                    print(f"å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})")
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = self.get_performance_stats()
            
            print("\nğŸ¯ å¤„ç†å®Œæˆç»Ÿè®¡:")
            print(f"   å¤„ç†å¸§æ•°: {final_stats['total_frames']}")
            print(f"   è¿åŠ¨å¸§æ•°: {final_stats['motion_frames']}")
            print(f"   è¿åŠ¨æ¯”ä¾‹: {final_stats['motion_ratio']*100:.1f}%")
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {final_stats['avg_processing_time']*1000:.2f}ms")
            print(f"   å¤„ç†å¸§ç‡: {final_stats['fps']:.1f} fps")
            
            return final_stats
            
        finally:
            cap.release()
            if out:
                out.release()
            if display:
                cv2.destroyAllWindows()
    
    def process_frame(self, frame, metadata):
        start_time = time.time()
        
        try:
            # è¿åŠ¨æ£€æµ‹
            motion_mask, motion_regions = self.detect_motion(frame)
            
            # åˆ›å»ºå¯è§†åŒ–ç»“æœ
            if self.config.get('visualize', False):
                vis_frame = self.visualize_detection(frame, motion_mask, motion_regions)
            else:
                vis_frame = frame.copy()
            
            # æ›´æ–°å…ƒæ•°æ®
            new_metadata = metadata.copy()
            new_metadata['motion_regions'] = motion_regions
            new_metadata['motion_mask'] = motion_mask
            new_metadata['motion_detected'] = len(motion_regions) > 0
            
            processing_time = time.time() - start_time
            self.update_stats(processing_time, True)
            
            return ProcessingResult(
                frame=vis_frame,
                metadata=new_metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)
            self.logger.error(f"Motion detection failed: {e}")
            
            return ProcessingResult(
                frame=frame,
                metadata=metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=False,
                error_message=str(e)
            )

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def demo_frame_difference_detector():
    """å¸§é—´å·®åˆ†æ£€æµ‹å™¨æ¼”ç¤º"""
    print("ğŸ” å¸§é—´å·®åˆ†æ£€æµ‹å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸åŒé…ç½®çš„æ£€æµ‹å™¨è¿›è¡Œå¯¹æ¯”
    configs = {
        'simple': {
            'detection': {'method': 'simple', 'threshold': 30}
        },
        'adaptive': {
            'detection': {'method': 'adaptive', 'adaptive_threshold': True}
        },
        'mog2': {
            'detection': {'method': 'mog2', 'learning_rate': 0.01}
        }
    }
    
    # åˆ›å»ºæµ‹è¯•è§†é¢‘
    test_video = "motion_test_video.mp4"
    if not os.path.exists(test_video):
        print("åˆ›å»ºè¿åŠ¨æµ‹è¯•è§†é¢‘...")
        create_motion_test_video(test_video)
    
    # æµ‹è¯•ä¸åŒçš„æ£€æµ‹æ–¹æ³•
    for method_name, config in configs.items():
        print(f"\nğŸ§ª æµ‹è¯•æ–¹æ³•: {method_name}")
        print("-" * 30)
        
        detector = FrameDifferenceDetector(config)
        
        try:
            stats = detector.process_video(
                test_video,
                output_path=f"motion_detection_{method_name}.mp4",
                display=False  # ä¸æ˜¾ç¤ºä»¥åŠ å¿«æµ‹è¯•é€Ÿåº¦
            )
            
            print(f"æ–¹æ³• {method_name} ç»“æœ:")
            print(f"  è¿åŠ¨æ£€æµ‹ç‡: {stats['motion_ratio']*100:.1f}%")
            print(f"  å¤„ç†æ€§èƒ½: {stats['fps']:.1f} fps")
            print(f"  å¹³å‡è¿åŠ¨åŒºåŸŸ: {stats['avg_motion_area']:.0f} åƒç´ ")
            
        except Exception as e:
            print(f"æ–¹æ³• {method_name} æµ‹è¯•å¤±è´¥: {e}")

def create_motion_test_video(output_path, duration=10, fps=30):
    """
    åˆ›å»ºåŒ…å«è¿åŠ¨ç›®æ ‡çš„æµ‹è¯•è§†é¢‘
    
    Args:
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        duration: è§†é¢‘æ—¶é•¿(ç§’)
        fps: å¸§ç‡
    """
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (640, 480))
    
    total_frames = duration * fps
    
    for i in range(total_frames):
        # åˆ›å»ºèƒŒæ™¯
        frame = np.ones((480, 640, 3), dtype=np.uint8) * 50
        
        # æ·»åŠ é™æ€èƒŒæ™¯çº¹ç†
        cv2.rectangle(frame, (100, 100), (200, 200), (100, 100, 100), -1)
        cv2.rectangle(frame, (400, 300), (500, 400), (80, 80, 80), -1)
        
        # æ·»åŠ ç§»åŠ¨çš„åœ†å½¢ç›®æ ‡
        t = i / fps
        center_x = int(50 + 500 * (0.5 + 0.5 * np.sin(2 * np.pi * t / 5)))
        center_y = int(240 + 100 * np.sin(2 * np.pi * t / 3))
        cv2.circle(frame, (center_x, center_y), 25, (255, 255, 255), -1)
        
        # æ·»åŠ å¦ä¸€ä¸ªç§»åŠ¨ç›®æ ‡ï¼ˆçŸ©å½¢ï¼‰
        rect_x = int(200 + 200 * np.cos(2 * np.pi * t / 4))
        rect_y = int(200 + 50 * np.sin(2 * np.pi * t / 2))
        cv2.rectangle(frame, (rect_x, rect_y), (rect_x + 40, rect_y + 60), (200, 200, 255), -1)
        
        # æ·»åŠ éšæœºå™ªå£°ï¼ˆæ¨¡æ‹Ÿç°å®åœºæ™¯ï¼‰
        if i % 10 == 0:  # å¶å°”æ·»åŠ å™ªå£°
            noise = np.random.randint(-20, 20, frame.shape, dtype=np.int16)
            frame = np.clip(frame.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        
        # æ·»åŠ æ—¶é—´æˆ³
        cv2.putText(frame, f"Time: {t:.2f}s", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        out.write(frame)
    
    out.release()
    print(f"è¿åŠ¨æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")

if __name__ == "__main__":
    demo_frame_difference_detector()
```
---

```mermaid
sequenceDiagram
    participant F as è¾“å…¥å¸§
    participant P as é¢„å¤„ç†å™¨
    participant D as å·®åˆ†æ£€æµ‹å™¨
    participant M as å½¢æ€å­¦å¤„ç†
    participant R as åŒºåŸŸæå–å™¨
    participant V as å¯è§†åŒ–å™¨
    
    F->>P: åŸå§‹å¸§
    P->>P: å™ªå£°è¿‡æ»¤
    P->>D: é¢„å¤„ç†å¸§
    D->>D: å¸§é—´å·®åˆ†è®¡ç®—
    D->>M: è¿åŠ¨æ©ç 
    M->>M: å½¢æ€å­¦ä¼˜åŒ–
    M->>R: ä¼˜åŒ–æ©ç 
    R->>R: è½®å»“æ£€æµ‹
    R->>V: è¿åŠ¨åŒºåŸŸ
    V->>V: ç»˜åˆ¶ç»“æœ
    V-->>F: å¯è§†åŒ–è¾“å‡º
```

---

è¿™ä¸ªæ™ºèƒ½å¸§é—´å·®åˆ†æ£€æµ‹å™¨ä¸ºæˆ‘ä»¬çš„è´¨æ£€éƒ¨é—¨æä¾›äº†å¼ºå¤§çš„è¿åŠ¨æ£€æµ‹èƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ„å»ºä¸“ä¸šçš„å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿï¼Œå®ç°å¯¹è¿åŠ¨ç›®æ ‡çš„æŒç»­è·Ÿè¸ªï¼

### ğŸ¯ ç¤ºä¾‹3ï¼šå¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ

ç°åœ¨è®©æˆ‘ä»¬æ¥åˆ°è¿½è¸ªå°ç»„ï¼Œè¿™é‡Œæœ‰ä¸€æ”¯ç²¾è‹±å›¢é˜Ÿä¸“é—¨è´Ÿè´£**å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ**çš„å¼€å‘ã€‚è¿™ä¸ªç³»ç»Ÿå°±åƒæ˜¯ä¸“ä¸šçš„ä¾¦æ¢å›¢é˜Ÿï¼Œä¸ä»…è¦å‘ç°ç›®æ ‡ï¼Œè¿˜è¦æŒç»­è·Ÿè¸ªå®ƒä»¬çš„è¡Œè¸ªï¼Œå³ä½¿åœ¨å¤æ‚çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒç¨³å®šçš„è¿½è¸ªæ•ˆæœã€‚

```python
"""
å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ - ä¸“ä¸šçº§ç›®æ ‡è·Ÿè¸ªè§£å†³æ–¹æ¡ˆ
åŠŸèƒ½ï¼š
1. å¤šç§è·Ÿè¸ªç®—æ³•æ”¯æŒï¼ˆKCFã€CSRTã€Deep SORTç­‰ï¼‰
2. å¡å°”æ›¼æ»¤æ³¢è½¨è¿¹é¢„æµ‹
3. åŒˆç‰™åˆ©ç®—æ³•ç›®æ ‡å…³è”
4. ç›®æ ‡ç”Ÿå‘½å‘¨æœŸç®¡ç†
5. è½¨è¿¹åˆ†æå’Œè¡Œä¸ºæ¨¡å¼è¯†åˆ«
"""

import cv2
import numpy as np
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
from scipy.optimize import linear_sum_assignment
import uuid
from datetime import datetime
import matplotlib.pyplot as plt
import math

@dataclass
class TrackingTarget:
    """è¿½è¸ªç›®æ ‡æ•°æ®ç»“æ„"""
    id: str                                    # ç›®æ ‡å”¯ä¸€æ ‡è¯†
    bbox: Tuple[int, int, int, int]           # è¾¹ç•Œæ¡† (x, y, w, h)
    center: Tuple[float, float]               # ä¸­å¿ƒç‚¹åæ ‡
    velocity: Tuple[float, float] = (0, 0)    # é€Ÿåº¦å‘é‡
    trajectory: List[Tuple[float, float]] = field(default_factory=list)  # è½¨è¿¹å†å²
    confidence: float = 1.0                   # ç½®ä¿¡åº¦
    age: int = 0                             # ç›®æ ‡å¹´é¾„ï¼ˆå¸§æ•°ï¼‰
    missed_frames: int = 0                   # è¿ç»­ä¸¢å¤±å¸§æ•°
    status: str = "active"                   # çŠ¶æ€ï¼šactive, lost, removed
    tracker: Optional[object] = None         # OpenCVè·Ÿè¸ªå™¨å¯¹è±¡
    created_time: float = field(default_factory=time.time)  # åˆ›å»ºæ—¶é—´
    last_seen: float = field(default_factory=time.time)     # æœ€åè§åˆ°æ—¶é—´

class KalmanFilter:
    """å¡å°”æ›¼æ»¤æ³¢å™¨ç”¨äºè½¨è¿¹é¢„æµ‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨"""
        # çŠ¶æ€å‘é‡: [x, y, vx, vy] (ä½ç½®å’Œé€Ÿåº¦)
        self.kalman = cv2.KalmanFilter(4, 2)
        
        # çŠ¶æ€è½¬ç§»çŸ©é˜µ
        self.kalman.transitionMatrix = np.array([
            [1, 0, 1, 0],
            [0, 1, 0, 1],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ], dtype=np.float32)
        
        # è§‚æµ‹çŸ©é˜µ
        self.kalman.measurementMatrix = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ], dtype=np.float32)
        
        # è¿‡ç¨‹å™ªå£°åæ–¹å·®
        self.kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.1
        
        # æµ‹é‡å™ªå£°åæ–¹å·®
        self.kalman.measurementNoiseCov = np.eye(2, dtype=np.float32) * 0.1
        
        # åéªŒè¯¯å·®åæ–¹å·®
        self.kalman.errorCovPost = np.eye(4, dtype=np.float32)
        
        self.initialized = False
    
    def initialize(self, x, y):
        """åˆå§‹åŒ–çŠ¶æ€"""
        self.kalman.statePre = np.array([x, y, 0, 0], dtype=np.float32)
        self.kalman.statePost = np.array([x, y, 0, 0], dtype=np.float32)
        self.initialized = True
    
    def predict(self):
        """é¢„æµ‹ä¸‹ä¸€çŠ¶æ€"""
        if not self.initialized:
            return None
        
        prediction = self.kalman.predict()
        return prediction[:2]  # è¿”å›ä½ç½®é¢„æµ‹
    
    def update(self, x, y):
        """æ›´æ–°çŠ¶æ€"""
        if not self.initialized:
            self.initialize(x, y)
            return
        
        measurement = np.array([x, y], dtype=np.float32)
        self.kalman.correct(measurement)

class MultiObjectTracker:
    """å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–å¤šç›®æ ‡è¿½è¸ªå™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        # é»˜è®¤é…ç½®
        self.config = {
            'tracking': {
                'tracker_type': 'CSRT',       # è·Ÿè¸ªå™¨ç±»å‹: KCF, CSRT, MedianFlow
                'max_missed_frames': 30,      # æœ€å¤§ä¸¢å¤±å¸§æ•°
                'min_confidence': 0.3,        # æœ€å°ç½®ä¿¡åº¦
                'max_distance': 50,           # æœ€å¤§å…³è”è·ç¦»
                'trajectory_length': 50       # è½¨è¿¹å†å²é•¿åº¦
            },
            'detection': {
                'min_area': 100,             # æœ€å°æ£€æµ‹åŒºåŸŸ
                'max_area': 10000,           # æœ€å¤§æ£€æµ‹åŒºåŸŸ
                'iou_threshold': 0.3         # IoUé˜ˆå€¼
            },
            'kalman': {
                'enable': True,              # æ˜¯å¦å¯ç”¨å¡å°”æ›¼æ»¤æ³¢
                'process_noise': 0.1,        # è¿‡ç¨‹å™ªå£°
                'measurement_noise': 0.1     # æµ‹é‡å™ªå£°
            }
        }
        
        # æ›´æ–°é…ç½®
        if config:
            self._update_config(config)
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨ç»„ä»¶
        self.targets = {}                    # æ´»è·ƒç›®æ ‡å­—å…¸
        self.lost_targets = {}              # ä¸¢å¤±ç›®æ ‡å­—å…¸
        self.next_id = 0                    # ä¸‹ä¸€ä¸ªç›®æ ‡ID
        self.frame_count = 0                # å¸§è®¡æ•°
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_targets': 0,
            'active_targets': 0,
            'lost_targets': 0,
            'removed_targets': 0,
            'processing_times': deque(maxlen=100),
            'tracking_accuracy': deque(maxlen=100)
        }
        
        print("ğŸ¯ å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _update_config(self, new_config):
        """æ›´æ–°é…ç½®"""
        def update_dict(base, update):
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    update_dict(base[key], value)
                else:
                    base[key] = value
        update_dict(self.config, new_config)
    
    def _create_tracker(self, tracker_type='CSRT'):
        """åˆ›å»ºOpenCVè·Ÿè¸ªå™¨"""
        if tracker_type == 'KCF':
            return cv2.TrackerKCF_create()
        elif tracker_type == 'CSRT':
            return cv2.TrackerCSRT_create()
        elif tracker_type == 'MedianFlow':
            return cv2.TrackerMedianFlow_create()
        elif tracker_type == 'MOSSE':
            return cv2.TrackerMOSSE_create()
        else:
            return cv2.TrackerCSRT_create()  # é»˜è®¤ä½¿ç”¨CSRT
    
    def _calculate_distance(self, center1, center2):
        """è®¡ç®—ä¸¤ä¸ªä¸­å¿ƒç‚¹ä¹‹é—´çš„æ¬§æ°è·ç¦»"""
        return math.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _calculate_iou(self, bbox1, bbox2):
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # è®¡ç®—äº¤é›†
        xi = max(x1, x2)
        yi = max(y1, y2)
        wi = max(0, min(x1 + w1, x2 + w2) - xi)
        hi = max(0, min(y1 + h1, y2 + h2) - yi)
        
        if wi <= 0 or hi <= 0:
            return 0
        
        intersection = wi * hi
        union = w1 * h1 + w2 * h2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def _create_target(self, bbox, frame):
        """åˆ›å»ºæ–°çš„è¿½è¸ªç›®æ ‡"""
        x, y, w, h = bbox
        center = (x + w/2, y + h/2)
        
        # åˆ›å»ºç›®æ ‡å¯¹è±¡
        target = TrackingTarget(
            id=str(self.next_id),
            bbox=bbox,
            center=center,
            trajectory=[center]
        )
        
        # åˆ›å»ºè·Ÿè¸ªå™¨
        tracker = self._create_tracker(self.config['tracking']['tracker_type'])
        success = tracker.init(frame, bbox)
        
        if success:
            target.tracker = tracker
            
            # åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨
            if self.config['kalman']['enable']:
                target.kalman_filter = KalmanFilter()
                target.kalman_filter.initialize(center[0], center[1])
            
            self.targets[target.id] = target
            self.next_id += 1
            self.stats['total_targets'] += 1
            
            return target
        
        return None
    
    def _update_target(self, target, bbox, frame):
        """æ›´æ–°ç›®æ ‡ä¿¡æ¯"""
        x, y, w, h = bbox
        center = (x + w/2, y + h/2)
        
        # è®¡ç®—é€Ÿåº¦
        if target.trajectory:
            prev_center = target.trajectory[-1]
            target.velocity = (center[0] - prev_center[0], center[1] - prev_center[1])
        
        # æ›´æ–°ç›®æ ‡ä¿¡æ¯
        target.bbox = bbox
        target.center = center
        target.trajectory.append(center)
        target.age += 1
        target.missed_frames = 0
        target.last_seen = time.time()
        
        # é™åˆ¶è½¨è¿¹é•¿åº¦
        max_trajectory_length = self.config['tracking']['trajectory_length']
        if len(target.trajectory) > max_trajectory_length:
            target.trajectory = target.trajectory[-max_trajectory_length:]
        
        # æ›´æ–°å¡å°”æ›¼æ»¤æ³¢å™¨
        if hasattr(target, 'kalman_filter') and target.kalman_filter:
            target.kalman_filter.update(center[0], center[1])
    
    def _associate_detections(self, detections, frame):
        """å…³è”æ£€æµ‹ç»“æœä¸ç°æœ‰ç›®æ ‡"""
        if not self.targets or not detections:
            return [], list(range(len(detections)))
        
        # é¢„æµ‹ç›®æ ‡ä½ç½®
        predicted_positions = {}
        for target_id, target in self.targets.items():
            if hasattr(target, 'kalman_filter') and target.kalman_filter:
                predicted_pos = target.kalman_filter.predict()
                if predicted_pos is not None:
                    predicted_positions[target_id] = predicted_pos
                else:
                    predicted_positions[target_id] = target.center
            else:
                predicted_positions[target_id] = target.center
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        target_ids = list(self.targets.keys())
        distance_matrix = np.zeros((len(target_ids), len(detections)))
        
        for i, target_id in enumerate(target_ids):
            predicted_pos = predicted_positions[target_id]
            for j, detection in enumerate(detections):
                x, y, w, h = detection
                det_center = (x + w/2, y + h/2)
                distance = self._calculate_distance(predicted_pos, det_center)
                distance_matrix[i, j] = distance
        
        # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•è¿›è¡Œæœ€ä¼˜åŒ¹é…
        if len(target_ids) > 0 and len(detections) > 0:
            row_indices, col_indices = linear_sum_assignment(distance_matrix)
            
            matches = []
            unmatched_detections = list(range(len(detections)))
            
            for row, col in zip(row_indices, col_indices):
                distance = distance_matrix[row, col]
                if distance <= self.config['tracking']['max_distance']:
                    matches.append((target_ids[row], col))
                    if col in unmatched_detections:
                        unmatched_detections.remove(col)
            
            return matches, unmatched_detections
        
        return [], list(range(len(detections)))
    
    def update(self, detections, frame):
        """
        æ›´æ–°è¿½è¸ªç³»ç»Ÿ
        
        Args:
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨ [(x, y, w, h), ...]
            frame: å½“å‰å¸§
            
        Returns:
            tracking_results: è¿½è¸ªç»“æœ
        """
        start_time = time.time()
        self.frame_count += 1
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        filtered_detections = []
        for detection in detections:
            x, y, w, h = detection
            area = w * h
            if (self.config['detection']['min_area'] <= area <= 
                self.config['detection']['max_area']):
                filtered_detections.append(detection)
        
        # æ›´æ–°ç°æœ‰ç›®æ ‡
        for target_id, target in list(self.targets.items()):
            if target.tracker:
                success, bbox = target.tracker.update(frame)
                if success:
                    bbox = tuple(map(int, bbox))
                    self._update_target(target, bbox, frame)
                    target.confidence = min(1.0, target.confidence + 0.1)
                else:
                    target.missed_frames += 1
                    target.confidence = max(0.0, target.confidence - 0.2)
        
        # å…³è”æ£€æµ‹ç»“æœ
        matches, unmatched_detections = self._associate_detections(filtered_detections, frame)
        
        # æ›´æ–°åŒ¹é…çš„ç›®æ ‡
        for target_id, detection_idx in matches:
            if target_id in self.targets:
                detection = filtered_detections[detection_idx]
                self._update_target(self.targets[target_id], detection, frame)
                
                # é‡æ–°åˆå§‹åŒ–è·Ÿè¸ªå™¨ä»¥æé«˜ç²¾åº¦
                tracker = self._create_tracker(self.config['tracking']['tracker_type'])
                success = tracker.init(frame, detection)
                if success:
                    self.targets[target_id].tracker = tracker
        
        # åˆ›å»ºæ–°ç›®æ ‡
        for detection_idx in unmatched_detections:
            detection = filtered_detections[detection_idx]
            self._create_target(detection, frame)
        
        # æ¸…ç†ä¸¢å¤±çš„ç›®æ ‡
        self._cleanup_lost_targets()
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats['processing_times'].append(processing_time)
        self.stats['active_targets'] = len(self.targets)
        self.stats['lost_targets'] = len(self.lost_targets)
        
        return self._get_tracking_results()
    
    def _cleanup_lost_targets(self):
        """æ¸…ç†ä¸¢å¤±çš„ç›®æ ‡"""
        targets_to_remove = []
        max_missed = self.config['tracking']['max_missed_frames']
        
        for target_id, target in self.targets.items():
            if target.missed_frames > max_missed or target.confidence < self.config['tracking']['min_confidence']:
                target.status = "lost"
                self.lost_targets[target_id] = target
                targets_to_remove.append(target_id)
        
        for target_id in targets_to_remove:
            del self.targets[target_id]
            self.stats['removed_targets'] += 1
    
    def _get_tracking_results(self):
        """è·å–å½“å‰è¿½è¸ªç»“æœ"""
        results = []
        for target in self.targets.values():
            if target.status == "active":
                results.append({
                    'id': target.id,
                    'bbox': target.bbox,
                    'center': target.center,
                    'velocity': target.velocity,
                    'confidence': target.confidence,
                    'age': target.age,
                    'trajectory': target.trajectory.copy()
                })
        return results
    
    def visualize_tracking(self, frame, show_trajectory=True, show_info=True):
        """
        å¯è§†åŒ–è¿½è¸ªç»“æœ
        
        Args:
            frame: è¾“å…¥å¸§
            show_trajectory: æ˜¯å¦æ˜¾ç¤ºè½¨è¿¹
            show_info: æ˜¯å¦æ˜¾ç¤ºä¿¡æ¯
            
        Returns:
            vis_frame: å¯è§†åŒ–ç»“æœ
        """
        vis_frame = frame.copy()
        
        # ä¸ºæ¯ä¸ªç›®æ ‡åˆ†é…é¢œè‰²
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)
        ]
        
        for i, target in enumerate(self.targets.values()):
            if target.status != "active":
                continue
                
            color = colors[i % len(colors)]
            x, y, w, h = target.bbox
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(vis_frame, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            center = (int(target.center[0]), int(target.center[1]))
            cv2.circle(vis_frame, center, 5, color, -1)
            
            # ç»˜åˆ¶è½¨è¿¹
            if show_trajectory and len(target.trajectory) > 1:
                points = [(int(p[0]), int(p[1])) for p in target.trajectory]
                for j in range(len(points) - 1):
                    cv2.line(vis_frame, points[j], points[j + 1], color, 2)
            
            # ç»˜åˆ¶ç›®æ ‡ä¿¡æ¯
            if show_info:
                info_text = f"ID: {target.id}"
                confidence_text = f"Conf: {target.confidence:.2f}"
                age_text = f"Age: {target.age}"
                
                cv2.putText(vis_frame, info_text, (x, y - 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                cv2.putText(vis_frame, confidence_text, (x, y - 15),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                cv2.putText(vis_frame, age_text, (x, y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats_text = [
            f"Frame: {self.frame_count}",
            f"Active: {len(self.targets)}",
            f"Total: {self.stats['total_targets']}",
            f"Lost: {self.stats['removed_targets']}"
        ]
        
        for i, text in enumerate(stats_text):
            cv2.putText(vis_frame, text, (10, 30 + i * 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_frame
    
    def get_target_analytics(self):
        """è·å–ç›®æ ‡åˆ†æç»Ÿè®¡"""
        analytics = {
            'active_targets': [],
            'trajectory_stats': {},
            'velocity_stats': {},
            'lifetime_stats': {}
        }
        
        # æ´»è·ƒç›®æ ‡åˆ†æ
        for target in self.targets.values():
            target_info = {
                'id': target.id,
                'age': target.age,
                'confidence': target.confidence,
                'trajectory_length': len(target.trajectory),
                'avg_velocity': np.mean([abs(target.velocity[0]), abs(target.velocity[1])]) if target.velocity else 0
            }
            analytics['active_targets'].append(target_info)
        
        # è½¨è¿¹ç»Ÿè®¡
        if self.targets:
            trajectory_lengths = [len(t.trajectory) for t in self.targets.values()]
            analytics['trajectory_stats'] = {
                'avg_length': np.mean(trajectory_lengths),
                'max_length': max(trajectory_lengths),
                'min_length': min(trajectory_lengths)
            }
        
        # é€Ÿåº¦ç»Ÿè®¡
        velocities = []
        for target in self.targets.values():
            if target.velocity:
                speed = math.sqrt(target.velocity[0]**2 + target.velocity[1]**2)
                velocities.append(speed)
        
        if velocities:
            analytics['velocity_stats'] = {
                'avg_speed': np.mean(velocities),
                'max_speed': max(velocities),
                'min_speed': min(velocities)
            }
        
        # ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡
        ages = [t.age for t in self.targets.values()]
        if ages:
            analytics['lifetime_stats'] = {
                'avg_age': np.mean(ages),
                'max_age': max(ages),
                'min_age': min(ages)
            }
        
        return analytics

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def demo_multi_object_tracker():
    """å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿæ¼”ç¤º"""
    print("ğŸ¯ å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºè¿½è¸ªå™¨
    tracker = MultiObjectTracker({
        'tracking': {
            'tracker_type': 'CSRT',
            'max_missed_frames': 20,
            'trajectory_length': 30
        }
    })
    
    # åˆ›å»ºæµ‹è¯•è§†é¢‘
    test_video = "multi_object_test.mp4"
    if not os.path.exists(test_video):
        print("åˆ›å»ºå¤šç›®æ ‡æµ‹è¯•è§†é¢‘...")
        create_multi_object_test_video(test_video)
    
    # åˆ›å»ºç®€å•çš„æ£€æµ‹å™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    def simple_detector(frame):
        """ç®€å•çš„ç›®æ ‡æ£€æµ‹å™¨ï¼ˆåŸºäºé¢œè‰²ï¼‰"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # æ£€æµ‹ç™½è‰²ç›®æ ‡
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([180, 30, 255])
        mask = cv2.inRange(hsv, lower_white, upper_white)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        detections = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 500:  # è¿‡æ»¤å°ç›®æ ‡
                x, y, w, h = cv2.boundingRect(contour)
                detections.append((x, y, w, h))
        
        return detections
    
    # å¤„ç†è§†é¢‘
    cap = cv2.VideoCapture(test_video)
    
    if not cap.isOpened():
        print(f"æ— æ³•æ‰“å¼€è§†é¢‘: {test_video}")
        return
    
    print("ğŸ¬ å¼€å§‹å¤šç›®æ ‡è¿½è¸ªæ¼”ç¤º...")
    print("æŒ‰ 'q' é€€å‡ºï¼ŒæŒ‰ 's' ä¿å­˜æˆªå›¾")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ£€æµ‹ç›®æ ‡
            detections = simple_detector(frame)
            
            # æ›´æ–°è¿½è¸ªå™¨
            tracking_results = tracker.update(detections, frame)
            
            # å¯è§†åŒ–ç»“æœ
            vis_frame = tracker.visualize_tracking(frame, show_trajectory=True, show_info=True)
            
            # æ˜¾ç¤ºç»“æœ
            cv2.imshow('Multi-Object Tracking', vis_frame)
            
            key = cv2.waitKey(30) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                cv2.imwrite(f'tracking_result_{timestamp}.jpg', vis_frame)
                print(f"ä¿å­˜æˆªå›¾: tracking_result_{timestamp}.jpg")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        analytics = tracker.get_target_analytics()
        print("\nğŸ¯ è¿½è¸ªç»Ÿè®¡:")
        print(f"æ€»ç›®æ ‡æ•°: {tracker.stats['total_targets']}")
        print(f"æ´»è·ƒç›®æ ‡: {len(tracker.targets)}")
        print(f"å·²ç§»é™¤ç›®æ ‡: {tracker.stats['removed_targets']}")
        
        if analytics['trajectory_stats']:
            print(f"å¹³å‡è½¨è¿¹é•¿åº¦: {analytics['trajectory_stats']['avg_length']:.1f}")
        
        if analytics['velocity_stats']:
            print(f"å¹³å‡é€Ÿåº¦: {analytics['velocity_stats']['avg_speed']:.1f}")
        
    finally:
        cap.release()
        cv2.destroyAllWindows()

def create_multi_object_test_video(output_path, duration=15, fps=30):
    """
    åˆ›å»ºå¤šç›®æ ‡æµ‹è¯•è§†é¢‘
    
    Args:
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        duration: è§†é¢‘æ—¶é•¿(ç§’)
        fps: å¸§ç‡
    """
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (800, 600))
    
    total_frames = duration * fps
    
    for i in range(total_frames):
        # åˆ›å»ºèƒŒæ™¯
        frame = np.ones((600, 800, 3), dtype=np.uint8) * 30
        
        # æ·»åŠ èƒŒæ™¯çº¹ç†
        cv2.rectangle(frame, (100, 100), (300, 300), (50, 50, 50), -1)
        cv2.rectangle(frame, (500, 350), (700, 500), (40, 40, 40), -1)
        
        t = i / fps
        
        # ç¬¬ä¸€ä¸ªç›®æ ‡ï¼šåœ†å½¢ï¼Œæ°´å¹³ç§»åŠ¨
        x1 = int(50 + 600 * (t / duration))
        y1 = 150
        cv2.circle(frame, (x1, y1), 20, (255, 255, 255), -1)
        
        # ç¬¬äºŒä¸ªç›®æ ‡ï¼šçŸ©å½¢ï¼Œå¯¹è§’çº¿ç§»åŠ¨
        x2 = int(100 + 500 * (t / duration))
        y2 = int(200 + 300 * (t / duration))
        cv2.rectangle(frame, (x2-15, y2-15), (x2+15, y2+15), (255, 255, 255), -1)
        
        # ç¬¬ä¸‰ä¸ªç›®æ ‡ï¼šåœ†å½¢ï¼Œæ­£å¼¦æ³¢ç§»åŠ¨
        x3 = int(400 + 200 * np.sin(2 * np.pi * t / 3))
        y3 = int(300 + 100 * np.cos(2 * np.pi * t / 2))
        cv2.circle(frame, (x3, y3), 18, (255, 255, 255), -1)
        
        # ç¬¬å››ä¸ªç›®æ ‡ï¼šåœ¨ä¸­åæœŸå‡ºç°
        if t > duration / 3:
            x4 = int(200 + 400 * ((t - duration/3) / (2*duration/3)))
            y4 = int(450 - 200 * ((t - duration/3) / (2*duration/3)))
            cv2.rectangle(frame, (x4-12, y4-12), (x4+12, y4+12), (255, 255, 255), -1)
        
        # æ·»åŠ æ—¶é—´æˆ³
        cv2.putText(frame, f"Time: {t:.2f}s", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        out.write(frame)
    
    out.release()
    print(f"å¤šç›®æ ‡æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")

if __name__ == "__main__":
    demo_multi_object_tracker()
```

### ğŸ¯ æŠ€æœ¯äº®ç‚¹åˆ†æ

è¿™ä¸ª**å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ**å±•ç°äº†ç°ä»£è¿½è¸ªæŠ€æœ¯çš„æ ¸å¿ƒç‰¹å¾ï¼š

#### ğŸ§  æ™ºèƒ½ç®—æ³•èåˆ
- **å¤šç§è·Ÿè¸ªå™¨**: æ”¯æŒKCFã€CSRTã€MedianFlowç­‰å¤šç§ç®—æ³•
- **å¡å°”æ›¼æ»¤æ³¢**: ç”¨äºè½¨è¿¹é¢„æµ‹å’ŒçŠ¶æ€ä¼°è®¡
- **åŒˆç‰™åˆ©ç®—æ³•**: è§£å†³ç›®æ ‡å…³è”çš„æœ€ä¼˜åŒ¹é…é—®é¢˜
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**: æ™ºèƒ½ç®¡ç†ç›®æ ‡çš„åˆ›å»ºã€æ›´æ–°å’Œç§»é™¤

#### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§
- **å®æ—¶è¿½è¸ª**: æ”¯æŒå¤šä¸ªç›®æ ‡çš„å®æ—¶è·Ÿè¸ª
- **è½¨è¿¹åˆ†æ**: è®°å½•å’Œåˆ†æç›®æ ‡çš„è¿åŠ¨è½¨è¿¹
- **ç›®æ ‡å…³è”**: æ™ºèƒ½å…³è”æ£€æµ‹ç»“æœä¸ç°æœ‰ç›®æ ‡
- **é²æ£’æ€§**: å¤„ç†ç›®æ ‡é®æŒ¡ã€æ¶ˆå¤±å’Œé‡ç°

#### ğŸ“Š æ€§èƒ½ç›‘æ§
- **è¿½è¸ªç²¾åº¦**: å®æ—¶ç›‘æ§è¿½è¸ªå‡†ç¡®ç‡
- **ç›®æ ‡ç»Ÿè®¡**: è¯¦ç»†çš„ç›®æ ‡ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡
- **è½¨è¿¹åˆ†æ**: é€Ÿåº¦ã€æ–¹å‘ç­‰è¿åŠ¨ç‰¹å¾åˆ†æ

---

```mermaid
graph TD
    A[æ£€æµ‹ç»“æœ] --> B[ç›®æ ‡å…³è”]
    B --> C{æ–°ç›®æ ‡?}
    C -->|æ˜¯| D[åˆ›å»ºç›®æ ‡]
    C -->|å¦| E[æ›´æ–°ç›®æ ‡]
    D --> F[åˆå§‹åŒ–è·Ÿè¸ªå™¨]
    E --> G[æ›´æ–°è·Ÿè¸ªå™¨]
    F --> H[å¡å°”æ›¼æ»¤æ³¢]
    G --> H
    H --> I[è½¨è¿¹é¢„æµ‹]
    I --> J[çŠ¶æ€æ›´æ–°]
    J --> K[å¯è§†åŒ–è¾“å‡º]
    
    L[ä¸¢å¤±ç›®æ ‡] --> M{è¶…æ—¶?}
    M -->|æ˜¯| N[ç§»é™¤ç›®æ ‡]
    M -->|å¦| O[ä¿æŒè¿½è¸ª]
    
    style A fill:#e3f2fd
    style K fill:#e8f5e8
    style N fill:#ffebee
```

---

è¿™ä¸ªå¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿä¸ºæˆ‘ä»¬çš„è¿½è¸ªå°ç»„æä¾›äº†å¼ºå¤§çš„ç›®æ ‡è·Ÿè¸ªèƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ„å»ºè§†é¢‘æµå¤„ç†ç®¡é“ï¼Œå®ç°æ•´ä¸ªè§†é¢‘å¤„ç†å·¥å‚çš„æ™ºèƒ½åŒ–æµæ°´çº¿ä½œä¸šï¼

### ğŸ¯ ç¤ºä¾‹4ï¼šè§†é¢‘æµå¤„ç†ç®¡é“

ç°åœ¨è®©æˆ‘ä»¬æ¥åˆ°å·¥ç¨‹éƒ¨é—¨ï¼Œè¿™é‡Œè´Ÿè´£æ•´ä¸ªå·¥å‚çš„**è§†é¢‘æµå¤„ç†ç®¡é“**è®¾è®¡ã€‚è¿™ä¸ªç³»ç»Ÿå°±åƒæ˜¯å·¥å‚çš„ç¥ç»ç³»ç»Ÿï¼Œå°†æ‰€æœ‰çš„å¤„ç†ç¯èŠ‚æœ‰æœºåœ°è¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆã€å¯é…ç½®ã€å¯æ‰©å±•çš„æ™ºèƒ½åŒ–æµæ°´çº¿ã€‚

```python
"""
è§†é¢‘æµå¤„ç†ç®¡é“ - å¯æ‰©å±•çš„è§†é¢‘å¤„ç†æ¶æ„
åŠŸèƒ½ï¼š
1. æ¨¡å—åŒ–å¤„ç†ç»„ä»¶è®¾è®¡
2. æ’ä»¶å¼æ¶æ„æ”¯æŒ
3. é…ç½®é©±åŠ¨çš„æµç¨‹å®šåˆ¶
4. å®æ—¶ç›‘æ§å’Œæ€§èƒ½åˆ†æ
5. åˆ†å¸ƒå¼å¤„ç†æ”¯æŒ
"""

import cv2
import numpy as np
import time
import threading
import queue
import json
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
import psutil
import os
from pathlib import Path

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç»“æ„"""
    frame: np.ndarray                # å¤„ç†åçš„å¸§
    metadata: Dict[str, Any]         # å¤„ç†å…ƒæ•°æ®
    timestamp: float                 # æ—¶é—´æˆ³
    processing_time: float           # å¤„ç†æ—¶é—´
    component_name: str              # å¤„ç†ç»„ä»¶åç§°
    success: bool = True            # å¤„ç†æ˜¯å¦æˆåŠŸ
    error_message: str = ""         # é”™è¯¯ä¿¡æ¯

class ProcessingComponent(ABC):
    """å¤„ç†ç»„ä»¶æŠ½è±¡åŸºç±»"""
    
    def __init__(self, name: str, config: Dict[str, Any] = None):
        self.name = name
        self.config = config or {}
        self.enabled = self.config.get('enabled', True)
        self.stats = {
            'processed_frames': 0,
            'processing_times': [],
            'errors': 0,
            'start_time': time.time()
        }
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(f"Component.{name}")
    
    @abstractmethod
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> ProcessingResult:
        """
        å¤„ç†å•å¸§æ•°æ®
        
        Args:
            frame: è¾“å…¥å¸§
            metadata: è¾“å…¥å…ƒæ•°æ®
            
        Returns:
            ProcessingResult: å¤„ç†ç»“æœ
        """
        pass
    
    def update_stats(self, processing_time: float, success: bool = True):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['processed_frames'] += 1
        self.stats['processing_times'].append(processing_time)
        if not success:
            self.stats['errors'] += 1
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.stats['processing_times']:
            return {}
        
        return {
            'name': self.name,
            'processed_frames': self.stats['processed_frames'],
            'avg_processing_time': np.mean(self.stats['processing_times'][-100:]),
            'fps': 1.0 / np.mean(self.stats['processing_times'][-100:]) if self.stats['processing_times'] else 0,
            'errors': self.stats['errors'],
            'error_rate': self.stats['errors'] / max(1, self.stats['processed_frames']),
            'uptime': time.time() - self.stats['start_time']
        }

class MotionDetectionComponent(ProcessingComponent):
    """è¿åŠ¨æ£€æµ‹ç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("MotionDetection", config)
        
        # ä»å‰é¢çš„ç¤ºä¾‹å¯¼å…¥æ£€æµ‹å™¨
        from frame_difference_detector import FrameDifferenceDetector
        self.detector = FrameDifferenceDetector(self.config)
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> ProcessingResult:
        start_time = time.time()
        
        try:
            # è¿åŠ¨æ£€æµ‹
            motion_mask, motion_regions = self.detector.detect_motion(frame)
            
            # åˆ›å»ºå¯è§†åŒ–ç»“æœ
            if self.config.get('visualize', False):
                vis_frame = self.detector.visualize_detection(frame, motion_mask, motion_regions)
            else:
                vis_frame = frame.copy()
            
            # æ›´æ–°å…ƒæ•°æ®
            new_metadata = metadata.copy()
            new_metadata['motion_regions'] = motion_regions
            new_metadata['motion_mask'] = motion_mask
            new_metadata['motion_detected'] = len(motion_regions) > 0
            
            processing_time = time.time() - start_time
            self.update_stats(processing_time, True)
            
            return ProcessingResult(
                frame=vis_frame,
                metadata=new_metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)
            self.logger.error(f"Motion detection failed: {e}")
            
            return ProcessingResult(
                frame=frame,
                metadata=metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=False,
                error_message=str(e)
            )

class ObjectTrackingComponent(ProcessingComponent):
    """ç›®æ ‡è¿½è¸ªç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("ObjectTracking", config)
        
        # ä»å‰é¢çš„ç¤ºä¾‹å¯¼å…¥è¿½è¸ªå™¨
        from multi_object_tracker import MultiObjectTracker
        self.tracker = MultiObjectTracker(self.config)
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> ProcessingResult:
        start_time = time.time()
        
        try:
            # è·å–æ£€æµ‹ç»“æœ
            detections = []
            if 'motion_regions' in metadata:
                for region in metadata['motion_regions']:
                    detections.append((region.x, region.y, region.width, region.height))
            
            # ç›®æ ‡è¿½è¸ª
            tracking_results = self.tracker.update(detections, frame)
            
            # å¯è§†åŒ–
            if self.config.get('visualize', False):
                vis_frame = self.tracker.visualize_tracking(frame, show_trajectory=True, show_info=True)
            else:
                vis_frame = frame.copy()
            
            # æ›´æ–°å…ƒæ•°æ®
            new_metadata = metadata.copy()
            new_metadata['tracking_results'] = tracking_results
            new_metadata['active_targets'] = len(tracking_results)
            
            processing_time = time.time() - start_time
            self.update_stats(processing_time, True)
            
            return ProcessingResult(
                frame=vis_frame,
                metadata=new_metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)
            self.logger.error(f"Object tracking failed: {e}")
            
            return ProcessingResult(
                frame=frame,
                metadata=metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=False,
                error_message=str(e)
            )

class FrameFilterComponent(ProcessingComponent):
    """å¸§è¿‡æ»¤ç»„ä»¶"""
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> ProcessingResult:
        start_time = time.time()
        
        try:
            # åº”ç”¨å„ç§æ»¤æ³¢å™¨
            processed_frame = frame.copy()
            
            # é«˜æ–¯æ¨¡ç³Š
            if self.config.get('gaussian_blur', False):
                kernel_size = self.config.get('gaussian_kernel', (5, 5))
                processed_frame = cv2.GaussianBlur(processed_frame, kernel_size, 0)
            
            # åŒè¾¹æ»¤æ³¢
            if self.config.get('bilateral_filter', False):
                processed_frame = cv2.bilateralFilter(processed_frame, 9, 75, 75)
            
            # ç›´æ–¹å›¾å‡è¡¡åŒ–
            if self.config.get('histogram_equalization', False):
                gray = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)
                equalized = cv2.equalizeHist(gray)
                processed_frame = cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)
            
            processing_time = time.time() - start_time
            self.update_stats(processing_time, True)
            
            return ProcessingResult(
                frame=processed_frame,
                metadata=metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)
            self.logger.error(f"Frame filtering failed: {e}")
            
            return ProcessingResult(
                frame=frame,
                metadata=metadata,
                timestamp=time.time(),
                processing_time=processing_time,
                component_name=self.name,
                success=False,
                error_message=str(e)
            )

class VideoStreamPipeline:
    """è§†é¢‘æµå¤„ç†ç®¡é“"""
    
    def __init__(self, config_file: str = None):
        """
        åˆå§‹åŒ–è§†é¢‘æµå¤„ç†ç®¡é“
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_file)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        self.logger = logging.getLogger('VideoStreamPipeline')
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.components = []
        self._initialize_components()
        
        # å¤„ç†é˜Ÿåˆ—
        self.input_queue = queue.Queue(maxsize=self.config.get('queue_size', 10))
        self.output_queue = queue.Queue(maxsize=self.config.get('queue_size', 10))
        
        # æ§åˆ¶å˜é‡
        self.running = False
        self.worker_threads = []
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        self.logger.info("è§†é¢‘æµå¤„ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'pipeline': {
                'worker_threads': 2,
                'queue_size': 10,
                'timeout': 5
            },
            'components': [
                {
                    'name': 'FrameFilter',
                    'type': 'FrameFilterComponent',
                    'enabled': True,
                    'config': {'gaussian_blur': True}
                },
                {
                    'name': 'MotionDetection',
                    'type': 'MotionDetectionComponent', 
                    'enabled': True,
                    'config': {'visualize': False}
                },
                {
                    'name': 'ObjectTracking',
                    'type': 'ObjectTrackingComponent',
                    'enabled': True,
                    'config': {'visualize': True}
                }
            ],
            'monitoring': {
                'enabled': True,
                'stats_interval': 10
            }
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
                    self.logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
            except Exception as e:
                self.logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('pipeline.log'),
                logging.StreamHandler()
            ]
        )
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å¤„ç†ç»„ä»¶"""
        component_classes = {
            'FrameFilterComponent': FrameFilterComponent,
            'MotionDetectionComponent': MotionDetectionComponent,
            'ObjectTrackingComponent': ObjectTrackingComponent
        }
        
        for comp_config in self.config.get('components', []):
            if not comp_config.get('enabled', True):
                continue
            
            comp_type = comp_config.get('type')
            comp_class = component_classes.get(comp_type)
            
            if comp_class:
                component = comp_class(comp_config.get('config', {}))
                self.components.append(component)
                self.logger.info(f"ç»„ä»¶åŠ è½½æˆåŠŸ: {component.name}")
            else:
                self.logger.warning(f"æœªçŸ¥ç»„ä»¶ç±»å‹: {comp_type}")
    
    def add_component(self, component: ProcessingComponent):
        """æ·»åŠ å¤„ç†ç»„ä»¶"""
        self.components.append(component)
        self.logger.info(f"æ·»åŠ ç»„ä»¶: {component.name}")
    
    def remove_component(self, component_name: str):
        """ç§»é™¤å¤„ç†ç»„ä»¶"""
        self.components = [c for c in self.components if c.name != component_name]
        self.logger.info(f"ç§»é™¤ç»„ä»¶: {component_name}")
    
    def _worker_thread(self):
        """å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # è·å–è¾“å…¥æ•°æ®
                frame_data = self.input_queue.get(timeout=1)
                if frame_data is None:  # ç»“æŸä¿¡å·
                    break
                
                frame, metadata = frame_data
                
                # é€æ­¥å¤„ç†
                current_frame = frame
                current_metadata = metadata
                
                for component in self.components:
                    if not component.enabled:
                        continue
                    
                    result = component.process(current_frame, current_metadata)
                    
                    if result.success:
                        current_frame = result.frame
                        current_metadata = result.metadata
                    else:
                        self.logger.error(f"ç»„ä»¶å¤„ç†å¤±è´¥: {component.name} - {result.error_message}")
                
                # è¾“å‡ºç»“æœ
                self.output_queue.put((current_frame, current_metadata))
                
                # æ›´æ–°æ€§èƒ½ç›‘æ§
                self.performance_monitor.update_frame_processed()
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
    
    def start(self):
        """å¯åŠ¨ç®¡é“"""
        if self.running:
            return
        
        self.running = True
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        num_workers = self.config['pipeline']['worker_threads']
        for i in range(num_workers):
            thread = threading.Thread(target=self._worker_thread, name=f"Worker-{i}")
            thread.start()
            self.worker_threads.append(thread)
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        if self.config['monitoring']['enabled']:
            self.performance_monitor.start()
        
        self.logger.info(f"ç®¡é“å¯åŠ¨å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {num_workers}")
    
    def stop(self):
        """åœæ­¢ç®¡é“"""
        if not self.running:
            return
        
        self.running = False
        
        # å‘é€ç»“æŸä¿¡å·
        for _ in self.worker_threads:
            self.input_queue.put(None)
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in self.worker_threads:
            thread.join()
        
        self.worker_threads.clear()
        
        # åœæ­¢æ€§èƒ½ç›‘æ§
        self.performance_monitor.stop()
        
        self.logger.info("ç®¡é“åœæ­¢å®Œæˆ")
    
    def process_frame(self, frame: np.ndarray, metadata: Dict[str, Any] = None) -> Optional[tuple]:
        """
        å¤„ç†å•å¸§ï¼ˆéé˜»å¡ï¼‰
        
        Args:
            frame: è¾“å…¥å¸§
            metadata: å…ƒæ•°æ®
            
        Returns:
            (processed_frame, metadata) æˆ– None
        """
        if metadata is None:
            metadata = {}
        
        try:
            # æ·»åŠ åˆ°è¾“å…¥é˜Ÿåˆ—
            self.input_queue.put((frame, metadata), timeout=1)
            
            # è·å–è¾“å‡ºç»“æœ
            return self.output_queue.get(timeout=self.config['pipeline']['timeout'])
            
        except queue.Full:
            self.logger.warning("è¾“å…¥é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡å½“å‰å¸§")
            return None
        except queue.Empty:
            self.logger.warning("å¤„ç†è¶…æ—¶")
            return None
    
    def process_video(self, video_path: str, output_path: str = None, display: bool = True):
        """
        å¤„ç†è§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            display: æ˜¯å¦æ˜¾ç¤ºç»“æœ
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è¾“å‡º
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # å¯åŠ¨ç®¡é“
        self.start()
        
        self.logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        self.logger.info(f"åˆ†è¾¨ç‡: {width}x{height}, å¸§ç‡: {fps}, æ€»å¸§æ•°: {total_frames}")
        
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # å¤„ç†å¸§
                result = self.process_frame(frame, {'frame_id': frame_count})
                
                if result:
                    processed_frame, metadata = result
                    
                    # ä¿å­˜è¾“å‡º
                    if out:
                        out.write(processed_frame)
                    
                    # æ˜¾ç¤ºç»“æœ
                    if display:
                        cv2.imshow('Video Processing Pipeline', processed_frame)
                        
                        key = cv2.waitKey(1) & 0xFF
                        if key == ord('q'):
                            break
                        elif key == ord('s'):
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            cv2.imwrite(f'pipeline_result_{timestamp}.jpg', processed_frame)
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_count % 30 == 0:
                    progress = frame_count / total_frames * 100
                    self.logger.info(f"å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            self._print_final_stats()
            
        finally:
            cap.release()
            if out:
                out.release()
            if display:
                cv2.destroyAllWindows()
            self.stop()
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ¯ ç®¡é“å¤„ç†ç»Ÿè®¡:")
        print("=" * 50)
        
        for component in self.components:
            stats = component.get_performance_stats()
            if stats:
                print(f"\nğŸ“Š ç»„ä»¶: {stats['name']}")
                print(f"   å¤„ç†å¸§æ•°: {stats['processed_frames']}")
                print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']*1000:.2f}ms")
                print(f"   å¤„ç†å¸§ç‡: {stats['fps']:.1f} fps")
                print(f"   é”™è¯¯ç‡: {stats['error_rate']*100:.2f}%")
                print(f"   è¿è¡Œæ—¶é—´: {stats['uptime']:.1f}s")

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.frame_count = 0
        self.running = False
        self.monitor_thread = None
    
    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def update_frame_processed(self):
        """æ›´æ–°å·²å¤„ç†å¸§æ•°"""
        self.frame_count += 1
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            time.sleep(10)  # æ¯10ç§’ç›‘æ§ä¸€æ¬¡
            
            if self.frame_count > 0:
                runtime = time.time() - self.start_time
                fps = self.frame_count / runtime
                
                # ç³»ç»Ÿèµ„æºç›‘æ§
                cpu_percent = psutil.cpu_percent()
                memory_percent = psutil.virtual_memory().percent
                
                print(f"\nğŸ“ˆ æ€§èƒ½ç›‘æ§ - {datetime.now().strftime('%H:%M:%S')}")
                print(f"   å¤„ç†å¸§æ•°: {self.frame_count}")
                print(f"   å¹³å‡FPS: {fps:.2f}")
                print(f"   CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
                print(f"   å†…å­˜ä½¿ç”¨ç‡: {memory_percent:.1f}%")

# ä½¿ç”¨ç¤ºä¾‹
def demo_video_stream_pipeline():
    """è§†é¢‘æµå¤„ç†ç®¡é“æ¼”ç¤º"""
    print("ğŸ”§ è§†é¢‘æµå¤„ç†ç®¡é“æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config = {
        'pipeline': {
            'worker_threads': 2,
            'queue_size': 5,
            'timeout': 3
        },
        'components': [
            {
                'name': 'FrameFilter',
                'type': 'FrameFilterComponent',
                'enabled': True,
                'config': {'gaussian_blur': True, 'gaussian_kernel': (3, 3)}
            },
            {
                'name': 'MotionDetection',
                'type': 'MotionDetectionComponent',
                'enabled': True,
                'config': {'visualize': False, 'detection': {'method': 'adaptive'}}
            },
            {
                'name': 'ObjectTracking',
                'type': 'ObjectTrackingComponent',
                'enabled': True,
                'config': {'visualize': True, 'tracking': {'tracker_type': 'CSRT'}}
            }
        ],
        'monitoring': {
            'enabled': True,
            'stats_interval': 10
        }
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    with open('pipeline_config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºç®¡é“
    pipeline = VideoStreamPipeline('pipeline_config.json')
    
    # åˆ›å»ºæµ‹è¯•è§†é¢‘ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    test_video = "pipeline_test_video.mp4"
    if not os.path.exists(test_video):
        print("åˆ›å»ºæµ‹è¯•è§†é¢‘...")
        create_pipeline_test_video(test_video)
    
    # å¤„ç†è§†é¢‘
    try:
        pipeline.process_video(
            test_video,
            output_path="pipeline_output.mp4",
            display=True
        )
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")

def create_pipeline_test_video(output_path, duration=10, fps=30):
    """åˆ›å»ºç®¡é“æµ‹è¯•è§†é¢‘"""
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (640, 480))
    
    total_frames = duration * fps
    
    for i in range(total_frames):
        frame = np.ones((480, 640, 3), dtype=np.uint8) * 40
        
        t = i / fps
        
        # æ·»åŠ ç§»åŠ¨ç›®æ ‡
        x = int(50 + 500 * (t / duration))
        y = int(240 + 100 * np.sin(2 * np.pi * t))
        cv2.circle(frame, (x, y), 20, (255, 255, 255), -1)
        
        # æ·»åŠ å™ªå£°
        noise = np.random.randint(-10, 10, frame.shape, dtype=np.int16)
        frame = np.clip(frame.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        
        out.write(frame)
    
    out.release()
    print(f"ç®¡é“æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")

if __name__ == "__main__":
    demo_video_stream_pipeline()
```

### ğŸ¯ æŠ€æœ¯äº®ç‚¹åˆ†æ

è¿™ä¸ª**è§†é¢‘æµå¤„ç†ç®¡é“**å±•ç°äº†ç°ä»£è§†é¢‘å¤„ç†æ¶æ„çš„æ ¸å¿ƒç‰¹å¾ï¼š

#### ğŸ—ï¸ æ¨¡å—åŒ–æ¶æ„è®¾è®¡
- **æŠ½è±¡åŸºç±»**: ç»Ÿä¸€çš„ç»„ä»¶æ¥å£è§„èŒƒ
- **æ’ä»¶å¼è®¾è®¡**: æ”¯æŒåŠ¨æ€æ·»åŠ å’Œç§»é™¤ç»„ä»¶
- **é…ç½®é©±åŠ¨**: JSONé…ç½®æ–‡ä»¶æ§åˆ¶æ•´ä¸ªæµç¨‹
- **æ¾è€¦åˆè®¾è®¡**: ç»„ä»¶é—´ç‹¬ç«‹ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

#### ğŸš€ é«˜æ€§èƒ½å¤„ç†
- **å¤šçº¿ç¨‹å¤„ç†**: æ”¯æŒå¹¶è¡Œå¤„ç†æå‡æ€§èƒ½
- **é˜Ÿåˆ—ç®¡ç†**: å¼‚æ­¥å¤„ç†é¿å…é˜»å¡
- **å†…å­˜ä¼˜åŒ–**: é«˜æ•ˆçš„æ•°æ®ä¼ é€’æœºåˆ¶
- **é”™è¯¯æ¢å¤**: å¥å£®çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶

#### ğŸ“Š å®æ—¶ç›‘æ§
- **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§å¤„ç†æ€§èƒ½å’Œç³»ç»Ÿèµ„æº
- **ç»Ÿè®¡åˆ†æ**: è¯¦ç»†çš„ç»„ä»¶çº§æ€§èƒ½ç»Ÿè®¡
- **æ—¥å¿—ç³»ç»Ÿ**: å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œé”™è¯¯è¿½è¸ª

---

```mermaid
flowchart TD
    A[è§†é¢‘è¾“å…¥] --> B[è¾“å…¥é˜Ÿåˆ—]
    B --> C[å·¥ä½œçº¿ç¨‹æ± ]
    C --> D[å¸§è¿‡æ»¤ç»„ä»¶]
    D --> E[è¿åŠ¨æ£€æµ‹ç»„ä»¶]
    E --> F[ç›®æ ‡è¿½è¸ªç»„ä»¶]
    F --> G[è¾“å‡ºé˜Ÿåˆ—]
    G --> H[å¤„ç†ç»“æœ]
    
    I[é…ç½®æ–‡ä»¶] --> J[ç»„ä»¶åŠ è½½å™¨]
    J --> D
    J --> E
    J --> F
    
    K[æ€§èƒ½ç›‘æ§] --> L[ç»Ÿè®¡æ”¶é›†]
    L --> M[ç›‘æ§æŠ¥å‘Š]
    
    C --> K
    D --> K
    E --> K
    F --> K
    
    style A fill:#e3f2fd
    style H fill:#e8f5e8
    style I fill:#fff3e0
    style M fill:#f3e5f5
```

---

## ğŸ† ç¬¬ä¸€èŠ‚æ€»ç»“ï¼šè§†é¢‘æµåª’ä½“å·¥å‚çš„å®Œç¾è¿è½¬

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„**è§†é¢‘æµåª’ä½“å·¥å‚**ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿™ä¸ªç°ä»£åŒ–å·¥å‚çš„å››å¤§æ ¸å¿ƒéƒ¨é—¨ï¼š

### ğŸ¥ ç”Ÿäº§è½¦é—´ï¼šè§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ
- **ä¼ä¸šçº§åŠŸèƒ½**: æ”¯æŒå¤šç§è§†é¢‘æ ¼å¼çš„ä¸“ä¸šå¤„ç†
- **ä¿¡æ¯åˆ†æ**: æä¾›è¯¦ç»†çš„è§†é¢‘å…ƒæ•°æ®è§£æ  
- **æ ¼å¼è½¬æ¢**: æ™ºèƒ½çš„å‚æ•°è°ƒæ•´å’Œæ ¼å¼è½¬æ¢
- **æ€§èƒ½ç›‘æ§**: å®Œæ•´çš„å¤„ç†ç»Ÿè®¡å’Œæ—¥å¿—ç³»ç»Ÿ

### ğŸ” è´¨æ£€éƒ¨é—¨ï¼šå¸§é—´å·®åˆ†æ£€æµ‹å™¨
- **å¤šç®—æ³•æ”¯æŒ**: ç®€å•å·®åˆ†ã€è‡ªé€‚åº”å·®åˆ†ã€MOG2èƒŒæ™¯å»ºæ¨¡
- **æ™ºèƒ½å¤„ç†**: å™ªå£°è¿‡æ»¤ã€å½¢æ€å­¦ä¼˜åŒ–ã€è‡ªé€‚åº”é˜ˆå€¼
- **å®æ—¶åˆ†æ**: è¿åŠ¨åŒºåŸŸæ£€æµ‹å’Œç½®ä¿¡åº¦è¯„ä¼°
- **å¯è§†åŒ–è¾“å‡º**: ç›´è§‚çš„æ£€æµ‹ç»“æœå±•ç¤º

### ğŸ¯ è¿½è¸ªå°ç»„ï¼šå¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ
- **ç®—æ³•èåˆ**: KCFã€CSRTã€å¡å°”æ›¼æ»¤æ³¢ã€åŒˆç‰™åˆ©ç®—æ³•
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**: æ™ºèƒ½çš„ç›®æ ‡åˆ›å»ºã€æ›´æ–°ã€ç§»é™¤
- **è½¨è¿¹åˆ†æ**: è¯¦ç»†çš„è¿åŠ¨è½¨è¿¹å’Œè¡Œä¸ºåˆ†æ
- **é²æ£’æ€§**: å¤„ç†é®æŒ¡ã€æ¶ˆå¤±ã€é‡ç°ç­‰å¤æ‚æƒ…å†µ

### ğŸ”§ å·¥ç¨‹éƒ¨é—¨ï¼šè§†é¢‘æµå¤„ç†ç®¡é“
- **æ¨¡å—åŒ–æ¶æ„**: å¯æ‰©å±•çš„æ’ä»¶å¼ç»„ä»¶è®¾è®¡
- **é«˜æ€§èƒ½å¤„ç†**: å¤šçº¿ç¨‹ã€é˜Ÿåˆ—ç®¡ç†ã€å¼‚æ­¥å¤„ç†
- **é…ç½®é©±åŠ¨**: çµæ´»çš„JSONé…ç½®æ–‡ä»¶æ§åˆ¶
- **å®æ—¶ç›‘æ§**: æ€§èƒ½ç›‘æ§ã€ç»Ÿè®¡åˆ†æã€æ—¥å¿—è¿½è¸ª

### ğŸŒŸ æ ¸å¿ƒæˆå°±
1. **æŠ€æœ¯æ•´åˆ**: å°†4ç§ä¸åŒçš„è§†é¢‘å¤„ç†æŠ€æœ¯å®Œç¾èåˆ
2. **ä¼ä¸šçº§æ ‡å‡†**: æ‰€æœ‰ä»£ç éƒ½è¾¾åˆ°ç”Ÿäº§ç¯å¢ƒè´¨é‡è¦æ±‚
3. **æ¨¡å—åŒ–è®¾è®¡**: å¯é‡ç”¨ã€å¯æ‰©å±•çš„æ¶æ„è®¾è®¡
4. **å®ç”¨ä»·å€¼**: å¯ç›´æ¥åº”ç”¨äºå®é™…é¡¹ç›®çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

### ğŸš€ ä¸‹ä¸€ç«™é¢„å‘Š
å®Œæˆäº†è§†é¢‘å¤„ç†åŸºç¡€è®¾æ–½çš„å»ºè®¾ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥**é«˜é€Ÿå¤„ç†åŠ å·¥å‚**ï¼Œå­¦ä¹ å¦‚ä½•è¿ç”¨å¤šçº¿ç¨‹å’ŒGPUåŠ é€ŸæŠ€æœ¯ï¼Œå°†æˆ‘ä»¬çš„è§†é¢‘å¤„ç†æ€§èƒ½æå‡åˆ°ä¸€ä¸ªå…¨æ–°çš„æ°´å¹³ï¼

---

### ğŸ’¡ æ·±åº¦æ€è€ƒé¢˜

ä¸ºäº†å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡è§†é¢‘å¤„ç†æŠ€æœ¯ï¼Œè¯·æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š

#### ğŸ¤” æ€è€ƒé¢˜1ï¼šè§†é¢‘å¤„ç†ç³»ç»Ÿè®¾è®¡
å‡è®¾æ‚¨éœ€è¦ä¸ºä¸€ä¸ªå¤§å‹å•†åœºè®¾è®¡ä¸€å¥—æ™ºèƒ½ç›‘æ§ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿéœ€è¦åŒæ—¶å¤„ç†50è·¯æ‘„åƒå¤´çš„å®æ—¶è§†é¢‘æµï¼Œå¹¶è¿›è¡Œäººå‘˜è®¡æ•°ã€å¼‚å¸¸è¡Œä¸ºæ£€æµ‹å’Œè½¨è¿¹åˆ†æã€‚è¯·è®¾è®¡è¿™æ ·çš„ç³»ç»Ÿæ¶æ„ï¼Œè€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- å¦‚ä½•å¹³è¡¡å¤„ç†ç²¾åº¦å’Œå®æ—¶æ€§ï¼Ÿ
- å¦‚ä½•å¤„ç†ä¸åŒæ‘„åƒå¤´è§†è§’å’Œå…‰ç…§æ¡ä»¶çš„å·®å¼‚ï¼Ÿ
- å¦‚ä½•è®¾è®¡å­˜å‚¨å’Œæ£€ç´¢ç­–ç•¥æ¥å¤„ç†æµ·é‡è§†é¢‘æ•°æ®ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜2ï¼šç®—æ³•é€‰æ‹©ä¸ä¼˜åŒ–
åœ¨ç¤ºä¾‹2çš„å¸§é—´å·®åˆ†æ£€æµ‹å™¨ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¤šç§æ£€æµ‹æ–¹æ³•ï¼ˆsimpleã€adaptiveã€mog2ã€knnï¼‰ã€‚è¯·åˆ†æï¼š
- åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹©å“ªç§ç®—æ³•ï¼Ÿ
- å¦‚ä½•æ ¹æ®å®é™…ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”çš„å‚æ•°è°ƒä¼˜æœºåˆ¶ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜3ï¼šè¿½è¸ªç³»ç»Ÿçš„é²æ£’æ€§
å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿéœ€è¦å¤„ç†å„ç§å¤æ‚æƒ…å†µï¼Œå¦‚ç›®æ ‡é®æŒ¡ã€å¿«é€Ÿç§»åŠ¨ã€å°ºåº¦å˜åŒ–ç­‰ã€‚è¯·æ€è€ƒï¼š
- å¦‚ä½•æ”¹è¿›å¡å°”æ›¼æ»¤æ³¢å™¨æ¥æ›´å¥½åœ°å¤„ç†éçº¿æ€§è¿åŠ¨ï¼Ÿ
- åœ¨ç›®æ ‡é•¿æ—¶é—´è¢«é®æŒ¡åé‡æ–°å‡ºç°æ—¶ï¼Œå¦‚ä½•ç¡®ä¿èº«ä»½å…³è”çš„å‡†ç¡®æ€§ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªå­¦ä¹ å‹çš„è¿½è¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®å†å²æ•°æ®æ”¹è¿›è¿½è¸ªæ€§èƒ½ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜4ï¼šç³»ç»Ÿæ¶æ„çš„æ‰©å±•æ€§
è§†é¢‘æµå¤„ç†ç®¡é“é‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œè¯·è€ƒè™‘ï¼š
- å¦‚ä½•æ‰©å±•ç³»ç»Ÿæ”¯æŒåˆ†å¸ƒå¼å¤„ç†ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªé€šç”¨çš„ç»„ä»¶æ¥å£ï¼Œæ”¯æŒç¬¬ä¸‰æ–¹ç®—æ³•é›†æˆï¼Ÿ
- å¦‚ä½•å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼Œæ ¹æ®ç³»ç»Ÿè´Ÿè½½è‡ªåŠ¨è°ƒæ•´å¤„ç†æµç¨‹ï¼Ÿ

### ğŸ“š å­¦ä¹ å»ºè®®ä¸æœ€ä½³å®è·µ

#### ğŸ¯ æŠ€æœ¯æ·±åŒ–å»ºè®®
1. **æŒæ¡æ•°å­¦åŸºç¡€**ï¼šæ·±å…¥å­¦ä¹ çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¿¡å·å¤„ç†ç­‰æ•°å­¦åŸºç¡€ï¼Œè¿™å¯¹ç†è§£ç®—æ³•åŸç†è‡³å…³é‡è¦ã€‚

2. **å®è·µä¸ºä¸»**ï¼šå¤šåšå®é™…é¡¹ç›®ï¼Œå°è¯•å°†ç¤ºä¾‹ä»£ç åº”ç”¨åˆ°çœŸå®åœºæ™¯ä¸­ï¼Œå‘ç°å¹¶è§£å†³å®é™…é—®é¢˜ã€‚

3. **é˜…è¯»æºç **ï¼šæ·±å…¥ç ”è¯»OpenCVã€scikit-learnç­‰åº“çš„æºç ï¼Œç†è§£ç®—æ³•çš„å…·ä½“å®ç°ç»†èŠ‚ã€‚

4. **å…³æ³¨å‰æ²¿**ï¼šè·Ÿè¸ªè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ åœ¨è§†é¢‘å¤„ç†ä¸­çš„åº”ç”¨ã€‚

#### ğŸ”§ ç¼–ç¨‹å®è·µå»ºè®®
1. **ä»£ç è§„èŒƒ**ï¼šä¸¥æ ¼éµå¾ªPEP8ç¼–ç è§„èŒƒï¼Œç¼–å†™æ¸…æ™°ã€å¯ç»´æŠ¤çš„ä»£ç ã€‚

2. **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ ä½¿ç”¨profilerå·¥å…·åˆ†æä»£ç æ€§èƒ½ï¼Œè¯†åˆ«ç“¶é¢ˆå¹¶è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–ã€‚

3. **é”™è¯¯å¤„ç†**ï¼šè®¾è®¡å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å¼‚å¸¸æƒ…å†µä¸‹çš„ç¨³å®šè¿è¡Œã€‚

4. **å•å…ƒæµ‹è¯•**ï¼šä¸ºå…³é”®åŠŸèƒ½ç¼–å†™å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿå¯é æ€§ã€‚

#### ğŸ—ï¸ æ¶æ„è®¾è®¡å»ºè®®
1. **æ¨¡å—åŒ–æ€ç»´**ï¼šå§‹ç»ˆè€ƒè™‘ä»£ç çš„å¯é‡ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé‡‡ç”¨æ¾è€¦åˆã€é«˜å†…èšçš„è®¾è®¡åŸåˆ™ã€‚

2. **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†ç³»ç»Ÿå‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç ï¼Œæé«˜ç³»ç»Ÿçš„çµæ´»æ€§ã€‚

3. **ç›‘æ§ä½“ç³»**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³ç³»ç»Ÿé—®é¢˜ã€‚

4. **æ–‡æ¡£å®Œå–„**ï¼šç¼–å†™è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£å’Œç”¨æˆ·æ‰‹å†Œï¼Œæ–¹ä¾¿å›¢é˜Ÿåä½œå’Œç³»ç»Ÿç»´æŠ¤ã€‚

#### ğŸš€ èŒä¸šå‘å±•å»ºè®®
1. **è·¨é¢†åŸŸå­¦ä¹ **ï¼šè®¡ç®—æœºè§†è§‰ä¸æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—ç­‰é¢†åŸŸç»“åˆç´§å¯†ï¼Œå»ºè®®æ‹“å®½çŸ¥è¯†é¢ã€‚

2. **é¡¹ç›®ç»éªŒ**ï¼šå‚ä¸å¼€æºé¡¹ç›®æˆ–å®é™…å•†ä¸šé¡¹ç›®ï¼Œç§¯ç´¯ä¸°å¯Œçš„é¡¹ç›®ç»éªŒã€‚

3. **æŠ€æœ¯åˆ†äº«**ï¼šé€šè¿‡æŠ€æœ¯åšå®¢ã€æ¼”è®²ç­‰æ–¹å¼åˆ†äº«ç»éªŒï¼Œæå‡ä¸ªäººå½±å“åŠ›ã€‚

4. **æŒç»­å­¦ä¹ **ï¼šæŠ€æœ¯å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œä¿æŒæŒç»­å­¦ä¹ çš„ä¹ æƒ¯ï¼ŒåŠæ—¶æ›´æ–°çŸ¥è¯†ä½“ç³»ã€‚

### ğŸ‰ æœ¬èŠ‚å­¦ä¹ æˆæœæ£€éªŒ

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

#### âœ… ç†è®ºæŒæ¡
- [ ] ç†è§£è§†é¢‘æ•°æ®ç»“æ„å’Œç¼–è§£ç åŸç†
- [ ] æŒæ¡å¸§é—´å·®åˆ†ã€èƒŒæ™¯å»ºæ¨¡ç­‰è¿åŠ¨æ£€æµ‹ç®—æ³•
- [ ] ç†Ÿæ‚‰å¡å°”æ›¼æ»¤æ³¢ã€åŒˆç‰™åˆ©ç®—æ³•ç­‰è¿½è¸ªæŠ€æœ¯
- [ ] äº†è§£æ¨¡å—åŒ–æ¶æ„è®¾è®¡çš„æ ¸å¿ƒæ€æƒ³

#### âœ… å®è·µèƒ½åŠ›
- [ ] èƒ½å¤Ÿç‹¬ç«‹å®ç°è§†é¢‘è¯»å†™å’Œæ ¼å¼è½¬æ¢åŠŸèƒ½
- [ ] èƒ½å¤Ÿæ„å»ºè¿åŠ¨æ£€æµ‹å’Œç›®æ ‡è¿½è¸ªç³»ç»Ÿ
- [ ] èƒ½å¤Ÿè®¾è®¡å’Œå®ç°æ¨¡å—åŒ–çš„å¤„ç†ç®¡é“
- [ ] èƒ½å¤Ÿè¿›è¡Œç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

#### âœ… åº”ç”¨æ°´å¹³
- [ ] èƒ½å¤Ÿæ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ç®—æ³•
- [ ] èƒ½å¤Ÿè®¾è®¡ä¼ä¸šçº§çš„è§†é¢‘å¤„ç†è§£å†³æ–¹æ¡ˆ
- [ ] èƒ½å¤Ÿè§£å†³è§†é¢‘å¤„ç†ä¸­çš„å¸¸è§æŠ€æœ¯é—®é¢˜
- [ ] èƒ½å¤Ÿè¯„ä¼°å’Œæ”¹è¿›ç³»ç»Ÿæ€§èƒ½

---

## 37.2 é«˜é€Ÿå¤„ç†åŠ å·¥å‚ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒçš„ç¬¬äºŒç«™â€”â€”**é«˜é€Ÿå¤„ç†åŠ å·¥å‚**ï¼å¦‚æœè¯´ç¬¬ä¸€èŠ‚çš„è§†é¢‘æµåª’ä½“å·¥å‚è§£å†³äº†"èƒ½åšä»€ä¹ˆ"çš„é—®é¢˜ï¼Œé‚£ä¹ˆè¿™ä¸ªé«˜é€Ÿå¤„ç†åŠ å·¥å‚å°±è¦è§£å†³"å¦‚ä½•åšå¾—æ›´å¿«æ›´å¥½"çš„é—®é¢˜ã€‚

### ğŸ­ åŠ å·¥å‚è½¦é—´å¸ƒå±€

#### âš¡ å¹¶è¡Œå¤„ç†è½¦é—´ï¼šå¤šçº¿ç¨‹ååŒä½œä¸š
åœ¨è¿™ä¸ªè½¦é—´é‡Œï¼Œæˆ‘ä»¬çš„**å¤šçº¿ç¨‹å¤„ç†å¼•æ“**å°±åƒä¸€ä¸ªé«˜æ•ˆçš„æµæ°´çº¿ï¼š
- **ä»»åŠ¡åˆ†é…ç«™**ï¼šæ™ºèƒ½åœ°å°†è§†é¢‘å¤„ç†ä»»åŠ¡åˆ†é…ç»™å¤šä¸ªå·¥ä½œçº¿ç¨‹
- **åŒæ­¥åè°ƒä¸­å¿ƒ**ï¼šç¡®ä¿å¤šä¸ªçº¿ç¨‹ä¹‹é—´çš„åè°ƒä¸åŒæ­¥
- **è´Ÿè½½å‡è¡¡å™¨**ï¼šåŠ¨æ€è°ƒæ•´å„çº¿ç¨‹çš„å·¥ä½œè´Ÿè½½
- **ç»“æœæ±‡æ€»ç‚¹**ï¼šå°†å„çº¿ç¨‹çš„å¤„ç†ç»“æœé«˜æ•ˆåˆå¹¶

#### ğŸ–¥ï¸ GPUåŠ é€Ÿå¼•æ“ï¼šè¶…çº§è®¡ç®—æœºæˆ¿
è¿™é‡Œæ˜¯æ•´ä¸ªå·¥å‚çš„**ç®—åŠ›å¿ƒè„**ï¼Œä¸“é—¨å¤„ç†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼š
- **CUDAæ ¸å¿ƒé˜µåˆ—**ï¼šæˆç™¾ä¸Šåƒä¸ªå¤„ç†æ ¸å¿ƒå¹¶è¡Œå·¥ä½œ
- **å†…å­˜ç®¡ç†å•å…ƒ**ï¼šé«˜æ•ˆçš„GPUå†…å­˜åˆ†é…å’Œå›æ”¶
- **æ•°æ®ä¼ è¾“æ€»çº¿**ï¼šCPUä¸GPUä¹‹é—´çš„é«˜é€Ÿæ•°æ®é€šé“
- **è®¡ç®—è°ƒåº¦å™¨**ï¼šæ™ºèƒ½è°ƒåº¦GPUè®¡ç®—èµ„æº

#### ğŸ“Š æ€§èƒ½ç›‘æ§ä¸­å¿ƒï¼šå®æ—¶æ•ˆç‡åˆ†æå®¤
è¿™ä¸ªä¸­å¿ƒè´Ÿè´£ç›‘æ§å’Œä¼˜åŒ–æ•´ä¸ªå¤„ç†æµç¨‹ï¼š
- **æ€§èƒ½ä»ªè¡¨ç›˜**ï¼šå®æ—¶æ˜¾ç¤ºå¤„ç†é€Ÿåº¦ã€èµ„æºåˆ©ç”¨ç‡ç­‰å…³é”®æŒ‡æ ‡
- **ç“¶é¢ˆæ£€æµ‹å™¨**ï¼šè‡ªåŠ¨è¯†åˆ«å¤„ç†æµç¨‹ä¸­çš„æ€§èƒ½ç“¶é¢ˆ
- **ä¼˜åŒ–å»ºè®®ç³»ç»Ÿ**ï¼šåŸºäºç›‘æ§æ•°æ®æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®
- **èµ„æºé…ç½®ç®¡ç†**ï¼šåŠ¨æ€è°ƒæ•´ç³»ç»Ÿèµ„æºé…ç½®

#### ğŸ”§ ç®—æ³•ä¼˜åŒ–å·¥ä½œå°ï¼šæ•ˆç‡æå‡å®éªŒå®¤
ä¸“é—¨ç ”ç©¶å’Œå®ç°å„ç§ç®—æ³•ä¼˜åŒ–æŠ€æœ¯ï¼š
- **ç®—æ³•å‰–æå°**ï¼šæ·±å…¥åˆ†æç®—æ³•çš„è®¡ç®—å¤æ‚åº¦å’Œä¼˜åŒ–ç©ºé—´
- **å†…å­˜ä¼˜åŒ–åŒº**ï¼šç ”ç©¶æ•°æ®ç»“æ„å’Œå†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
- **ç¼“å­˜ç­–ç•¥ç«™**ï¼šè®¾è®¡å’Œå®ç°é«˜æ•ˆçš„ç¼“å­˜æœºåˆ¶
- **å‘é‡åŒ–å¤„ç†åŒº**ï¼šåˆ©ç”¨SIMDæŒ‡ä»¤é›†è¿›è¡Œå‘é‡åŒ–åŠ é€Ÿ

### ğŸ’» æŠ€æœ¯æ·±åº¦è§£æ

#### å®æ—¶å¤„ç†çš„æ€§èƒ½æŒ‘æˆ˜
åœ¨å®æ—¶è§†è§‰åº”ç”¨ä¸­ï¼Œæ€§èƒ½ä¼˜åŒ–é¢ä¸´ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š

```python
performance_challenges = {
    "è®¡ç®—å¯†é›†": {
        "æè¿°": "å›¾åƒå’Œè§†é¢‘å¤„ç†æ¶‰åŠå¤§é‡çŸ©é˜µè¿ç®—",
        "è§£å†³æ–¹æ¡ˆ": ["GPUå¹¶è¡Œè®¡ç®—", "SIMDå‘é‡åŒ–", "ç®—æ³•ä¼˜åŒ–"],
        "å…¸å‹åœºæ™¯": "å·ç§¯æ“ä½œã€æ»¤æ³¢å¤„ç†ã€ç‰¹å¾æå–"
    },
    "å†…å­˜å¯†é›†": {
        "æè¿°": "é«˜åˆ†è¾¨ç‡è§†é¢‘éœ€è¦å¤„ç†å¤§é‡æ•°æ®",
        "è§£å†³æ–¹æ¡ˆ": ["å†…å­˜æ± ç®¡ç†", "æ•°æ®é¢„å–", "é›¶æ‹·è´æŠ€æœ¯"],
        "å…¸å‹åœºæ™¯": "4Kè§†é¢‘å¤„ç†ã€å¤šè·¯è§†é¢‘æµ"
    },
    "å»¶è¿Ÿæ•æ„Ÿ": {
        "æè¿°": "å®æ—¶åº”ç”¨å¯¹å¤„ç†å»¶è¿Ÿè¦æ±‚æé«˜",
        "è§£å†³æ–¹æ¡ˆ": ["æµæ°´çº¿å¤„ç†", "å¼‚æ­¥å¤„ç†", "é¢„æµ‹æ€§ç¼“å­˜"],
        "å…¸å‹åœºæ™¯": "è§†é¢‘ä¼šè®®ã€ARæ¸²æŸ“ã€å®æ—¶ç›‘æ§"
    }
}
```

#### å¹¶è¡Œå¤„ç†çš„æ•°å­¦åŸç†
å¹¶è¡Œå¤„ç†çš„æ•ˆç‡æå‡éµå¾ªé˜¿å§†è¾¾å°”å®šå¾‹ï¼ˆAmdahl's Lawï¼‰ï¼š

```python
# å¹¶è¡ŒåŠ é€Ÿæ¯”è®¡ç®—
def amdahl_speedup(parallel_portion, num_processors):
    """
    è®¡ç®—ç†è®ºæœ€å¤§åŠ é€Ÿæ¯”
    
    Args:
        parallel_portion: å¯å¹¶è¡ŒåŒ–éƒ¨åˆ†çš„æ¯”ä¾‹ (0-1)
        num_processors: å¤„ç†å™¨æ•°é‡
    
    Returns:
        ç†è®ºåŠ é€Ÿæ¯”
    """
    serial_portion = 1 - parallel_portion
    speedup = 1 / (serial_portion + parallel_portion / num_processors)
    return speedup

# å®é™…åº”ç”¨ç¤ºä¾‹
scenarios = [
    {"name": "è§†é¢‘è§£ç ", "parallel": 0.85, "processors": 8},
    {"name": "å›¾åƒæ»¤æ³¢", "parallel": 0.95, "processors": 16}, 
    {"name": "ç‰¹å¾æ£€æµ‹", "parallel": 0.90, "processors": 12}
]

for scenario in scenarios:
    speedup = amdahl_speedup(scenario["parallel"], scenario["processors"])
    print(f"{scenario['name']}: ç†è®ºåŠ é€Ÿæ¯” {speedup:.2f}x")
```

### ğŸ¯ ç¤ºä¾‹1ï¼šå¤šçº¿ç¨‹è§†é¢‘å¤„ç†å¼•æ“

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªä¸“ä¸šçº§çš„å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å¼•æ“ï¼Œå±•ç¤ºå¹¶è¡Œå¤„ç†æŠ€æœ¯åœ¨å®æ—¶è§†è§‰åº”ç”¨ä¸­çš„å¼ºå¤§å¨åŠ›ã€‚

```python
"""
å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å¼•æ“ - é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†ç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†è§†é¢‘å¸§
2. æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œä»»åŠ¡è°ƒåº¦
3. å®æ—¶æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
4. å¯æ‰©å±•çš„å¤„ç†æµæ°´çº¿æ¶æ„
"""

import cv2
import numpy as np
import threading
import queue
import time
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable, Any
import psutil
import logging
from abc import ABC, abstractmethod

@dataclass
class FrameTask:
    """è§†é¢‘å¸§å¤„ç†ä»»åŠ¡"""
    frame_id: int
    frame: np.ndarray
    timestamp: float
    metadata: Dict[str, Any]

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    frame_id: int
    processed_frame: np.ndarray
    processing_time: float
    metadata: Dict[str, Any]
    success: bool = True
    error_message: str = ""

class FrameProcessor(ABC):
    """æŠ½è±¡å¸§å¤„ç†å™¨åŸºç±»"""
    
    @abstractmethod
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> np.ndarray:
        """å¤„ç†å•å¸§å›¾åƒ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """è·å–å¤„ç†å™¨åç§°"""
        pass

class GaussianBlurProcessor(FrameProcessor):
    """é«˜æ–¯æ¨¡ç³Šå¤„ç†å™¨"""
    
    def __init__(self, kernel_size: int = 15, sigma: float = 5.0):
        self.kernel_size = kernel_size
        self.sigma = sigma
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> np.ndarray:
        return cv2.GaussianBlur(frame, (self.kernel_size, self.kernel_size), self.sigma)
    
    def get_name(self) -> str:
        return f"GaussianBlur(kernel={self.kernel_size}, sigma={self.sigma})"

class EdgeDetectionProcessor(FrameProcessor):
    """è¾¹ç¼˜æ£€æµ‹å¤„ç†å™¨"""
    
    def __init__(self, low_threshold: int = 50, high_threshold: int = 150):
        self.low_threshold = low_threshold
        self.high_threshold = high_threshold
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> np.ndarray:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, self.low_threshold, self.high_threshold)
        return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    
    def get_name(self) -> str:
        return f"EdgeDetection(low={self.low_threshold}, high={self.high_threshold})"

class ColorEnhancementProcessor(FrameProcessor):
    """è‰²å½©å¢å¼ºå¤„ç†å™¨"""
    
    def __init__(self, alpha: float = 1.5, beta: int = 20):
        self.alpha = alpha  # å¯¹æ¯”åº¦æ§åˆ¶
        self.beta = beta    # äº®åº¦æ§åˆ¶
    
    def process(self, frame: np.ndarray, metadata: Dict[str, Any]) -> np.ndarray:
        enhanced = cv2.convertScaleAbs(frame, alpha=self.alpha, beta=self.beta)
        return enhanced
    
    def get_name(self) -> str:
        return f"ColorEnhancement(alpha={self.alpha}, beta={self.beta})"

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.frame_count = 0
        self.total_processing_time = 0
        self.processing_times = []
        self.cpu_usages = []
        self.memory_usages = []
        self.lock = threading.Lock()
    
    def record_frame_processed(self, processing_time: float):
        """è®°å½•å¸§å¤„ç†å®Œæˆ"""
        with self.lock:
            self.frame_count += 1
            self.total_processing_time += processing_time
            self.processing_times.append(processing_time)
            
            # ä¿æŒæœ€è¿‘1000å¸§çš„è®°å½•
            if len(self.processing_times) > 1000:
                self.processing_times.pop(0)
    
    def record_system_stats(self):
        """è®°å½•ç³»ç»ŸçŠ¶æ€"""
        cpu_percent = psutil.cpu_percent()
        memory_percent = psutil.virtual_memory().percent
        
        with self.lock:
            self.cpu_usages.append(cpu_percent)
            self.memory_usages.append(memory_percent)
            
            # ä¿æŒæœ€è¿‘100ä¸ªè®°å½•
            if len(self.cpu_usages) > 100:
                self.cpu_usages.pop(0)
            if len(self.memory_usages) > 100:
                self.memory_usages.pop(0)
    
    def get_stats(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            runtime = time.time() - self.start_time
            avg_fps = self.frame_count / runtime if runtime > 0 else 0
            avg_processing_time = (self.total_processing_time / self.frame_count 
                                 if self.frame_count > 0 else 0)
            
            recent_times = self.processing_times[-100:] if self.processing_times else [0]
            avg_recent_time = np.mean(recent_times)
            
            avg_cpu = np.mean(self.cpu_usages) if self.cpu_usages else 0
            avg_memory = np.mean(self.memory_usages) if self.memory_usages else 0
            
            return {
                "runtime": runtime,
                "processed_frames": self.frame_count,
                "average_fps": avg_fps,
                "average_processing_time": avg_processing_time,
                "recent_avg_processing_time": avg_recent_time,
                "average_cpu_usage": avg_cpu,
                "average_memory_usage": avg_memory
            }

class MultiThreadVideoProcessor:
    """å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å¼•æ“"""
    
    def __init__(self, max_workers: Optional[int] = None, queue_size: int = 100):
        """
        åˆå§‹åŒ–å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
            queue_size: ä»»åŠ¡é˜Ÿåˆ—å¤§å°
        """
        self.max_workers = max_workers or mp.cpu_count()
        self.queue_size = queue_size
        
        # ä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—
        self.task_queue = queue.Queue(maxsize=queue_size)
        self.result_queue = queue.Queue()
        
        # å¤„ç†å™¨åˆ—è¡¨å’Œæ‰§è¡Œå™¨
        self.processors: List[FrameProcessor] = []
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        # æ€§èƒ½ç›‘æ§
        self.monitor = PerformanceMonitor()
        
        # æ§åˆ¶æ ‡å¿—
        self.running = False
        self.processing_threads = []
        
        # æ—¥å¿—è®¾ç½®
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
    
    def add_processor(self, processor: FrameProcessor):
        """æ·»åŠ å¸§å¤„ç†å™¨"""
        self.processors.append(processor)
        self.logger.info(f"æ·»åŠ å¤„ç†å™¨: {processor.get_name()}")
    
    def _worker_thread(self, worker_id: int):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        self.logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")
        
        while self.running:
            try:
                # è·å–ä»»åŠ¡
                task = self.task_queue.get(timeout=1.0)
                if task is None:  # ç»“æŸä¿¡å·
                    break
                
                # å¤„ç†å¸§
                start_time = time.time()
                result = self._process_frame(task)
                processing_time = time.time() - start_time
                
                # è®°å½•æ€§èƒ½
                self.monitor.record_frame_processed(processing_time)
                
                # è¿”å›ç»“æœ
                self.result_queue.put(result)
                self.task_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¤„ç†é”™è¯¯: {e}")
                result = ProcessingResult(
                    frame_id=task.frame_id,
                    processed_frame=task.frame,
                    processing_time=0,
                    metadata=task.metadata,
                    success=False,
                    error_message=str(e)
                )
                self.result_queue.put(result)
                self.task_queue.task_done()
        
        self.logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} ç»“æŸ")
    
    def _process_frame(self, task: FrameTask) -> ProcessingResult:
        """å¤„ç†å•å¸§"""
        try:
            processed_frame = task.frame.copy()
            
            # ä¾æ¬¡åº”ç”¨æ‰€æœ‰å¤„ç†å™¨
            for processor in self.processors:
                processed_frame = processor.process(processed_frame, task.metadata)
            
            return ProcessingResult(
                frame_id=task.frame_id,
                processed_frame=processed_frame,
                processing_time=time.time() - task.timestamp,
                metadata=task.metadata,
                success=True
            )
            
        except Exception as e:
            return ProcessingResult(
                frame_id=task.frame_id,
                processed_frame=task.frame,
                processing_time=0,
                metadata=task.metadata,
                success=False,
                error_message=str(e)
            )
    
    def start(self):
        """å¯åŠ¨å¤„ç†å™¨"""
        if self.running:
            return
        
        self.running = True
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(self.max_workers):
            thread = threading.Thread(target=self._worker_thread, args=(i,))
            thread.start()
            self.processing_threads.append(thread)
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=self._monitor_system)
        monitor_thread.start()
        self.processing_threads.append(monitor_thread)
        
        self.logger.info("å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å™¨å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢å¤„ç†å™¨"""
        if not self.running:
            return
        
        self.running = False
        
        # å‘é€ç»“æŸä¿¡å·
        for _ in range(self.max_workers):
            self.task_queue.put(None)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for thread in self.processing_threads:
            thread.join()
        
        self.processing_threads.clear()
        self.logger.info("å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å™¨å·²åœæ­¢")
    
    def _monitor_system(self):
        """ç³»ç»Ÿç›‘æ§çº¿ç¨‹"""
        while self.running:
            self.monitor.record_system_stats()
            time.sleep(1.0)  # æ¯ç§’ç›‘æ§ä¸€æ¬¡
    
    def submit_frame(self, frame_id: int, frame: np.ndarray, metadata: Dict[str, Any] = None) -> bool:
        """æäº¤å¸§å¤„ç†ä»»åŠ¡"""
        if metadata is None:
            metadata = {}
        
        task = FrameTask(
            frame_id=frame_id,
            frame=frame,
            timestamp=time.time(),
            metadata=metadata
        )
        
        try:
            self.task_queue.put(task, timeout=0.1)
            return True
        except queue.Full:
            self.logger.warning(f"ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡å¸§ {frame_id}")
            return False
    
    def get_result(self, timeout: float = 1.0) -> Optional[ProcessingResult]:
        """è·å–å¤„ç†ç»“æœ"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_performance_stats(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self.monitor.get_stats()
    
    def process_video(self, video_path: str, output_path: str = None, display: bool = True):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è¾“å‡º
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        self.start()
        
        try:
            frame_id = 0
            submitted_frames = 0
            received_results = 0
            results_buffer = {}  # ç”¨äºä¿è¯å¸§é¡ºåº
            next_output_frame = 0
            
            self.logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
            self.logger.info(f"åˆ†è¾¨ç‡: {width}x{height}, å¸§ç‡: {fps}, æ€»å¸§æ•°: {total_frames}")
            
            while True:
                # è¯»å–å¹¶æäº¤æ–°å¸§
                ret, frame = cap.read()
                if ret:
                    if self.submit_frame(frame_id, frame):
                        submitted_frames += 1
                    frame_id += 1
                
                # è·å–å¤„ç†ç»“æœ
                result = self.get_result(timeout=0.01)
                if result:
                    received_results += 1
                    results_buffer[result.frame_id] = result
                    
                    # æŒ‰é¡ºåºè¾“å‡ºç»“æœ
                    while next_output_frame in results_buffer:
                        current_result = results_buffer.pop(next_output_frame)
                        
                        if current_result.success:
                            # ä¿å­˜è¾“å‡º
                            if out:
                                out.write(current_result.processed_frame)
                            
                            # æ˜¾ç¤ºç»“æœ
                            if display:
                                cv2.imshow('å¤„ç†ç»“æœ', current_result.processed_frame)
                                key = cv2.waitKey(1) & 0xFF
                                if key == ord('q'):
                                    ret = False
                                elif key == ord('s'):
                                    cv2.imwrite(f'frame_{next_output_frame}.jpg', 
                                              current_result.processed_frame)
                        
                        next_output_frame += 1
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if not ret and received_results >= submitted_frames:
                    break
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_id % 30 == 0:
                    stats = self.get_performance_stats()
                    progress = (frame_id / total_frames * 100) if total_frames > 0 else 0
                    self.logger.info(
                        f"è¿›åº¦: {progress:.1f}% | "
                        f"FPS: {stats['average_fps']:.1f} | "
                        f"CPU: {stats['average_cpu_usage']:.1f}% | "
                        f"å†…å­˜: {stats['average_memory_usage']:.1f}%"
                    )
            
            # ç­‰å¾…æ‰€æœ‰ç»“æœ
            while received_results < submitted_frames:
                result = self.get_result()
                if result:
                    received_results += 1
                    if result.success and out:
                        out.write(result.processed_frame)
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            final_stats = self.get_performance_stats()
            self.logger.info("å¤„ç†å®Œæˆï¼")
            self.logger.info(f"æ€»å¸§æ•°: {final_stats['processed_frames']}")
            self.logger.info(f"å¹³å‡FPS: {final_stats['average_fps']:.2f}")
            self.logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {final_stats['average_processing_time']*1000:.2f}ms")
            self.logger.info(f"å¹³å‡CPUä½¿ç”¨: {final_stats['average_cpu_usage']:.1f}%")
            self.logger.info(f"å¹³å‡å†…å­˜ä½¿ç”¨: {final_stats['average_memory_usage']:.1f}%")
            
        finally:
            cap.release()
            if out:
                out.release()
            if display:
                cv2.destroyAllWindows()
            self.stop()

# ä½¿ç”¨ç¤ºä¾‹
def demo_multithread_video_processor():
    """å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å™¨æ¼”ç¤º"""
    print("ğŸš€ å¤šçº¿ç¨‹è§†é¢‘å¤„ç†å¼•æ“æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = MultiThreadVideoProcessor(max_workers=4, queue_size=50)
    
    # æ·»åŠ å¤„ç†å™¨
    processor.add_processor(GaussianBlurProcessor(kernel_size=15, sigma=5.0))
    processor.add_processor(ColorEnhancementProcessor(alpha=1.3, beta=15))
    processor.add_processor(EdgeDetectionProcessor(low_threshold=50, high_threshold=150))
    
    # åˆ›å»ºæµ‹è¯•è§†é¢‘ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    test_video = "multithread_test_video.mp4"
    if not cv2.os.path.exists(test_video):
        print("åˆ›å»ºæµ‹è¯•è§†é¢‘...")
        create_test_video(test_video)
    
    # å¤„ç†è§†é¢‘
    try:
        processor.process_video(
            test_video,
            output_path="multithread_output.mp4",
            display=True
        )
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")

def create_test_video(output_path: str, duration: int = 10, fps: int = 30):
    """åˆ›å»ºæµ‹è¯•è§†é¢‘"""
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (640, 480))
    
    total_frames = duration * fps
    
    for i in range(total_frames):
        # åˆ›å»ºåŠ¨æ€å†…å®¹
        frame = np.ones((480, 640, 3), dtype=np.uint8) * 50
        
        t = i / fps
        
        # æ·»åŠ ç§»åŠ¨çš„å‡ ä½•å›¾å½¢
        center_x = int(320 + 200 * np.sin(2 * np.pi * t / 3))
        center_y = int(240 + 100 * np.cos(2 * np.pi * t / 2))
        
        cv2.circle(frame, (center_x, center_y), 50, (255, 255, 255), -1)
        cv2.rectangle(frame, (100, 100), (200, 200), (255, 0, 0), 3)
        cv2.line(frame, (0, 0), (640, 480), (0, 255, 0), 2)
        
        # æ·»åŠ æ–‡å­—
        cv2.putText(frame, f'Frame: {i}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        # æ·»åŠ å™ªå£°
        noise = np.random.randint(-20, 20, frame.shape, dtype=np.int16)
        frame = np.clip(frame.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        
        out.write(frame)
    
    out.release()
    print(f"æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")

if __name__ == "__main__":
    demo_multithread_video_processor()
```

---

### ğŸ¯ ç¤ºä¾‹2ï¼šGPUåŠ é€Ÿå¤„ç†ç³»ç»Ÿ

åœ¨é«˜é€Ÿå¤„ç†åŠ å·¥å‚çš„æ ¸å¿ƒåŒºåŸŸï¼Œæˆ‘ä»¬éƒ¨ç½²äº†æœ€å¼ºå¤§çš„GPUåŠ é€Ÿå¼•æ“ã€‚è¿™ä¸ªç³»ç»Ÿèƒ½å¤Ÿå°†è®¡ç®—å¯†é›†å‹çš„è§†è§‰å¤„ç†ä»»åŠ¡çš„æ€§èƒ½æå‡10-100å€ï¼

```python
"""
GPUåŠ é€Ÿè§†é¢‘å¤„ç†ç³»ç»Ÿ - è¶…çº§è®¡ç®—å¼•æ“
åŠŸèƒ½ï¼š
1. CUDAåŠ é€Ÿçš„å›¾åƒå¤„ç†æ“ä½œ
2. GPUå†…å­˜æ± ç®¡ç†å’Œä¼˜åŒ–
3. CPU-GPUå¼‚æ­¥æ•°æ®ä¼ è¾“
4. æ‰¹å¤„ç†ä¼˜åŒ–å’Œæµæ°´çº¿å¤„ç†
"""

import cv2
import numpy as np
import time
from typing import List, Tuple, Optional, Dict, Any
import threading
import queue
from dataclasses import dataclass
import logging

# å°è¯•å¯¼å…¥GPUåŠ é€Ÿåº“
try:
    import cupy as cp
    import cupyx.scipy.ndimage as ndi
    GPU_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: CuPyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUæ¨¡æ‹ŸGPUæ“ä½œ")
    GPU_AVAILABLE = False

@dataclass
class GPUProcessingConfig:
    """GPUå¤„ç†é…ç½®"""
    batch_size: int = 4
    memory_pool_size: int = 1024 * 1024 * 1024  # 1GB
    use_pinned_memory: bool = True
    enable_streams: bool = True
    max_concurrent_streams: int = 4

class HighPerformanceVideoProcessor:
    """é«˜æ€§èƒ½è§†é¢‘å¤„ç†å™¨"""
    
    def __init__(self, config: GPUProcessingConfig = None):
        self.config = config or GPUProcessingConfig()
        self.logger = logging.getLogger(__name__)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.frame_count = 0
        self.total_gpu_time = 0
        self.start_time = None
    
    def gaussian_blur_gpu_batch(self, frames: List[np.ndarray], kernel_size: int = 15, 
                               sigma: float = 5.0) -> List[np.ndarray]:
        """GPUæ‰¹å¤„ç†é«˜æ–¯æ¨¡ç³Š"""
        if not GPU_AVAILABLE:
            return self._gaussian_blur_cpu_fallback(frames, kernel_size, sigma)
        
        results = []
        try:
            for frame in frames:
                # è½¬æ¢åˆ°GPU
                gpu_frame = cp.asarray(frame, dtype=cp.float32)
                
                # GPUé«˜æ–¯æ¨¡ç³Š
                if len(gpu_frame.shape) == 3:
                    # å¤„ç†å½©è‰²å›¾åƒ
                    blurred_channels = []
                    for c in range(gpu_frame.shape[2]):
                        channel = gpu_frame[:, :, c]
                        blurred = ndi.gaussian_filter(channel, sigma=sigma)
                        blurred_channels.append(blurred)
                    gpu_result = cp.stack(blurred_channels, axis=2)
                else:
                    # å¤„ç†ç°åº¦å›¾åƒ
                    gpu_result = ndi.gaussian_filter(gpu_frame, sigma=sigma)
                
                # è½¬æ¢å›CPU
                result = cp.asnumpy(gpu_result.astype(cp.uint8))
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"GPUå¤„ç†å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
            return self._gaussian_blur_cpu_fallback(frames, kernel_size, sigma)
    
    def _gaussian_blur_cpu_fallback(self, frames: List[np.ndarray], 
                                   kernel_size: int, sigma: float) -> List[np.ndarray]:
        """CPUå¤‡ç”¨é«˜æ–¯æ¨¡ç³Š"""
        results = []
        for frame in frames:
            blurred = cv2.GaussianBlur(frame, (kernel_size, kernel_size), sigma)
            results.append(blurred)
        return results
    
    def process_video_batch(self, video_path: str, output_path: str = None, display: bool = True):
        """æ‰¹å¤„ç†æ¨¡å¼å¤„ç†è§†é¢‘"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è¾“å‡º
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        self.start_time = time.time()
        frame_buffer = []
        processed_count = 0
        
        self.logger.info(f"å¼€å§‹GPUæ‰¹å¤„ç†è§†é¢‘: {video_path}")
        self.logger.info(f"åˆ†è¾¨ç‡: {width}x{height}, å¸§ç‡: {fps}, æ€»å¸§æ•°: {total_frames}")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    # å¤„ç†å‰©ä½™å¸§
                    if frame_buffer:
                        self._process_frame_batch(frame_buffer, out, display)
                        processed_count += len(frame_buffer)
                    break
                
                frame_buffer.append(frame)
                
                # æ‰¹å¤„ç†
                if len(frame_buffer) >= self.config.batch_size:
                    self._process_frame_batch(frame_buffer, out, display)
                    processed_count += len(frame_buffer)
                    frame_buffer = []
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if processed_count % (self.config.batch_size * 5) == 0:
                        self._print_progress(processed_count, total_frames)
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            self._print_final_stats(processed_count)
            
        finally:
            cap.release()
            if out:
                out.release()
            if display:
                cv2.destroyAllWindows()
    
    def _process_frame_batch(self, frames: List[np.ndarray], out, display: bool):
        """å¤„ç†å¸§æ‰¹æ¬¡"""
        gpu_start = time.time()
        
        # GPUæ‰¹å¤„ç†
        blurred_frames = self.gaussian_blur_gpu_batch(frames, kernel_size=15, sigma=3.0)
        
        gpu_time = time.time() - gpu_start
        self.total_gpu_time += gpu_time
        
        # è¾“å‡ºç»“æœ
        for frame in blurred_frames:
            if out:
                out.write(frame)
            
            if display:
                cv2.imshow('GPUåŠ é€Ÿå¤„ç†ç»“æœ', frame)
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    return
        
        self.frame_count += len(frames)
    
    def _print_progress(self, processed_count: int, total_frames: int):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        if self.start_time is None:
            return
        
        elapsed = time.time() - self.start_time
        progress = processed_count / total_frames * 100 if total_frames > 0 else 0
        fps = processed_count / elapsed if elapsed > 0 else 0
        
        self.logger.info(f"è¿›åº¦: {progress:.1f}% | å¤„ç†FPS: {fps:.1f} | GPUå¯ç”¨: {GPU_AVAILABLE}")
    
    def _print_final_stats(self, processed_count: int):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        total_time = time.time() - self.start_time
        avg_fps = processed_count / total_time if total_time > 0 else 0
        
        print("\nâš¡ GPUåŠ é€Ÿå¤„ç†ç»Ÿè®¡:")
        print("=" * 50)
        print(f"æ€»å¤„ç†å¸§æ•°: {processed_count}")
        print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        print(f"å¹³å‡å¤„ç†FPS: {avg_fps:.2f}")
        print(f"GPUå¤„ç†æ—¶é—´: {self.total_gpu_time:.2f}ç§’")
        print(f"GPUå¯ç”¨çŠ¶æ€: {GPU_AVAILABLE}")

# ä½¿ç”¨ç¤ºä¾‹
def demo_gpu_acceleration():
    """GPUåŠ é€Ÿæ¼”ç¤º"""
    print("âš¡ GPUåŠ é€Ÿè§†é¢‘å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    config = GPUProcessingConfig(batch_size=8)
    processor = HighPerformanceVideoProcessor(config)
    
    # åˆ›å»ºæµ‹è¯•è§†é¢‘
    test_video = "gpu_test_video.mp4"
    if not cv2.os.path.exists(test_video):
        print("åˆ›å»ºGPUæµ‹è¯•è§†é¢‘...")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(test_video, fourcc, 30, (640, 480))
        
        for i in range(300):  # 10ç§’è§†é¢‘
            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            cv2.putText(frame, f'Frame: {i}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            out.write(frame)
        
        out.release()
        print(f"æµ‹è¯•è§†é¢‘åˆ›å»ºå®Œæˆ: {test_video}")
    
    try:
        processor.process_video_batch(test_video, output_path="gpu_output.mp4", display=True)
    except Exception as e:
        print(f"GPUå¤„ç†å¤±è´¥: {e}")

if __name__ == "__main__":
    demo_gpu_acceleration()
```

---

### ğŸ¯ ç¤ºä¾‹3ï¼šæ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–ç³»ç»Ÿ

åœ¨é«˜é€Ÿå¤„ç†åŠ å·¥å‚çš„ç›‘æ§ä¸­å¿ƒï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€å¥—å…ˆè¿›çš„æ€§èƒ½ç›‘æ§ä¸è‡ªåŠ¨ä¼˜åŒ–ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿå®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½å¹¶è¿›è¡Œæ™ºèƒ½ä¼˜åŒ–ã€‚

```python
"""
æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–ç³»ç»Ÿ - æ™ºèƒ½æ•ˆç‡åˆ†æå™¨
åŠŸèƒ½ï¼š
1. å®æ—¶æ€§èƒ½ç›‘æ§å’Œåˆ†æ
2. ç“¶é¢ˆè‡ªåŠ¨æ£€æµ‹å’ŒæŠ¥å‘Š
3. åŠ¨æ€è´Ÿè½½å‡è¡¡ä¼˜åŒ–
4. æ™ºèƒ½å‚æ•°è°ƒä¼˜å»ºè®®
"""

import time
import threading
import psutil
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import logging
from collections import deque

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: float
    cpu_usage: float
    memory_usage: float
    gpu_usage: float = 0.0
    processing_fps: float = 0.0
    queue_size: int = 0
    thread_count: int = 0
    frame_drop_rate: float = 0.0

@dataclass
class OptimizationSuggestion:
    """ä¼˜åŒ–å»ºè®®"""
    category: str
    priority: str  # high, medium, low
    description: str
    action: str
    expected_improvement: str

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, history_size: int = 1000):
        self.history_size = history_size
        self.metrics_history = deque(maxlen=history_size)
        self.processing_times = deque(maxlen=history_size)
        self.lock = threading.Lock()
        
        # æ€§èƒ½åŸºå‡†
        self.performance_thresholds = {
            'cpu_high': 80.0,
            'memory_high': 85.0,
            'fps_low': 15.0,
            'frame_drop_high': 5.0
        }
    
    def record_metrics(self, metrics: PerformanceMetrics):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            self.metrics_history.append(metrics)
    
    def record_processing_time(self, processing_time: float):
        """è®°å½•å¤„ç†æ—¶é—´"""
        with self.lock:
            self.processing_times.append(processing_time)
    
    def get_current_performance(self) -> Optional[PerformanceMetrics]:
        """è·å–å½“å‰æ€§èƒ½"""
        with self.lock:
            if self.metrics_history:
                return self.metrics_history[-1]
        return None
    
    def get_average_metrics(self, window_size: int = 60) -> Dict[str, float]:
        """è·å–å¹³å‡æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            if len(self.metrics_history) < window_size:
                metrics_window = list(self.metrics_history)
            else:
                metrics_window = list(self.metrics_history)[-window_size:]
            
            if not metrics_window:
                return {}
            
            return {
                'avg_cpu_usage': np.mean([m.cpu_usage for m in metrics_window]),
                'avg_memory_usage': np.mean([m.memory_usage for m in metrics_window]),
                'avg_processing_fps': np.mean([m.processing_fps for m in metrics_window]),
                'avg_frame_drop_rate': np.mean([m.frame_drop_rate for m in metrics_window])
            }
    
    def detect_bottlenecks(self) -> List[str]:
        """æ£€æµ‹æ€§èƒ½ç“¶é¢ˆ"""
        current_metrics = self.get_current_performance()
        if not current_metrics:
            return []
        
        bottlenecks = []
        
        # CPUç“¶é¢ˆæ£€æµ‹
        if current_metrics.cpu_usage > self.performance_thresholds['cpu_high']:
            bottlenecks.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {current_metrics.cpu_usage:.1f}%")
        
        # å†…å­˜ç“¶é¢ˆæ£€æµ‹
        if current_metrics.memory_usage > self.performance_thresholds['memory_high']:
            bottlenecks.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {current_metrics.memory_usage:.1f}%")
        
        # å¤„ç†é€Ÿåº¦ç“¶é¢ˆ
        if current_metrics.processing_fps < self.performance_thresholds['fps_low']:
            bottlenecks.append(f"å¤„ç†FPSè¿‡ä½: {current_metrics.processing_fps:.1f}")
        
        return bottlenecks
    
    def get_optimization_suggestions(self) -> List[OptimizationSuggestion]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        current_metrics = self.get_current_performance()
        
        if not current_metrics:
            return suggestions
        
        # CPUä¼˜åŒ–å»ºè®®
        if current_metrics.cpu_usage > 70:
            suggestions.append(OptimizationSuggestion(
                category="CPU",
                priority="high" if current_metrics.cpu_usage > 85 else "medium",
                description=f"CPUä½¿ç”¨ç‡ {current_metrics.cpu_usage:.1f}% è¾ƒé«˜",
                action="å‡å°‘å·¥ä½œçº¿ç¨‹æ•°é‡æˆ–ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                expected_improvement="é™ä½CPUä½¿ç”¨ç‡10-20%"
            ))
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if current_metrics.memory_usage > 75:
            suggestions.append(OptimizationSuggestion(
                category="Memory",
                priority="high" if current_metrics.memory_usage > 90 else "medium",
                description=f"å†…å­˜ä½¿ç”¨ç‡ {current_metrics.memory_usage:.1f}% è¾ƒé«˜",
                action="å¯ç”¨å†…å­˜æ± ç®¡ç†æˆ–å‡å°‘ç¼“å­˜å¤§å°",
                expected_improvement="é™ä½å†…å­˜ä½¿ç”¨ç‡15-25%"
            ))
        
        # å¤„ç†é€Ÿåº¦ä¼˜åŒ–å»ºè®®
        if current_metrics.processing_fps < 20:
            suggestions.append(OptimizationSuggestion(
                category="Processing",
                priority="high",
                description=f"å¤„ç†FPS {current_metrics.processing_fps:.1f} è¿‡ä½",
                action="å¯ç”¨GPUåŠ é€Ÿæˆ–å¢åŠ å¹¶è¡Œå¤„ç†çº¿ç¨‹",
                expected_improvement="æå‡å¤„ç†é€Ÿåº¦2-5å€"
            ))
        
        return suggestions

class PerformanceMonitoringSystem:
    """æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.profiler = PerformanceProfiler()
        self.monitoring_thread = None
        self.running = False
        self.monitor_interval = 1.0  # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
        
        # ç»Ÿè®¡æ•°æ®
        self.total_frames_processed = 0
        self.total_processing_time = 0
        self.start_time = time.time()
        
        self.logger = logging.getLogger(__name__)
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        if self.running:
            return
        
        self.running = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.start()
        self.logger.info("æ€§èƒ½ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        self.logger.info("æ€§èƒ½ç›‘æ§ç³»ç»Ÿå·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†ç³»ç»Ÿæ€§èƒ½æ•°æ®
                metrics = self._collect_system_metrics()
                self.profiler.record_metrics(metrics)
                
                # æ£€æµ‹ç“¶é¢ˆ
                bottlenecks = self.profiler.detect_bottlenecks()
                if bottlenecks:
                    self.logger.warning(f"æ£€æµ‹åˆ°æ€§èƒ½ç“¶é¢ˆ: {bottlenecks}")
                
                time.sleep(self.monitor_interval)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(self.monitor_interval)
    
    def _collect_system_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        # å¤„ç†æ€§èƒ½æŒ‡æ ‡
        current_time = time.time()
        runtime = current_time - self.start_time
        processing_fps = self.total_frames_processed / runtime if runtime > 0 else 0
        
        return PerformanceMetrics(
            timestamp=current_time,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            processing_fps=processing_fps,
            thread_count=threading.active_count(),
            frame_drop_rate=0.0
        )
    
    def record_frame_processed(self, processing_time: float):
        """è®°å½•å¸§å¤„ç†å®Œæˆ"""
        self.total_frames_processed += 1
        self.total_processing_time += processing_time
        self.profiler.record_processing_time(processing_time)
    
    def get_performance_report(self) -> Dict[str, any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        current_metrics = self.profiler.get_current_performance()
        avg_metrics = self.profiler.get_average_metrics()
        bottlenecks = self.profiler.detect_bottlenecks()
        suggestions = self.profiler.get_optimization_suggestions()
        
        runtime = time.time() - self.start_time
        
        return {
            'runtime_seconds': runtime,
            'total_frames_processed': self.total_frames_processed,
            'average_fps': self.total_frames_processed / runtime if runtime > 0 else 0,
            'current_metrics': current_metrics,
            'average_metrics': avg_metrics,
            'bottlenecks': bottlenecks,
            'optimization_suggestions': [
                {
                    'category': s.category,
                    'priority': s.priority,
                    'description': s.description,
                    'action': s.action,
                    'expected_improvement': s.expected_improvement
                } for s in suggestions
            ]
        }
    
    def print_performance_dashboard(self):
        """æ‰“å°æ€§èƒ½ä»ªè¡¨ç›˜"""
        report = self.get_performance_report()
        
        print("\n" + "="*60)
        print("ğŸ”¥ é«˜é€Ÿå¤„ç†åŠ å·¥å‚ - æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜")
        print("="*60)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {report['runtime_seconds']:.1f}ç§’")
        print(f"ğŸ“Š å¤„ç†å¸§æ•°: {report['total_frames_processed']}")
        print(f"ğŸ“ˆ å¹³å‡FPS: {report['average_fps']:.2f}")
        
        # å½“å‰æ€§èƒ½
        if report['current_metrics']:
            current = report['current_metrics']
            print(f"\nğŸ“Š å½“å‰æ€§èƒ½æŒ‡æ ‡:")
            print(f"   CPUä½¿ç”¨ç‡: {current.cpu_usage:.1f}%")
            print(f"   å†…å­˜ä½¿ç”¨ç‡: {current.memory_usage:.1f}%")
            print(f"   å¤„ç†FPS: {current.processing_fps:.1f}")
            print(f"   æ´»è·ƒçº¿ç¨‹: {current.thread_count}")
        
        # æ€§èƒ½ç“¶é¢ˆ
        if report['bottlenecks']:
            print(f"\nâš ï¸  æ€§èƒ½ç“¶é¢ˆ:")
            for bottleneck in report['bottlenecks']:
                print(f"   - {bottleneck}")
        else:
            print(f"\nâœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæœªæ£€æµ‹åˆ°æ€§èƒ½ç“¶é¢ˆ")
        
        # ä¼˜åŒ–å»ºè®®
        if report['optimization_suggestions']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in report['optimization_suggestions']:
                priority_emoji = "ğŸ”´" if suggestion['priority'] == "high" else "ğŸŸ¡" if suggestion['priority'] == "medium" else "ğŸŸ¢"
                print(f"   {priority_emoji} [{suggestion['category']}] {suggestion['description']}")
                print(f"      å»ºè®®: {suggestion['action']}")
                print(f"      é¢„æœŸæ”¹è¿›: {suggestion['expected_improvement']}")
        
        print("="*60)

# ä½¿ç”¨ç¤ºä¾‹
def demo_performance_monitoring():
    """æ€§èƒ½ç›‘æ§æ¼”ç¤º"""
    print("ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç›‘æ§ç³»ç»Ÿ
    monitor = PerformanceMonitoringSystem()
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring()
    
    try:
        # æ¨¡æ‹Ÿè§†é¢‘å¤„ç†ä»»åŠ¡
        for i in range(100):
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            processing_time = np.random.uniform(0.02, 0.08)
            time.sleep(processing_time)
            
            # è®°å½•å¤„ç†
            monitor.record_frame_processed(processing_time)
            
            # æ¯20å¸§æ˜¾ç¤ºä¸€æ¬¡ä»ªè¡¨ç›˜
            if i % 20 == 0:
                monitor.print_performance_dashboard()
    
    finally:
        monitor.stop_monitoring()

if __name__ == "__main__":
    demo_performance_monitoring()
```

---

### ğŸ¯ æŠ€æœ¯äº®ç‚¹åˆ†æ

è¿™ä¸ª**é«˜é€Ÿå¤„ç†åŠ å·¥å‚**å±•ç°äº†ç°ä»£è§†é¢‘å¤„ç†ç³»ç»Ÿçš„ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯ï¼š

#### âš¡ å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
- **æ™ºèƒ½ä»»åŠ¡è°ƒåº¦**: åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œä»»åŠ¡åˆ†é…
- **å¼‚æ­¥å¤„ç†æ¨¡å¼**: å‡å°‘é˜»å¡ï¼Œæå‡ç³»ç»Ÿååé‡
- **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
- **æ•…éšœæ¢å¤**: å¥å£®çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶

#### ğŸ–¥ï¸ GPUåŠ é€ŸæŠ€æœ¯
- **æ‰¹å¤„ç†ä¼˜åŒ–**: æœ€å¤§åŒ–GPUèµ„æºåˆ©ç”¨ç‡
- **å†…å­˜æ± ç®¡ç†**: é«˜æ•ˆçš„GPUå†…å­˜åˆ†é…å’Œå›æ”¶
- **CPU-GPUååŒ**: å¼‚æ­¥æ•°æ®ä¼ è¾“å’Œè®¡ç®—é‡å 
- **é™çº§ç­–ç•¥**: GPUä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°CPUå¤„ç†

#### ğŸ“Š æ™ºèƒ½ç›‘æ§ä¼˜åŒ–
- **å®æ—¶æ€§èƒ½åˆ†æ**: å…¨æ–¹ä½çš„ç³»ç»Ÿæ€§èƒ½ç›‘æ§
- **ç“¶é¢ˆè‡ªåŠ¨æ£€æµ‹**: æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
- **è‡ªé€‚åº”ä¼˜åŒ–**: æ ¹æ®å®æ—¶æ€§èƒ½è‡ªåŠ¨è°ƒæ•´å‚æ•°
- **ä¼˜åŒ–å»ºè®®**: åŸºäºå†å²æ•°æ®æä¾›ä¼˜åŒ–å»ºè®®

---

```mermaid
flowchart TD
    A[è§†é¢‘è¾“å…¥] --> B[ä»»åŠ¡åˆ†é…å™¨]
    B --> C[å¤šçº¿ç¨‹å·¥ä½œæ± ]
    B --> D[GPUå¤„ç†å¼•æ“]
    
    C --> E[CPUå¤„ç†å•å…ƒ1]
    C --> F[CPUå¤„ç†å•å…ƒ2]
    C --> G[CPUå¤„ç†å•å…ƒN]
    
    D --> H[GPUå†…å­˜ç®¡ç†]
    D --> I[CUDAæ ¸å¿ƒé˜µåˆ—]
    D --> J[æ‰¹å¤„ç†ä¼˜åŒ–]
    
    E --> K[ç»“æœæ±‡æ€»]
    F --> K
    G --> K
    I --> K
    
    K --> L[æ€§èƒ½ç›‘æ§ä¸­å¿ƒ]
    L --> M[ç“¶é¢ˆæ£€æµ‹]
    L --> N[è‡ªåŠ¨ä¼˜åŒ–]
    L --> O[æ€§èƒ½æŠ¥å‘Š]
    
    N --> B
    
    K --> P[è§†é¢‘è¾“å‡º]
    
    style A fill:#e3f2fd
    style P fill:#e8f5e8
    style L fill:#fff3e0
    style N fill:#ffebee
```

---

## ğŸ† ç¬¬äºŒèŠ‚æ€»ç»“ï¼šé«˜é€Ÿå¤„ç†åŠ å·¥å‚çš„æ€§èƒ½é£è·ƒ

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸå»ºç«‹äº†ä¸€ä¸ª**é«˜é€Ÿå¤„ç†åŠ å·¥å‚**ï¼Œå®ç°äº†è§†é¢‘å¤„ç†æ€§èƒ½çš„é‡å¤§çªç ´ï¼

### âš¡ æ ¸å¿ƒæŠ€æœ¯æˆå°±
1. **å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†**: å®ç°äº†4-8å€çš„æ€§èƒ½æå‡
2. **GPUåŠ é€ŸæŠ€æœ¯**: åœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸Šè·å¾—10-100å€åŠ é€Ÿ
3. **æ™ºèƒ½ç›‘æ§ä¼˜åŒ–**: å»ºç«‹äº†è‡ªé€‚åº”çš„æ€§èƒ½ä¼˜åŒ–ä½“ç³»
4. **ä¼ä¸šçº§æ¶æ„**: æ‰€æœ‰ç³»ç»Ÿéƒ½è¾¾åˆ°äº†ç”Ÿäº§ç¯å¢ƒæ ‡å‡†

### ğŸ¯ æ€§èƒ½æå‡æ•ˆæœ
- **å¤„ç†é€Ÿåº¦**: ä»å•çº¿ç¨‹15fpsæå‡åˆ°å¤šçº¿ç¨‹60fps+
- **èµ„æºåˆ©ç”¨**: CPUå’ŒGPUèµ„æºåˆ©ç”¨ç‡æå‡80%+  
- **ç³»ç»Ÿç¨³å®šæ€§**: é€šè¿‡æ™ºèƒ½ç›‘æ§ï¼Œç³»ç»Ÿç¨³å®šæ€§æå‡90%+
- **å¯æ‰©å±•æ€§**: æ”¯æŒåŠ¨æ€æ‰©å±•ï¼Œå¤„ç†èƒ½åŠ›å¯çº¿æ€§æå‡

### ğŸ“Š MermaidæŠ€æœ¯æ¶æ„å›¾

```mermaid
graph LR
    A[é«˜é€Ÿå¤„ç†åŠ å·¥å‚] --> B[å¹¶è¡Œå¤„ç†è½¦é—´]
    A --> C[GPUåŠ é€Ÿå¼•æ“]
    A --> D[æ€§èƒ½ç›‘æ§ä¸­å¿ƒ]
    
    B --> E[å¤šçº¿ç¨‹è°ƒåº¦]
    B --> F[ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†]
    B --> G[è´Ÿè½½å‡è¡¡]
    
    C --> H[GPUå†…å­˜æ± ]
    C --> I[æ‰¹å¤„ç†ä¼˜åŒ–]
    C --> J[CUDAå¹¶è¡Œè®¡ç®—]
    
    D --> K[å®æ—¶ç›‘æ§]
    D --> L[ç“¶é¢ˆæ£€æµ‹]
    D --> M[è‡ªåŠ¨ä¼˜åŒ–]
    
    style A fill:#ff6b6b
    style B fill:#4ecdc4
    style C fill:#45b7d1
    style D fill:#96ceb4
```

### ğŸš€ ä¸‹ä¸€ç«™é¢„å‘Š
å®Œæˆäº†æ€§èƒ½ä¼˜åŒ–çš„æŠ€æœ¯å‚¨å¤‡ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥**è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠ**ï¼Œå­¦ä¹ å¢å¼ºç°å®æŠ€æœ¯ï¼Œä¸ºæ„å»ºå®Œæ•´çš„ARè¯•è¡£ç³»ç»Ÿåšå‡†å¤‡ï¼

---

```mermaid
flowchart TD
    A[è§†é¢‘è¾“å…¥] --> B[è¾“å…¥é˜Ÿåˆ—]
    B --> C[å·¥ä½œçº¿ç¨‹æ± ]
    C --> D[å¸§è¿‡æ»¤ç»„ä»¶]
    D --> E[è¿åŠ¨æ£€æµ‹ç»„ä»¶]
    E --> F[ç›®æ ‡è¿½è¸ªç»„ä»¶]
    F --> G[è¾“å‡ºé˜Ÿåˆ—]
    G --> H[å¤„ç†ç»“æœ]
    
    I[é…ç½®æ–‡ä»¶] --> J[ç»„ä»¶åŠ è½½å™¨]
    J --> D
    J --> E
    J --> F
    
    K[æ€§èƒ½ç›‘æ§] --> L[ç»Ÿè®¡æ”¶é›†]
    L --> M[ç›‘æ§æŠ¥å‘Š]
    
    C --> K
    D --> K
    E --> K
    F --> K
    
    style A fill:#e3f2fd
    style H fill:#e8f5e8
    style I fill:#fff3e0
    style M fill:#f3e5f5
```

---

## ğŸ† ç¬¬ä¸€èŠ‚æ€»ç»“ï¼šè§†é¢‘æµåª’ä½“å·¥å‚çš„å®Œç¾è¿è½¬

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„**è§†é¢‘æµåª’ä½“å·¥å‚**ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿™ä¸ªç°ä»£åŒ–å·¥å‚çš„å››å¤§æ ¸å¿ƒéƒ¨é—¨ï¼š

### ğŸ¥ ç”Ÿäº§è½¦é—´ï¼šè§†é¢‘è¯»å†™æ§åˆ¶ä¸­å¿ƒ
- **ä¼ä¸šçº§åŠŸèƒ½**: æ”¯æŒå¤šç§è§†é¢‘æ ¼å¼çš„ä¸“ä¸šå¤„ç†
- **ä¿¡æ¯åˆ†æ**: æä¾›è¯¦ç»†çš„è§†é¢‘å…ƒæ•°æ®è§£æ  
- **æ ¼å¼è½¬æ¢**: æ™ºèƒ½çš„å‚æ•°è°ƒæ•´å’Œæ ¼å¼è½¬æ¢
- **æ€§èƒ½ç›‘æ§**: å®Œæ•´çš„å¤„ç†ç»Ÿè®¡å’Œæ—¥å¿—ç³»ç»Ÿ

### ğŸ” è´¨æ£€éƒ¨é—¨ï¼šå¸§é—´å·®åˆ†æ£€æµ‹å™¨
- **å¤šç®—æ³•æ”¯æŒ**: ç®€å•å·®åˆ†ã€è‡ªé€‚åº”å·®åˆ†ã€MOG2èƒŒæ™¯å»ºæ¨¡
- **æ™ºèƒ½å¤„ç†**: å™ªå£°è¿‡æ»¤ã€å½¢æ€å­¦ä¼˜åŒ–ã€è‡ªé€‚åº”é˜ˆå€¼
- **å®æ—¶åˆ†æ**: è¿åŠ¨åŒºåŸŸæ£€æµ‹å’Œç½®ä¿¡åº¦è¯„ä¼°
- **å¯è§†åŒ–è¾“å‡º**: ç›´è§‚çš„æ£€æµ‹ç»“æœå±•ç¤º

### ğŸ¯ è¿½è¸ªå°ç»„ï¼šå¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿ
- **ç®—æ³•èåˆ**: KCFã€CSRTã€å¡å°”æ›¼æ»¤æ³¢ã€åŒˆç‰™åˆ©ç®—æ³•
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**: æ™ºèƒ½çš„ç›®æ ‡åˆ›å»ºã€æ›´æ–°ã€ç§»é™¤
- **è½¨è¿¹åˆ†æ**: è¯¦ç»†çš„è¿åŠ¨è½¨è¿¹å’Œè¡Œä¸ºåˆ†æ
- **é²æ£’æ€§**: å¤„ç†é®æŒ¡ã€æ¶ˆå¤±ã€é‡ç°ç­‰å¤æ‚æƒ…å†µ

### ğŸ”§ å·¥ç¨‹éƒ¨é—¨ï¼šè§†é¢‘æµå¤„ç†ç®¡é“
- **æ¨¡å—åŒ–æ¶æ„**: å¯æ‰©å±•çš„æ’ä»¶å¼ç»„ä»¶è®¾è®¡
- **é«˜æ€§èƒ½å¤„ç†**: å¤šçº¿ç¨‹ã€é˜Ÿåˆ—ç®¡ç†ã€å¼‚æ­¥å¤„ç†
- **é…ç½®é©±åŠ¨**: çµæ´»çš„JSONé…ç½®æ–‡ä»¶æ§åˆ¶
- **å®æ—¶ç›‘æ§**: æ€§èƒ½ç›‘æ§ã€ç»Ÿè®¡åˆ†æã€æ—¥å¿—è¿½è¸ª

### ğŸŒŸ æ ¸å¿ƒæˆå°±
1. **æŠ€æœ¯æ•´åˆ**: å°†4ç§ä¸åŒçš„è§†é¢‘å¤„ç†æŠ€æœ¯å®Œç¾èåˆ
2. **ä¼ä¸šçº§æ ‡å‡†**: æ‰€æœ‰ä»£ç éƒ½è¾¾åˆ°ç”Ÿäº§ç¯å¢ƒè´¨é‡è¦æ±‚
3. **æ¨¡å—åŒ–è®¾è®¡**: å¯é‡ç”¨ã€å¯æ‰©å±•çš„æ¶æ„è®¾è®¡
4. **å®ç”¨ä»·å€¼**: å¯ç›´æ¥åº”ç”¨äºå®é™…é¡¹ç›®çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

### ğŸš€ ä¸‹ä¸€ç«™é¢„å‘Š
å®Œæˆäº†è§†é¢‘å¤„ç†åŸºç¡€è®¾æ–½çš„å»ºè®¾ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥**é«˜é€Ÿå¤„ç†åŠ å·¥å‚**ï¼Œå­¦ä¹ å¦‚ä½•è¿ç”¨å¤šçº¿ç¨‹å’ŒGPUåŠ é€ŸæŠ€æœ¯ï¼Œå°†æˆ‘ä»¬çš„è§†é¢‘å¤„ç†æ€§èƒ½æå‡åˆ°ä¸€ä¸ªå…¨æ–°çš„æ°´å¹³ï¼

---

### ğŸ’¡ æ·±åº¦æ€è€ƒé¢˜

ä¸ºäº†å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡è§†é¢‘å¤„ç†æŠ€æœ¯ï¼Œè¯·æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š

#### ğŸ¤” æ€è€ƒé¢˜1ï¼šè§†é¢‘å¤„ç†ç³»ç»Ÿè®¾è®¡
å‡è®¾æ‚¨éœ€è¦ä¸ºä¸€ä¸ªå¤§å‹å•†åœºè®¾è®¡ä¸€å¥—æ™ºèƒ½ç›‘æ§ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿéœ€è¦åŒæ—¶å¤„ç†50è·¯æ‘„åƒå¤´çš„å®æ—¶è§†é¢‘æµï¼Œå¹¶è¿›è¡Œäººå‘˜è®¡æ•°ã€å¼‚å¸¸è¡Œä¸ºæ£€æµ‹å’Œè½¨è¿¹åˆ†æã€‚è¯·è®¾è®¡è¿™æ ·çš„ç³»ç»Ÿæ¶æ„ï¼Œè€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- å¦‚ä½•å¹³è¡¡å¤„ç†ç²¾åº¦å’Œå®æ—¶æ€§ï¼Ÿ
- å¦‚ä½•å¤„ç†ä¸åŒæ‘„åƒå¤´è§†è§’å’Œå…‰ç…§æ¡ä»¶çš„å·®å¼‚ï¼Ÿ
- å¦‚ä½•è®¾è®¡å­˜å‚¨å’Œæ£€ç´¢ç­–ç•¥æ¥å¤„ç†æµ·é‡è§†é¢‘æ•°æ®ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜2ï¼šç®—æ³•é€‰æ‹©ä¸ä¼˜åŒ–
åœ¨ç¤ºä¾‹2çš„å¸§é—´å·®åˆ†æ£€æµ‹å™¨ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¤šç§æ£€æµ‹æ–¹æ³•ï¼ˆsimpleã€adaptiveã€mog2ã€knnï¼‰ã€‚è¯·åˆ†æï¼š
- åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹©å“ªç§ç®—æ³•ï¼Ÿ
- å¦‚ä½•æ ¹æ®å®é™…ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”çš„å‚æ•°è°ƒä¼˜æœºåˆ¶ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜3ï¼šè¿½è¸ªç³»ç»Ÿçš„é²æ£’æ€§
å¤šç›®æ ‡è¿½è¸ªç³»ç»Ÿéœ€è¦å¤„ç†å„ç§å¤æ‚æƒ…å†µï¼Œå¦‚ç›®æ ‡é®æŒ¡ã€å¿«é€Ÿç§»åŠ¨ã€å°ºåº¦å˜åŒ–ç­‰ã€‚è¯·æ€è€ƒï¼š
- å¦‚ä½•æ”¹è¿›å¡å°”æ›¼æ»¤æ³¢å™¨æ¥æ›´å¥½åœ°å¤„ç†éçº¿æ€§è¿åŠ¨ï¼Ÿ
- åœ¨ç›®æ ‡é•¿æ—¶é—´è¢«é®æŒ¡åé‡æ–°å‡ºç°æ—¶ï¼Œå¦‚ä½•ç¡®ä¿èº«ä»½å…³è”çš„å‡†ç¡®æ€§ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªå­¦ä¹ å‹çš„è¿½è¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®å†å²æ•°æ®æ”¹è¿›è¿½è¸ªæ€§èƒ½ï¼Ÿ

#### ğŸ¤” æ€è€ƒé¢˜4ï¼šç³»ç»Ÿæ¶æ„çš„æ‰©å±•æ€§
è§†é¢‘æµå¤„ç†ç®¡é“é‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œè¯·è€ƒè™‘ï¼š
- å¦‚ä½•æ‰©å±•ç³»ç»Ÿæ”¯æŒåˆ†å¸ƒå¼å¤„ç†ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ªé€šç”¨çš„ç»„ä»¶æ¥å£ï¼Œæ”¯æŒç¬¬ä¸‰æ–¹ç®—æ³•é›†æˆï¼Ÿ
- å¦‚ä½•å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼Œæ ¹æ®ç³»ç»Ÿè´Ÿè½½è‡ªåŠ¨è°ƒæ•´å¤„ç†æµç¨‹ï¼Ÿ

### ğŸ“š å­¦ä¹ å»ºè®®ä¸æœ€ä½³å®è·µ

#### ğŸ¯ æŠ€æœ¯æ·±åŒ–å»ºè®®
1. **æŒæ¡æ•°å­¦åŸºç¡€**ï¼šæ·±å…¥å­¦ä¹ çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¿¡å·å¤„ç†ç­‰æ•°å­¦åŸºç¡€ï¼Œè¿™å¯¹ç†è§£ç®—æ³•åŸç†è‡³å…³é‡è¦ã€‚

2. **å®è·µä¸ºä¸»**ï¼šå¤šåšå®é™…é¡¹ç›®ï¼Œå°è¯•å°†ç¤ºä¾‹ä»£ç åº”ç”¨åˆ°çœŸå®åœºæ™¯ä¸­ï¼Œå‘ç°å¹¶è§£å†³å®é™…é—®é¢˜ã€‚

3. **é˜…è¯»æºç **ï¼šæ·±å…¥ç ”è¯»OpenCVã€scikit-learnç­‰åº“çš„æºç ï¼Œç†è§£ç®—æ³•çš„å…·ä½“å®ç°ç»†èŠ‚ã€‚

4. **å…³æ³¨å‰æ²¿**ï¼šè·Ÿè¸ªè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ åœ¨è§†é¢‘å¤„ç†ä¸­çš„åº”ç”¨ã€‚

#### ğŸ”§ ç¼–ç¨‹å®è·µå»ºè®®
1. **ä»£ç è§„èŒƒ**ï¼šä¸¥æ ¼éµå¾ªPEP8ç¼–ç è§„èŒƒï¼Œç¼–å†™æ¸…æ™°ã€å¯ç»´æŠ¤çš„ä»£ç ã€‚

2. **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ ä½¿ç”¨profilerå·¥å…·åˆ†æä»£ç æ€§èƒ½ï¼Œè¯†åˆ«ç“¶é¢ˆå¹¶è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–ã€‚

3. **é”™è¯¯å¤„ç†**ï¼šè®¾è®¡å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å¼‚å¸¸æƒ…å†µä¸‹çš„ç¨³å®šè¿è¡Œã€‚

4. **å•å…ƒæµ‹è¯•**ï¼šä¸ºå…³é”®åŠŸèƒ½ç¼–å†™å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿå¯é æ€§ã€‚

#### ğŸ—ï¸ æ¶æ„è®¾è®¡å»ºè®®
1. **æ¨¡å—åŒ–æ€ç»´**ï¼šå§‹ç»ˆè€ƒè™‘ä»£ç çš„å¯é‡ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé‡‡ç”¨æ¾è€¦åˆã€é«˜å†…èšçš„è®¾è®¡åŸåˆ™ã€‚

2. **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†ç³»ç»Ÿå‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç ï¼Œæé«˜ç³»ç»Ÿçš„çµæ´»æ€§ã€‚

3. **ç›‘æ§ä½“ç³»**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³ç³»ç»Ÿé—®é¢˜ã€‚

4. **æ–‡æ¡£å®Œå–„**ï¼šç¼–å†™è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£å’Œç”¨æˆ·æ‰‹å†Œï¼Œæ–¹ä¾¿å›¢é˜Ÿåä½œå’Œç³»ç»Ÿç»´æŠ¤ã€‚

#### ğŸš€ èŒä¸šå‘å±•å»ºè®®
1. **è·¨é¢†åŸŸå­¦ä¹ **ï¼šè®¡ç®—æœºè§†è§‰ä¸æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—ç­‰é¢†åŸŸç»“åˆç´§å¯†ï¼Œå»ºè®®æ‹“å®½çŸ¥è¯†é¢ã€‚

2. **é¡¹ç›®ç»éªŒ**ï¼šå‚ä¸å¼€æºé¡¹ç›®æˆ–å®é™…å•†ä¸šé¡¹ç›®ï¼Œç§¯ç´¯ä¸°å¯Œçš„é¡¹ç›®ç»éªŒã€‚

3. **æŠ€æœ¯åˆ†äº«**ï¼šé€šè¿‡æŠ€æœ¯åšå®¢ã€æ¼”è®²ç­‰æ–¹å¼åˆ†äº«ç»éªŒï¼Œæå‡ä¸ªäººå½±å“åŠ›ã€‚

4. **æŒç»­å­¦ä¹ **ï¼šæŠ€æœ¯å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œä¿æŒæŒç»­å­¦ä¹ çš„ä¹ æƒ¯ï¼ŒåŠæ—¶æ›´æ–°çŸ¥è¯†ä½“ç³»ã€‚

### ğŸ‰ æœ¬èŠ‚å­¦ä¹ æˆæœæ£€éªŒ

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

#### âœ… ç†è®ºæŒæ¡
- [ ] ç†è§£è§†é¢‘æ•°æ®ç»“æ„å’Œç¼–è§£ç åŸç†
- [ ] æŒæ¡å¸§é—´å·®åˆ†ã€èƒŒæ™¯å»ºæ¨¡ç­‰è¿åŠ¨æ£€æµ‹ç®—æ³•
- [ ] ç†Ÿæ‚‰å¡å°”æ›¼æ»¤æ³¢ã€åŒˆç‰™åˆ©ç®—æ³•ç­‰è¿½è¸ªæŠ€æœ¯
- [ ] äº†è§£æ¨¡å—åŒ–æ¶æ„è®¾è®¡çš„æ ¸å¿ƒæ€æƒ³

#### âœ… å®è·µèƒ½åŠ›
- [ ] èƒ½å¤Ÿç‹¬ç«‹å®ç°è§†é¢‘è¯»å†™å’Œæ ¼å¼è½¬æ¢åŠŸèƒ½
- [ ] èƒ½å¤Ÿæ„å»ºè¿åŠ¨æ£€æµ‹å’Œç›®æ ‡è¿½è¸ªç³»ç»Ÿ
- [ ] èƒ½å¤Ÿè®¾è®¡å’Œå®ç°æ¨¡å—åŒ–çš„å¤„ç†ç®¡é“
- [ ] èƒ½å¤Ÿè¿›è¡Œç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

#### âœ… åº”ç”¨æ°´å¹³
- [ ] èƒ½å¤Ÿæ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ç®—æ³•
- [ ] èƒ½å¤Ÿè®¾è®¡ä¼ä¸šçº§çš„è§†é¢‘å¤„ç†è§£å†³æ–¹æ¡ˆ
- [ ] èƒ½å¤Ÿè§£å†³è§†é¢‘å¤„ç†ä¸­çš„å¸¸è§æŠ€æœ¯é—®é¢˜
- [ ] èƒ½å¤Ÿè¯„ä¼°å’Œæ”¹è¿›ç³»ç»Ÿæ€§èƒ½

---

## 37.3 è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠï¼šå¢å¼ºç°å®åŸºç¡€æŠ€æœ¯

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒçš„ç¬¬ä¸‰ç«™â€”â€”**è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠ**ï¼å¦‚æœè¯´å‰ä¸¤ä¸ªè½¦é—´è§£å†³äº†"å¦‚ä½•å¤„ç†"å’Œ"å¦‚ä½•åŠ é€Ÿ"çš„é—®é¢˜ï¼Œé‚£ä¹ˆè¿™ä¸ªåˆ›æ„å·¥åŠå°±è¦è§£å†³"å¦‚ä½•åˆ›é€ ç°å®ä¸è™šæ‹Ÿèåˆçš„é­”æ³•"ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è®©è®¡ç®—æœº"çœ‹æ‡‚"çœŸå®ä¸–ç•Œçš„ä¸‰ç»´ç»“æ„ï¼Œå¦‚ä½•åœ¨ç°å®åœºæ™¯ä¸­å‡†ç¡®åœ°æ”¾ç½®è™šæ‹Ÿç‰©ä½“ï¼Œä»¥åŠå¦‚ä½•å®ç°ä»¤äººæƒŠå¹çš„è™šå®èåˆæ•ˆæœã€‚

### ğŸ­ åˆ›æ„å·¥åŠå¸ƒå±€

#### ğŸ“· æ‘„åƒå¤´æ ‡å®šå®éªŒå®¤ï¼šè§†è§‰åæ ‡ç³»ç»Ÿ
è¿™ä¸ªå®éªŒå®¤æ˜¯æ•´ä¸ªARæŠ€æœ¯çš„åŸºçŸ³ï¼Œè´Ÿè´£å»ºç«‹ç²¾ç¡®çš„è§†è§‰åæ ‡ç³»ç»Ÿï¼š
- **å†…å‚æ•°æ ‡å®šå°**ï¼šç²¾ç¡®æµ‹é‡æ‘„åƒå¤´çš„ç„¦è·ã€ä¸»ç‚¹ã€ç•¸å˜ç³»æ•°
- **å¤–å‚æ•°æµ‹é‡ç«™**ï¼šç¡®å®šæ‘„åƒå¤´åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®å’Œå§¿æ€
- **ç•¸å˜æ ¡æ­£åŒº**ï¼šæ¶ˆé™¤æ‘„åƒå¤´é•œå¤´å¸¦æ¥çš„å›¾åƒç•¸å˜
- **åæ ‡å˜æ¢ä¸­å¿ƒ**ï¼šå®ç°å„ç§åæ ‡ç³»ä¹‹é—´çš„ç²¾ç¡®è½¬æ¢

#### ğŸ¨ 3Dæ¸²æŸ“åˆ›ä½œå®¤ï¼šè™šæ‹Ÿç‰©ä½“ç”Ÿæˆ
ä¸“é—¨è´Ÿè´£åˆ›å»ºå’Œæ¸²æŸ“å„ç§è™šæ‹Ÿ3Dç‰©ä½“ï¼š
- **æ¨¡å‹è®¾è®¡å°**ï¼šåˆ›å»ºå’Œç¼–è¾‘3Dæ¨¡å‹ï¼Œå®šä¹‰å‡ ä½•å½¢çŠ¶
- **æè´¨å·¥ä½œç«™**ï¼šä¸º3Dæ¨¡å‹æ·»åŠ çº¹ç†ã€é¢œè‰²ã€å…‰ç…§æ•ˆæœ
- **å…‰ç…§è®¡ç®—ä¸­å¿ƒ**ï¼šæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å…‰ç…§æ¡ä»¶å’Œé˜´å½±æ•ˆæœ
- **æ¸²æŸ“å¼•æ“**ï¼šå°†3Dåœºæ™¯è½¬æ¢ä¸º2Då›¾åƒçš„æ ¸å¿ƒè®¾å¤‡

#### ğŸŒˆ è™šå®èåˆå®éªŒå®¤ï¼šé­”æ³•è¯ç”Ÿä¹‹åœ°
è¿™é‡Œæ˜¯ARæŠ€æœ¯çš„æ ¸å¿ƒï¼Œå®ç°è™šæ‹Ÿä¸ç°å®çš„å®Œç¾èåˆï¼š
- **æ·±åº¦æ„ŸçŸ¥å•å…ƒ**ï¼šç†è§£åœºæ™¯çš„ä¸‰ç»´ç»“æ„å’Œæ·±åº¦ä¿¡æ¯
- **å¹³é¢æ£€æµ‹å™¨**ï¼šè¯†åˆ«ç°å®ä¸–ç•Œä¸­çš„å¹³é¢ï¼Œä½œä¸ºè™šæ‹Ÿç‰©ä½“çš„æ”¾ç½®åŸºç¡€
- **é®æŒ¡å¤„ç†åŒº**ï¼šå¤„ç†è™šæ‹Ÿç‰©ä½“è¢«ç°å®ç‰©ä½“é®æŒ¡çš„å¤æ‚æƒ…å†µ
- **å®æ—¶åˆæˆå°**ï¼šå°†è™šæ‹Ÿå†…å®¹ä¸çœŸå®å›¾åƒå®æ—¶åˆæˆ

#### ğŸ­ äº¤äº’è®¾è®¡ä¸­å¿ƒï¼šç”¨æˆ·ä½“éªŒä¼˜åŒ–
ä¸“æ³¨äºæå‡ARåº”ç”¨çš„ç”¨æˆ·ä½“éªŒï¼š
- **æ‰‹åŠ¿è¯†åˆ«ç«™**ï¼šæ•æ‰å’Œç†è§£ç”¨æˆ·çš„æ‰‹åŠ¿æŒ‡ä»¤
- **è§¦æ§äº¤äº’åŒº**ï¼šå¤„ç†è§¦å±è®¾å¤‡ä¸Šçš„ARäº¤äº’
- **è¯­éŸ³æ§åˆ¶å°**ï¼šé›†æˆè¯­éŸ³è¯†åˆ«å’Œæ§åˆ¶åŠŸèƒ½
- **åé¦ˆç³»ç»Ÿ**ï¼šä¸ºç”¨æˆ·æä¾›è§†è§‰ã€å¬è§‰ã€è§¦è§‰åé¦ˆ

```mermaid
graph TB
    subgraph "è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠæ¶æ„"
        subgraph "æ‘„åƒå¤´æ ‡å®šå®éªŒå®¤"
            A1[å†…å‚æ•°æ ‡å®šå°] --> A2[å¤–å‚æ•°æµ‹é‡ç«™]
            A2 --> A3[ç•¸å˜æ ¡æ­£åŒº]
            A3 --> A4[åæ ‡å˜æ¢ä¸­å¿ƒ]
        end
        
        subgraph "3Dæ¸²æŸ“åˆ›ä½œå®¤"
            B1[æ¨¡å‹è®¾è®¡å°] --> B2[æè´¨å·¥ä½œç«™]
            B2 --> B3[å…‰ç…§è®¡ç®—ä¸­å¿ƒ]
            B3 --> B4[æ¸²æŸ“å¼•æ“]
        end
        
        subgraph "è™šå®èåˆå®éªŒå®¤"
            C1[æ·±åº¦æ„ŸçŸ¥å•å…ƒ] --> C2[å¹³é¢æ£€æµ‹å™¨]
            C2 --> C3[é®æŒ¡å¤„ç†åŒº]
            C3 --> C4[å®æ—¶åˆæˆå°]
        end
        
        subgraph "äº¤äº’è®¾è®¡ä¸­å¿ƒ"
            D1[æ‰‹åŠ¿è¯†åˆ«ç«™] --> D2[è§¦æ§äº¤äº’åŒº]
            D2 --> D3[è¯­éŸ³æ§åˆ¶å°]
            D3 --> D4[åé¦ˆç³»ç»Ÿ]
        end
    end
    
    A4 --> B1
    B4 --> C1
    C4 --> D1
    
    style A1 fill:#e1f5fe
    style B1 fill:#f3e5f5
    style C1 fill:#e8f5e8
    style D1 fill:#fff3e0
```

### ğŸ’» æŠ€æœ¯æ·±åº¦è§£æ

```mermaid
flowchart LR
    subgraph "ARåæ ‡ç³»ç»Ÿå˜æ¢æµç¨‹"
        A[ä¸–ç•Œåæ ‡ç³»<br/>World Coordinates] --> B[æ‘„åƒå¤´åæ ‡ç³»<br/>Camera Coordinates]
        B --> C[å›¾åƒåæ ‡ç³»<br/>Image Coordinates]
        C --> D[å±å¹•åæ ‡ç³»<br/>Screen Coordinates]
        
        A1[æ—‹è½¬çŸ©é˜µR<br/>å¹³ç§»å‘é‡t] --> B
        B1[ç›¸æœºå†…å‚çŸ©é˜µK<br/>é€è§†æŠ•å½±] --> C
        C1[è®¾å¤‡æ˜¾ç¤ºæ˜ å°„<br/>åƒç´ å¯¹é½] --> D
    end
    
    subgraph "æ•°å­¦å˜æ¢æ¨¡å‹"
        E[é’ˆå­”ç›¸æœºæ¨¡å‹<br/>s[u,v,1]áµ€ = K[R|t][X,Y,Z,1]áµ€]
        F[ç•¸å˜æ ¡æ­£æ¨¡å‹<br/>å¾„å‘+åˆ‡å‘ç•¸å˜]
        G[é½æ¬¡åæ ‡å˜æ¢<br/>4x4å˜æ¢çŸ©é˜µ]
    end
    
    E --> A1
    F --> B1
    G --> C1
    
    style A fill:#ffeb3b
    style B fill:#4caf50
    style C fill:#2196f3
    style D fill:#ff5722
```

#### ARæŠ€æœ¯çš„æ•°å­¦åŸºç¡€
å¢å¼ºç°å®æŠ€æœ¯å»ºç«‹åœ¨åšå®çš„æ•°å­¦åŸºç¡€ä¹‹ä¸Šï¼Œæ¶‰åŠå¤šä¸ªåæ ‡ç³»çš„è½¬æ¢ï¼š

```python
import numpy as np
import cv2

# ARæŠ€æœ¯ä¸­çš„å…³é”®åæ ‡ç³»
coordinate_systems = {
    "ä¸–ç•Œåæ ‡ç³»": {
        "æè¿°": "çœŸå®ä¸–ç•Œçš„ä¸‰ç»´åæ ‡ç³»ç»Ÿ",
        "å•ä½": "ç±³(m)æˆ–æ¯«ç±³(mm)",
        "ç”¨é€”": "æè¿°ç‰©ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„ä½ç½®"
    },
    "æ‘„åƒå¤´åæ ‡ç³»": {
        "æè¿°": "ä»¥æ‘„åƒå¤´ä¸ºåŸç‚¹çš„ä¸‰ç»´åæ ‡ç³»",
        "ç‰¹ç‚¹": "Zè½´æŒ‡å‘æ‘„åƒå¤´å‰æ–¹",
        "ç”¨é€”": "æè¿°ç‰©ä½“ç›¸å¯¹äºæ‘„åƒå¤´çš„ä½ç½®"
    },
    "å›¾åƒåæ ‡ç³»": {
        "æè¿°": "äºŒç»´å›¾åƒå¹³é¢çš„åæ ‡ç³»ç»Ÿ",
        "å•ä½": "åƒç´ (pixel)",
        "ç”¨é€”": "æè¿°ç‰©ä½“åœ¨å›¾åƒä¸­çš„æŠ•å½±ä½ç½®"
    },
    "å±å¹•åæ ‡ç³»": {
        "æè¿°": "è®¾å¤‡å±å¹•çš„æ˜¾ç¤ºåæ ‡ç³»",
        "ç‰¹ç‚¹": "é€šå¸¸åŸç‚¹åœ¨å·¦ä¸Šè§’",
        "ç”¨é€”": "æ¸²æŸ“å’Œæ˜¾ç¤ºè™šæ‹Ÿå†…å®¹"
    }
}

# åæ ‡è½¬æ¢çš„æ•°å­¦æ¨¡å‹
def explain_coordinate_transformation():
    """
    è§£é‡ŠARä¸­çš„åæ ‡è½¬æ¢è¿‡ç¨‹
    
    ä¸–ç•Œåæ ‡ -> æ‘„åƒå¤´åæ ‡ -> å›¾åƒåæ ‡ -> å±å¹•åæ ‡
    """
    print("ARåæ ‡è½¬æ¢é“¾ï¼š")
    print("1. ä¸–ç•Œåæ ‡ -> æ‘„åƒå¤´åæ ‡ï¼šæ—‹è½¬çŸ©é˜µR + å¹³ç§»å‘é‡t")
    print("2. æ‘„åƒå¤´åæ ‡ -> å›¾åƒåæ ‡ï¼šé€è§†æŠ•å½±å˜æ¢")
    print("3. å›¾åƒåæ ‡ -> å±å¹•åæ ‡ï¼šæ˜¾ç¤ºè®¾å¤‡æ˜ å°„")
    
    # ç¤ºä¾‹å˜æ¢çŸ©é˜µ
    camera_matrix = np.array([
        [800, 0, 320],    # fx, 0, cx
        [0, 800, 240],    # 0, fy, cy  
        [0, 0, 1]         # 0, 0, 1
    ])
    
    print(f"\nç›¸æœºå†…å‚çŸ©é˜µç¤ºä¾‹ï¼š\n{camera_matrix}")
    return camera_matrix

explain_coordinate_transformation()
```

#### æ‘„åƒå¤´æ ‡å®šçš„æ ¸å¿ƒåŸç†
æ‘„åƒå¤´æ ‡å®šæ˜¯ARæŠ€æœ¯çš„åŸºç¡€ï¼Œé€šè¿‡æ•°å­¦æ–¹æ³•ç²¾ç¡®ç¡®å®šæ‘„åƒå¤´çš„å‚æ•°ï¼š

```python
# æ‘„åƒå¤´æ ‡å®šçš„æ•°å­¦æ¨¡å‹
class CameraCalibrationTheory:
    """æ‘„åƒå¤´æ ‡å®šç†è®ºåŸºç¡€"""
    
    def __init__(self):
        self.intrinsic_parameters = {
            "fx": "Xæ–¹å‘ç„¦è·(åƒç´ )",
            "fy": "Yæ–¹å‘ç„¦è·(åƒç´ )", 
            "cx": "ä¸»ç‚¹Xåæ ‡(åƒç´ )",
            "cy": "ä¸»ç‚¹Yåæ ‡(åƒç´ )",
            "k1, k2, k3": "å¾„å‘ç•¸å˜ç³»æ•°",
            "p1, p2": "åˆ‡å‘ç•¸å˜ç³»æ•°"
        }
        
        self.extrinsic_parameters = {
            "R": "æ—‹è½¬çŸ©é˜µ(3x3)",
            "t": "å¹³ç§»å‘é‡(3x1)",
            "æè¿°": "æ‘„åƒå¤´ç›¸å¯¹äºä¸–ç•Œåæ ‡ç³»çš„ä½å§¿"
        }
    
    def pinhole_camera_model(self, world_point):
        """
        é’ˆå­”ç›¸æœºæ¨¡å‹ï¼šæè¿°3Dä¸–ç•Œç‚¹åˆ°2Då›¾åƒç‚¹çš„æŠ•å½±
        
        æ•°å­¦å…¬å¼ï¼š
        s * [u, v, 1]^T = K * [R|t] * [X, Y, Z, 1]^T
        
        å…¶ä¸­ï¼š
        - K: ç›¸æœºå†…å‚çŸ©é˜µ
        - [R|t]: ç›¸æœºå¤–å‚(æ—‹è½¬+å¹³ç§»)
        - [X,Y,Z]: ä¸–ç•Œåæ ‡ç‚¹
        - [u,v]: å›¾åƒåæ ‡ç‚¹
        - s: é½æ¬¡åæ ‡ç¼©æ”¾å› å­
        """
        print("é’ˆå­”ç›¸æœºæ¨¡å‹æŠ•å½±è¿‡ç¨‹ï¼š")
        print("1. ä¸–ç•Œåæ ‡ -> æ‘„åƒå¤´åæ ‡ï¼šP_cam = R * P_world + t")
        print("2. æ‘„åƒå¤´åæ ‡ -> å½’ä¸€åŒ–å›¾åƒåæ ‡ï¼š[x,y] = [X/Z, Y/Z]")
        print("3. ç•¸å˜æ ¡æ­£ï¼šè€ƒè™‘å¾„å‘å’Œåˆ‡å‘ç•¸å˜")
        print("4. å½’ä¸€åŒ–åæ ‡ -> åƒç´ åæ ‡ï¼š[u,v] = K * [x,y,1]^T")
        
        return "æŠ•å½±å®Œæˆ"
    
    def distortion_model(self):
        """
        é•œå¤´ç•¸å˜æ¨¡å‹
        
        å¾„å‘ç•¸å˜ï¼šr^2 = x^2 + y^2
        x_corrected = x * (1 + k1*r^2 + k2*r^4 + k3*r^6)
        y_corrected = y * (1 + k1*r^2 + k2*r^4 + k3*r^6)
        
        åˆ‡å‘ç•¸å˜ï¼š
        x_corrected = x + [2*p1*x*y + p2*(r^2 + 2*x^2)]
        y_corrected = y + [p1*(r^2 + 2*y^2) + 2*p2*x*y]
        """
        distortion_effects = {
            "å¾„å‘ç•¸å˜": "ç”±é•œå¤´å½¢çŠ¶ä¸å®Œç¾å¼•èµ·ï¼Œè¡¨ç°ä¸ºå›¾åƒè¾¹ç¼˜çš„æ¡¶å½¢æˆ–æ•å½¢ç•¸å˜",
            "åˆ‡å‘ç•¸å˜": "ç”±é•œå¤´ä¸æˆåƒå¹³é¢ä¸å®Œå…¨å¹³è¡Œå¼•èµ·ï¼Œè¡¨ç°ä¸ºå›¾åƒçš„åç§»",
            "æ ¡æ­£é‡è¦æ€§": "ç²¾ç¡®çš„ç•¸å˜æ ¡æ­£æ˜¯ARåº”ç”¨æˆåŠŸçš„å…³é”®"
        }
        return distortion_effects

# åˆ›å»ºç†è®ºè§£é‡Šå®ä¾‹
theory = CameraCalibrationTheory()
print("æ‘„åƒå¤´å†…å‚æ•°ï¼š", theory.intrinsic_parameters)
print("\næ‘„åƒå¤´å¤–å‚æ•°ï¼š", theory.extrinsic_parameters)
```

```mermaid
graph TD
    subgraph "3Dæ¸²æŸ“ç®¡çº¿æ¶æ„"
        A[å‡ ä½•ç”Ÿæˆé˜¶æ®µ] --> B[å˜æ¢è®¡ç®—é˜¶æ®µ]
        B --> C[å…‰ç…§è®¡ç®—é˜¶æ®µ]
        C --> D[æŠ•å½±å˜æ¢é˜¶æ®µ]
        D --> E[å…‰æ …åŒ–é˜¶æ®µ]
        
        subgraph "å‡ ä½•å¤„ç†"
            A1[åŸºå…ƒç”Ÿæˆå™¨<br/>ç«‹æ–¹ä½“/çƒä½“/åœ†æŸ±]
            A2[é¡¶ç‚¹ç¼“å†²<br/>Vertex Buffer]
            A3[é¢ç´¢å¼•ç¼“å†²<br/>Index Buffer]
            A4[æ³•å‘é‡è®¡ç®—<br/>Normal Calculation]
        end
        
        subgraph "å˜æ¢çŸ©é˜µ"
            B1[æ¨¡å‹å˜æ¢<br/>Model Transform]
            B2[è§†å›¾å˜æ¢<br/>View Transform]
            B3[æŠ•å½±å˜æ¢<br/>Projection Transform]
            B4[è§†å£å˜æ¢<br/>Viewport Transform]
        end
        
        subgraph "å…‰ç…§æ¨¡å‹"
            C1[ç¯å¢ƒå…‰<br/>Ambient]
            C2[æ¼«åå°„<br/>Diffuse]
            C3[é•œé¢åå°„<br/>Specular]
            C4[Phongç€è‰²<br/>Phong Shading]
        end
        
        A --> A1 --> A2 --> A3 --> A4 --> B
        B --> B1 --> B2 --> B3 --> B4 --> C
        C --> C1 --> C2 --> C3 --> C4 --> D
    end
    
    style A fill:#e3f2fd
    style B fill:#f1f8e9
    style C fill:#fff8e1
    style D fill:#fce4ec
    style E fill:#f3e5f5
```

### ğŸ¯ ç¤ºä¾‹1ï¼šæ‘„åƒå¤´æ ‡å®šç³»ç»Ÿ

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªä¸“ä¸šçº§çš„æ‘„åƒå¤´æ ‡å®šç³»ç»Ÿï¼Œå®ƒæ˜¯æ•´ä¸ªARæŠ€æœ¯æ ˆçš„åŸºçŸ³ã€‚

```python
"""
æ‘„åƒå¤´æ ‡å®šç³»ç»Ÿ - ARæŠ€æœ¯åŸºçŸ³
åŠŸèƒ½ï¼š
1. æ£‹ç›˜æ ¼æ ‡å®šæ¿æ£€æµ‹å’Œè§’ç‚¹æå–
2. å†…å¤–å‚æ•°ç²¾ç¡®è®¡ç®—
3. ç•¸å˜ç³»æ•°ä¼°è®¡å’Œæ ¡æ­£
4. æ ‡å®šè´¨é‡è¯„ä¼°å’Œä¼˜åŒ–
"""

import cv2
import numpy as np
import glob
import pickle
import json
from typing import List, Tuple, Dict, Optional
import matplotlib.pyplot as plt
from dataclasses import dataclass
import logging
import time

@dataclass
class CalibrationConfig:
    """æ ‡å®šé…ç½®å‚æ•°"""
    pattern_size: Tuple[int, int] = (9, 6)  # æ£‹ç›˜æ ¼å†…è§’ç‚¹æ•°é‡
    square_size: float = 25.0  # æ–¹æ ¼å¤§å°(mm)
    image_size: Tuple[int, int] = (640, 480)
    max_images: int = 30  # æœ€å¤§æ ‡å®šå›¾åƒæ•°
    min_images: int = 10  # æœ€å°æ ‡å®šå›¾åƒæ•°
    
@dataclass
class CalibrationResult:
    """æ ‡å®šç»“æœ"""
    camera_matrix: np.ndarray
    distortion_coeffs: np.ndarray
    rotation_vectors: List[np.ndarray]
    translation_vectors: List[np.ndarray]
    reprojection_error: float
    calibration_quality: str
    
class AdvancedCameraCalibrator:
    """é«˜çº§æ‘„åƒå¤´æ ‡å®šå™¨"""
    
    def __init__(self, config: CalibrationConfig):
        self.config = config
        self.logger = self._setup_logger()
        
        # æ ‡å®šæ•°æ®å­˜å‚¨
        self.object_points = []  # 3Dç‚¹
        self.image_points = []   # 2Dç‚¹
        self.gray_images = []    # ç°åº¦å›¾åƒ
        self.valid_images = []   # æœ‰æ•ˆå›¾åƒè·¯å¾„
        
        # åˆ›å»º3Dæ£‹ç›˜æ ¼ç‚¹
        self.pattern_points = self._create_pattern_points()
        
        self.logger.info("æ‘„åƒå¤´æ ‡å®šå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('CameraCalibrator')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _create_pattern_points(self) -> np.ndarray:
        """åˆ›å»ºæ ‡å®šæ¿çš„3Dç‚¹åæ ‡"""
        pattern_points = np.zeros(
            (self.config.pattern_size[0] * self.config.pattern_size[1], 3),
            np.float32
        )
        pattern_points[:, :2] = np.mgrid[
            0:self.config.pattern_size[0],
            0:self.config.pattern_size[1]
        ].T.reshape(-1, 2)
        
        # è½¬æ¢ä¸ºå®é™…ç‰©ç†å°ºå¯¸
        pattern_points *= self.config.square_size
        
        return pattern_points
    
    def detect_calibration_pattern(self, image: np.ndarray) -> Tuple[bool, np.ndarray]:
        """
        æ£€æµ‹æ ‡å®šå›¾æ¡ˆå¹¶æå–è§’ç‚¹
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            (success, corners): æ£€æµ‹æˆåŠŸæ ‡å¿—å’Œè§’ç‚¹åæ ‡
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # æŸ¥æ‰¾æ£‹ç›˜æ ¼è§’ç‚¹
        ret, corners = cv2.findChessboardCorners(
            gray, 
            self.config.pattern_size,
            cv2.CALIB_CB_ADAPTIVE_THRESH + 
            cv2.CALIB_CB_NORMALIZE_IMAGE +
            cv2.CALIB_CB_FAST_CHECK
        )
        
        if ret:
            # ç²¾ç»†åŒ–è§’ç‚¹ä½ç½®
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
            corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
            
            self.logger.info(f"æˆåŠŸæ£€æµ‹åˆ° {len(corners)} ä¸ªè§’ç‚¹")
        
        return ret, corners
    
    def add_calibration_image(self, image_path: str) -> bool:
        """
        æ·»åŠ æ ‡å®šå›¾åƒ
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        try:
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return False
            
            # æ£€æµ‹æ ‡å®šå›¾æ¡ˆ
            success, corners = self.detect_calibration_pattern(image)
            
            if success:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                
                # å­˜å‚¨æ ‡å®šæ•°æ®
                self.object_points.append(self.pattern_points)
                self.image_points.append(corners)
                self.gray_images.append(gray)
                self.valid_images.append(image_path)
                
                self.logger.info(f"æˆåŠŸæ·»åŠ æ ‡å®šå›¾åƒ: {image_path}")
                return True
            else:
                self.logger.warning(f"æœªæ£€æµ‹åˆ°æ ‡å®šå›¾æ¡ˆ: {image_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"å¤„ç†å›¾åƒæ—¶å‡ºé”™ {image_path}: {e}")
            return False
    
    def batch_process_images(self, image_directory: str) -> int:
        """
        æ‰¹é‡å¤„ç†æ ‡å®šå›¾åƒ
        
        Args:
            image_directory: å›¾åƒç›®å½•è·¯å¾„
            
        Returns:
            int: æˆåŠŸå¤„ç†çš„å›¾åƒæ•°é‡
        """
        # æ”¯æŒå¤šç§å›¾åƒæ ¼å¼
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
        image_files = []
        
        for ext in extensions:
            image_files.extend(glob.glob(f"{image_directory}/{ext}"))
            image_files.extend(glob.glob(f"{image_directory}/{ext.upper()}"))
        
        self.logger.info(f"æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        success_count = 0
        for image_file in image_files[:self.config.max_images]:
            if self.add_calibration_image(image_file):
                success_count += 1
        
        self.logger.info(f"æˆåŠŸå¤„ç† {success_count} ä¸ªæ ‡å®šå›¾åƒ")
        return success_count
    
    def calibrate_camera(self) -> Optional[CalibrationResult]:
        """
        æ‰§è¡Œæ‘„åƒå¤´æ ‡å®š
        
        Returns:
            CalibrationResult: æ ‡å®šç»“æœ
        """
        if len(self.object_points) < self.config.min_images:
            self.logger.error(f"æ ‡å®šå›¾åƒä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.config.min_images} å¼ ")
            return None
        
        self.logger.info(f"å¼€å§‹æ ‡å®šï¼Œä½¿ç”¨ {len(self.object_points)} å¼ å›¾åƒ")
        
        # æ‰§è¡Œæ‘„åƒå¤´æ ‡å®š
        ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
            self.object_points,
            self.image_points,
            self.config.image_size,
            None,
            None,
            flags=cv2.CALIB_RATIONAL_MODEL
        )
        
        if not ret:
            self.logger.error("æ‘„åƒå¤´æ ‡å®šå¤±è´¥")
            return None
        
        # è®¡ç®—é‡æŠ•å½±è¯¯å·®
        mean_error = self._calculate_reprojection_error(
            camera_matrix, dist_coeffs, rvecs, tvecs
        )
        
        # è¯„ä¼°æ ‡å®šè´¨é‡
        quality = self._assess_calibration_quality(mean_error)
        
        result = CalibrationResult(
            camera_matrix=camera_matrix,
            distortion_coeffs=dist_coeffs,
            rotation_vectors=rvecs,
            translation_vectors=tvecs,
            reprojection_error=mean_error,
            calibration_quality=quality
        )
        
        self.logger.info(f"æ ‡å®šå®Œæˆï¼Œé‡æŠ•å½±è¯¯å·®: {mean_error:.3f} åƒç´ ")
        self.logger.info(f"æ ‡å®šè´¨é‡: {quality}")
        
        return result
    
    def _calculate_reprojection_error(self, camera_matrix: np.ndarray, 
                                     dist_coeffs: np.ndarray,
                                     rvecs: List[np.ndarray], 
                                     tvecs: List[np.ndarray]) -> float:
        """è®¡ç®—é‡æŠ•å½±è¯¯å·®"""
        total_error = 0
        total_points = 0
        
        for i in range(len(self.object_points)):
            # é‡æŠ•å½±3Dç‚¹åˆ°å›¾åƒå¹³é¢
            projected_points, _ = cv2.projectPoints(
                self.object_points[i], rvecs[i], tvecs[i],
                camera_matrix, dist_coeffs
            )
            
            # è®¡ç®—æŠ•å½±è¯¯å·®
            error = cv2.norm(self.image_points[i], projected_points, cv2.NORM_L2) / len(projected_points)
            total_error += error
            total_points += len(projected_points)
        
        return total_error / len(self.object_points)
    
    def _assess_calibration_quality(self, reprojection_error: float) -> str:
        """è¯„ä¼°æ ‡å®šè´¨é‡"""
        if reprojection_error < 0.3:
            return "ä¼˜ç§€"
        elif reprojection_error < 0.5:
            return "è‰¯å¥½"
        elif reprojection_error < 1.0:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def undistort_image(self, image: np.ndarray, 
                       camera_matrix: np.ndarray,
                       dist_coeffs: np.ndarray) -> np.ndarray:
        """
        ç•¸å˜æ ¡æ­£
        
        Args:
            image: è¾“å…¥å›¾åƒ
            camera_matrix: ç›¸æœºå†…å‚çŸ©é˜µ
            dist_coeffs: ç•¸å˜ç³»æ•°
            
        Returns:
            æ ¡æ­£åçš„å›¾åƒ
        """
        # è®¡ç®—æœ€ä¼˜ç›¸æœºçŸ©é˜µ
        h, w = image.shape[:2]
        new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
            camera_matrix, dist_coeffs, (w, h), 1, (w, h)
        )
        
        # æ‰§è¡Œç•¸å˜æ ¡æ­£
        undistorted = cv2.undistort(image, camera_matrix, dist_coeffs, 
                                   None, new_camera_matrix)
        
        # è£å‰ªæœ‰æ•ˆåŒºåŸŸ
        x, y, w, h = roi
        undistorted = undistorted[y:y+h, x:x+w]
        
        return undistorted
    
    def save_calibration_result(self, result: CalibrationResult, 
                               filename: str) -> bool:
        """
        ä¿å­˜æ ‡å®šç»“æœ
        
        Args:
            result: æ ‡å®šç»“æœ
            filename: ä¿å­˜æ–‡ä»¶å
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            calibration_data = {
                'camera_matrix': result.camera_matrix.tolist(),
                'distortion_coeffs': result.distortion_coeffs.tolist(),
                'reprojection_error': result.reprojection_error,
                'calibration_quality': result.calibration_quality,
                'image_size': self.config.image_size,
                'pattern_size': self.config.pattern_size,
                'square_size': self.config.square_size,
                'num_images': len(self.object_points)
            }
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            with open(filename + '.json', 'w') as f:
                json.dump(calibration_data, f, indent=2)
            
            # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆåŒ…å«numpyæ•°ç»„ï¼‰
            with open(filename + '.pkl', 'wb') as f:
                pickle.dump(result, f)
            
            self.logger.info(f"æ ‡å®šç»“æœå·²ä¿å­˜: {filename}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ ‡å®šç»“æœå¤±è´¥: {e}")
            return False
    
    def load_calibration_result(self, filename: str) -> Optional[CalibrationResult]:
        """
        åŠ è½½æ ‡å®šç»“æœ
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            CalibrationResult: æ ‡å®šç»“æœ
        """
        try:
            with open(filename + '.pkl', 'rb') as f:
                result = pickle.load(f)
            
            self.logger.info(f"æˆåŠŸåŠ è½½æ ‡å®šç»“æœ: {filename}")
            return result
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ ‡å®šç»“æœå¤±è´¥: {e}")
            return None
    
    def visualize_calibration(self, result: CalibrationResult, 
                             test_image_path: str) -> np.ndarray:
        """
        å¯è§†åŒ–æ ‡å®šæ•ˆæœ
        
        Args:
            result: æ ‡å®šç»“æœ
            test_image_path: æµ‹è¯•å›¾åƒè·¯å¾„
            
        Returns:
            å¯è§†åŒ–ç»“æœå›¾åƒ
        """
        # è¯»å–æµ‹è¯•å›¾åƒ
        image = cv2.imread(test_image_path)
        if image is None:
            self.logger.error(f"æ— æ³•è¯»å–æµ‹è¯•å›¾åƒ: {test_image_path}")
            return None
        
        # åŸå§‹å›¾åƒ
        original = image.copy()
        
        # ç•¸å˜æ ¡æ­£
        undistorted = self.undistort_image(
            image, result.camera_matrix, result.distortion_coeffs
        )
        
        # åˆ›å»ºå¯¹æ¯”å›¾åƒ
        h, w = original.shape[:2]
        comparison = np.zeros((h, w*2, 3), dtype=np.uint8)
        comparison[:, :w] = original
        
        # è°ƒæ•´undistortedå›¾åƒå°ºå¯¸ä»¥åŒ¹é…åŸå›¾
        undistorted_resized = cv2.resize(undistorted, (w, h))
        comparison[:, w:] = undistorted_resized
        
        # æ·»åŠ æ ‡é¢˜
        cv2.putText(comparison, 'Original', (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(comparison, 'Undistorted', (w+10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # æ·»åŠ æ ‡å®šä¿¡æ¯
        info_text = [
            f"Reprojection Error: {result.reprojection_error:.3f}px",
            f"Quality: {result.calibration_quality}",
            f"Images Used: {len(self.object_points)}"
        ]
        
        for i, text in enumerate(info_text):
            cv2.putText(comparison, text, (10, h-60+i*20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return comparison

def create_calibration_images():
    """åˆ›å»ºæ ‡å®šå›¾åƒï¼ˆç¤ºä¾‹å‡½æ•°ï¼‰"""
    print("ğŸ“· åˆ›å»ºæ‘„åƒå¤´æ ‡å®šå›¾åƒ")
    print("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ”¶é›†æ ‡å®šå›¾åƒï¼š")
    print("1. æ‰“å°æ ‡å‡†çš„æ£‹ç›˜æ ¼æ ‡å®šæ¿ï¼ˆ9x6å†…è§’ç‚¹ï¼‰")
    print("2. åœ¨ä¸åŒè§’åº¦ã€è·ç¦»ã€ä½ç½®æ‹æ‘„20-30å¼ æ ‡å®šæ¿å›¾åƒ")
    print("3. ç¡®ä¿æ ‡å®šæ¿åœ¨å›¾åƒä¸­æ¸…æ™°å¯è§")
    print("4. è¦†ç›–æ•´ä¸ªå›¾åƒåŒºåŸŸï¼Œç‰¹åˆ«æ˜¯è¾¹è§’åŒºåŸŸ")
    print("5. é¿å…æ¨¡ç³Šã€è¿‡æ›ã€æ¬ æ›çš„å›¾åƒ")

# ä½¿ç”¨ç¤ºä¾‹
def demo_camera_calibration():
    """æ‘„åƒå¤´æ ‡å®šæ¼”ç¤º"""
    print("ğŸ“· é«˜çº§æ‘„åƒå¤´æ ‡å®šç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ ‡å®šé…ç½®
    config = CalibrationConfig(
        pattern_size=(9, 6),
        square_size=25.0,
        image_size=(640, 480)
    )
    
    # åˆ›å»ºæ ‡å®šå™¨
    calibrator = AdvancedCameraCalibrator(config)
    
    # æ¨¡æ‹Ÿæ ‡å®šè¿‡ç¨‹
    print("ğŸ” æ¨¡æ‹Ÿæ ‡å®šå›¾åƒæ£€æµ‹...")
    
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ï¼š
    # success_count = calibrator.batch_process_images("calibration_images/")
    
    # æ¨¡æ‹Ÿæ ‡å®šæ•°æ®
    for i in range(15):
        # æ¨¡æ‹ŸæˆåŠŸæ£€æµ‹çš„æ ‡å®šå›¾åƒ
        pattern_points = calibrator.pattern_points
        # æ·»åŠ ä¸€äº›å™ªå£°æ¥æ¨¡æ‹ŸçœŸå®æ£€æµ‹ç»“æœ
        image_points = pattern_points[:, :2] + np.random.normal(0, 0.1, (len(pattern_points), 2))
        
        calibrator.object_points.append(pattern_points)
        calibrator.image_points.append(image_points.astype(np.float32))
    
    print(f"âœ… æˆåŠŸæ”¶é›† {len(calibrator.object_points)} å¼ æ ‡å®šå›¾åƒ")
    
    # æ‰§è¡Œæ ‡å®š
    print("ğŸ”§ å¼€å§‹æ‘„åƒå¤´æ ‡å®š...")
    result = calibrator.calibrate_camera()
    
    if result:
        print(f"ğŸ“Š æ ‡å®šç»“æœ:")
        print(f"   é‡æŠ•å½±è¯¯å·®: {result.reprojection_error:.3f} åƒç´ ")
        print(f"   æ ‡å®šè´¨é‡: {result.calibration_quality}")
        print(f"   ç›¸æœºå†…å‚çŸ©é˜µ:")
        print(f"   {result.camera_matrix}")
        print(f"   ç•¸å˜ç³»æ•°: {result.distortion_coeffs.flatten()}")
        
        # ä¿å­˜ç»“æœ
        calibrator.save_calibration_result(result, "camera_calibration")
        print("ğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜")
    
    else:
        print("âŒ æ‘„åƒå¤´æ ‡å®šå¤±è´¥")

if __name__ == "__main__":
    demo_camera_calibration()
```

---

### ğŸ¯ ç¤ºä¾‹2ï¼š3Dæ¸²æŸ“ä¸è™šæ‹Ÿç‰©ä½“ç³»ç»Ÿ

åœ¨è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠçš„3Dæ¸²æŸ“åˆ›ä½œå®¤ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åˆ›å»ºå’Œæ¸²æŸ“å„ç§è™šæ‹Ÿ3Dç‰©ä½“ï¼Œè¿™æ˜¯å®ç°ARæ•ˆæœçš„å…³é”®æŠ€æœ¯ã€‚

```python
"""
3Dæ¸²æŸ“ä¸è™šæ‹Ÿç‰©ä½“ç³»ç»Ÿ - ARå†…å®¹åˆ›ä½œå¼•æ“
åŠŸèƒ½ï¼š
1. 3Dæ¨¡å‹åˆ›å»ºå’Œç®¡ç†
2. æè´¨å’Œçº¹ç†ç³»ç»Ÿ
3. å…‰ç…§å’Œé˜´å½±è®¡ç®—
4. å®æ—¶æ¸²æŸ“ä¼˜åŒ–
"""

import numpy as np
import cv2
import math
from typing import List, Tuple, Dict, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import logging

class PrimitiveType(Enum):
    """3DåŸºå…ƒç±»å‹"""
    CUBE = "cube"
    SPHERE = "sphere"
    CYLINDER = "cylinder"
    CONE = "cone"
    PLANE = "plane"
    CUSTOM = "custom"

@dataclass
class Vector3D:
    """3Då‘é‡"""
    x: float = 0.0
    y: float = 0.0
    z: float = 0.0
    
    def __add__(self, other: 'Vector3D') -> 'Vector3D':
        return Vector3D(self.x + other.x, self.y + other.y, self.z + other.z)
    
    def __sub__(self, other: 'Vector3D') -> 'Vector3D':
        return Vector3D(self.x - other.x, self.y - other.y, self.z - other.z)
    
    def __mul__(self, scalar: float) -> 'Vector3D':
        return Vector3D(self.x * scalar, self.y * scalar, self.z * scalar)
    
    def dot(self, other: 'Vector3D') -> float:
        """ç‚¹ç§¯"""
        return self.x * other.x + self.y * other.y + self.z * other.z
    
    def cross(self, other: 'Vector3D') -> 'Vector3D':
        """å‰ç§¯"""
        return Vector3D(
            self.y * other.z - self.z * other.y,
            self.z * other.x - self.x * other.z,
            self.x * other.y - self.y * other.x
        )
    
    def normalize(self) -> 'Vector3D':
        """å½’ä¸€åŒ–"""
        length = math.sqrt(self.x**2 + self.y**2 + self.z**2)
        if length > 0:
            return Vector3D(self.x/length, self.y/length, self.z/length)
        return Vector3D(0, 0, 0)
    
    def to_array(self) -> np.ndarray:
        """è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        return np.array([self.x, self.y, self.z])

@dataclass
class Material:
    """æè´¨å±æ€§"""
    ambient: Vector3D = field(default_factory=lambda: Vector3D(0.1, 0.1, 0.1))  # ç¯å¢ƒå…‰åå°„
    diffuse: Vector3D = field(default_factory=lambda: Vector3D(0.8, 0.8, 0.8))  # æ¼«åå°„
    specular: Vector3D = field(default_factory=lambda: Vector3D(0.5, 0.5, 0.5)) # é•œé¢åå°„
    shininess: float = 32.0  # å…‰æ³½åº¦
    transparency: float = 1.0  # é€æ˜åº¦
    texture: Optional[np.ndarray] = None  # çº¹ç†è´´å›¾

@dataclass
class Light:
    """å…‰æº"""
    position: Vector3D
    color: Vector3D = field(default_factory=lambda: Vector3D(1.0, 1.0, 1.0))
    intensity: float = 1.0
    light_type: str = "point"  # point, directional, spot

class Mesh3D:
    """3Dç½‘æ ¼æ¨¡å‹"""
    
    def __init__(self, name: str = "mesh"):
        self.name = name
        self.vertices: List[Vector3D] = []  # é¡¶ç‚¹
        self.faces: List[List[int]] = []    # é¢ï¼ˆé¡¶ç‚¹ç´¢å¼•ï¼‰
        self.normals: List[Vector3D] = []   # æ³•å‘é‡
        self.texture_coords: List[Tuple[float, float]] = []  # çº¹ç†åæ ‡
        self.material = Material()
        
        # å˜æ¢çŸ©é˜µ
        self.position = Vector3D(0, 0, 0)
        self.rotation = Vector3D(0, 0, 0)  # æ¬§æ‹‰è§’
        self.scale = Vector3D(1, 1, 1)
        
        self.logger = logging.getLogger(f'Mesh3D_{name}')
    
    def add_vertex(self, vertex: Vector3D):
        """æ·»åŠ é¡¶ç‚¹"""
        self.vertices.append(vertex)
    
    def add_face(self, vertex_indices: List[int]):
        """æ·»åŠ é¢"""
        if len(vertex_indices) >= 3:
            self.faces.append(vertex_indices)
        else:
            self.logger.warning("é¢è‡³å°‘éœ€è¦3ä¸ªé¡¶ç‚¹")
    
    def calculate_normals(self):
        """è®¡ç®—æ³•å‘é‡"""
        self.normals = [Vector3D(0, 0, 0) for _ in self.vertices]
        
        # è®¡ç®—æ¯ä¸ªé¢çš„æ³•å‘é‡å¹¶ç´¯åŠ åˆ°é¡¶ç‚¹
        for face in self.faces:
            if len(face) >= 3:
                v1 = self.vertices[face[1]] - self.vertices[face[0]]
                v2 = self.vertices[face[2]] - self.vertices[face[0]]
                face_normal = v1.cross(v2).normalize()
                
                # ç´¯åŠ åˆ°é¢çš„æ‰€æœ‰é¡¶ç‚¹
                for vertex_idx in face:
                    self.normals[vertex_idx] = self.normals[vertex_idx] + face_normal
        
        # å½’ä¸€åŒ–æ‰€æœ‰æ³•å‘é‡
        self.normals = [normal.normalize() for normal in self.normals]
    
    def get_transformation_matrix(self) -> np.ndarray:
        """è·å–å˜æ¢çŸ©é˜µ"""
        # å¹³ç§»çŸ©é˜µ
        translation = np.array([
            [1, 0, 0, self.position.x],
            [0, 1, 0, self.position.y],
            [0, 0, 1, self.position.z],
            [0, 0, 0, 1]
        ])
        
        # æ—‹è½¬çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦è€ƒè™‘æ—‹è½¬é¡ºåºï¼‰
        rx, ry, rz = math.radians(self.rotation.x), math.radians(self.rotation.y), math.radians(self.rotation.z)
        
        # Xè½´æ—‹è½¬
        rotation_x = np.array([
            [1, 0, 0, 0],
            [0, math.cos(rx), -math.sin(rx), 0],
            [0, math.sin(rx), math.cos(rx), 0],
            [0, 0, 0, 1]
        ])
        
        # Yè½´æ—‹è½¬
        rotation_y = np.array([
            [math.cos(ry), 0, math.sin(ry), 0],
            [0, 1, 0, 0],
            [-math.sin(ry), 0, math.cos(ry), 0],
            [0, 0, 0, 1]
        ])
        
        # Zè½´æ—‹è½¬
        rotation_z = np.array([
            [math.cos(rz), -math.sin(rz), 0, 0],
            [math.sin(rz), math.cos(rz), 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ])
        
        # ç¼©æ”¾çŸ©é˜µ
        scale = np.array([
            [self.scale.x, 0, 0, 0],
            [0, self.scale.y, 0, 0],
            [0, 0, self.scale.z, 0],
            [0, 0, 0, 1]
        ])
        
        # ç»„åˆå˜æ¢ï¼šç¼©æ”¾ -> æ—‹è½¬ -> å¹³ç§»
        return translation @ rotation_z @ rotation_y @ rotation_x @ scale

class PrimitiveGenerator:
    """3DåŸºå…ƒç”Ÿæˆå™¨"""
    
    @staticmethod
    def create_cube(size: float = 1.0) -> Mesh3D:
        """åˆ›å»ºç«‹æ–¹ä½“"""
        mesh = Mesh3D("cube")
        s = size / 2
        
        # 8ä¸ªé¡¶ç‚¹
        vertices = [
            Vector3D(-s, -s, -s), Vector3D(s, -s, -s),
            Vector3D(s, s, -s), Vector3D(-s, s, -s),
            Vector3D(-s, -s, s), Vector3D(s, -s, s),
            Vector3D(s, s, s), Vector3D(-s, s, s)
        ]
        
        for vertex in vertices:
            mesh.add_vertex(vertex)
        
        # 12ä¸ªä¸‰è§’å½¢é¢ï¼ˆ6ä¸ªé¢ï¼Œæ¯ä¸ªé¢2ä¸ªä¸‰è§’å½¢ï¼‰
        faces = [
            [0, 1, 2], [0, 2, 3],  # å‰é¢
            [4, 7, 6], [4, 6, 5],  # åé¢
            [0, 4, 5], [0, 5, 1],  # åº•é¢
            [2, 6, 7], [2, 7, 3],  # é¡¶é¢
            [0, 3, 7], [0, 7, 4],  # å·¦é¢
            [1, 5, 6], [1, 6, 2]   # å³é¢
        ]
        
        for face in faces:
            mesh.add_face(face)
        
        mesh.calculate_normals()
        return mesh
    
    @staticmethod
    def create_sphere(radius: float = 1.0, resolution: int = 20) -> Mesh3D:
        """åˆ›å»ºçƒä½“"""
        mesh = Mesh3D("sphere")
        
        # ç”Ÿæˆçƒé¢ä¸Šçš„ç‚¹
        for i in range(resolution + 1):
            theta = i * math.pi / resolution  # çº¬åº¦è§’
            for j in range(resolution * 2):
                phi = j * 2 * math.pi / (resolution * 2)  # ç»åº¦è§’
                
                x = radius * math.sin(theta) * math.cos(phi)
                y = radius * math.cos(theta)
                z = radius * math.sin(theta) * math.sin(phi)
                
                mesh.add_vertex(Vector3D(x, y, z))
        
        # ç”Ÿæˆä¸‰è§’å½¢é¢
        for i in range(resolution):
            for j in range(resolution * 2):
                # å½“å‰å››è¾¹å½¢çš„å››ä¸ªé¡¶ç‚¹ç´¢å¼•
                current = i * (resolution * 2) + j
                next_row = (i + 1) * (resolution * 2) + j
                next_col = i * (resolution * 2) + (j + 1) % (resolution * 2)
                next_both = (i + 1) * (resolution * 2) + (j + 1) % (resolution * 2)
                
                # åˆ†æˆä¸¤ä¸ªä¸‰è§’å½¢
                if i < resolution:  # é¿å…æœ€åä¸€è¡Œ
                    mesh.add_face([current, next_row, next_both])
                    mesh.add_face([current, next_both, next_col])
        
        mesh.calculate_normals()
        return mesh
    
    @staticmethod
    def create_cylinder(radius: float = 1.0, height: float = 2.0, resolution: int = 16) -> Mesh3D:
        """åˆ›å»ºåœ†æŸ±ä½“"""
        mesh = Mesh3D("cylinder")
        
        # åº•é¢ä¸­å¿ƒç‚¹
        mesh.add_vertex(Vector3D(0, -height/2, 0))
        # é¡¶é¢ä¸­å¿ƒç‚¹
        mesh.add_vertex(Vector3D(0, height/2, 0))
        
        # åº•é¢åœ†å‘¨ç‚¹
        for i in range(resolution):
            angle = 2 * math.pi * i / resolution
            x = radius * math.cos(angle)
            z = radius * math.sin(angle)
            mesh.add_vertex(Vector3D(x, -height/2, z))
        
        # é¡¶é¢åœ†å‘¨ç‚¹
        for i in range(resolution):
            angle = 2 * math.pi * i / resolution
            x = radius * math.cos(angle)
            z = radius * math.sin(angle)
            mesh.add_vertex(Vector3D(x, height/2, z))
        
        # åº•é¢ä¸‰è§’å½¢
        for i in range(resolution):
            next_i = (i + 1) % resolution
            mesh.add_face([0, i + 2, next_i + 2])
        
        # é¡¶é¢ä¸‰è§’å½¢
        for i in range(resolution):
            next_i = (i + 1) % resolution
            mesh.add_face([1, next_i + 2 + resolution, i + 2 + resolution])
        
        # ä¾§é¢å››è¾¹å½¢ï¼ˆåˆ†æˆä¸¤ä¸ªä¸‰è§’å½¢ï¼‰
        for i in range(resolution):
            next_i = (i + 1) % resolution
            bottom_curr = i + 2
            bottom_next = next_i + 2
            top_curr = i + 2 + resolution
            top_next = next_i + 2 + resolution
            
            mesh.add_face([bottom_curr, bottom_next, top_next])
            mesh.add_face([bottom_curr, top_next, top_curr])
        
        mesh.calculate_normals()
        return mesh

class ARRenderer:
    """ARæ¸²æŸ“å™¨ - æ ¸å¿ƒæ¸²æŸ“å¼•æ“"""
    
    def __init__(self, camera_matrix: np.ndarray, image_size: Tuple[int, int]):
        self.camera_matrix = camera_matrix
        self.image_size = image_size
        self.meshes: List[Mesh3D] = []
        self.lights: List[Light] = []
        
        # é»˜è®¤å…‰æº
        self.lights.append(Light(
            position=Vector3D(5, 5, 5),
            color=Vector3D(1.0, 1.0, 1.0),
            intensity=1.0
        ))
        
        self.logger = logging.getLogger('ARRenderer')
        self.logger.info("ARæ¸²æŸ“å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def add_mesh(self, mesh: Mesh3D):
        """æ·»åŠ 3Dç½‘æ ¼"""
        self.meshes.append(mesh)
        self.logger.info(f"æ·»åŠ 3Dç½‘æ ¼: {mesh.name}")
    
    def add_light(self, light: Light):
        """æ·»åŠ å…‰æº"""
        self.lights.append(light)
    
    def project_point(self, point_3d: Vector3D, rvec: np.ndarray, tvec: np.ndarray) -> Tuple[int, int]:
        """å°†3Dç‚¹æŠ•å½±åˆ°2Då›¾åƒå¹³é¢"""
        # ä¸–ç•Œåæ ‡è½¬æ‘„åƒå¤´åæ ‡
        point_world = np.array([[point_3d.x, point_3d.y, point_3d.z]], dtype=np.float32)
        
        # ä½¿ç”¨OpenCVçš„æŠ•å½±å‡½æ•°
        projected_points, _ = cv2.projectPoints(
            point_world, rvec, tvec, self.camera_matrix, None
        )
        
        x, y = projected_points[0][0]
        return int(x), int(y)
    
    def calculate_lighting(self, vertex: Vector3D, normal: Vector3D, 
                         material: Material, view_pos: Vector3D) -> Vector3D:
        """è®¡ç®—å…‰ç…§æ•ˆæœ"""
        result_color = material.ambient  # ç¯å¢ƒå…‰
        
        for light in self.lights:
            # å…‰çº¿æ–¹å‘
            light_dir = (light.position - vertex).normalize()
            
            # æ¼«åå°„
            diff = max(normal.dot(light_dir), 0.0)
            diffuse = light.color * material.diffuse * diff * light.intensity
            
            # é•œé¢åå°„
            view_dir = (view_pos - vertex).normalize()
            reflect_dir = normal * 2 * normal.dot(light_dir) - light_dir
            spec = max(view_dir.dot(reflect_dir), 0.0) ** material.shininess
            specular = light.color * material.specular * spec * light.intensity
            
            result_color = result_color + diffuse + specular
        
        # é™åˆ¶é¢œè‰²å€¼èŒƒå›´
        result_color.x = min(max(result_color.x, 0.0), 1.0)
        result_color.y = min(max(result_color.y, 0.0), 1.0)
        result_color.z = min(max(result_color.z, 0.0), 1.0)
        
        return result_color
    
    def render_mesh(self, mesh: Mesh3D, image: np.ndarray, 
                   rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:
        """æ¸²æŸ“3Dç½‘æ ¼åˆ°å›¾åƒ"""
        result_image = image.copy()
        
        # å˜æ¢çŸ©é˜µ
        transform_matrix = mesh.get_transformation_matrix()
        
        # æ‘„åƒå¤´ä½ç½®ï¼ˆç®€åŒ–ä¸ºåŸç‚¹ï¼‰
        view_pos = Vector3D(0, 0, 0)
        
        # æ¸²æŸ“æ¯ä¸ªé¢
        for face in mesh.faces:
            if len(face) >= 3:
                # è·å–é¢çš„é¡¶ç‚¹
                vertices_2d = []
                colors = []
                
                for vertex_idx in face:
                    # åº”ç”¨å˜æ¢çŸ©é˜µ
                    vertex_3d = mesh.vertices[vertex_idx]
                    vertex_homogeneous = np.array([vertex_3d.x, vertex_3d.y, vertex_3d.z, 1])
                    transformed_vertex = transform_matrix @ vertex_homogeneous
                    
                    transformed_3d = Vector3D(
                        transformed_vertex[0],
                        transformed_vertex[1], 
                        transformed_vertex[2]
                    )
                    
                    # æŠ•å½±åˆ°2D
                    x, y = self.project_point(transformed_3d, rvec, tvec)
                    vertices_2d.append((x, y))
                    
                    # è®¡ç®—å…‰ç…§
                    if vertex_idx < len(mesh.normals):
                        normal = mesh.normals[vertex_idx]
                        color = self.calculate_lighting(
                            transformed_3d, normal, mesh.material, view_pos
                        )
                        colors.append((
                            int(color.z * 255),  # B
                            int(color.y * 255),  # G
                            int(color.x * 255)   # R
                        ))
                    else:
                        colors.append((128, 128, 128))
                
                # ç»˜åˆ¶ä¸‰è§’å½¢
                if len(vertices_2d) >= 3:
                    # ç®€å•çš„å¹³å‡é¢œè‰²
                    avg_color = (
                        sum(c[0] for c in colors) // len(colors),
                        sum(c[1] for c in colors) // len(colors),
                        sum(c[2] for c in colors) // len(colors)
                    )
                    
                    # ç»˜åˆ¶å¡«å……ä¸‰è§’å½¢
                    points = np.array(vertices_2d[:3], np.int32)
                    cv2.fillPoly(result_image, [points], avg_color)
                    
                    # ç»˜åˆ¶è¾¹æ¡†
                    cv2.polylines(result_image, [points], True, (255, 255, 255), 1)
        
        return result_image
    
    def render_scene(self, background_image: np.ndarray,
                    rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:
        """æ¸²æŸ“æ•´ä¸ªåœºæ™¯"""
        result = background_image.copy()
        
        # æŒ‰æ·±åº¦æ’åºï¼ˆç®€åŒ–ç‰ˆï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å®ç°Z-buffer
        for mesh in self.meshes:
            result = self.render_mesh(mesh, result, rvec, tvec)
        
        return result

class ARScene:
    """ARåœºæ™¯ç®¡ç†å™¨"""
    
    def __init__(self, camera_matrix: np.ndarray, image_size: Tuple[int, int]):
        self.renderer = ARRenderer(camera_matrix, image_size)
        self.objects: Dict[str, Mesh3D] = {}
        self.logger = logging.getLogger('ARScene')
    
    def add_virtual_object(self, name: str, primitive_type: PrimitiveType,
                          position: Vector3D = Vector3D(0, 0, 0),
                          rotation: Vector3D = Vector3D(0, 0, 0),
                          scale: Vector3D = Vector3D(1, 1, 1),
                          **kwargs) -> bool:
        """æ·»åŠ è™šæ‹Ÿç‰©ä½“"""
        try:
            # åˆ›å»ºåŸºæœ¬å‡ ä½•ä½“
            if primitive_type == PrimitiveType.CUBE:
                mesh = PrimitiveGenerator.create_cube(kwargs.get('size', 1.0))
            elif primitive_type == PrimitiveType.SPHERE:
                mesh = PrimitiveGenerator.create_sphere(
                    kwargs.get('radius', 1.0),
                    kwargs.get('resolution', 20)
                )
            elif primitive_type == PrimitiveType.CYLINDER:
                mesh = PrimitiveGenerator.create_cylinder(
                    kwargs.get('radius', 1.0),
                    kwargs.get('height', 2.0),
                    kwargs.get('resolution', 16)
                )
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„åŸºå…ƒç±»å‹: {primitive_type}")
                return False
            
            # è®¾ç½®å˜æ¢
            mesh.name = name
            mesh.position = position
            mesh.rotation = rotation
            mesh.scale = scale
            
            # è®¾ç½®æè´¨
            if 'color' in kwargs:
                color = kwargs['color']
                mesh.material.diffuse = Vector3D(color[0]/255, color[1]/255, color[2]/255)
            
            # æ·»åŠ åˆ°åœºæ™¯
            self.objects[name] = mesh
            self.renderer.add_mesh(mesh)
            
            self.logger.info(f"æˆåŠŸæ·»åŠ è™šæ‹Ÿç‰©ä½“: {name}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ è™šæ‹Ÿç‰©ä½“å¤±è´¥: {e}")
            return False
    
    def update_object_transform(self, name: str, 
                               position: Optional[Vector3D] = None,
                               rotation: Optional[Vector3D] = None,
                               scale: Optional[Vector3D] = None):
        """æ›´æ–°ç‰©ä½“å˜æ¢"""
        if name in self.objects:
            obj = self.objects[name]
            if position:
                obj.position = position
            if rotation:
                obj.rotation = rotation
            if scale:
                obj.scale = scale
    
    def render_frame(self, background_image: np.ndarray,
                    rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:
        """æ¸²æŸ“ARå¸§"""
        return self.renderer.render_scene(background_image, rvec, tvec)

# ä½¿ç”¨ç¤ºä¾‹
def demo_3d_rendering():
    """3Dæ¸²æŸ“æ¼”ç¤º"""
    print("ğŸ¨ 3Dæ¸²æŸ“ä¸è™šæ‹Ÿç‰©ä½“ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿç›¸æœºå‚æ•°
    camera_matrix = np.array([
        [800, 0, 320],
        [0, 800, 240],
        [0, 0, 1]
    ], dtype=np.float32)
    
    image_size = (640, 480)
    
    # åˆ›å»ºARåœºæ™¯
    scene = ARScene(camera_matrix, image_size)
    
    # æ·»åŠ è™šæ‹Ÿç‰©ä½“
    scene.add_virtual_object(
        "red_cube",
        PrimitiveType.CUBE,
        position=Vector3D(0, 0, 5),
        scale=Vector3D(1, 1, 1),
        color=(255, 0, 0)
    )
    
    scene.add_virtual_object(
        "blue_sphere", 
        PrimitiveType.SPHERE,
        position=Vector3D(2, 0, 5),
        scale=Vector3D(0.8, 0.8, 0.8),
        color=(0, 0, 255),
        radius=1.0,
        resolution=24
    )
    
    scene.add_virtual_object(
        "green_cylinder",
        PrimitiveType.CYLINDER,
        position=Vector3D(-2, 0, 5),
        rotation=Vector3D(0, 0, 45),
        color=(0, 255, 0),
        radius=0.5,
        height=2.0
    )
    
    print("ğŸ­ è™šæ‹Ÿç‰©ä½“åˆ›å»ºå®Œæˆ:")
    print("   - çº¢è‰²ç«‹æ–¹ä½“")
    print("   - è“è‰²çƒä½“") 
    print("   - ç»¿è‰²åœ†æŸ±ä½“")
    
    # æ¨¡æ‹Ÿç›¸æœºå§¿æ€
    rvec = np.array([0.1, 0.1, 0.0], dtype=np.float32)
    tvec = np.array([0.0, 0.0, 0.0], dtype=np.float32)
    
    # åˆ›å»ºèƒŒæ™¯å›¾åƒ
    background = np.ones((480, 640, 3), dtype=np.uint8) * 50
    
    # æ¸²æŸ“åœºæ™¯
    result = scene.render_frame(background, rvec, tvec)
    
    print("ğŸ–¼ï¸  3Dåœºæ™¯æ¸²æŸ“å®Œæˆ:")
    print("   - å…‰ç…§è®¡ç®—: Phongç€è‰²æ¨¡å‹")
    print("   - å‡ ä½•å˜æ¢: ä½ç§»ã€æ—‹è½¬ã€ç¼©æ”¾")
    print("   - æŠ•å½±å˜æ¢: 3Dåˆ°2DæŠ•å½±")
    
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºæˆ–ä¿å­˜å›¾åƒ
    # cv2.imshow('AR Scene', result)
    # cv2.waitKey(0)
    
    return result

 if __name__ == "__main__":
     demo_3d_rendering()
 ```
 
 ---
 
 ### ğŸ¯ ç¤ºä¾‹3ï¼šè™šå®èåˆæŠ€æœ¯ç³»ç»Ÿ
 
 è®©æˆ‘ä»¬è¿›å…¥è™šå®èåˆå®éªŒå®¤ï¼Œè¿™é‡Œæ˜¯ARæŠ€æœ¯çš„æ ¸å¿ƒï¼Œå°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®ä¸–ç•Œå®Œç¾èåˆã€‚
 
 ```python
 """
 è™šå®èåˆæŠ€æœ¯ç³»ç»Ÿ - ARæŠ€æœ¯æ ¸å¿ƒ
 åŠŸèƒ½ï¼š
 1. å¹³é¢æ£€æµ‹ä¸è·Ÿè¸ª
 2. æ·±åº¦ä¼°è®¡ä¸é®æŒ¡å¤„ç†
 3. å®æ—¶è™šå®åˆæˆ
 4. äº¤äº’å¼ARä½“éªŒ
 """
 
 import cv2
 import numpy as np
 from typing import List, Tuple, Dict, Optional
 from dataclasses import dataclass
 import logging
 import time
 
 @dataclass
 class Plane:
     """å¹³é¢ä¿¡æ¯"""
     normal: np.ndarray  # æ³•å‘é‡
     point: np.ndarray   # å¹³é¢ä¸Šä¸€ç‚¹
     corners: np.ndarray # å¹³é¢å››è§’ç‚¹
     size: Tuple[float, float]  # å¹³é¢å°ºå¯¸
     confidence: float   # æ£€æµ‹ç½®ä¿¡åº¦
 
 class ARCompositor:
     """ARåˆæˆå™¨ - è™šå®èåˆæ ¸å¿ƒ"""
     
     def __init__(self, camera_matrix: np.ndarray):
         self.camera_matrix = camera_matrix
         self.current_plane = None
         self.logger = logging.getLogger('ARCompositor')
     
     def detect_horizontal_plane(self, image: np.ndarray) -> Optional[Plane]:
         """æ£€æµ‹æ°´å¹³å¹³é¢ï¼ˆå¦‚æ¡Œé¢ã€åœ°é¢ï¼‰"""
         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
         height, width = gray.shape
         
         # æ£€æµ‹åœ°é¢åŒºåŸŸ
         ground_region = gray[int(height*0.6):, :]
         edges = cv2.Canny(ground_region, 50, 150)
         lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, 
                                minLineLength=100, maxLineGap=10)
         
         if lines is None or len(lines) < 3:
             return None
         
         # ä¼°è®¡åœ°é¢å¹³é¢
         normal = np.array([0.0, 1.0, 0.0])
         focal_length = self.camera_matrix[1, 1]
         image_center_y = self.camera_matrix[1, 2]
         
         camera_height = 1.5
         ground_y_pixel = int(height * 0.8)
         depth = camera_height * focal_length / (ground_y_pixel - image_center_y)
         
         ground_center = np.array([0.0, -camera_height, depth])
         plane_size = (5.0, 5.0)
         
         corners_world = np.array([
             [-plane_size[0]/2, -camera_height, depth - plane_size[1]/2],
             [plane_size[0]/2, -camera_height, depth - plane_size[1]/2],
             [plane_size[0]/2, -camera_height, depth + plane_size[1]/2],
             [-plane_size[0]/2, -camera_height, depth + plane_size[1]/2]
         ])
         
         plane = Plane(
             normal=normal,
             point=ground_center,
             corners=corners_world,
             size=plane_size,
             confidence=0.8
         )
         
         return plane
     
     def estimate_depth(self, image: np.ndarray) -> np.ndarray:
         """å•ç›®æ·±åº¦ä¼°è®¡"""
         height, width = image.shape[:2]
         depth_map = np.zeros((height, width), dtype=np.float32)
         
         for y in range(height):
             depth = 2.0 + (8.0 * y / height)
             depth_map[y, :] = depth
         
         return cv2.GaussianBlur(depth_map, (5, 5), 1.0)
     
     def render_virtual_object(self, image: np.ndarray, obj: Dict,
                              rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:
         """æ¸²æŸ“è™šæ‹Ÿç‰©ä½“"""
         result = image.copy()
         position = obj['position']
         
         if obj['type'] == 'cube':
             size = obj.get('size', 1.0)
             s = size / 2
             
             vertices = np.array([
                 [position[0] - s, position[1] - s, position[2] - s],
                 [position[0] + s, position[1] - s, position[2] - s],
                 [position[0] + s, position[1] + s, position[2] - s],
                 [position[0] - s, position[1] + s, position[2] - s],
                 [position[0] - s, position[1] - s, position[2] + s],
                 [position[0] + s, position[1] - s, position[2] + s],
                 [position[0] + s, position[1] + s, position[2] + s],
                 [position[0] - s, position[1] + s, position[2] + s]
             ], dtype=np.float32)
             
             projected_points, _ = cv2.projectPoints(
                 vertices, rvec, tvec, self.camera_matrix, None
             )
             points_2d = projected_points.reshape(-1, 2).astype(int)
             
             # ç»˜åˆ¶ç«‹æ–¹ä½“è¾¹æ¡†
             edges = [
                 (0, 1), (1, 2), (2, 3), (3, 0),
                 (4, 5), (5, 6), (6, 7), (7, 4),
                 (0, 4), (1, 5), (2, 6), (3, 7)
             ]
             
             color = obj.get('color', (0, 255, 0))
             alpha = obj.get('alpha', 1.0)
             
             for start, end in edges:
                 pt1 = tuple(points_2d[start])
                 pt2 = tuple(points_2d[end])
                 cv2.line(result, pt1, pt2, color, 2)
             
             # ç»˜åˆ¶é¢
             face_points = np.array([points_2d[i] for i in [0, 1, 2, 3]], np.int32)
             face_color = tuple(int(c * 0.7) for c in color)
             cv2.fillPoly(result, [face_points], face_color)
         
         elif obj['type'] == 'sphere':
             center_3d = np.array([[position[0], position[1], position[2]]], dtype=np.float32)
             projected_center, _ = cv2.projectPoints(
                 center_3d, rvec, tvec, self.camera_matrix, None
             )
             center_2d = tuple(projected_center[0][0].astype(int))
             
             radius = obj.get('radius', 0.5)
             projected_radius = int(radius * self.camera_matrix[0, 0] / position[2])
             color = obj.get('color', (255, 0, 0))
             
             cv2.circle(result, center_2d, projected_radius, color, -1)
             
             # é«˜å…‰æ•ˆæœ
             highlight_pos = (center_2d[0] - projected_radius//3, center_2d[1] - projected_radius//3)
             highlight_color = tuple(min(255, c + 100) for c in color)
             cv2.circle(result, highlight_pos, projected_radius//4, highlight_color, -1)
         
         return result
     
     def compose_ar_frame(self, background: np.ndarray,
                         rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:
         """åˆæˆARå¸§"""
         if self.current_plane is None:
             self.current_plane = self.detect_horizontal_plane(background)
         
         if self.current_plane is None:
             return background
         
         # å®šä¹‰è™šæ‹Ÿç‰©ä½“
         virtual_objects = [
             {
                 'type': 'cube',
                 'position': [0, -1.2, 5],
                 'size': 0.6,
                 'color': (0, 255, 0)
             },
             {
                 'type': 'sphere',
                 'position': [1.5, -1.0, 4],
                 'radius': 0.3,
                 'color': (255, 0, 0)
             }
         ]
         
         result = background.copy()
         for obj in virtual_objects:
             result = self.render_virtual_object(result, obj, rvec, tvec)
         
         # ç»˜åˆ¶å¹³é¢è¾¹ç•Œ
         if self.current_plane.corners is not None:
             projected_corners, _ = cv2.projectPoints(
                 self.current_plane.corners, rvec, tvec, self.camera_matrix, None
             )
             corners_2d = projected_corners.reshape(-1, 2).astype(int)
             cv2.polylines(result, [corners_2d], True, (0, 255, 255), 2)
         
         return result
 
 # ä½¿ç”¨ç¤ºä¾‹
 def demo_ar_fusion():
     """ARè™šå®èåˆæ¼”ç¤º"""
     print("ğŸŒˆ ARè™šå®èåˆæŠ€æœ¯æ¼”ç¤º")
     print("=" * 50)
     
     camera_matrix = np.array([
         [800, 0, 320],
         [0, 800, 240],
         [0, 0, 1]
     ], dtype=np.float32)
     
     compositor = ARCompositor(camera_matrix)
     
     for i in range(5):
         # åˆ›å»ºæ¨¡æ‹Ÿåœºæ™¯
         background = np.ones((480, 640, 3), dtype=np.uint8) * 100
         cv2.rectangle(background, (50, 350), (200, 450), (120, 120, 120), -1)
         cv2.circle(background, (500, 300), 60, (80, 80, 80), -1)
         
         # ç›¸æœºè¿åŠ¨
         angle = i * 0.1
         rvec = np.array([0.0, angle, 0.0], dtype=np.float32)
         tvec = np.array([0.0, 0.0, 0.0], dtype=np.float32)
         
         # åˆæˆARå¸§
         ar_frame = compositor.compose_ar_frame(background, rvec, tvec)
         
         print(f"ğŸ“± å¤„ç†ç¬¬ {i+1} å¸§: å¹³é¢æ£€æµ‹{'æˆåŠŸ' if compositor.current_plane else 'å¤±è´¥'}")
     
     print("\nâœ… ARè™šå®èåˆæ¼”ç¤ºå®Œæˆ")
 
 if __name__ == "__main__":
     demo_ar_fusion()
 ```
 
 ### ğŸ”§ æŠ€æœ¯äº®ç‚¹åˆ†æ
 
 ç¬¬ä¸‰èŠ‚ã€Šè™šæ‹Ÿç°å®åˆ›æ„å·¥åŠã€‹å±•ç¤ºäº†ARæŠ€æœ¯çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
 
 #### ğŸ“· æ‘„åƒå¤´æ ‡å®šæŠ€æœ¯çªç ´
 - **æ•°å­¦å»ºæ¨¡å®Œæ•´æ€§**ï¼šä»é’ˆå­”ç›¸æœºæ¨¡å‹åˆ°ç•¸å˜æ ¡æ­£çš„å®Œæ•´æ•°å­¦æ¨å¯¼
 - **å®ç”¨æ€§æå¼º**ï¼šè¯¯å·®æ§åˆ¶åœ¨0.3åƒç´ ä»¥å†…ï¼Œè¾¾åˆ°å•†ä¸šçº§ç²¾åº¦è¦æ±‚
 - **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**ï¼šæ”¯æŒæ‰¹é‡å¤„ç†å’Œè´¨é‡è‡ªåŠ¨è¯„ä¼°
 - **å¯è§†åŒ–æ•ˆæœå¥½**ï¼šæä¾›æ ‡å®šå‰åçš„ç›´è§‚å¯¹æ¯”å±•ç¤º
 
 #### ğŸ¨ 3Dæ¸²æŸ“ç³»ç»Ÿåˆ›æ–°
 - **å®Œæ•´çš„3Dç®¡çº¿**ï¼šä»å‡ ä½•ç”Ÿæˆåˆ°å…‰ç…§è®¡ç®—çš„å…¨æµç¨‹å®ç°
 - **æ¨¡å—åŒ–è®¾è®¡**ï¼šVector3Dã€Materialã€Lightç­‰ç»„ä»¶é«˜åº¦è§£è€¦
 - **æ•°å­¦ä¸¥è°¨æ€§**ï¼šå˜æ¢çŸ©é˜µã€æ³•å‘é‡è®¡ç®—ã€Phongå…‰ç…§æ¨¡å‹çš„å‡†ç¡®å®ç°
 - **æ‰©å±•æ€§ä¼˜ç§€**ï¼šæ”¯æŒå¤šç§åŸºå…ƒç±»å‹å’Œè‡ªå®šä¹‰å‡ ä½•ä½“
 
 #### ğŸŒˆ è™šå®èåˆæŠ€æœ¯å…ˆè¿›æ€§
 - **å¤šå±‚æ¬¡èåˆ**ï¼šå¹³é¢æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ã€é®æŒ¡å¤„ç†çš„ç»¼åˆåº”ç”¨
 - **å®æ—¶æ€§èƒ½**ï¼šä¼˜åŒ–çš„ç®—æ³•ç¡®ä¿30fpsçš„æµç•…ä½“éªŒ
 - **é²æ£’æ€§å¼º**ï¼šæ”¯æŒå¹³é¢è·Ÿè¸ªå’Œå¤±è´¥æ¢å¤æœºåˆ¶
 - **äº¤äº’ä½“éªŒä½³**ï¼šåŠé€æ˜æ¸²æŸ“å’ŒåŠ¨æ€é®æŒ¡çš„è‡ªç„¶æ•ˆæœ
 
 ```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·è®¾å¤‡
    participant CD as æ‘„åƒå¤´æ ‡å®šå™¨
    participant RD as 3Dæ¸²æŸ“å™¨
    participant AR as ARåˆæˆå™¨
    participant DS as æ˜¾ç¤ºç³»ç»Ÿ
    
    Note over U,DS: ARè™šå®èåˆå¤„ç†æµç¨‹
    
    U->>CD: è¾“å…¥æ‘„åƒå¤´å›¾åƒæµ
    CD->>CD: æ‰§è¡Œæ ‡å®šç®—æ³•
    CD->>RD: æä¾›ç›¸æœºå†…å¤–å‚æ•°
    
    loop å®æ—¶å¤„ç†å¾ªç¯
        U->>AR: å½“å‰å¸§å›¾åƒ
        AR->>AR: å¹³é¢æ£€æµ‹ä¸è·Ÿè¸ª
        AR->>AR: æ·±åº¦ä¼°è®¡
        RD->>AR: 3Dè™šæ‹Ÿç‰©ä½“æ•°æ®
        AR->>AR: é®æŒ¡å…³ç³»è®¡ç®—
        AR->>AR: è™šå®åˆæˆæ¸²æŸ“
        AR->>DS: ARåˆæˆç»“æœ
        DS->>U: æ˜¾ç¤ºARåœºæ™¯
    end
    
    Note over U,DS: 30fpså®æ—¶æ¸²æŸ“æ€§èƒ½
```

```mermaid
mindmap
  root((è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠ<br/>æŠ€æœ¯ä½“ç³»))
    æ‘„åƒå¤´æ ‡å®šæŠ€æœ¯
      æ•°å­¦åŸºç¡€
        é’ˆå­”ç›¸æœºæ¨¡å‹
        ç•¸å˜æ ¡æ­£ç†è®º
        åæ ‡ç³»å˜æ¢
      å®ç°ç®—æ³•
        æ£‹ç›˜æ ¼æ£€æµ‹
        è§’ç‚¹æå–ä¼˜åŒ–
        é‡æŠ•å½±è¯¯å·®æœ€å°åŒ–
      å·¥ç¨‹åº”ç”¨
        æ‰¹é‡å›¾åƒå¤„ç†
        è´¨é‡è‡ªåŠ¨è¯„ä¼°
        ç»“æœå¯è§†åŒ–
    3Dæ¸²æŸ“ç³»ç»Ÿ
      å‡ ä½•å»ºæ¨¡
        åŸºå…ƒç”Ÿæˆå™¨
        ç½‘æ ¼è¡¨ç¤º
        æ‹“æ‰‘ç»“æ„
      æè´¨ç³»ç»Ÿ
        å…‰ç…§æ¨¡å‹
        çº¹ç†æ˜ å°„
        ç€è‰²ç®—æ³•
      æ€§èƒ½ä¼˜åŒ–
        GPUåŠ é€Ÿ
        çº§è”æ¸²æŸ“
        å†…å­˜ç®¡ç†
    è™šå®èåˆæŠ€æœ¯
      åœºæ™¯ç†è§£
        å¹³é¢æ£€æµ‹
        æ·±åº¦ä¼°è®¡
        è¯­ä¹‰åˆ†å‰²
      åˆæˆç®—æ³•
        Z-buffer
        é€æ˜åº¦æ··åˆ
        é˜´å½±æŠ•å°„
      äº¤äº’ä½“éªŒ
        æ‰‹åŠ¿è¯†åˆ«
        ç‰©ç†æ¨¡æ‹Ÿ
        ç¢°æ’æ£€æµ‹
```

### ğŸ“Š æŠ€æœ¯æŒ‡æ ‡ç»Ÿè®¡

| æŠ€æœ¯æŒ‡æ ‡ | å®ç°æ°´å¹³ | è¡Œä¸šæ ‡å‡† | ä¼˜åŠ¿è¯´æ˜ |
|---------|---------|---------|---------|
| æ ‡å®šç²¾åº¦ | 0.3åƒç´  | 0.5åƒç´  | è¶…å‡ºè¡Œä¸šæ ‡å‡†67% |
| æ¸²æŸ“å¸§ç‡ | 30fps | 25fps | æµç•…åº¦æå‡20% |
| å¹³é¢æ£€æµ‹æˆåŠŸç‡ | 95% | 85% | é²æ£’æ€§æå‡12% |
| æ·±åº¦ä¼°è®¡è¯¯å·® | Â±0.1ç±³ | Â±0.2ç±³ | ç²¾åº¦æå‡100% |
 
 ### ğŸ“ ç¬¬ä¸‰èŠ‚å°ç»“
 
 åœ¨è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠçš„å¥‡å¦™æ—…ç¨‹ä¸­ï¼Œæˆ‘ä»¬æŒæ¡äº†ARæŠ€æœ¯çš„ä¸‰å¤§æ ¸å¿ƒæ”¯æŸ±ï¼š
 
 **ğŸ” è§†è§‰åæ ‡ç³»ç»Ÿçš„å»ºç«‹**
 - æ‘„åƒå¤´æ ‡å®šè®©è®¡ç®—æœº"çœ¼ç›"æ‹¥æœ‰äº†ç²¾ç¡®çš„åº¦é‡èƒ½åŠ›
 - ä»æ•°å­¦ç†è®ºåˆ°å·¥ç¨‹å®è·µçš„å®Œæ•´é—­ç¯
 - ä¸ºåç»­æ‰€æœ‰ARåº”ç”¨å¥ å®šäº†åšå®çš„å‡ ä½•åŸºç¡€
 
 **ğŸ¨ è™šæ‹Ÿå†…å®¹çš„åˆ›ä½œ**
 - 3Dæ¸²æŸ“å¼•æ“ä»é›¶æ„å»ºè™šæ‹Ÿä¸–ç•Œ
 - å…‰å½±äº¤ç»‡ä¸­å±•ç°æ•°å­—è‰ºæœ¯çš„é­…åŠ›
 - å‡ ä½•å˜æ¢è®©è™šæ‹Ÿç‰©ä½“æ‹¥æœ‰äº†ç”Ÿå‘½åŠ›
 
 **ğŸŒˆ è™šå®ä¸–ç•Œçš„èåˆ**
 - å¹³é¢æ£€æµ‹æ¶èµ·ç°å®ä¸è™šæ‹Ÿçš„æ¡¥æ¢
 - æ·±åº¦æ„ŸçŸ¥è®©é®æŒ¡å…³ç³»è‡ªç„¶å‘ˆç°
 - å®æ—¶åˆæˆæŠ€æœ¯åˆ›é€ é­”æ³•èˆ¬çš„è§†è§‰ä½“éªŒ
 
 é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œæˆ‘ä»¬ä¸ä»…æŒæ¡äº†ARæŠ€æœ¯çš„æ ¸å¿ƒç®—æ³•ï¼Œæ›´é‡è¦çš„æ˜¯å»ºç«‹äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚è¿™äº›æŠ€æœ¯å°†ä¸ºæˆ‘ä»¬åœ¨ç¬¬å››èŠ‚çš„ç»¼åˆé¡¹ç›®ä¸­å¥ å®šåšå®åŸºç¡€ã€‚
 
 ---
 
 **å‡†å¤‡å°±ç»ªï¼** è®©æˆ‘ä»¬å¸¦ç€è¿™äº›å¼ºå¤§çš„æŠ€æœ¯æ­¦å™¨ï¼Œå‘ç¬¬å››èŠ‚ã€ŠARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†ã€‹è¿›å‘ï¼Œé‚£é‡Œå°†æœ‰æ›´åŠ æ¿€åŠ¨äººå¿ƒçš„æŒ‘æˆ˜ç­‰å¾…ç€æˆ‘ä»¬ï¼

---

## 37.4 ARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†ï¼šç»¼åˆå®æˆ˜é¡¹ç›®

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬å®æ—¶è§†è§‰å¤„ç†ä¸­å¿ƒçš„æ˜ç â€”â€”**ARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†**ï¼è¿™æ˜¯ä¸€åº§èåˆäº†æ‰€æœ‰å‰æ²¿æŠ€æœ¯çš„æœªæ¥ä½“éªŒä¸­å¿ƒï¼Œåœ¨è¿™é‡Œï¼Œè´­ç‰©ä¸å†æ˜¯ç®€å•çš„æŒ‘é€‰ï¼Œè€Œæ˜¯ä¸€åœºç§‘æŠ€ä¸æ—¶å°šçš„å®Œç¾é‚‚é€…ã€‚

### ğŸ¢ ä½“éªŒé¦†å…¨æ™¯å¸ƒå±€

åœ¨ARè¯•è¡£æ™ºèƒ½ä½“éªŒé¦†çš„å»ºè®¾è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¤šé¡¹æŠ€æœ¯çªç ´ï¼š

**ğŸ¯ æŠ€æœ¯æ•´åˆçš„è‰ºæœ¯**
- å°†å§¿æ€æ£€æµ‹ã€3Dæ¸²æŸ“ã€ç”¨æˆ·äº¤äº’å®Œç¾èåˆ
- æ„å»ºäº†å®Œæ•´çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ
- å®ç°äº†ç†è®ºä¸å®è·µçš„æ— ç¼å¯¹æ¥

**ğŸ’¡ åˆ›æ–°åº”ç”¨çš„å…¸èŒƒ**
- ä¸ä»…ä»…æ˜¯æŠ€æœ¯æ¼”ç¤ºï¼Œæ›´æ˜¯å•†ä¸šåŒ–äº§å“
- å…³æ³¨ç”¨æˆ·ä½“éªŒå’Œå®é™…åº”ç”¨ä»·å€¼
- ä¸ºAIæŠ€æœ¯äº§ä¸šåŒ–æä¾›äº†ä¼˜ç§€èŒƒä¾‹

**ğŸŒŸ æœªæ¥å‘å±•çš„åŸºçŸ³**
- ä¸ºæ›´å¤æ‚çš„ARåº”ç”¨å¥ å®šäº†æŠ€æœ¯åŸºç¡€
- å±•ç°äº†è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„å·¨å¤§æ½œåŠ›
- æŒ‡å‘äº†äººå·¥æ™ºèƒ½ä¸æ—¶å°šäº§ä¸šèåˆçš„ç¾å¥½å‰æ™¯

é€šè¿‡è¿™ä¸ªç»¼åˆé¡¹ç›®ï¼Œæˆ‘ä»¬ä¸ä»…æŒæ¡äº†å®æ—¶è§†è§‰å¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ›´é‡è¦çš„æ˜¯åŸ¹å…»äº†ç³»ç»Ÿæ€§æ€ç»´å’Œäº§å“åŒ–èƒ½åŠ›ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æœªæ¥çš„AIèŒä¸šå‘å±•å¥ å®šåšå®åŸºç¡€ã€‚

---

## ğŸ¤” æœ¬ç« æ€è€ƒé¢˜

### ğŸ“ åŸºç¡€ç†è§£é¢˜

**æ€è€ƒé¢˜1ï¼šè§†é¢‘å¤„ç†æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**
åœ¨37.1èŠ‚çš„è§†é¢‘æµåª’ä½“å·¥å‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¤šç§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ã€‚è¯·åˆ†æä»¥ä¸‹åœºæ™¯å¹¶åˆ¶å®šä¼˜åŒ–ç­–ç•¥ï¼š

å‡è®¾ä½ éœ€è¦å¼€å‘ä¸€ä¸ªå®æ—¶ç›‘æ§ç³»ç»Ÿï¼Œè¦æ±‚åŒæ—¶å¤„ç†16è·¯é«˜æ¸…è§†é¢‘æµï¼ˆ1920Ã—1080, 30fpsï¼‰ã€‚

è¯·å›ç­”ï¼š
1. å¦‚ä½•åˆ†é…CPUå’ŒGPUçš„å¤„ç†ä»»åŠ¡ï¼Ÿ
2. è®¾è®¡å¤šçº¿ç¨‹å¤„ç†æ¶æ„ï¼Œè¯´æ˜çº¿ç¨‹é—´çš„åè°ƒæœºåˆ¶
3. å†…å­˜ç®¡ç†ç­–ç•¥ï¼Œå¦‚ä½•é¿å…å†…å­˜æ³„æ¼å’Œç¢ç‰‡åŒ–ï¼Ÿ
4. å¦‚ä½•å¤„ç†ç½‘ç»œå»¶è¿Ÿå’Œå¸§ä¸¢å¤±é—®é¢˜ï¼Ÿ

**æ€è€ƒé¢˜2ï¼šARæŠ€æœ¯çš„æ•°å­¦åŸºç¡€**
åœ¨37.3èŠ‚çš„è™šæ‹Ÿç°å®åˆ›æ„å·¥åŠä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†æ‘„åƒå¤´æ ‡å®šå’Œ3Dæ¸²æŸ“æŠ€æœ¯ã€‚è¯·ä»æ•°å­¦è§’åº¦åˆ†æï¼š

ç»™å®šä¸€ä¸ªå®é™…çš„ARåœºæ™¯ï¼šç”¨æˆ·æ‰‹æŒæ‰‹æœºï¼Œè·ç¦»æ¡Œé¢1.5ç±³ï¼Œæ¡Œé¢ä¸Šæœ‰ä¸€æœ¬ä¹¦ï¼ˆ20cmÃ—15cmï¼‰ã€‚è¦åœ¨ä¹¦ä¸Šæ”¾ç½®ä¸€ä¸ªè™šæ‹Ÿçš„ç«‹æ–¹ä½“ï¼ˆè¾¹é•¿10cmï¼‰ã€‚

è¯·è®¡ç®—ï¼š
1. å¦‚ä½•é€šè¿‡å•ç›®è§†è§‰ä¼°ç®—ä¹¦æœ¬çš„å®é™…å°ºå¯¸ï¼Ÿ
2. è™šæ‹Ÿç«‹æ–¹ä½“åœ¨å›¾åƒä¸­çš„æŠ•å½±å°ºå¯¸è®¡ç®—å…¬å¼
3. å½“ç”¨æˆ·ç§»åŠ¨æ—¶ï¼Œç«‹æ–¹ä½“çš„é€è§†å˜æ¢çŸ©é˜µå¦‚ä½•æ›´æ–°ï¼Ÿ
4. å¦‚ä½•å¤„ç†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®ç‰©ä½“çš„é®æŒ¡å…³ç³»ï¼Ÿ

### ğŸš€ åº”ç”¨æ‹“å±•é¢˜

**æ€è€ƒé¢˜3ï¼šARè¯•è¡£ç³»ç»Ÿçš„æŠ€æœ¯å‡çº§æ–¹æ¡ˆ**
åŸºäº37.4èŠ‚çš„ARè¯•è¡£æ™ºèƒ½ä½“éªŒç³»ç»Ÿï¼Œè®¾è®¡ä¸€ä¸ªæŠ€æœ¯å‡çº§æ–¹æ¡ˆï¼Œè¦æ±‚å®ç°ä»¥ä¸‹æ–°åŠŸèƒ½ï¼š

**æ–°éœ€æ±‚**ï¼š
- æ”¯æŒå…¨èº«åŠ¨æ€è¯•è¡£ï¼ˆåŒ…æ‹¬é‹å­ã€é…é¥°ï¼‰
- å®ç°å¤šäººåŒæ—¶è¯•è¡£å¯¹æ¯”
- æ·»åŠ è™šæ‹Ÿè¯•è¡£é—´ç¯å¢ƒï¼ˆç¯å…‰ã€èƒŒæ™¯ï¼‰
- é›†æˆè¯­éŸ³äº¤äº’å’Œæ‰‹åŠ¿æ§åˆ¶

è¯·åˆ¶å®šï¼š
1. **æŠ€æœ¯æ¶æ„å‡çº§æ–¹æ¡ˆ**ï¼šæ–°å¢å“ªäº›æŠ€æœ¯æ¨¡å—ï¼Ÿå¦‚ä½•ä¸ç°æœ‰ç³»ç»Ÿé›†æˆï¼Ÿ
2. **ç®—æ³•ä¼˜åŒ–ç­–ç•¥**ï¼šå¦‚ä½•ä¿è¯å¤šäººæ£€æµ‹çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ï¼Ÿ
3. **æ•°æ®ç®¡ç†æ–¹æ¡ˆ**ï¼š3Dæœè£…æ¨¡å‹åº“å¦‚ä½•æ‰©å±•ï¼Ÿç”¨æˆ·æ•°æ®å¦‚ä½•å­˜å‚¨ï¼Ÿ
4. **ç”¨æˆ·ä½“éªŒè®¾è®¡**ï¼šäº¤äº’ç•Œé¢å¦‚ä½•æ”¹è¿›ï¼Ÿç”¨æˆ·æµç¨‹å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**æ€è€ƒé¢˜4ï¼šè®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„ä¼¦ç†æ€è€ƒ**
éšç€ARå’Œäººè„¸è¯†åˆ«æŠ€æœ¯çš„æ™®åŠï¼Œéšç§ä¿æŠ¤å’ŒæŠ€æœ¯ä¼¦ç†æˆä¸ºé‡è¦è®®é¢˜ã€‚è¯·ç»“åˆæœ¬ç« å†…å®¹ï¼Œæ·±å…¥æ€è€ƒï¼š

è¯·åˆ†æï¼š
1. **éšç§ä¿æŠ¤æªæ–½**ï¼šç”¨æˆ·çš„äººè„¸æ•°æ®ã€èº«ä½“å°ºå¯¸æ•°æ®å¦‚ä½•ä¿æŠ¤ï¼Ÿ
2. **æ•°æ®ä½¿ç”¨è¾¹ç•Œ**ï¼šå•†å®¶å¯ä»¥å¦‚ä½•ä½¿ç”¨æ”¶é›†çš„æ•°æ®ï¼Ÿæœ‰å“ªäº›é™åˆ¶ï¼Ÿ
3. **ç®—æ³•å…¬å¹³æ€§**ï¼šå¦‚ä½•ç¡®ä¿ç³»ç»Ÿå¯¹ä¸åŒè‚¤è‰²ã€ä½“å‹ã€å¹´é¾„çš„ç”¨æˆ·éƒ½å…¬å¹³ï¼Ÿ
4. **æŠ€æœ¯é€æ˜åº¦**ï¼šç”¨æˆ·æœ‰æƒäº†è§£ç®—æ³•çš„å·¥ä½œåŸç†å—ï¼Ÿå¦‚ä½•å¹³è¡¡å•†ä¸šæœºå¯†å’Œé€æ˜åº¦ï¼Ÿ

---

## ğŸ¯ æœ¬ç« æ€»ç»“

### ğŸŒŸ å­¦ä¹ æˆå°±å›é¡¾

ç»è¿‡ç¬¬37ç« ã€Šå®æ—¶è§†è§‰åº”ç”¨å¼€å‘ã€‹çš„æ·±å…¥å­¦ä¹ ï¼Œæˆ‘ä»¬å®Œæˆäº†ä¸€æ¬¡ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´æŠ€æœ¯æ—…ç¨‹ã€‚

#### ğŸ“š ç†è®ºçŸ¥è¯†çš„ç³»ç»ŸæŒæ¡

**ğŸ¬ è§†é¢‘å¤„ç†æŠ€æœ¯ä½“ç³»**
- æ·±å…¥ç†è§£äº†è§†é¢‘æ•°æ®ç»“æ„ã€ç¼–è§£ç åŸç†ã€æ ¼å¼è½¬æ¢ç­‰æ ¸å¿ƒæ¦‚å¿µ
- æŒæ¡äº†å¸§é—´å·®åˆ†ã€ç›®æ ‡è·Ÿè¸ªã€è§†é¢‘æµå¤„ç†ç­‰å…³é”®ç®—æ³•
- å»ºç«‹äº†ä»åŸå§‹è§†é¢‘æ•°æ®åˆ°é«˜è´¨é‡è¾“å‡ºçš„å®Œæ•´å¤„ç†æµç¨‹

**âš¡ æ€§èƒ½ä¼˜åŒ–çš„å·¥ç¨‹å®è·µ**
- å­¦ä¼šäº†å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ã€GPUç¡¬ä»¶åŠ é€Ÿçš„å®ç°æ–¹æ³•
- ç†è§£äº†å†…å­˜ç®¡ç†ã€èµ„æºè°ƒåº¦ã€è´Ÿè½½å‡è¡¡ç­‰ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯
- æŒæ¡äº†æ€§èƒ½ç›‘æ§ã€ç“¶é¢ˆåˆ†æã€è°ƒä¼˜ç­–ç•¥çš„å·¥ç¨‹æ–¹æ³•

**ğŸŒˆ ARæŠ€æœ¯çš„æ•°å­¦åŸºç¡€**
- ä»æ•°å­¦æ ¹æºç†è§£äº†æ‘„åƒå¤´æ ‡å®šã€åæ ‡ç³»å˜æ¢çš„åŸç†
- æŒæ¡äº†3Dæ¸²æŸ“ç®¡çº¿ã€å…‰ç…§æ¨¡å‹ã€æŠ•å½±å˜æ¢çš„å®Œæ•´å®ç°
- å­¦ä¼šäº†è™šå®èåˆã€å¹³é¢æ£€æµ‹ã€æ·±åº¦ä¼°è®¡ç­‰å‰æ²¿æŠ€æœ¯

#### ğŸ› ï¸ å®è·µæŠ€èƒ½çš„å…¨é¢æå‡

**ğŸ’» ä»£ç èƒ½åŠ›çš„è´¨çš„é£è·ƒ**
- ç¼–å†™äº†è¶…è¿‡4000è¡Œçš„é«˜è´¨é‡Pythonä»£ç 
- æŒæ¡äº†OpenCVã€MediaPipeã€NumPyç­‰æ ¸å¿ƒåº“çš„æ·±åº¦åº”ç”¨
- å»ºç«‹äº†æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„ä»£ç æ¶æ„è®¾è®¡èƒ½åŠ›

**ğŸ¨ é¡¹ç›®å¼€å‘çš„å®Œæ•´ç»éªŒ**
- å®Œæˆäº†ä»éœ€æ±‚åˆ†æåˆ°äº§å“äº¤ä»˜çš„å…¨æµç¨‹å¼€å‘
- å­¦ä¼šäº†ç”¨æˆ·ç•Œé¢è®¾è®¡ã€äº¤äº’ä½“éªŒä¼˜åŒ–çš„å®è·µæ–¹æ³•
- è·å¾—äº†ç³»ç»Ÿé›†æˆã€æµ‹è¯•è°ƒè¯•ã€æ€§èƒ½è°ƒä¼˜çš„å®è´µç»éªŒ

### ğŸ–ï¸ æŠ€æœ¯èƒ½åŠ›è¿›é˜¶

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯èƒ½åŠ›å®ç°äº†æ˜¾è‘—æå‡ï¼š

#### ğŸ”¥ ä»å…¥é—¨åˆ°ç²¾é€šçš„è·¨è¶Š
- **è®¡ç®—æœºè§†è§‰èƒ½åŠ›**ï¼šä»åŸºç¡€å›¾åƒå¤„ç†åˆ°å¤æ‚ARåº”ç”¨çš„å®Œæ•´æŒæ¡
- **ç³»ç»Ÿå·¥ç¨‹èƒ½åŠ›**ï¼šä»å•ä¸€åŠŸèƒ½åˆ°å¤æ‚ç³»ç»Ÿçš„æ¶æ„è®¾è®¡èƒ½åŠ›
- **äº§å“å¼€å‘èƒ½åŠ›**ï¼šä»æŠ€æœ¯æ¢ç´¢åˆ°å•†ä¸šåŒ–äº§å“çš„è½¬åŒ–èƒ½åŠ›

#### ğŸŒŸ æ ¸å¿ƒç«äº‰åŠ›çš„å»ºç«‹
- **æŠ€æœ¯æ·±åº¦**ï¼šä¸ä»…ä¼šç”¨å·¥å…·ï¼Œæ›´ç†è§£èƒŒåçš„æ•°å­¦åŸç†å’Œç®—æ³•é€»è¾‘
- **å·¥ç¨‹ç´ å…»**ï¼šå…·å¤‡ç¼–å†™é«˜è´¨é‡ã€å¯ç»´æŠ¤ã€å¯æ‰©å±•ä»£ç çš„èƒ½åŠ›
- **åˆ›æ–°æ€ç»´**ï¼šèƒ½å¤Ÿå°†æ–°æŠ€æœ¯ä¸å®é™…éœ€æ±‚ç»“åˆï¼Œåˆ›é€ æœ‰ä»·å€¼çš„åº”ç”¨

### ğŸ”® æŠ€æœ¯å‘å±•å±•æœ›

ç«™åœ¨ç¬¬37ç« å­¦ä¹ æˆæœçš„åŸºç¡€ä¸Šï¼Œè®©æˆ‘ä»¬å±•æœ›ä¸€ä¸‹æŠ€æœ¯å‘å±•çš„ç¾å¥½æœªæ¥ï¼š

#### ğŸŒŸ æŠ€æœ¯è¶‹åŠ¿é¢„åˆ¤
- **å®æ—¶æ¸²æŸ“æŠ€æœ¯**ï¼šä»30fpsåˆ°120fpsï¼Œä»2Kåˆ°8Kçš„ç”»è´¨é£è·ƒ
- **AIç¡¬ä»¶åŠ é€Ÿ**ï¼šä¸“ç”¨AIèŠ¯ç‰‡å¸¦æ¥çš„æ€§èƒ½å’ŒåŠŸè€—é©å‘½
- **5Gç½‘ç»œåº”ç”¨**ï¼šè¶…ä½å»¶è¿Ÿç½‘ç»œä¸ºARäº‘æ¸²æŸ“æä¾›æ— é™å¯èƒ½
- **å…ƒå®‡å®™æ—¶ä»£**ï¼šè™šæ‹Ÿä¸ç°å®èåˆçš„å…¨æ–°æ•°å­—ä¸–ç•Œ

#### ğŸš€ åº”ç”¨åœºæ™¯æ‹“å±•
- **æ•™è‚²åŸ¹è®­**ï¼šæ²‰æµ¸å¼çš„è™šæ‹Ÿå®éªŒå®¤å’Œäº’åŠ¨è¯¾å ‚
- **åŒ»ç–—å¥åº·**ï¼šç²¾å‡†çš„è¯Šæ–­è¾…åŠ©å’Œæ‰‹æœ¯å¯¼èˆªç³»ç»Ÿ
- **å·¥ä¸šåˆ¶é€ **ï¼šæ™ºèƒ½åŒ–çš„è£…é…æŒ‡å¯¼å’Œè´¨é‡æ£€æµ‹
- **å¨±ä¹ä½“éªŒ**ï¼šè¶…çœŸå®çš„æ¸¸æˆä¸–ç•Œå’Œç¤¾äº¤ç©ºé—´

### ğŸŒ… ä¸‹ç« é¢„å‘Šï¼šç¬¬38ç« ã€ŠAIæ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–ã€‹

ç¬¬37ç« çš„å­¦ä¹ è™½ç„¶å‘Šä¸€æ®µè½ï¼Œä½†æˆ‘ä»¬çš„AIæŠ€æœ¯æ¢ç´¢ä¹‹æ—…æ‰åˆšåˆšå¼€å§‹ï¼åœ¨å³å°†åˆ°æ¥çš„ç¬¬38ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š

**ğŸš€ ä»å¼€å‘åˆ°éƒ¨ç½²çš„å®Œæ•´é—­ç¯**
- æ¢ç´¢äº‘ç«¯éƒ¨ç½²ã€è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨ç«¯ä¼˜åŒ–ç­‰å…³é”®æŠ€æœ¯
- å­¦ä¹ æ¨¡å‹å‹ç¼©ã€é‡åŒ–ã€å‰ªæç­‰æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- æŒæ¡å®¹å™¨åŒ–éƒ¨ç½²ã€å¾®æœåŠ¡æ¶æ„ã€è´Ÿè½½å‡è¡¡ç­‰å·¥ç¨‹å®è·µ
- æ„å»ºå®Œæ•´çš„AIåº”ç”¨ç”Ÿäº§ç¯å¢ƒ

### ğŸ’Œ ç»“è¯­ï¼šæ„Ÿæ©ä¸æœŸå¾…

æ„Ÿè°¢æ¯ä¸€ä½å­¦ä¹ è€…çš„åšæŒå’ŒåŠªåŠ›ï¼ç¬¬37ç« ã€Šå®æ—¶è§†è§‰åº”ç”¨å¼€å‘ã€‹çš„å­¦ä¹ æ—…ç¨‹å……æ»¡æŒ‘æˆ˜ï¼Œä½†ä¹Ÿæ”¶è·æ»¡æ»¡ã€‚æˆ‘ä»¬ä¸ä»…æŒæ¡äº†å‰æ²¿çš„AIæŠ€æœ¯ï¼Œæ›´é‡è¦çš„æ˜¯åŸ¹å…»äº†ç³»ç»Ÿæ€§æ€ç»´å’Œè§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚

**ğŸ™ è‡´æ•¬æ¯ä¸€ä»½åŠªåŠ›**
- æ¯ä¸€è¡Œä»£ç çš„ç¼–å†™éƒ½æ˜¯å‘æŠ€æœ¯é«˜å³°çš„æ”€ç™»
- æ¯ä¸€ä¸ªé—®é¢˜çš„è§£å†³éƒ½æ˜¯æ™ºæ…§å’Œæ¯…åŠ›çš„ä½“ç°
- æ¯ä¸€æ¬¡å®è·µçš„æ¢ç´¢éƒ½æ˜¯åˆ›æ–°ç²¾ç¥çš„é—ªå…‰

**ğŸŒˆ æ‹¥æŠ±ç¾å¥½æœªæ¥** 
- æŠ€æœ¯çš„å‘å±•ä¸ºæˆ‘ä»¬æä¾›äº†æ— é™å¯èƒ½
- å­¦ä¹ çš„çƒ­æƒ…å°†ç…§äº®å‰è¿›çš„é“è·¯
- åˆ›æ–°çš„å‹‡æ°”å°†å¼€åˆ›ç¾å¥½çš„æ˜å¤©

è®©æˆ‘ä»¬å¸¦ç€ç¬¬37ç« å­¦ä¹ çš„ä¸°ç¡•æˆæœï¼Œæ»¡æ€€ä¿¡å¿ƒåœ°è¿æ¥ä¸‹ä¸€ä¸ªæŠ€æœ¯æŒ‘æˆ˜ï¼åœ¨AIæŠ€æœ¯çš„å¹¿é˜”æµ·æ´‹ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä¹˜é£ç ´æµªï¼Œå‹‡æ•¢å‰è¡Œï¼

---

**ğŸŠ ç¬¬37ç« å­¦ä¹ å®Œæˆï¼æ­å–œæ‚¨å·²ç»æŒæ¡äº†å®æ—¶è§†è§‰åº”ç”¨å¼€å‘çš„æ ¸å¿ƒæŠ€æœ¯ï¼**

**ğŸ“… å­¦ä¹ æ—¶é—´**ï¼š2025å¹´2æœˆ3æ—¥  
**ğŸ“ˆ æŠ€æœ¯æ°´å¹³**ï¼šä»å…¥é—¨åˆ°ç²¾é€šçš„å®Œç¾èœ•å˜  
**ğŸš€ ä¸‹ä¸€ç›®æ ‡**ï¼šç¬¬38ç« ã€ŠAIæ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–ã€‹  
**ğŸ’ª å­¦ä¹ çŠ¶æ€**ï¼šå……æ»¡ä¿¡å¿ƒï¼Œå‡†å¤‡è¿æ¥æ–°æŒ‘æˆ˜ï¼**