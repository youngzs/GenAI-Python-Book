# ç¬¬35ç« ï¼šOpenCVä¸å›¾åƒå¤„ç†åŸºç¡€

> "åœ¨æ•°å­—æ—¶ä»£ï¼Œæ¯ä¸€å¼ å›¾ç‰‡éƒ½æ˜¯æ•°æ®çš„è‰ºæœ¯å“ï¼Œè€Œæˆ‘ä»¬å³å°†å»ºç«‹ä¸€åº§ç°ä»£åŒ–çš„æ•°å­—ç›¸æœºå·¥å‚ï¼Œç”¨ä»£ç çš„é­”æ³•è®©æœºå™¨æ‹¥æœ‰'çœ‹è§'çš„èƒ½åŠ›ã€‚"

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### çŸ¥è¯†ç›®æ ‡
- æŒæ¡OpenCVåº“çš„åŸºæœ¬æ¦‚å¿µå’Œæ ¸å¿ƒåŠŸèƒ½æ¶æ„
- ç†è§£è®¡ç®—æœºè§†è§‰ä¸­å›¾åƒå¤„ç†çš„åŸºæœ¬åŸç†å’Œæ•°å­¦åŸºç¡€
- ç†Ÿæ‚‰å¸¸ç”¨çš„å›¾åƒé¢„å¤„ç†æŠ€æœ¯å’Œç®—æ³•å®ç°
- æŒæ¡ç‰¹å¾æ£€æµ‹ä¸åŒ¹é…çš„åŸºæœ¬æ–¹æ³•å’Œåº”ç”¨åœºæ™¯

### æŠ€èƒ½ç›®æ ‡  
- èƒ½å¤Ÿæ­å»ºå’Œé…ç½®å®Œæ•´çš„OpenCVå¼€å‘ç¯å¢ƒ
- èƒ½å¤Ÿè¿›è¡ŒåŸºæœ¬çš„å›¾åƒè¯»å–ã€æ˜¾ç¤ºã€ä¿å­˜ç­‰æ ¸å¿ƒæ“ä½œ
- èƒ½å¤Ÿè¿ç”¨å›¾åƒæ»¤æ³¢ã€å½¢æ€å­¦æ“ä½œç­‰æŠ€æœ¯å¤„ç†å„ç±»å›¾åƒ
- èƒ½å¤Ÿå®ç°è¾¹ç¼˜æ£€æµ‹ã€è§’ç‚¹æ£€æµ‹ç­‰ç‰¹å¾æå–åŠŸèƒ½
- èƒ½å¤Ÿå¼€å‘å®Œæ•´çš„å›¾åƒå¤„ç†åº”ç”¨é¡¹ç›®å¹¶è¿›è¡Œä¼˜åŒ–

### ç´ å…»ç›®æ ‡
- åŸ¹å…»è§£å†³å®é™…å›¾åƒå¤„ç†é—®é¢˜çš„å·¥ç¨‹æ€ç»´å’Œç³»ç»Ÿæ€§æ€è€ƒèƒ½åŠ›
- å»ºç«‹è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸“ä¸šç´ å…»å’ŒæŠ€æœ¯åˆ¤æ–­åŠ›
- åŸ¹å…»ä¼ä¸šçº§é¡¹ç›®å¼€å‘çš„è§„èŒƒæ„è¯†å’Œè´¨é‡æ ‡å‡†
- å»ºç«‹ä»æŠ€æœ¯å­¦ä¹ åˆ°äº§å“å¼€å‘çš„å®Œæ•´æ€ç»´é“¾æ¡

## ğŸ­ ç« èŠ‚å¯¼å…¥ï¼šæ¬¢è¿æ¥åˆ°æ•°å­—ç›¸æœºå·¥å‚

### ğŸ¬ å¼€ç¯‡æ•…äº‹ï¼šæœªæ¥å·¥å‚çš„ç¬¬ä¸€å¤©

æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨åˆšåˆšè¢«è˜ä¸ºä¸–ç•Œä¸Šæœ€å…ˆè¿›çš„"æ•°å­—ç›¸æœºå·¥å‚"çš„é¦–å¸­å·¥ç¨‹å¸ˆã€‚è¿™åº§å·¥å‚ä¸ç”Ÿäº§ä¼ ç»Ÿçš„ç›¸æœºï¼Œè€Œæ˜¯ä¸“é—¨ç”Ÿäº§èƒ½å¤Ÿ"ç†è§£"å›¾åƒçš„æ™ºèƒ½ç³»ç»Ÿã€‚

èµ°è¿›å·¥å‚å¤§é—¨ï¼Œæ‚¨ä¼šçœ‹åˆ°ï¼š

ğŸ­ **æ‹ç…§è½¦é—´** - è¿™é‡Œæœ‰æœ€å…ˆè¿›çš„å›¾åƒé‡‡é›†è®¾å¤‡ï¼Œèƒ½å¤Ÿä»å„ç§æ¥æºè·å–æ•°å­—å›¾åƒ  
ğŸ–¼ï¸ **ç…§ç‰‡å±•ç¤ºå…** - é…å¤‡äº†é«˜æ¸…æ˜¾ç¤ºç³»ç»Ÿï¼Œå¯ä»¥å®æ—¶å±•ç¤ºå¤„ç†è¿‡ç¨‹å’Œç»“æœ  
ğŸ–¨ï¸ **ç…§ç‰‡æ‰“å°è½¦é—´** - æ‹¥æœ‰å¤šç§è¾“å‡ºæ ¼å¼çš„ä¿å­˜ç³»ç»Ÿï¼Œç¡®ä¿æˆæœèƒ½å¤Ÿå®Œç¾ä¿å­˜  
âœ¨ **ç¾é¢œå·¥å‚** - è¿™é‡Œæ˜¯å›¾åƒå¤„ç†çš„æ ¸å¿ƒï¼Œå„ç§æ»¤æ³¢ã€å¢å¼ºã€ä¼˜åŒ–ç®—æ³•åœ¨è¿™é‡Œè¿è½¬  
ğŸ” **å›¾åƒä¾¦æ¢å±€** - ä¸“é—¨è´Ÿè´£åœ¨å›¾åƒä¸­å¯»æ‰¾è¾¹ç¼˜ã€è§’ç‚¹ã€ç‰¹å¾ç­‰é‡è¦ä¿¡æ¯

### ğŸ¯ å·¥å‚çš„ä½¿å‘½

ä½œä¸ºé¦–å¸­å·¥ç¨‹å¸ˆï¼Œæ‚¨çš„ä½¿å‘½æ˜¯ï¼š
1. **å»ºç«‹æ ‡å‡†åŒ–ç”Ÿäº§çº¿** - è®¾è®¡å¯é‡å¤ã€å¯æ‰©å±•çš„å›¾åƒå¤„ç†æµç¨‹
2. **ä¿è¯äº§å“è´¨é‡** - ç¡®ä¿æ¯ä¸ªå¤„ç†æ­¥éª¤éƒ½è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†
3. **ä¼˜åŒ–ç”Ÿäº§æ•ˆç‡** - è®©ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å¤§é‡å›¾åƒè€Œä¸é™ä½è´¨é‡
4. **åˆ›æ–°äº§å“åŠŸèƒ½** - å¼€å‘å‡ºå…·æœ‰å•†ä¸šä»·å€¼çš„æ™ºèƒ½å›¾åƒåº”ç”¨

### ğŸ”§ æ‚¨çš„å·¥å…·ç®±ï¼šOpenCV

åœ¨è¿™åº§æ•°å­—å·¥å‚ä¸­ï¼ŒOpenCVå°±æ˜¯æ‚¨æœ€é‡è¦çš„å·¥å…·ç®±ï¼š

```python
# è¿™å°±æ˜¯æ‚¨çš„é­”æ³•å·¥å…·ç®±
import cv2
import numpy as np
import matplotlib.pyplot as plt

# è®©æˆ‘ä»¬å¼€å§‹è¿™æ®µç¥å¥‡çš„æ—…ç¨‹ï¼
print("æ¬¢è¿æ¥åˆ°æ•°å­—ç›¸æœºå·¥å‚ï¼")
print(f"å·¥å‚ç‰ˆæœ¬ï¼šOpenCV {cv2.__version__}")
print("å‡†å¤‡å¼€å§‹ç”Ÿäº§æ‚¨çš„ç¬¬ä¸€ä¸ªæ™ºèƒ½è§†è§‰äº§å“...")
```

## ğŸ“š ç¬¬ä¸€èŠ‚ï¼šæ•°å­—ç›¸æœºå·¥å‚çš„åŸºç¡€è®¾æ–½å»ºè®¾

### 35.1 OpenCVç¯å¢ƒæ­å»ºä¸åŸºç¡€æ“ä½œ

#### ğŸ—ï¸ å·¥å‚åŸºç¡€è®¾æ–½è§„åˆ’

åœ¨å»ºè®¾æˆ‘ä»¬çš„æ•°å­—ç›¸æœºå·¥å‚ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ‰€æœ‰çš„åŸºç¡€è®¾æ–½éƒ½å·²å°±ä½ã€‚å°±åƒå»ºè®¾çœŸå®çš„å·¥å‚éœ€è¦ç”µåŠ›ã€æ°´æºã€äº¤é€šä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ•°å­—å·¥å‚ä¹Ÿéœ€è¦åˆé€‚çš„è½¯ä»¶ç¯å¢ƒå’Œå¼€å‘å·¥å…·ã€‚

#### ğŸ“‹ å·¥å‚å»ºè®¾éœ€æ±‚æ¸…å•

```bash
# æ•°å­—ç›¸æœºå·¥å‚å»ºè®¾éœ€æ±‚æ¸…å•
# ================================

# 1. åŸºç¡€å¹³å°ï¼ˆPythonç¯å¢ƒï¼‰
Python >= 3.8

# 2. æ ¸å¿ƒç”Ÿäº§è®¾å¤‡ï¼ˆOpenCVï¼‰
opencv-python >= 4.8.0

# 3. æ•°æ®å¤„ç†è½¦é—´ï¼ˆNumPyï¼‰
numpy >= 1.24.0

# 4. å¯è§†åŒ–å±•ç¤ºå…ï¼ˆMatplotlibï¼‰
matplotlib >= 3.7.0

# 5. å¼€å‘è°ƒè¯•å·¥å…·ï¼ˆJupyterï¼‰
jupyter >= 1.0.0

# 6. å›¾åƒæ ¼å¼æ”¯æŒï¼ˆPillowï¼‰
pillow >= 9.5.0
```

#### ğŸ”§ å·¥å‚å»ºè®¾æ­¥éª¤

##### æ­¥éª¤1ï¼šç¯å¢ƒéªŒè¯å’Œå‡†å¤‡

é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ç°æœ‰ç¯å¢ƒæ˜¯å¦ç¬¦åˆå»ºå‚è¦æ±‚ï¼š

```python
# ç¤ºä¾‹1ï¼šå·¥å‚ç¯å¢ƒæ£€æŸ¥ç³»ç»Ÿ
import sys
import subprocess
import pkg_resources

def check_factory_environment():
    """
    æ•°å­—ç›¸æœºå·¥å‚ç¯å¢ƒæ£€æŸ¥ç³»ç»Ÿ
    æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…æ˜¯å¦å·²æ­£ç¡®å®‰è£…
    """
    print("ğŸ­ æ•°å­—ç›¸æœºå·¥å‚ç¯å¢ƒæ£€æŸ¥ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = sys.version_info
    print(f"ğŸ Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    if python_version >= (3, 8):
        print("âœ… Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.8)")
    else:
        print("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œè¯·å‡çº§åˆ°3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    
    # æ£€æŸ¥å¿…éœ€çš„è½¯ä»¶åŒ…
    required_packages = {
        'opencv-python': '4.8.0',
        'numpy': '1.24.0', 
        'matplotlib': '3.7.0',
        'jupyter': '1.0.0',
        'pillow': '9.5.0'
    }
    
    print("\nğŸ“¦ æ£€æŸ¥å·¥å‚è®¾å¤‡ï¼ˆè½¯ä»¶åŒ…ï¼‰:")
    all_installed = True
    
    for package, min_version in required_packages.items():
        try:
            # å°è¯•å¯¼å…¥åŒ…
            if package == 'opencv-python':
                import cv2
                version = cv2.__version__
                package_name = 'OpenCV'
            elif package == 'numpy':
                import numpy as np
                version = np.__version__
                package_name = 'NumPy'
            elif package == 'matplotlib':
                import matplotlib
                version = matplotlib.__version__
                package_name = 'Matplotlib'
            elif package == 'jupyter':
                import jupyter
                version = jupyter.__version__
                package_name = 'Jupyter'
            elif package == 'pillow':
                import PIL
                version = PIL.__version__
                package_name = 'Pillow'
            
            print(f"âœ… {package_name}: {version}")
            
        except ImportError:
            print(f"âŒ {package_name}: æœªå®‰è£…")
            all_installed = False
    
    print("\n" + "=" * 50)
    if all_installed:
        print("ğŸ‰ æ­å–œï¼å·¥å‚å»ºè®¾ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹ç”Ÿäº§ï¼")
        return True
    else:
        print("âš ï¸  å·¥å‚å»ºè®¾ä¸å®Œæ•´ï¼Œè¯·å®‰è£…ç¼ºå¤±çš„è®¾å¤‡åé‡è¯•")
        return False

# è¿è¡Œç¯å¢ƒæ£€æŸ¥
if __name__ == "__main__":
    check_factory_environment()
```

##### æ­¥éª¤2ï¼šä¸€é”®å¼å·¥å‚å»ºè®¾è„šæœ¬

å¦‚æœç¯å¢ƒæ£€æŸ¥å‘ç°ç¼ºå¤±ç»„ä»¶ï¼Œä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œä¸€é”®å®‰è£…ï¼š

```python
# ç¤ºä¾‹2ï¼šæ•°å­—ç›¸æœºå·¥å‚ä¸€é”®å»ºè®¾è„šæœ¬
import subprocess
import sys
import os

def install_factory_equipment():
    """
    æ•°å­—ç›¸æœºå·¥å‚ä¸€é”®å»ºè®¾è„šæœ¬
    è‡ªåŠ¨å®‰è£…æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…
    """
    print("ğŸ—ï¸ æ•°å­—ç›¸æœºå·¥å‚å»ºè®¾è„šæœ¬å¯åŠ¨")
    print("=" * 50)
    
    # å®šä¹‰éœ€è¦å®‰è£…çš„è®¾å¤‡æ¸…å•
    equipment_list = [
        "opencv-python>=4.8.0",
        "numpy>=1.24.0", 
        "matplotlib>=3.7.0",
        "jupyter>=1.0.0",
        "pillow>=9.5.0",
        "ipykernel",  # Jupyterå†…æ ¸æ”¯æŒ
        "seaborn",    # å¢å¼ºçš„å¯è§†åŒ–æ”¯æŒ
    ]
    
    print("ğŸ“‹ å¼€å§‹å®‰è£…å·¥å‚è®¾å¤‡:")
    
    for equipment in equipment_list:
        print(f"\nğŸ”§ æ­£åœ¨å®‰è£…: {equipment}")
        try:
            # ä½¿ç”¨pipå®‰è£…åŒ…
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", equipment
            ])
            print(f"âœ… {equipment} å®‰è£…æˆåŠŸ")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ {equipment} å®‰è£…å¤±è´¥: {e}")
            return False
    
    # éªŒè¯å®‰è£…
    print("\nğŸ” éªŒè¯å·¥å‚è®¾å¤‡å®‰è£…...")
    if check_factory_environment():
        print("\nğŸ‰ æ•°å­—ç›¸æœºå·¥å‚å»ºè®¾å®Œæˆï¼")
        print("ğŸš€ æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹ç”Ÿäº§æ™ºèƒ½è§†è§‰äº§å“äº†ï¼")
        return True
    else:
        print("\nâš ï¸ å·¥å‚å»ºè®¾éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return False

def create_factory_workspace():
    """
    åˆ›å»ºæ ‡å‡†åŒ–çš„å·¥å‚å·¥ä½œç©ºé—´
    """
    print("\nğŸ“ åˆ›å»ºå·¥å‚å·¥ä½œç©ºé—´...")
    
    # å®šä¹‰å·¥å‚ç›®å½•ç»“æ„
    factory_structure = {
        "digital_camera_factory": {
            "workshops": {
                "capture_workshop": ["input_images"],
                "processing_workshop": ["enhanced_images"],
                "output_workshop": ["final_products"]
            },
            "tools": ["utilities", "filters", "detectors"],
            "tests": ["unit_tests", "integration_tests"],
            "docs": ["manuals", "specifications"],
            "projects": ["document_scanner", "image_enhancer"]
        }
    }
    
    def create_dirs(structure, base_path=""):
        for name, content in structure.items():
            current_path = os.path.join(base_path, name)
            os.makedirs(current_path, exist_ok=True)
            print(f"ğŸ“‚ åˆ›å»ºç›®å½•: {current_path}")
            
            if isinstance(content, dict):
                create_dirs(content, current_path)
            elif isinstance(content, list):
                for subdir in content:
                    subdir_path = os.path.join(current_path, subdir)
                    os.makedirs(subdir_path, exist_ok=True)
                    print(f"ğŸ“‚ åˆ›å»ºå­ç›®å½•: {subdir_path}")
    
    try:
        create_dirs(factory_structure)
        
        # åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶
        requirements_content = """# æ•°å­—ç›¸æœºå·¥å‚è®¾å¤‡æ¸…å•
opencv-python>=4.8.0
numpy>=1.24.0
matplotlib>=3.7.0
jupyter>=1.0.0
pillow>=9.5.0
seaborn>=0.12.0
ipykernel>=6.0.0
"""
        
        with open("digital_camera_factory/requirements.txt", "w", encoding="utf-8") as f:
            f.write(requirements_content)
        
        print("ğŸ“„ åˆ›å»ºè®¾å¤‡æ¸…å•: requirements.txt")
        print("âœ… å·¥å‚å·¥ä½œç©ºé—´åˆ›å»ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å·¥ä½œç©ºé—´åˆ›å»ºå¤±è´¥: {e}")

# è¿è¡Œå·¥å‚å»ºè®¾
if __name__ == "__main__":
    print("ğŸ­ æ¬¢è¿ä½¿ç”¨æ•°å­—ç›¸æœºå·¥å‚å»ºè®¾ç³»ç»Ÿï¼")
    print("\nè¯·é€‰æ‹©æ“ä½œ:")
    print("1. æ£€æŸ¥ç°æœ‰ç¯å¢ƒ")
    print("2. ä¸€é”®å»ºè®¾å·¥å‚")
    print("3. åˆ›å»ºå·¥ä½œç©ºé—´")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ")
    
    if choice == "1":
        check_factory_environment()
    elif choice == "2":
        install_factory_equipment()
    elif choice == "3":
        create_factory_workspace()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œç¨‹åºé€€å‡º")
```

#### ğŸ–¼ï¸ æ‹ç…§è½¦é—´ï¼šå›¾åƒè¾“å…¥ç³»ç»Ÿ

ç°åœ¨å·¥å‚åŸºç¡€è®¾æ–½å·²ç»å»ºè®¾å®Œæˆï¼Œè®©æˆ‘ä»¬æ¥å»ºè®¾ç¬¬ä¸€ä¸ªè½¦é—´â€”â€”æ‹ç…§è½¦é—´ï¼è¿™ä¸ªè½¦é—´è´Ÿè´£ä»å„ç§æ¥æºè·å–å›¾åƒæ•°æ®ã€‚

```python
# ç¤ºä¾‹3ï¼šæ‹ç…§è½¦é—´ - å›¾åƒè¾“å…¥ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

class CaptureWorkshop:
    """
    æ‹ç…§è½¦é—´ç±»
    è´Ÿè´£ä»å„ç§æ¥æºè·å–å’Œç®¡ç†å›¾åƒæ•°æ®
    """
    
    def __init__(self, workspace_path="digital_camera_factory/workshops/capture_workshop"):
        """
        åˆå§‹åŒ–æ‹ç…§è½¦é—´
        
        Parameters:
        workspace_path (str): è½¦é—´å·¥ä½œç›®å½•è·¯å¾„
        """
        self.workspace_path = Path(workspace_path)
        self.workspace_path.mkdir(parents=True, exist_ok=True)
        
        # è½¦é—´çŠ¶æ€ä¿¡æ¯
        self.captured_images = []
        self.image_info = {}
        
        print("ğŸ­ æ‹ç…§è½¦é—´åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {self.workspace_path}")
    
    def capture_from_file(self, image_path, display_info=True):
        """
        ä»æ–‡ä»¶è·å–å›¾åƒï¼ˆç›¸å½“äºä»åº•ç‰‡æ‰«æï¼‰
        
        Parameters:
        image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
        display_info (bool): æ˜¯å¦æ˜¾ç¤ºå›¾åƒä¿¡æ¯
        
        Returns:
        numpy.ndarray: å›¾åƒæ•°æ®
        """
        print(f"ğŸ“¸ æ‹ç…§è½¦é—´ï¼šæ­£åœ¨ä»æ–‡ä»¶è·å–å›¾åƒ {image_path}")
        
        try:
            # ä½¿ç”¨OpenCVè¯»å–å›¾åƒ
            image = cv2.imread(str(image_path))
            
            if image is None:
                print(f"âŒ é”™è¯¯ï¼šæ— æ³•è¯»å–å›¾åƒæ–‡ä»¶ {image_path}")
                return None
            
            # è½¬æ¢é¢œè‰²ç©ºé—´ï¼ˆOpenCVé»˜è®¤ä½¿ç”¨BGRï¼Œæˆ‘ä»¬è½¬æ¢ä¸ºRGBç”¨äºæ˜¾ç¤ºï¼‰
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # è®°å½•å›¾åƒä¿¡æ¯
            image_id = len(self.captured_images)
            image_info = {
                'id': image_id,
                'path': image_path,
                'shape': image.shape,
                'dtype': image.dtype,
                'size_bytes': image.nbytes,
                'color_space': 'BGR'
            }
            
            self.captured_images.append(image)
            self.image_info[image_id] = image_info
            
            if display_info:
                self._display_image_info(image_info, image_rgb)
            
            print(f"âœ… å›¾åƒè·å–æˆåŠŸï¼åˆ†é…ID: {image_id}")
            return image_rgb
            
        except Exception as e:
            print(f"âŒ æ‹ç…§è½¦é—´é”™è¯¯: {e}")
            return None
    
    def capture_from_camera(self, camera_id=0, save_path=None):
        """
        ä»æ‘„åƒå¤´å®æ—¶è·å–å›¾åƒ
        
        Parameters:
        camera_id (int): æ‘„åƒå¤´ID
        save_path (str): ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
        numpy.ndarray: å›¾åƒæ•°æ®
        """
        print(f"ğŸ“¹ æ‹ç…§è½¦é—´ï¼šæ­£åœ¨å¯åŠ¨æ‘„åƒå¤´ {camera_id}")
        
        try:
            # æ‰“å¼€æ‘„åƒå¤´
            cap = cv2.VideoCapture(camera_id)
            
            if not cap.isOpened():
                print(f"âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")
                return None
            
            print("ğŸ“¸ æŒ‰ç©ºæ ¼é”®æ‹ç…§ï¼ŒæŒ‰ESCé”®é€€å‡º")
            
            while True:
                # è¯»å–ä¸€å¸§
                ret, frame = cap.read()
                
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                    break
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('æ‹ç…§è½¦é—´ - å®æ—¶ç”»é¢ (ç©ºæ ¼æ‹ç…§, ESCé€€å‡º)', frame)
                
                key = cv2.waitKey(1) & 0xFF
                
                # ç©ºæ ¼é”®æ‹ç…§
                if key == ord(' '):
                    # è½¬æ¢é¢œè‰²ç©ºé—´
                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ä¿å­˜å›¾åƒ
                    if save_path:
                        cv2.imwrite(str(save_path), frame)
                        print(f"ğŸ’¾ å›¾åƒå·²ä¿å­˜åˆ°: {save_path}")
                    
                    # è®°å½•å›¾åƒä¿¡æ¯
                    image_id = len(self.captured_images)
                    image_info = {
                        'id': image_id,
                        'path': save_path or 'camera_capture',
                        'shape': frame.shape,
                        'dtype': frame.dtype,
                        'size_bytes': frame.nbytes,
                        'color_space': 'BGR'
                    }
                    
                    self.captured_images.append(frame)
                    self.image_info[image_id] = image_info
                    
                    print(f"âœ… æ‹ç…§æˆåŠŸï¼å›¾åƒID: {image_id}")
                    
                    # é‡Šæ”¾æ‘„åƒå¤´
                    cap.release()
                    cv2.destroyAllWindows()
                    
                    return image_rgb
                
                # ESCé”®é€€å‡º
                elif key == 27:
                    break
            
            # é‡Šæ”¾æ‘„åƒå¤´
            cap.release()
            cv2.destroyAllWindows()
            return None
            
        except Exception as e:
            print(f"âŒ æ‹ç…§è½¦é—´æ‘„åƒå¤´é”™è¯¯: {e}")
            return None
    
    def create_test_image(self, width=640, height=480, pattern='gradient'):
        """
        åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆç”¨äºå·¥å‚è°ƒè¯•ï¼‰
        
        Parameters:
        width (int): å›¾åƒå®½åº¦
        height (int): å›¾åƒé«˜åº¦
        pattern (str): å›¾åƒæ¨¡å¼ ('gradient', 'checkerboard', 'noise')
        
        Returns:
        numpy.ndarray: æµ‹è¯•å›¾åƒ
        """
        print(f"ğŸ§ª æ‹ç…§è½¦é—´ï¼šæ­£åœ¨åˆ›å»ºæµ‹è¯•å›¾åƒ ({width}x{height}, {pattern})")
        
        if pattern == 'gradient':
            # åˆ›å»ºæ¢¯åº¦å›¾åƒ
            image = np.zeros((height, width, 3), dtype=np.uint8)
            for i in range(height):
                intensity = int(255 * i / height)
                image[i, :] = [intensity, intensity // 2, 255 - intensity]
                
        elif pattern == 'checkerboard':
            # åˆ›å»ºæ£‹ç›˜å›¾åƒ
            image = np.zeros((height, width, 3), dtype=np.uint8)
            square_size = 50
            for i in range(0, height, square_size):
                for j in range(0, width, square_size):
                    if (i // square_size + j // square_size) % 2 == 0:
                        image[i:i+square_size, j:j+square_size] = [255, 255, 255]
                        
        elif pattern == 'noise':
            # åˆ›å»ºå™ªå£°å›¾åƒ
            image = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
            
        else:
            # é»˜è®¤çº¯è‰²å›¾åƒ
            image = np.full((height, width, 3), 128, dtype=np.uint8)
        
        # è®°å½•å›¾åƒä¿¡æ¯
        image_id = len(self.captured_images)
        image_info = {
            'id': image_id,
            'path': f'test_image_{pattern}',
            'shape': image.shape,
            'dtype': image.dtype,
            'size_bytes': image.nbytes,
            'color_space': 'RGB'
        }
        
        self.captured_images.append(image)
        self.image_info[image_id] = image_info
        
        print(f"âœ… æµ‹è¯•å›¾åƒåˆ›å»ºæˆåŠŸï¼å›¾åƒID: {image_id}")
        return image
    
    def _display_image_info(self, image_info, image):
        """
        æ˜¾ç¤ºå›¾åƒè¯¦ç»†ä¿¡æ¯
        """
        print("\nğŸ“Š å›¾åƒä¿¡æ¯:")
        print(f"   ID: {image_info['id']}")
        print(f"   å°ºå¯¸: {image_info['shape'][1]} x {image_info['shape'][0]} åƒç´ ")
        print(f"   é€šé“æ•°: {image_info['shape'][2] if len(image_info['shape']) > 2 else 1}")
        print(f"   æ•°æ®ç±»å‹: {image_info['dtype']}")
        print(f"   æ–‡ä»¶å¤§å°: {image_info['size_bytes'] / 1024:.2f} KB")
        print(f"   é¢œè‰²ç©ºé—´: {image_info['color_space']}")
    
    def get_workshop_status(self):
        """
        è·å–æ‹ç…§è½¦é—´çŠ¶æ€
        """
        print("\nğŸ­ æ‹ç…§è½¦é—´çŠ¶æ€æŠ¥å‘Š")
        print("=" * 40)
        print(f"ğŸ“¸ å·²å¤„ç†å›¾åƒæ•°é‡: {len(self.captured_images)}")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {self.workspace_path}")
        
        if self.captured_images:
            total_size = sum(info['size_bytes'] for info in self.image_info.values())
            print(f"ğŸ’¾ æ€»æ•°æ®é‡: {total_size / 1024 / 1024:.2f} MB")
            
            print("\nğŸ“‹ å›¾åƒæ¸…å•:")
            for image_id, info in self.image_info.items():
                print(f"   [{image_id}] {info['path']} - {info['shape'][1]}x{info['shape'][0]}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ‹ç…§è½¦é—´
    workshop = CaptureWorkshop()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = workshop.create_test_image(pattern='gradient')
    
    # æ˜¾ç¤ºå·¥å‚çŠ¶æ€
    workshop.get_workshop_status()
```

è¿™ä¸ª"æ‹ç…§è½¦é—´"ç±»å±•ç¤ºäº†å¦‚ä½•ç”¨ä¼ä¸šçº§çš„ä»£ç ç»“æ„æ¥å¤„ç†å›¾åƒè¾“å…¥ï¼ŒåŒ…å«äº†å®Œæ•´çš„é”™è¯¯å¤„ç†ã€ä¿¡æ¯è®°å½•å’ŒçŠ¶æ€ç®¡ç†ã€‚

#### ğŸ–¼ï¸ ç…§ç‰‡å±•ç¤ºå…ï¼šå›¾åƒæ˜¾ç¤ºç³»ç»Ÿ

æ‹ç…§è½¦é—´è·å–äº†å›¾åƒåï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸“ä¸šçš„å±•ç¤ºç³»ç»Ÿæ¥æŸ¥çœ‹å’Œåˆ†æè¿™äº›å›¾åƒã€‚ç…§ç‰‡å±•ç¤ºå…å°±æ˜¯æˆ‘ä»¬çš„å¯è§†åŒ–ä¸­å¿ƒï¼

```python
# ç¤ºä¾‹4ï¼šç…§ç‰‡å±•ç¤ºå… - å›¾åƒæ˜¾ç¤ºç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns

class DisplayGallery:
    """
    ç…§ç‰‡å±•ç¤ºå…ç±»
    è´Ÿè´£å›¾åƒçš„ä¸“ä¸šåŒ–æ˜¾ç¤ºã€åˆ†æå’Œå¯è§†åŒ–
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç…§ç‰‡å±•ç¤ºå…
        """
        # è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®seabornæ ·å¼
        sns.set_style("whitegrid")
        
        print("ğŸ–¼ï¸ ç…§ç‰‡å±•ç¤ºå…åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“º é«˜æ¸…æ˜¾ç¤ºç³»ç»Ÿå·²å°±ç»ª")
    
    def show_single_image(self, image, title="å›¾åƒå±•ç¤º", figsize=(10, 8), 
                         color_space='RGB', show_info=True):
        """
        åœ¨å±•ç¤ºå…å±•ç¤ºå•å¼ å›¾åƒ
        
        Parameters:
        image (numpy.ndarray): å›¾åƒæ•°æ®
        title (str): å›¾åƒæ ‡é¢˜
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        color_space (str): é¢œè‰²ç©ºé—´ ('RGB', 'BGR', 'GRAY')
        show_info (bool): æ˜¯å¦æ˜¾ç¤ºå›¾åƒä¿¡æ¯
        """
        print(f"ğŸ–¼ï¸ å±•ç¤ºå…ï¼šæ­£åœ¨å±•ç¤ºå›¾åƒ - {title}")
        
        fig, ax = plt.subplots(1, 1, figsize=figsize)
        
        # æ ¹æ®é¢œè‰²ç©ºé—´å¤„ç†å›¾åƒæ˜¾ç¤º
        if color_space == 'BGR':
            # OpenCVé»˜è®¤æ˜¯BGRï¼Œè½¬æ¢ä¸ºRGBæ˜¾ç¤º
            display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        elif color_space == 'GRAY':
            # ç°åº¦å›¾åƒ
            display_image = image
            ax.imshow(display_image, cmap='gray')
        else:
            # RGBæˆ–å…¶ä»–
            display_image = image
            ax.imshow(display_image)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ˜¾ç¤ºå±æ€§
        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
        ax.axis('off')  # éšè—åæ ‡è½´
        
        # æ˜¾ç¤ºå›¾åƒä¿¡æ¯
        if show_info and image is not None:
            info_text = f"å°ºå¯¸: {image.shape[1]}Ã—{image.shape[0]}"
            if len(image.shape) > 2:
                info_text += f" | é€šé“: {image.shape[2]}"
            info_text += f" | ç±»å‹: {image.dtype}"
            
            ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                   verticalalignment='top', fontsize=10)
        
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… å›¾åƒå±•ç¤ºå®Œæˆ")
    
    def show_image_comparison(self, images, titles, figsize=(15, 5), 
                            color_space='RGB', suptitle="å›¾åƒå¯¹æ¯”å±•ç¤º"):
        """
        åœ¨å±•ç¤ºå…åŒæ—¶å±•ç¤ºå¤šå¼ å›¾åƒè¿›è¡Œå¯¹æ¯”
        
        Parameters:
        images (list): å›¾åƒåˆ—è¡¨
        titles (list): æ ‡é¢˜åˆ—è¡¨
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        color_space (str): é¢œè‰²ç©ºé—´
        suptitle (str): æ€»æ ‡é¢˜
        """
        print(f"ğŸ–¼ï¸ å±•ç¤ºå…ï¼šæ­£åœ¨è¿›è¡Œå›¾åƒå¯¹æ¯”å±•ç¤º")
        
        num_images = len(images)
        fig, axes = plt.subplots(1, num_images, figsize=figsize)
        
        # å¦‚æœåªæœ‰ä¸€å¼ å›¾åƒï¼Œaxesä¸æ˜¯åˆ—è¡¨
        if num_images == 1:
            axes = [axes]
        
        for i, (image, title) in enumerate(zip(images, titles)):
            if image is not None:
                # å¤„ç†é¢œè‰²ç©ºé—´
                if color_space == 'BGR':
                    display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                elif color_space == 'GRAY':
                    display_image = image
                    axes[i].imshow(display_image, cmap='gray')
                else:
                    display_image = image
                    
                if color_space != 'GRAY':
                    axes[i].imshow(display_image)
                
                axes[i].set_title(title, fontsize=12, fontweight='bold')
                axes[i].axis('off')
                
                # æ·»åŠ å›¾åƒä¿¡æ¯
                info_text = f"{image.shape[1]}Ã—{image.shape[0]}"
                axes[i].text(0.02, 0.98, info_text, transform=axes[i].transAxes,
                           bbox=dict(boxstyle="round,pad=0.2", facecolor="lightblue", alpha=0.7),
                           verticalalignment='top', fontsize=8)
            else:
                axes[i].text(0.5, 0.5, 'å›¾åƒåŠ è½½å¤±è´¥', ha='center', va='center',
                           transform=axes[i].transAxes, fontsize=12)
                axes[i].set_title(title, fontsize=12)
                axes[i].axis('off')
        
        fig.suptitle(suptitle, fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… å¯¹æ¯”å±•ç¤ºå®Œæˆ")
    
    def show_image_histogram(self, image, title="å›¾åƒç›´æ–¹å›¾åˆ†æ", 
                           color_space='RGB', bins=256):
        """
        åœ¨å±•ç¤ºå…æ˜¾ç¤ºå›¾åƒçš„ç›´æ–¹å›¾åˆ†æ
        
        Parameters:
        image (numpy.ndarray): å›¾åƒæ•°æ®
        title (str): æ ‡é¢˜
        color_space (str): é¢œè‰²ç©ºé—´
        bins (int): ç›´æ–¹å›¾binæ•°é‡
        """
        print(f"ğŸ“Š å±•ç¤ºå…ï¼šæ­£åœ¨è¿›è¡Œç›´æ–¹å›¾åˆ†æ")
        
        if color_space == 'GRAY':
            # ç°åº¦å›¾åƒç›´æ–¹å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            
            # æ˜¾ç¤ºå›¾åƒ
            ax1.imshow(image, cmap='gray')
            ax1.set_title('åŸå§‹å›¾åƒ', fontsize=12)
            ax1.axis('off')
            
            # æ˜¾ç¤ºç›´æ–¹å›¾
            hist = cv2.calcHist([image], [0], None, [bins], [0, 256])
            ax2.plot(hist, color='black')
            ax2.set_title('ç°åº¦ç›´æ–¹å›¾', fontsize=12)
            ax2.set_xlabel('åƒç´ å€¼')
            ax2.set_ylabel('é¢‘æ¬¡')
            ax2.grid(True, alpha=0.3)
            
        else:
            # å½©è‰²å›¾åƒç›´æ–¹å›¾
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            
            # å¤„ç†é¢œè‰²ç©ºé—´
            if color_space == 'BGR':
                display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                colors = ['red', 'green', 'blue']
                channels = ['R', 'G', 'B']
            else:
                display_image = image
                colors = ['red', 'green', 'blue']
                channels = ['R', 'G', 'B']
            
            # æ˜¾ç¤ºåŸå›¾
            axes[0, 0].imshow(display_image)
            axes[0, 0].set_title('åŸå§‹å›¾åƒ', fontsize=12)
            axes[0, 0].axis('off')
            
            # åˆ†åˆ«æ˜¾ç¤ºå„é€šé“ç›´æ–¹å›¾
            for i, (color, channel) in enumerate(zip(colors, channels)):
                hist = cv2.calcHist([image], [i], None, [bins], [0, 256])
                
                if i == 0:  # Ré€šé“
                    axes[0, 1].plot(hist, color=color, alpha=0.7, label=f'{channel}é€šé“')
                    axes[0, 1].set_title('å•é€šé“ç›´æ–¹å›¾', fontsize=12)
                elif i == 1:  # Gé€šé“
                    axes[1, 0].plot(hist, color=color, alpha=0.7, label=f'{channel}é€šé“')
                    axes[1, 0].set_title('å•é€šé“ç›´æ–¹å›¾', fontsize=12)
                else:  # Bé€šé“
                    axes[1, 1].plot(hist, color=color, alpha=0.7, label=f'{channel}é€šé“')
                    axes[1, 1].set_title('å•é€šé“ç›´æ–¹å›¾', fontsize=12)
                
                # è®¾ç½®å›¾è¡¨å±æ€§
                current_ax = axes[0, 1] if i == 0 else (axes[1, 0] if i == 1 else axes[1, 1])
                current_ax.set_xlabel('åƒç´ å€¼')
                current_ax.set_ylabel('é¢‘æ¬¡')
                current_ax.grid(True, alpha=0.3)
                current_ax.legend()
        
        fig.suptitle(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… ç›´æ–¹å›¾åˆ†æå®Œæˆ")
    
    def show_image_with_roi(self, image, roi_boxes, roi_labels=None, 
                          title="å›¾åƒåŒºåŸŸæ ‡æ³¨", color_space='RGB'):
        """
        åœ¨å±•ç¤ºå…æ˜¾ç¤ºå¸¦æœ‰æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰æ ‡æ³¨çš„å›¾åƒ
        
        Parameters:
        image (numpy.ndarray): å›¾åƒæ•°æ®
        roi_boxes (list): ROIæ¡†åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡†ä¸º(x, y, width, height)
        roi_labels (list): ROIæ ‡ç­¾åˆ—è¡¨
        title (str): æ ‡é¢˜
        color_space (str): é¢œè‰²ç©ºé—´
        """
        print(f"ğŸ” å±•ç¤ºå…ï¼šæ­£åœ¨æ˜¾ç¤ºROIæ ‡æ³¨å›¾åƒ")
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        # å¤„ç†é¢œè‰²ç©ºé—´
        if color_space == 'BGR':
            display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        elif color_space == 'GRAY':
            display_image = image
            ax.imshow(display_image, cmap='gray')
        else:
            display_image = image
            
        if color_space != 'GRAY':
            ax.imshow(display_image)
        
        # ç»˜åˆ¶ROIæ¡†
        colors = ['red', 'green', 'blue', 'yellow', 'magenta', 'cyan']
        
        for i, (x, y, w, h) in enumerate(roi_boxes):
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶çŸ©å½¢æ¡†
            rect = Rectangle((x, y), w, h, linewidth=2, 
                           edgecolor=color, facecolor='none')
            ax.add_patch(rect)
            
            # æ·»åŠ æ ‡ç­¾
            if roi_labels and i < len(roi_labels):
                label = roi_labels[i]
            else:
                label = f'ROI-{i+1}'
            
            ax.text(x, y-5, label, fontsize=10, color=color, 
                   fontweight='bold', bbox=dict(boxstyle="round,pad=0.2", 
                   facecolor='white', edgecolor=color, alpha=0.8))
        
        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
        ax.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… ROIæ ‡æ³¨æ˜¾ç¤ºå®Œæˆ")
    
    def create_image_gallery(self, images, titles, cols=3, figsize_per_image=(4, 3),
                           suptitle="å›¾åƒç”»å»Š", color_space='RGB'):
        """
        åˆ›å»ºå›¾åƒç”»å»Šå±•ç¤º
        
        Parameters:
        images (list): å›¾åƒåˆ—è¡¨
        titles (list): æ ‡é¢˜åˆ—è¡¨
        cols (int): åˆ—æ•°
        figsize_per_image (tuple): æ¯å¼ å›¾åƒçš„å°ºå¯¸
        suptitle (str): æ€»æ ‡é¢˜
        color_space (str): é¢œè‰²ç©ºé—´
        """
        print(f"ğŸ–¼ï¸ å±•ç¤ºå…ï¼šæ­£åœ¨åˆ›å»ºå›¾åƒç”»å»Š")
        
        num_images = len(images)
        rows = (num_images + cols - 1) // cols
        
        fig_width = cols * figsize_per_image[0]
        fig_height = rows * figsize_per_image[1]
        
        fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))
        
        # å¤„ç†axesçš„ç»´åº¦
        if rows == 1 and cols == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        elif cols == 1:
            axes = axes.reshape(-1, 1)
        
        for i in range(num_images):
            row = i // cols
            col = i % cols
            
            if rows == 1:
                ax = axes[col] if cols > 1 else axes[0]
            else:
                ax = axes[row, col]
            
            image = images[i]
            title = titles[i] if i < len(titles) else f'å›¾åƒ-{i+1}'
            
            if image is not None:
                # å¤„ç†é¢œè‰²ç©ºé—´
                if color_space == 'BGR':
                    display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                elif color_space == 'GRAY':
                    display_image = image
                    ax.imshow(display_image, cmap='gray')
                else:
                    display_image = image
                    
                if color_space != 'GRAY':
                    ax.imshow(display_image)
                
                ax.set_title(title, fontsize=10)
            else:
                ax.text(0.5, 0.5, 'åŠ è½½å¤±è´¥', ha='center', va='center',
                       transform=ax.transAxes)
                ax.set_title(title, fontsize=10)
            
            ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_images, rows * cols):
            row = i // cols
            col = i % cols
            if rows == 1:
                ax = axes[col] if cols > 1 else axes[0]
            else:
                ax = axes[row, col]
            ax.axis('off')
        
        fig.suptitle(suptitle, fontsize=16, fontweight='bold', y=0.95)
        plt.tight_layout()
        plt.show()
        
        print(f"âœ… å›¾åƒç”»å»Šå±•ç¤ºå®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå±•ç¤ºå…
    gallery = DisplayGallery()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image1 = workshop.create_test_image(pattern='gradient')
    test_image2 = workshop.create_test_image(pattern='checkerboard')
    
    # å•å›¾åƒå±•ç¤º
    gallery.show_single_image(test_image1, "æ¢¯åº¦æµ‹è¯•å›¾åƒ")
    
    # å¯¹æ¯”å±•ç¤º
    gallery.show_image_comparison([test_image1, test_image2], 
                                 ["æ¢¯åº¦å›¾åƒ", "æ£‹ç›˜å›¾åƒ"], 
                                 suptitle="æµ‹è¯•å›¾åƒå¯¹æ¯”")
    
    # ç›´æ–¹å›¾åˆ†æ
    gallery.show_image_histogram(test_image1, "æ¢¯åº¦å›¾åƒç›´æ–¹å›¾åˆ†æ")
```

#### ğŸ–¨ï¸ ç…§ç‰‡æ‰“å°è½¦é—´ï¼šå›¾åƒä¿å­˜ç³»ç»Ÿ

æœ‰äº†ä¸“ä¸šçš„å±•ç¤ºç³»ç»Ÿï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå¯é çš„ä¿å­˜ç³»ç»Ÿæ¥ç¡®ä¿å¤„ç†ç»“æœèƒ½å¤Ÿå®Œç¾ä¿å­˜ã€‚

```python
# ç¤ºä¾‹5ï¼šç…§ç‰‡æ‰“å°è½¦é—´ - å›¾åƒä¿å­˜ç³»ç»Ÿ
import cv2
import numpy as np
import os
from datetime import datetime
from pathlib import Path
import json

class PrintingWorkshop:
    """
    ç…§ç‰‡æ‰“å°è½¦é—´ç±»
    è´Ÿè´£å›¾åƒçš„ä¿å­˜ã€æ ¼å¼è½¬æ¢å’Œæ‰¹é‡è¾“å‡º
    """
    
    def __init__(self, output_path="digital_camera_factory/workshops/output_workshop"):
        """
        åˆå§‹åŒ–ç…§ç‰‡æ‰“å°è½¦é—´
        
        Parameters:
        output_path (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.output_path = Path(output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„è¾“å‡ºæ ¼å¼
        self.supported_formats = {
            '.jpg': 'JPEGæ ¼å¼ - æœ‰æŸå‹ç¼©ï¼Œé€‚åˆç…§ç‰‡',
            '.jpeg': 'JPEGæ ¼å¼ - æœ‰æŸå‹ç¼©ï¼Œé€‚åˆç…§ç‰‡',
            '.png': 'PNGæ ¼å¼ - æ— æŸå‹ç¼©ï¼Œæ”¯æŒé€æ˜',
            '.bmp': 'BMPæ ¼å¼ - æ— å‹ç¼©ï¼Œå ç”¨ç©ºé—´å¤§',
            '.tiff': 'TIFFæ ¼å¼ - æ— æŸå‹ç¼©ï¼Œä¸“ä¸šç”¨é€”',
            '.webp': 'WebPæ ¼å¼ - é«˜æ•ˆå‹ç¼©ï¼Œwebå‹å¥½'
        }
        
        # ä¿å­˜è®°å½•
        self.save_history = []
        
        print("ğŸ–¨ï¸ ç…§ç‰‡æ‰“å°è½¦é—´åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
        print(f"ğŸ“„ æ”¯æŒæ ¼å¼: {list(self.supported_formats.keys())}")
    
    def save_image(self, image, filename, quality=95, color_space='RGB',
                   add_timestamp=False, metadata=None):
        """
        ä¿å­˜å•å¼ å›¾åƒ
        
        Parameters:
        image (numpy.ndarray): å›¾åƒæ•°æ®
        filename (str): æ–‡ä»¶åï¼ˆå«æ‰©å±•åï¼‰
        quality (int): ä¿å­˜è´¨é‡ (1-100ï¼Œä»…å¯¹JPEGæœ‰æ•ˆ)
        color_space (str): å›¾åƒé¢œè‰²ç©ºé—´
        add_timestamp (bool): æ˜¯å¦æ·»åŠ æ—¶é—´æˆ³
        metadata (dict): é™„åŠ å…ƒæ•°æ®
        
        Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ–¨ï¸ æ‰“å°è½¦é—´ï¼šæ­£åœ¨ä¿å­˜å›¾åƒ {filename}")
        
        # å¤„ç†æ–‡ä»¶å
        if add_timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            name, ext = os.path.splitext(filename)
            filename = f"{name}_{timestamp}{ext}"
        
        # å®Œæ•´æ–‡ä»¶è·¯å¾„
        file_path = self.output_path / filename
        
        # æ£€æŸ¥æ ¼å¼æ”¯æŒ
        ext = os.path.splitext(filename)[1].lower()
        if ext not in self.supported_formats:
            print(f"âš ï¸ è­¦å‘Šï¼šæ ¼å¼ {ext} å¯èƒ½ä¸è¢«æ”¯æŒï¼Œå°è¯•ä¿å­˜...")
        
        try:
            # å¤„ç†é¢œè‰²ç©ºé—´è½¬æ¢
            if color_space == 'RGB':
                # RGBè½¬BGRï¼ˆOpenCVé»˜è®¤ï¼‰
                save_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            elif color_space == 'GRAY':
                save_image = image
            else:
                save_image = image
            
            # è®¾ç½®ä¿å­˜å‚æ•°
            save_params = []
            
            if ext in ['.jpg', '.jpeg']:
                save_params = [cv2.IMWRITE_JPEG_QUALITY, quality]
            elif ext == '.png':
                # PNGå‹ç¼©çº§åˆ« (0-9)
                compression = int((100 - quality) / 100 * 9)
                save_params = [cv2.IMWRITE_PNG_COMPRESSION, compression]
            elif ext == '.webp':
                save_params = [cv2.IMWRITE_WEBP_QUALITY, quality]
            
            # ä¿å­˜å›¾åƒ
            success = cv2.imwrite(str(file_path), save_image, save_params)
            
            if success:
                # è®°å½•ä¿å­˜ä¿¡æ¯
                save_info = {
                    'filename': filename,
                    'file_path': str(file_path),
                    'timestamp': datetime.now().isoformat(),
                    'original_shape': image.shape,
                    'color_space': color_space,
                    'quality': quality,
                    'file_size_bytes': file_path.stat().st_size,
                    'metadata': metadata or {}
                }
                
                self.save_history.append(save_info)
                
                print(f"âœ… å›¾åƒä¿å­˜æˆåŠŸ: {file_path}")
                print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {save_info['file_size_bytes'] / 1024:.2f} KB")
                
                # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
                if metadata:
                    self._save_metadata(file_path, save_info)
                
                return str(file_path)
            else:
                print(f"âŒ å›¾åƒä¿å­˜å¤±è´¥: {file_path}")
                return None
                
        except Exception as e:
            print(f"âŒ æ‰“å°è½¦é—´é”™è¯¯: {e}")
            return None
    
    def save_image_batch(self, images, base_filename, quality=95, 
                        color_space='RGB', format_type='.jpg'):
        """
        æ‰¹é‡ä¿å­˜å›¾åƒ
        
        Parameters:
        images (list): å›¾åƒåˆ—è¡¨
        base_filename (str): åŸºç¡€æ–‡ä»¶å
        quality (int): ä¿å­˜è´¨é‡
        color_space (str): é¢œè‰²ç©ºé—´
        format_type (str): ä¿å­˜æ ¼å¼
        
        Returns:
        list: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print(f"ğŸ–¨ï¸ æ‰“å°è½¦é—´ï¼šå¼€å§‹æ‰¹é‡ä¿å­˜ {len(images)} å¼ å›¾åƒ")
        
        saved_paths = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for i, image in enumerate(images):
            if image is not None:
                filename = f"{base_filename}_{timestamp}_{i+1:03d}{format_type}"
                
                path = self.save_image(
                    image=image,
                    filename=filename,
                    quality=quality,
                    color_space=color_space,
                    metadata={'batch_id': timestamp, 'sequence': i+1}
                )
                
                if path:
                    saved_paths.append(path)
            else:
                print(f"âš ï¸ è·³è¿‡ç©ºå›¾åƒ (ç´¢å¼•: {i})")
        
        print(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {len(saved_paths)} å¼ å›¾åƒ")
        return saved_paths
    
    def convert_format(self, input_path, output_format, quality=95):
        """
        è½¬æ¢å›¾åƒæ ¼å¼
        
        Parameters:
        input_path (str): è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_format (str): è¾“å‡ºæ ¼å¼ (å¦‚ '.png')
        quality (int): è½¬æ¢è´¨é‡
        
        Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ”„ æ‰“å°è½¦é—´ï¼šæ­£åœ¨è½¬æ¢æ ¼å¼ {input_path} -> {output_format}")
        
        try:
            # è¯»å–åŸå›¾åƒ
            image = cv2.imread(str(input_path))
            if image is None:
                print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {input_path}")
                return None
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            input_path = Path(input_path)
            output_filename = input_path.stem + output_format
            
            # ä¿å­˜è½¬æ¢åçš„å›¾åƒ
            output_path = self.save_image(
                image=image,
                filename=output_filename,
                quality=quality,
                color_space='BGR',
                metadata={
                    'original_file': str(input_path),
                    'conversion': f"{input_path.suffix} -> {output_format}"
                }
            )
            
            print(f"âœ… æ ¼å¼è½¬æ¢æˆåŠŸ: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ æ ¼å¼è½¬æ¢é”™è¯¯: {e}")
            return None
    
    def _save_metadata(self, image_path, save_info):
        """
        ä¿å­˜å›¾åƒå…ƒæ•°æ®
        """
        metadata_path = image_path.with_suffix('.json')
        
        try:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(save_info, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {e}")
    
    def get_workshop_status(self):
        """
        è·å–æ‰“å°è½¦é—´çŠ¶æ€
        """
        print("\nğŸ–¨ï¸ ç…§ç‰‡æ‰“å°è½¦é—´çŠ¶æ€æŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
        print(f"ğŸ“„ å·²ä¿å­˜æ–‡ä»¶æ•°: {len(self.save_history)}")
        
        if self.save_history:
            total_size = sum(info['file_size_bytes'] for info in self.save_history)
            print(f"ğŸ’¾ æ€»è¾“å‡ºå¤§å°: {total_size / 1024 / 1024:.2f} MB")
            
            # æŒ‰æ ¼å¼ç»Ÿè®¡
            format_stats = {}
            for info in self.save_history:
                ext = os.path.splitext(info['filename'])[1].lower()
                format_stats[ext] = format_stats.get(ext, 0) + 1
            
            print("\nğŸ“Š æ ¼å¼ç»Ÿè®¡:")
            for fmt, count in format_stats.items():
                print(f"   {fmt}: {count} ä¸ªæ–‡ä»¶")
            
            print("\nğŸ“‹ æœ€è¿‘ä¿å­˜çš„æ–‡ä»¶:")
            for info in self.save_history[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ª
                size_kb = info['file_size_bytes'] / 1024
                print(f"   {info['filename']} - {size_kb:.1f} KB")
        
        print(f"\nğŸ“ æ”¯æŒçš„è¾“å‡ºæ ¼å¼:")
        for fmt, desc in self.supported_formats.items():
            print(f"   {fmt}: {desc}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ‰“å°è½¦é—´
    printer = PrintingWorkshop()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(pattern='gradient')
    
    # ä¿å­˜å•å¼ å›¾åƒ
    saved_path = printer.save_image(
        image=test_image,
        filename="test_gradient.jpg",
        quality=90,
        add_timestamp=True,
        metadata={'test_type': 'gradient', 'quality_test': True}
    )
    
    # æ˜¾ç¤ºè½¦é—´çŠ¶æ€
    printer.get_workshop_status()
```

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å®Œæˆäº†æ•°å­—ç›¸æœºå·¥å‚çš„ä¸‰ä¸ªæ ¸å¿ƒè½¦é—´ï¼š

1. **ğŸ­ æ‹ç…§è½¦é—´** - è´Ÿè´£å›¾åƒè¾“å…¥å’Œè·å–
2. **ğŸ–¼ï¸ ç…§ç‰‡å±•ç¤ºå…** - è´Ÿè´£å›¾åƒæ˜¾ç¤ºå’Œåˆ†æ 
3. **ğŸ–¨ï¸ ç…§ç‰‡æ‰“å°è½¦é—´** - è´Ÿè´£å›¾åƒä¿å­˜å’Œæ ¼å¼è½¬æ¢

æ¯ä¸ªè½¦é—´éƒ½é‡‡ç”¨äº†ä¼ä¸šçº§çš„ä»£ç ç»“æ„ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†ã€çŠ¶æ€ç®¡ç†å’ŒåŠŸèƒ½æ‰©å±•èƒ½åŠ›ã€‚æ¥ä¸‹æ¥æˆ‘å°†å¼€å§‹ç¬¬äºŒèŠ‚çš„å›¾åƒé¢„å¤„ç†æŠ€æœ¯...

## ğŸ“š ç¬¬äºŒèŠ‚ï¼šç…§ç‰‡ç¾é¢œå·¥å‚çš„å›¾åƒå¤„ç†æŠ€æœ¯

### 35.2 å›¾åƒé¢„å¤„ç†æŠ€æœ¯

æˆ‘ä»¬çš„æ•°å­—ç›¸æœºå·¥å‚å·²ç»å»ºç«‹äº†å®Œå–„çš„åŸºç¡€è®¾æ–½ï¼Œç°åœ¨æ˜¯æ—¶å€™å»ºè®¾æœ€æ ¸å¿ƒçš„éƒ¨åˆ†â€”â€”**ç…§ç‰‡ç¾é¢œå·¥å‚**ï¼è¿™é‡Œæ˜¯æ•´ä¸ªå·¥å‚çš„æŠ€æœ¯å¿ƒè„ï¼Œè´Ÿè´£å°†åŸå§‹å›¾åƒè½¬åŒ–ä¸ºå®Œç¾çš„è§†è§‰ä½œå“ã€‚

#### ğŸ­ ç¾é¢œå·¥å‚æ€»è§ˆ

æƒ³è±¡ä¸€ä¸‹æœ€å…ˆè¿›çš„ç…§ç‰‡ç¾é¢œå·¥å‚ï¼Œå®ƒæœ‰å‡ ä¸ªä¸“ä¸šçš„å¤„ç†è½¦é—´ï¼š

ğŸ¨ **è°ƒè‰²è½¦é—´** - è´Ÿè´£é¢œè‰²ç©ºé—´è½¬æ¢å’Œè‰²å½©è°ƒæ•´  
ğŸ§½ **ç£¨çš®è½¦é—´** - ä¸“é—¨è¿›è¡Œå›¾åƒæ»¤æ³¢å’Œé™å™ªå¤„ç†  
âœ¨ **ç¾ç™½è½¦é—´** - è¿›è¡Œäº®åº¦å¯¹æ¯”åº¦è°ƒæ•´å’Œç›´æ–¹å›¾å‡è¡¡åŒ–  
ğŸ”ª **é”åŒ–è½¦é—´** - è´Ÿè´£å›¾åƒè¾¹ç¼˜å¢å¼ºå’Œç»†èŠ‚ä¼˜åŒ–  

æ¯ä¸ªè½¦é—´éƒ½æœ‰è‡ªå·±çš„ä¸“ä¸šè®¾å¤‡å’Œå·¥è‰ºæµç¨‹ï¼Œè®©æˆ‘ä»¬é€ä¸€æ¢ç´¢å®ƒä»¬ï¼

#### ğŸ¨ è°ƒè‰²è½¦é—´ï¼šé¢œè‰²ç©ºé—´è½¬æ¢æŠ€æœ¯

é¢œè‰²ç©ºé—´å°±åƒæ˜¯ä¸åŒçš„"è°ƒè‰²æ¿"ï¼Œæ¯ç§è°ƒè‰²æ¿éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç”¨é€”å’Œä¼˜åŠ¿ã€‚

```python
# ç¤ºä¾‹6ï¼šè°ƒè‰²è½¦é—´ - é¢œè‰²ç©ºé—´è½¬æ¢ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class ColorWorkshop:
    """
    è°ƒè‰²è½¦é—´ç±»
    è´Ÿè´£å„ç§é¢œè‰²ç©ºé—´çš„è½¬æ¢å’Œè‰²å½©å¤„ç†
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–è°ƒè‰²è½¦é—´
        """
        # æ”¯æŒçš„é¢œè‰²ç©ºé—´è½¬æ¢
        self.color_spaces = {
            'RGB': 'æ ‡å‡†RGBé¢œè‰²ç©ºé—´ - é€‚åˆæ˜¾ç¤º',
            'BGR': 'OpenCVé»˜è®¤é¢œè‰²ç©ºé—´ - è“ç»¿çº¢',
            'HSV': 'HSVé¢œè‰²ç©ºé—´ - é€‚åˆé¢œè‰²åˆ†æ',
            'LAB': 'LABé¢œè‰²ç©ºé—´ - æ„ŸçŸ¥å‡åŒ€',
            'GRAY': 'ç°åº¦ç©ºé—´ - å•é€šé“å¤„ç†',
            'YUV': 'YUVé¢œè‰²ç©ºé—´ - è§†é¢‘ç¼–ç '
        }
        
        print("ğŸ¨ è°ƒè‰²è½¦é—´åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ–Œï¸ è°ƒè‰²æ¿ç³»ç»Ÿå·²å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„é¢œè‰²ç©ºé—´: {list(self.color_spaces.keys())}")
    
    def show_color_spaces(self, image, figsize=(16, 12)):
        """
        å±•ç¤ºå›¾åƒåœ¨ä¸åŒé¢œè‰²ç©ºé—´ä¸‹çš„è¡¨ç°
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ (RGBæ ¼å¼)
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ¨ è°ƒè‰²è½¦é—´ï¼šæ­£åœ¨å±•ç¤ºä¸åŒé¢œè‰²ç©ºé—´æ•ˆæœ")
        
        # ç¡®ä¿è¾“å…¥æ˜¯RGBæ ¼å¼
        if len(image.shape) == 3:
            rgb_image = image.copy()
        else:
            print("âŒ è¾“å…¥å›¾åƒå¿…é¡»æ˜¯å½©è‰²å›¾åƒ")
            return
        
        # è½¬æ¢ä¸ºBGRæ ¼å¼ä¾›OpenCVä½¿ç”¨
        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
        
        # å‡†å¤‡ä¸åŒé¢œè‰²ç©ºé—´çš„å›¾åƒ
        color_images = {}
        
        # RGB (åŸå›¾)
        color_images['RGB'] = rgb_image
        
        # BGR
        color_images['BGR'] = bgr_image
        
        # HSV
        hsv_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV)
        color_images['HSV'] = hsv_image
        
        # LAB
        lab_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2LAB)
        color_images['LAB'] = lab_image
        
        # ç°åº¦
        gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)
        color_images['GRAY'] = gray_image
        
        # YUV
        yuv_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2YUV)
        color_images['YUV'] = yuv_image
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        for i, (space_name, img) in enumerate(color_images.items()):
            ax = axes[i]
            
            if space_name == 'GRAY':
                ax.imshow(img, cmap='gray')
            else:
                # å¯¹äºéRGBç©ºé—´ï¼Œæˆ‘ä»¬æ˜¾ç¤ºæ¯ä¸ªé€šé“
                if space_name == 'RGB':
                    ax.imshow(img)
                elif space_name == 'BGR':
                    # BGRæ˜¾ç¤ºæ—¶è½¬æ¢ä¸ºRGB
                    display_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    ax.imshow(display_img)
                else:
                    # å…¶ä»–é¢œè‰²ç©ºé—´æ˜¾ç¤ºåŸå§‹æ•°æ®
                    ax.imshow(img)
            
            ax.set_title(f'{space_name} é¢œè‰²ç©ºé—´', fontsize=12, fontweight='bold')
            ax.axis('off')
            
            # æ·»åŠ é¢œè‰²ç©ºé—´è¯´æ˜
            desc = self.color_spaces.get(space_name, '')
            ax.text(0.02, 0.02, desc, transform=ax.transAxes,
                   bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.8),
                   fontsize=8, verticalalignment='bottom')
        
        fig.suptitle('ğŸ¨ è°ƒè‰²è½¦é—´ - é¢œè‰²ç©ºé—´å±•ç¤º', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… é¢œè‰²ç©ºé—´å±•ç¤ºå®Œæˆ")
    
    def analyze_hsv_channels(self, image, figsize=(15, 10)):
        """
        åˆ†æHSVé¢œè‰²ç©ºé—´çš„å„ä¸ªé€šé“
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ (RGBæ ¼å¼)
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ” è°ƒè‰²è½¦é—´ï¼šæ­£åœ¨åˆ†æHSVé€šé“")
        
        # è½¬æ¢ä¸ºHSV
        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        hsv_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2HSV)
        
        # åˆ†ç¦»HSVé€šé“
        h_channel = hsv_image[:, :, 0]  # è‰²è°ƒ (Hue)
        s_channel = hsv_image[:, :, 1]  # é¥±å’Œåº¦ (Saturation)
        v_channel = hsv_image[:, :, 2]  # æ˜åº¦ (Value)
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # åŸå§‹RGBå›¾åƒ
        axes[0, 0].imshow(image)
        axes[0, 0].set_title('åŸå§‹RGBå›¾åƒ', fontsize=12, fontweight='bold')
        axes[0, 0].axis('off')
        
        # Hé€šé“ (è‰²è°ƒ)
        axes[0, 1].imshow(h_channel, cmap='hsv')
        axes[0, 1].set_title('Hé€šé“ (è‰²è°ƒ)\nå€¼åŸŸ: 0-179Â°', fontsize=12, fontweight='bold')
        axes[0, 1].axis('off')
        
        # Sé€šé“ (é¥±å’Œåº¦)
        axes[1, 0].imshow(s_channel, cmap='gray')
        axes[1, 0].set_title('Sé€šé“ (é¥±å’Œåº¦)\nå€¼åŸŸ: 0-255', fontsize=12, fontweight='bold')
        axes[1, 0].axis('off')
        
        # Vé€šé“ (æ˜åº¦)
        axes[1, 1].imshow(v_channel, cmap='gray')
        axes[1, 1].set_title('Vé€šé“ (æ˜åº¦)\nå€¼åŸŸ: 0-255', fontsize=12, fontweight='bold')
        axes[1, 1].axis('off')
        
        fig.suptitle('ğŸ¨ HSVé€šé“åˆ†æ - ç†è§£è‰²å½©çš„ä¸‰ä¸ªç»´åº¦', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š HSVé€šé“ç»Ÿè®¡:")
        print(f"   Hé€šé“ (è‰²è°ƒ): èŒƒå›´ {h_channel.min()}-{h_channel.max()}, å¹³å‡ {h_channel.mean():.1f}")
        print(f"   Sé€šé“ (é¥±å’Œåº¦): èŒƒå›´ {s_channel.min()}-{s_channel.max()}, å¹³å‡ {s_channel.mean():.1f}")
        print(f"   Vé€šé“ (æ˜åº¦): èŒƒå›´ {v_channel.min()}-{v_channel.max()}, å¹³å‡ {v_channel.mean():.1f}")
        
        print("âœ… HSVé€šé“åˆ†æå®Œæˆ")
        
        return hsv_image, (h_channel, s_channel, v_channel)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè°ƒè‰²è½¦é—´
    color_workshop = ColorWorkshop()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(640, 480, 'gradient')
    
    # å±•ç¤ºä¸åŒé¢œè‰²ç©ºé—´
    color_workshop.show_color_spaces(test_image)
    
    # åˆ†æHSVé€šé“
    hsv_img, channels = color_workshop.analyze_hsv_channels(test_image)
```

#### ğŸ§½ ç£¨çš®è½¦é—´ï¼šå›¾åƒæ»¤æ³¢å’Œé™å™ªæŠ€æœ¯

ç£¨çš®è½¦é—´æ˜¯ç¾é¢œå·¥å‚çš„æ ¸å¿ƒï¼Œè´Ÿè´£å»é™¤å›¾åƒä¸­çš„å™ªå£°å’Œç‘•ç–µï¼Œè®©å›¾åƒå˜å¾—æ›´åŠ å¹³æ»‘å’Œæ¸…æ´ã€‚

```python
# ç¤ºä¾‹7ï¼šç£¨çš®è½¦é—´ - å›¾åƒæ»¤æ³¢å’Œé™å™ªç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class FilteringWorkshop:
    """
    ç£¨çš®è½¦é—´ç±»
    è´Ÿè´£å›¾åƒæ»¤æ³¢ã€é™å™ªå’Œå¹³æ»‘å¤„ç†
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç£¨çš®è½¦é—´
        """
        # æ”¯æŒçš„æ»¤æ³¢æŠ€æœ¯
        self.filter_types = {
            'blur': 'å‡å€¼æ»¤æ³¢ - ç®€å•å¹³å‡',
            'gaussian': 'é«˜æ–¯æ»¤æ³¢ - åŠ æƒå¹³å‡',
            'median': 'ä¸­å€¼æ»¤æ³¢ - å»é™¤æ¤’ç›å™ªå£°',
            'bilateral': 'åŒè¾¹æ»¤æ³¢ - ä¿æŒè¾¹ç¼˜',
            'morphology': 'å½¢æ€å­¦æ»¤æ³¢ - ç»“æ„å¤„ç†'
        }
        
        print("ğŸ§½ ç£¨çš®è½¦é—´åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ”§ æ»¤æ³¢è®¾å¤‡å·²å‡†å¤‡å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„æ»¤æ³¢æŠ€æœ¯: {list(self.filter_types.keys())}")
    
    def add_noise_to_image(self, image, noise_type='gaussian', intensity=0.1):
        """
        å‘å›¾åƒæ·»åŠ å™ªå£°ï¼ˆç”¨äºæ¼”ç¤ºæ»¤æ³¢æ•ˆæœï¼‰
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        noise_type (str): å™ªå£°ç±»å‹ ('gaussian', 'salt_pepper', 'uniform')
        intensity (float): å™ªå£°å¼ºåº¦
        
        Returns:
        numpy.ndarray: å¸¦å™ªå£°çš„å›¾åƒ
        """
        print(f"ğŸ­ ç£¨çš®è½¦é—´ï¼šæ­£åœ¨æ·»åŠ  {noise_type} å™ªå£° (å¼ºåº¦: {intensity})")
        
        noisy_image = image.copy().astype(np.float32)
        
        if noise_type == 'gaussian':
            # é«˜æ–¯å™ªå£°
            noise = np.random.normal(0, intensity * 255, image.shape)
            noisy_image = noisy_image + noise
            
        elif noise_type == 'salt_pepper':
            # æ¤’ç›å™ªå£°
            noise = np.random.random(image.shape)
            noisy_image[noise < intensity/2] = 0  # ç›å™ªå£° (é»‘ç‚¹)
            noisy_image[noise > 1 - intensity/2] = 255  # æ¤’å™ªå£° (ç™½ç‚¹)
            
        elif noise_type == 'uniform':
            # å‡åŒ€å™ªå£°
            noise = np.random.uniform(-intensity * 255, intensity * 255, image.shape)
            noisy_image = noisy_image + noise
        
        # é™åˆ¶åƒç´ å€¼èŒƒå›´
        noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)
        
        print(f"âœ… å™ªå£°æ·»åŠ å®Œæˆ")
        return noisy_image
    
    def apply_filters_comparison(self, image, figsize=(18, 12)):
        """
        å¯¹æ¯”å±•ç¤ºä¸åŒæ»¤æ³¢æŠ€æœ¯çš„æ•ˆæœ
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ§½ ç£¨çš®è½¦é—´ï¼šæ­£åœ¨è¿›è¡Œæ»¤æ³¢æŠ€æœ¯å¯¹æ¯”")
        
        # æ·»åŠ å™ªå£°
        noisy_image = self.add_noise_to_image(image, 'gaussian', 0.15)
        
        # å‡†å¤‡ä¸åŒçš„æ»¤æ³¢ç»“æœ
        filtered_images = {}
        
        # åŸå›¾å’Œå™ªå£°å›¾
        filtered_images['åŸå§‹å›¾åƒ'] = image
        filtered_images['å™ªå£°å›¾åƒ'] = noisy_image
        
        # å‡å€¼æ»¤æ³¢
        blur_result = cv2.blur(noisy_image, (5, 5))
        filtered_images['å‡å€¼æ»¤æ³¢'] = blur_result
        
        # é«˜æ–¯æ»¤æ³¢
        gaussian_result = cv2.GaussianBlur(noisy_image, (5, 5), 0)
        filtered_images['é«˜æ–¯æ»¤æ³¢'] = gaussian_result
        
        # ä¸­å€¼æ»¤æ³¢
        median_result = cv2.medianBlur(noisy_image, 5)
        filtered_images['ä¸­å€¼æ»¤æ³¢'] = median_result
        
        # åŒè¾¹æ»¤æ³¢
        bilateral_result = cv2.bilateralFilter(noisy_image, 9, 75, 75)
        filtered_images['åŒè¾¹æ»¤æ³¢'] = bilateral_result
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        for i, (filter_name, filtered_img) in enumerate(filtered_images.items()):
            ax = axes[i]
            
            if len(filtered_img.shape) == 3:
                # å½©è‰²å›¾åƒ
                if filter_name in ['åŸå§‹å›¾åƒ', 'å™ªå£°å›¾åƒ']:
                    ax.imshow(filtered_img)
                else:
                    # OpenCVæ»¤æ³¢ç»“æœå¯èƒ½æ˜¯BGRï¼Œè½¬æ¢ä¸ºRGBæ˜¾ç¤º
                    if filtered_img.shape[2] == 3:
                        display_img = cv2.cvtColor(filtered_img, cv2.COLOR_BGR2RGB)
                        ax.imshow(display_img)
                    else:
                        ax.imshow(filtered_img)
            else:
                # ç°åº¦å›¾åƒ
                ax.imshow(filtered_img, cmap='gray')
            
            ax.set_title(filter_name, fontsize=12, fontweight='bold')
            ax.axis('off')
            
            # æ·»åŠ æ»¤æ³¢è¯´æ˜
            if filter_name in ['å‡å€¼æ»¤æ³¢', 'é«˜æ–¯æ»¤æ³¢', 'ä¸­å€¼æ»¤æ³¢', 'åŒè¾¹æ»¤æ³¢']:
                descriptions = {
                    'å‡å€¼æ»¤æ³¢': 'ç®€å•å¹³å‡\nå¿«é€Ÿä½†æ¨¡ç³Š',
                    'é«˜æ–¯æ»¤æ³¢': 'åŠ æƒå¹³å‡\nè‡ªç„¶å¹³æ»‘',
                    'ä¸­å€¼æ»¤æ³¢': 'æ’åºå–ä¸­å€¼\nå»æ¤’ç›å™ªå£°',
                    'åŒè¾¹æ»¤æ³¢': 'ä¿æŒè¾¹ç¼˜\né«˜è´¨é‡é™å™ª'
                }
                desc = descriptions.get(filter_name, '')
                ax.text(0.02, 0.98, desc, transform=ax.transAxes,
                       bbox=dict(boxstyle="round,pad=0.2", facecolor="yellow", alpha=0.7),
                       fontsize=8, verticalalignment='top')
        
        fig.suptitle('ğŸ§½ ç£¨çš®è½¦é—´ - æ»¤æ³¢æŠ€æœ¯å¯¹æ¯”å±•ç¤º', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… æ»¤æ³¢å¯¹æ¯”å±•ç¤ºå®Œæˆ")
        
        return filtered_images
    
    def demonstrate_bilateral_filter(self, image, figsize=(15, 10)):
        """
        è¯¦ç»†æ¼”ç¤ºåŒè¾¹æ»¤æ³¢çš„å‚æ•°æ•ˆæœ
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ” ç£¨çš®è½¦é—´ï¼šæ­£åœ¨æ¼”ç¤ºåŒè¾¹æ»¤æ³¢å‚æ•°æ•ˆæœ")
        
        # æ·»åŠ å™ªå£°
        noisy_image = self.add_noise_to_image(image, 'gaussian', 0.1)
        
        # ä¸åŒå‚æ•°çš„åŒè¾¹æ»¤æ³¢
        bilateral_results = {}
        
        # åŸå›¾å’Œå™ªå£°å›¾
        bilateral_results['åŸå§‹å›¾åƒ'] = image
        bilateral_results['å™ªå£°å›¾åƒ'] = noisy_image
        
        # ä¸åŒçš„åŒè¾¹æ»¤æ³¢å‚æ•°
        params = [
            (9, 20, 20),   # ä½å¼ºåº¦
            (9, 50, 50),   # ä¸­å¼ºåº¦
            (9, 80, 80),   # é«˜å¼ºåº¦
            (15, 50, 50),  # å¤§æ»¤æ³¢æ ¸
        ]
        
        param_names = ['ä½å¼ºåº¦', 'ä¸­å¼ºåº¦', 'é«˜å¼ºåº¦', 'å¤§æ»¤æ³¢æ ¸']
        
        for i, (d, sigma_color, sigma_space) in enumerate(params):
            result = cv2.bilateralFilter(noisy_image, d, sigma_color, sigma_space)
            bilateral_results[f'åŒè¾¹æ»¤æ³¢-{param_names[i]}'] = result
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        for i, (name, img) in enumerate(bilateral_results.items()):
            ax = axes[i]
            
            if len(img.shape) == 3:
                ax.imshow(img)
            else:
                ax.imshow(img, cmap='gray')
            
            ax.set_title(name, fontsize=12, fontweight='bold')
            ax.axis('off')
            
            # æ·»åŠ å‚æ•°è¯´æ˜
            if i >= 2 and i < 6:  # åŒè¾¹æ»¤æ³¢ç»“æœ
                param_idx = i - 2
                d, sigma_color, sigma_space = params[param_idx]
                param_text = f"d={d}\nÏƒ_color={sigma_color}\nÏƒ_space={sigma_space}"
                ax.text(0.02, 0.98, param_text, transform=ax.transAxes,
                       bbox=dict(boxstyle="round,pad=0.2", facecolor="lightgreen", alpha=0.7),
                       fontsize=8, verticalalignment='top')
        
        fig.suptitle('ğŸ§½ åŒè¾¹æ»¤æ³¢å‚æ•°æ•ˆæœå¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("ğŸ“ åŒè¾¹æ»¤æ³¢å‚æ•°è¯´æ˜:")
        print("   d: æ»¤æ³¢å™¨ç›´å¾„ (å½±å“å¤„ç†èŒƒå›´)")
        print("   Ïƒ_color: é¢œè‰²ç›¸ä¼¼æ€§æ ‡å‡†å·® (æ§åˆ¶é¢œè‰²å·®å¼‚å®¹å¿åº¦)")
        print("   Ïƒ_space: ç©ºé—´æ ‡å‡†å·® (æ§åˆ¶è·ç¦»æƒé‡)")
        print("âœ… åŒè¾¹æ»¤æ³¢æ¼”ç¤ºå®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç£¨çš®è½¦é—´
    filter_workshop = FilteringWorkshop()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(640, 480, 'checkerboard')
    
    # æ»¤æ³¢å¯¹æ¯”æ¼”ç¤º
    results = filter_workshop.apply_filters_comparison(test_image)
    
    # åŒè¾¹æ»¤æ³¢è¯¦ç»†æ¼”ç¤º
    filter_workshop.demonstrate_bilateral_filter(test_image)
```

#### âœ¨ ç¾ç™½è½¦é—´ï¼šå›¾åƒå¢å¼ºæŠ€æœ¯

ç¾ç™½è½¦é—´è´Ÿè´£è°ƒæ•´å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦å’Œè‰²å½©å¹³è¡¡ï¼Œè®©å›¾åƒå‘ˆç°å‡ºæ›´å¥½çš„è§†è§‰æ•ˆæœã€‚

```python
# ç¤ºä¾‹8ï¼šç¾ç™½è½¦é—´ - å›¾åƒå¢å¼ºç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class EnhancementWorkshop:
    """
    ç¾ç™½è½¦é—´ç±»
    è´Ÿè´£å›¾åƒäº®åº¦ã€å¯¹æ¯”åº¦å’Œè‰²å½©å¢å¼º
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç¾ç™½è½¦é—´
        """
        # æ”¯æŒçš„å¢å¼ºæŠ€æœ¯
        self.enhancement_types = {
            'histogram_equalization': 'ç›´æ–¹å›¾å‡è¡¡åŒ–',
            'clahe': 'è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–',
            'gamma_correction': 'ä¼½é©¬æ ¡æ­£',
            'contrast_brightness': 'å¯¹æ¯”åº¦äº®åº¦è°ƒæ•´',
            'color_balance': 'è‰²å½©å¹³è¡¡'
        }
        
        print("âœ¨ ç¾ç™½è½¦é—´åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ’¡ å›¾åƒå¢å¼ºè®¾å¤‡å·²å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„å¢å¼ºæŠ€æœ¯: {list(self.enhancement_types.keys())}")
    
    def adjust_brightness_contrast(self, image, brightness=0, contrast=1.0):
        """
        è°ƒæ•´å›¾åƒçš„äº®åº¦å’Œå¯¹æ¯”åº¦
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        brightness (int): äº®åº¦è°ƒæ•´ (-100 åˆ° 100)
        contrast (float): å¯¹æ¯”åº¦è°ƒæ•´ (0.5 åˆ° 3.0)
        
        Returns:
        numpy.ndarray: è°ƒæ•´åçš„å›¾åƒ
        """
        print(f"ğŸ’¡ ç¾ç™½è½¦é—´ï¼šè°ƒæ•´äº®åº¦ {brightness}, å¯¹æ¯”åº¦ {contrast}")
        
        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°è¿›è¡Œè®¡ç®—
        adjusted = image.astype(np.float32)
        
        # åº”ç”¨å¯¹æ¯”åº¦å’Œäº®åº¦è°ƒæ•´
        # å…¬å¼: new_pixel = contrast * old_pixel + brightness
        adjusted = contrast * adjusted + brightness
        
        # é™åˆ¶åƒç´ å€¼èŒƒå›´
        adjusted = np.clip(adjusted, 0, 255)
        
        return adjusted.astype(np.uint8)
    
    def gamma_correction(self, image, gamma=1.0):
        """
        ä¼½é©¬æ ¡æ­£
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        gamma (float): ä¼½é©¬å€¼ (< 1 å˜äº®, > 1 å˜æš—)
        
        Returns:
        numpy.ndarray: æ ¡æ­£åçš„å›¾åƒ
        """
        print(f"ğŸ”† ç¾ç™½è½¦é—´ï¼šåº”ç”¨ä¼½é©¬æ ¡æ­£ Î³={gamma}")
        
        # æ„å»ºæŸ¥æ‰¾è¡¨
        inv_gamma = 1.0 / gamma
        table = np.array([((i / 255.0) ** inv_gamma) * 255 
                         for i in np.arange(0, 256)]).astype("uint8")
        
        # åº”ç”¨æŸ¥æ‰¾è¡¨
        corrected = cv2.LUT(image, table)
        
        return corrected
    
    def histogram_equalization(self, image):
        """
        ç›´æ–¹å›¾å‡è¡¡åŒ–
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        
        Returns:
        numpy.ndarray: å‡è¡¡åŒ–åçš„å›¾åƒ
        """
        print("ğŸ“Š ç¾ç™½è½¦é—´ï¼šåº”ç”¨ç›´æ–¹å›¾å‡è¡¡åŒ–")
        
        if len(image.shape) == 3:
            # å½©è‰²å›¾åƒï¼šåœ¨YUVç©ºé—´è¿›è¡Œå‡è¡¡åŒ–
            yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)
            yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])
            equalized = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)
        else:
            # ç°åº¦å›¾åƒï¼šç›´æ¥å‡è¡¡åŒ–
            equalized = cv2.equalizeHist(image)
        
        return equalized
    
    def clahe_enhancement(self, image, clip_limit=2.0, tile_grid_size=(8, 8)):
        """
        è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ– (CLAHE)
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        clip_limit (float): å¯¹æ¯”åº¦é™åˆ¶
        tile_grid_size (tuple): ç½‘æ ¼å¤§å°
        
        Returns:
        numpy.ndarray: å¢å¼ºåçš„å›¾åƒ
        """
        print(f"ğŸ“ˆ ç¾ç™½è½¦é—´ï¼šåº”ç”¨CLAHEå¢å¼º (clip_limit={clip_limit})")
        
        # åˆ›å»ºCLAHEå¯¹è±¡
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
        
        if len(image.shape) == 3:
            # å½©è‰²å›¾åƒï¼šåœ¨LABç©ºé—´çš„Lé€šé“è¿›è¡ŒCLAHE
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            lab[:, :, 0] = clahe.apply(lab[:, :, 0])
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        else:
            # ç°åº¦å›¾åƒï¼šç›´æ¥åº”ç”¨CLAHE
            enhanced = clahe.apply(image)
        
        return enhanced
    
    def enhancement_comparison(self, image, figsize=(20, 15)):
        """
        å¯¹æ¯”å±•ç¤ºä¸åŒå¢å¼ºæŠ€æœ¯çš„æ•ˆæœ
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("âœ¨ ç¾ç™½è½¦é—´ï¼šæ­£åœ¨è¿›è¡Œå¢å¼ºæŠ€æœ¯å¯¹æ¯”")
        
        # åˆ›å»ºä¸€ä¸ªåæš—çš„å›¾åƒç”¨äºæ¼”ç¤º
        dark_image = self.adjust_brightness_contrast(image, brightness=-50, contrast=0.7)
        
        # å‡†å¤‡ä¸åŒçš„å¢å¼ºç»“æœ
        enhanced_images = {}
        
        # åŸå›¾å’Œæš—å›¾
        enhanced_images['åŸå§‹å›¾åƒ'] = image
        enhanced_images['åæš—å›¾åƒ'] = dark_image
        
        # äº®åº¦å¯¹æ¯”åº¦è°ƒæ•´
        brightness_enhanced = self.adjust_brightness_contrast(dark_image, brightness=30, contrast=1.3)
        enhanced_images['äº®åº¦å¯¹æ¯”åº¦è°ƒæ•´'] = brightness_enhanced
        
        # ä¼½é©¬æ ¡æ­£
        gamma_enhanced = self.gamma_correction(dark_image, gamma=0.7)
        enhanced_images['ä¼½é©¬æ ¡æ­£'] = gamma_enhanced
        
        # ç›´æ–¹å›¾å‡è¡¡åŒ–
        hist_enhanced = self.histogram_equalization(dark_image)
        enhanced_images['ç›´æ–¹å›¾å‡è¡¡åŒ–'] = hist_enhanced
        
        # CLAHEå¢å¼º
        clahe_enhanced = self.clahe_enhancement(dark_image, clip_limit=3.0)
        enhanced_images['CLAHEå¢å¼º'] = clahe_enhanced
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        for i, (enhancement_name, enhanced_img) in enumerate(enhanced_images.items()):
            ax = axes[i]
            
            if len(enhanced_img.shape) == 3:
                ax.imshow(enhanced_img)
            else:
                ax.imshow(enhanced_img, cmap='gray')
            
            ax.set_title(enhancement_name, fontsize=12, fontweight='bold')
            ax.axis('off')
            
            # æ·»åŠ æŠ€æœ¯è¯´æ˜
            descriptions = {
                'äº®åº¦å¯¹æ¯”åº¦è°ƒæ•´': 'çº¿æ€§è°ƒæ•´\n+30äº®åº¦, 1.3å¯¹æ¯”åº¦',
                'ä¼½é©¬æ ¡æ­£': 'éçº¿æ€§è°ƒæ•´\nÎ³=0.7 æäº®',
                'ç›´æ–¹å›¾å‡è¡¡åŒ–': 'å…¨å±€å¯¹æ¯”åº¦å¢å¼º\nå¯èƒ½è¿‡åº¦å¢å¼º',
                'CLAHEå¢å¼º': 'å±€éƒ¨è‡ªé€‚åº”å¢å¼º\né¿å…è¿‡åº¦å¢å¼º'
            }
            
            if enhancement_name in descriptions:
                desc = descriptions[enhancement_name]
                ax.text(0.02, 0.98, desc, transform=ax.transAxes,
                       bbox=dict(boxstyle="round,pad=0.2", facecolor="lightblue", alpha=0.7),
                       fontsize=8, verticalalignment='top')
        
        fig.suptitle('âœ¨ ç¾ç™½è½¦é—´ - å›¾åƒå¢å¼ºæŠ€æœ¯å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… å¢å¼ºæŠ€æœ¯å¯¹æ¯”å®Œæˆ")
        
        return enhanced_images
    
    def interactive_gamma_correction(self, image, gamma_values=[0.5, 0.8, 1.0, 1.2, 1.5, 2.0]):
        """
        äº¤äº’å¼ä¼½é©¬æ ¡æ­£æ¼”ç¤º
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        gamma_values (list): è¦æ¼”ç¤ºçš„ä¼½é©¬å€¼åˆ—è¡¨
        """
        print("ğŸšï¸ ç¾ç™½è½¦é—´ï¼šä¼½é©¬æ ¡æ­£å‚æ•°æ¼”ç¤º")
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, gamma in enumerate(gamma_values):
            if i < len(axes):
                ax = axes[i]
                
                # åº”ç”¨ä¼½é©¬æ ¡æ­£
                corrected = self.gamma_correction(image, gamma)
                
                if len(corrected.shape) == 3:
                    ax.imshow(corrected)
                else:
                    ax.imshow(corrected, cmap='gray')
                
                # è®¾ç½®æ ‡é¢˜
                if gamma < 1.0:
                    effect = "(å˜äº®)"
                elif gamma > 1.0:
                    effect = "(å˜æš—)"
                else:
                    effect = "(åŸå§‹)"
                
                ax.set_title(f'Î³ = {gamma} {effect}', fontsize=12, fontweight='bold')
                ax.axis('off')
                
                # æ·»åŠ æ•ˆæœè¯´æ˜
                if gamma != 1.0:
                    effect_desc = "æäº®æš—éƒ¨" if gamma < 1.0 else "å‹æš—äº®éƒ¨"
                    ax.text(0.02, 0.98, effect_desc, transform=ax.transAxes,
                           bbox=dict(boxstyle="round,pad=0.2", 
                                   facecolor="yellow" if gamma < 1.0 else "orange", 
                                   alpha=0.7),
                           fontsize=8, verticalalignment='top')
        
        fig.suptitle('ğŸšï¸ ä¼½é©¬æ ¡æ­£å‚æ•°æ•ˆæœå¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("ğŸ“ ä¼½é©¬æ ¡æ­£åŸç†:")
        print("   Î³ < 1.0: æäº®å›¾åƒï¼Œå¢å¼ºæš—éƒ¨ç»†èŠ‚")
        print("   Î³ = 1.0: ä¿æŒåŸå§‹å›¾åƒ")
        print("   Î³ > 1.0: å‹æš—å›¾åƒï¼Œå¢å¼ºå¯¹æ¯”åº¦")
        print("âœ… ä¼½é©¬æ ¡æ­£æ¼”ç¤ºå®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç¾ç™½è½¦é—´
    enhancement_workshop = EnhancementWorkshop()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(640, 480, 'gradient')
    
    # å¢å¼ºæŠ€æœ¯å¯¹æ¯”
    enhanced_results = enhancement_workshop.enhancement_comparison(test_image)
    
    # ä¼½é©¬æ ¡æ­£æ¼”ç¤º
    enhancement_workshop.interactive_gamma_correction(test_image)
```

ç°åœ¨æˆ‘ä»¬çš„ç…§ç‰‡ç¾é¢œå·¥å‚å·²ç»å…·å¤‡äº†ä¸‰ä¸ªä¸“ä¸šè½¦é—´ï¼šè°ƒè‰²è½¦é—´ã€ç£¨çš®è½¦é—´å’Œç¾ç™½è½¦é—´ã€‚æ¯ä¸ªè½¦é—´éƒ½æœ‰è‡ªå·±çš„ä¸“ä¸šè®¾å¤‡å’Œå¤„ç†æµç¨‹ï¼Œèƒ½å¤Ÿå°†åŸå§‹å›¾åƒè½¬åŒ–ä¸ºé«˜è´¨é‡çš„è§†è§‰ä½œå“ã€‚æ¥ä¸‹æ¥æˆ‘å°†ç»§ç»­æ·»åŠ ç¬¬ä¸‰èŠ‚çš„ç‰¹å¾æ£€æµ‹ä¸åŒ¹é…å†…å®¹... 

## ğŸ“š ç¬¬ä¸‰èŠ‚ï¼šå›¾åƒä¾¦æ¢å±€çš„ç‰¹å¾æ£€æµ‹æŠ€æœ¯

### 35.3 ç‰¹å¾æ£€æµ‹ä¸åŒ¹é…

ç»è¿‡ç¾é¢œå·¥å‚çš„ç²¾å¿ƒå¤„ç†ï¼Œæˆ‘ä»¬çš„å›¾åƒå·²ç»å˜å¾—å®Œç¾æ— ç‘•ã€‚ä½†æ˜¯ï¼ŒçœŸæ­£çš„æ™ºèƒ½è§†è§‰åº”ç”¨éœ€è¦çš„ä¸ä»…ä»…æ˜¯ç¾ä¸½çš„å›¾åƒï¼Œæ›´éœ€è¦èƒ½å¤Ÿ"ç†è§£"å›¾åƒå†…å®¹çš„èƒ½åŠ›ã€‚è¿™å°±éœ€è¦æˆ‘ä»¬çš„**å›¾åƒä¾¦æ¢å±€**ï¼

#### ğŸ•µï¸ å›¾åƒä¾¦æ¢å±€æ€»è§ˆ

æƒ³è±¡ä¸€ä¸‹ä¸€ä¸ªé«˜ç§‘æŠ€çš„å›¾åƒä¾¦æ¢å±€ï¼Œè¿™é‡Œæœ‰æœ€ä¸“ä¸šçš„"ä¾¦æ¢"ä»¬åœ¨å·¥ä½œï¼š

ğŸ” **è¾¹ç¼˜ä¾¦æ¢éƒ¨** - ä¸“é—¨è´Ÿè´£å¯»æ‰¾å›¾åƒä¸­çš„è¾¹ç¼˜å’Œè½®å»“  
ğŸ“ **è§’ç‚¹ä¾¦æ¢éƒ¨** - æ“…é•¿å‘ç°å›¾åƒä¸­çš„é‡è¦è§’ç‚¹å’Œå…³é”®ç‚¹  
ğŸ”— **åŒ¹é…ä¾¦æ¢éƒ¨** - è´Ÿè´£åœ¨ä¸åŒå›¾åƒé—´å¯»æ‰¾ç›¸ä¼¼çš„ç‰¹å¾  
ğŸ“‹ **æ¨¡æ¿ä¾¦æ¢éƒ¨** - ä¸“é—¨è¿›è¡Œæ¨¡æ¿åŒ¹é…å’Œç›®æ ‡å®šä½  

æ¯ä¸ªéƒ¨é—¨éƒ½æœ‰è‡ªå·±çš„ä¸“ä¸šæŠ€æœ¯å’Œæ£€æµ‹è®¾å¤‡ï¼Œè®©æˆ‘ä»¬ä¸€ä¸€æ¢ç´¢è¿™äº›"ä¾¦æ¢"çš„å·¥ä½œæ–¹å¼ï¼

#### ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šè¾¹ç¼˜æ£€æµ‹æŠ€æœ¯

è¾¹ç¼˜æ˜¯å›¾åƒä¸­æœ€é‡è¦çš„ç‰¹å¾ä¹‹ä¸€ï¼Œå®ƒä»¬å®šä¹‰äº†ç‰©ä½“çš„è½®å»“å’Œå½¢çŠ¶ã€‚

```python
# ç¤ºä¾‹9ï¼šè¾¹ç¼˜ä¾¦æ¢éƒ¨ - è¾¹ç¼˜æ£€æµ‹ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class EdgeDetectionDepartment:
    """
    è¾¹ç¼˜ä¾¦æ¢éƒ¨ç±»
    è´Ÿè´£å„ç§è¾¹ç¼˜æ£€æµ‹ç®—æ³•çš„å®ç°å’Œåº”ç”¨
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–è¾¹ç¼˜ä¾¦æ¢éƒ¨
        """
        # æ”¯æŒçš„è¾¹ç¼˜æ£€æµ‹ç®—æ³•
        self.edge_algorithms = {
            'canny': 'Cannyè¾¹ç¼˜æ£€æµ‹ - å¤šé˜¶æ®µæœ€ä¼˜è¾¹ç¼˜æ£€æµ‹',
            'sobel': 'Sobelè¾¹ç¼˜æ£€æµ‹ - æ¢¯åº¦æ–¹å‘æ•æ„Ÿ',
            'laplacian': 'Laplacianè¾¹ç¼˜æ£€æµ‹ - äºŒé˜¶å¯¼æ•°',
            'scharr': 'Scharrè¾¹ç¼˜æ£€æµ‹ - æ”¹è¿›çš„Sobel',
            'roberts': 'Robertsè¾¹ç¼˜æ£€æµ‹ - å¯¹è§’çº¿æ¢¯åº¦'
        }
        
        print("ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ•µï¸ è¾¹ç¼˜æ£€æµ‹è®¾å¤‡å·²å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„æ£€æµ‹ç®—æ³•: {list(self.edge_algorithms.keys())}")
    
    def canny_edge_detection(self, image, low_threshold=50, high_threshold=150, 
                           kernel_size=3, l2_gradient=False):
        """
        Cannyè¾¹ç¼˜æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        low_threshold (int): ä½é˜ˆå€¼
        high_threshold (int): é«˜é˜ˆå€¼
        kernel_size (int): Sobelæ ¸å¤§å°
        l2_gradient (bool): æ˜¯å¦ä½¿ç”¨L2æ¢¯åº¦
        
        Returns:
        numpy.ndarray: è¾¹ç¼˜å›¾åƒ
        """
        print(f"ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒCannyæ£€æµ‹ (é˜ˆå€¼: {low_threshold}-{high_threshold})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # åº”ç”¨é«˜æ–¯æ¨¡ç³Šä»¥å‡å°‘å™ªå£°
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Cannyè¾¹ç¼˜æ£€æµ‹
        edges = cv2.Canny(blurred, low_threshold, high_threshold, 
                         apertureSize=kernel_size, L2gradient=l2_gradient)
        
        print(f"âœ… Cannyæ£€æµ‹å®Œæˆï¼Œå‘ç°è¾¹ç¼˜ç‚¹æ•°: {np.sum(edges > 0)}")
        return edges
    
    def sobel_edge_detection(self, image, ksize=3, scale=1, delta=0):
        """
        Sobelè¾¹ç¼˜æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        ksize (int): Sobelæ ¸å¤§å°
        scale (float): ç¼©æ”¾å› å­
        delta (float): åç§»å€¼
        
        Returns:
        tuple: (æ¢¯åº¦x, æ¢¯åº¦y, æ¢¯åº¦å¹…å€¼)
        """
        print(f"ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒSobelæ£€æµ‹ (æ ¸å¤§å°: {ksize})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # è®¡ç®—xå’Œyæ–¹å‘çš„æ¢¯åº¦
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize, scale=scale, delta=delta)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize, scale=scale, delta=delta)
        
        # è®¡ç®—æ¢¯åº¦å¹…å€¼
        magnitude = np.sqrt(grad_x**2 + grad_y**2)
        magnitude = np.uint8(np.clip(magnitude, 0, 255))
        
        # è½¬æ¢ä¸ºuint8æ ¼å¼
        grad_x = np.uint8(np.clip(np.abs(grad_x), 0, 255))
        grad_y = np.uint8(np.clip(np.abs(grad_y), 0, 255))
        
        print(f"âœ… Sobelæ£€æµ‹å®Œæˆ")
        return grad_x, grad_y, magnitude
    
    def laplacian_edge_detection(self, image, ksize=1, scale=1, delta=0):
        """
        Laplacianè¾¹ç¼˜æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        ksize (int): æ ¸å¤§å°
        scale (float): ç¼©æ”¾å› å­
        delta (float): åç§»å€¼
        
        Returns:
        numpy.ndarray: è¾¹ç¼˜å›¾åƒ
        """
        print(f"ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒLaplacianæ£€æµ‹")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
        blurred = cv2.GaussianBlur(gray, (3, 3), 0)
        
        # Laplacianè¾¹ç¼˜æ£€æµ‹
        laplacian = cv2.Laplacian(blurred, cv2.CV_64F, ksize=ksize, scale=scale, delta=delta)
        laplacian = np.uint8(np.clip(np.abs(laplacian), 0, 255))
        
        print(f"âœ… Laplacianæ£€æµ‹å®Œæˆ")
        return laplacian
    
    def comprehensive_edge_comparison(self, image, figsize=(20, 15)):
        """
        ç»¼åˆå¯¹æ¯”ä¸åŒè¾¹ç¼˜æ£€æµ‹ç®—æ³•
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šæ­£åœ¨è¿›è¡Œè¾¹ç¼˜æ£€æµ‹ç®—æ³•å¯¹æ¯”")
        
        # å‡†å¤‡ä¸åŒç®—æ³•çš„ç»“æœ
        edge_results = {}
        
        # åŸå§‹å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        edge_results['åŸå§‹å›¾åƒ'] = image
        edge_results['ç°åº¦å›¾åƒ'] = gray
        
        # Cannyè¾¹ç¼˜æ£€æµ‹
        canny_edges = self.canny_edge_detection(image, 50, 150)
        edge_results['Cannyè¾¹ç¼˜æ£€æµ‹'] = canny_edges
        
        # Sobelè¾¹ç¼˜æ£€æµ‹
        sobel_x, sobel_y, sobel_magnitude = self.sobel_edge_detection(image)
        edge_results['Sobel-Xæ¢¯åº¦'] = sobel_x
        edge_results['Sobel-Yæ¢¯åº¦'] = sobel_y
        edge_results['Sobelæ¢¯åº¦å¹…å€¼'] = sobel_magnitude
        
        # Laplacianè¾¹ç¼˜æ£€æµ‹
        laplacian_edges = self.laplacian_edge_detection(image)
        edge_results['Laplacianè¾¹ç¼˜æ£€æµ‹'] = laplacian_edges
        
        # Scharrè¾¹ç¼˜æ£€æµ‹
        scharr_x = cv2.Scharr(gray, cv2.CV_64F, 1, 0)
        scharr_y = cv2.Scharr(gray, cv2.CV_64F, 0, 1)
        scharr_magnitude = np.sqrt(scharr_x**2 + scharr_y**2)
        scharr_magnitude = np.uint8(np.clip(scharr_magnitude, 0, 255))
        edge_results['Scharrè¾¹ç¼˜æ£€æµ‹'] = scharr_magnitude
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(3, 3, figsize=figsize)
        axes = axes.flatten()
        
        for i, (method_name, result_img) in enumerate(edge_results.items()):
            if i < len(axes):
                ax = axes[i]
                
                if method_name == 'åŸå§‹å›¾åƒ':
                    if len(result_img.shape) == 3:
                        ax.imshow(result_img)
                    else:
                        ax.imshow(result_img, cmap='gray')
                else:
                    ax.imshow(result_img, cmap='gray')
                
                ax.set_title(method_name, fontsize=12, fontweight='bold')
                ax.axis('off')
                
                # æ·»åŠ ç®—æ³•è¯´æ˜
                descriptions = {
                    'Cannyè¾¹ç¼˜æ£€æµ‹': 'å¤šé˜¶æ®µæ£€æµ‹\næŠ—å™ªå£°èƒ½åŠ›å¼º',
                    'Sobel-Xæ¢¯åº¦': 'æ°´å¹³æ–¹å‘\næ¢¯åº¦æ£€æµ‹',
                    'Sobel-Yæ¢¯åº¦': 'å‚ç›´æ–¹å‘\næ¢¯åº¦æ£€æµ‹',
                    'Sobelæ¢¯åº¦å¹…å€¼': 'Xå’ŒYæ¢¯åº¦\nçš„åˆæˆç»“æœ',
                    'Laplacianè¾¹ç¼˜æ£€æµ‹': 'äºŒé˜¶å¯¼æ•°\nå¯¹å™ªå£°æ•æ„Ÿ',
                    'Scharrè¾¹ç¼˜æ£€æµ‹': 'æ”¹è¿›çš„Sobel\næ›´ç²¾ç¡®çš„æ¢¯åº¦'
                }
                
                if method_name in descriptions:
                    desc = descriptions[method_name]
                    ax.text(0.02, 0.98, desc, transform=ax.transAxes,
                           bbox=dict(boxstyle="round,pad=0.2", facecolor="lightblue", alpha=0.7),
                           fontsize=8, verticalalignment='top')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(edge_results), len(axes)):
            axes[i].axis('off')
        
        fig.suptitle('ğŸ” è¾¹ç¼˜ä¾¦æ¢éƒ¨ - è¾¹ç¼˜æ£€æµ‹ç®—æ³•å…¨é¢å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… è¾¹ç¼˜æ£€æµ‹å¯¹æ¯”å®Œæˆ")
        return edge_results
    
    def canny_parameter_tuning(self, image, figsize=(18, 12)):
        """
        Cannyè¾¹ç¼˜æ£€æµ‹å‚æ•°è°ƒä¼˜æ¼”ç¤º
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸšï¸ è¾¹ç¼˜ä¾¦æ¢éƒ¨ï¼šCannyå‚æ•°è°ƒä¼˜æ¼”ç¤º")
        
        # ä¸åŒå‚æ•°ç»„åˆ
        threshold_combinations = [
            (30, 80),    # ä½é˜ˆå€¼ç»„åˆ
            (50, 100),   # ä¸­ç­‰é˜ˆå€¼ç»„åˆ
            (50, 150),   # æ ‡å‡†é˜ˆå€¼ç»„åˆ
            (100, 200),  # é«˜é˜ˆå€¼ç»„åˆ
            (150, 250),  # å¾ˆé«˜é˜ˆå€¼ç»„åˆ
        ]
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        # æ˜¾ç¤ºåŸå›¾
        if len(image.shape) == 3:
            axes[0].imshow(image)
        else:
            axes[0].imshow(image, cmap='gray')
        axes[0].set_title('åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')
        axes[0].axis('off')
        
        # ä¸åŒå‚æ•°çš„Cannyæ£€æµ‹ç»“æœ
        for i, (low_thresh, high_thresh) in enumerate(threshold_combinations):
            if i + 1 < len(axes):
                ax = axes[i + 1]
                
                # åº”ç”¨Cannyæ£€æµ‹
                edges = self.canny_edge_detection(image, low_thresh, high_thresh)
                
                ax.imshow(edges, cmap='gray')
                ax.set_title(f'é˜ˆå€¼: {low_thresh}-{high_thresh}', fontsize=12, fontweight='bold')
                ax.axis('off')
                
                # ç»Ÿè®¡è¾¹ç¼˜ç‚¹æ•°
                edge_count = np.sum(edges > 0)
                ax.text(0.02, 0.98, f'è¾¹ç¼˜ç‚¹æ•°: {edge_count}', transform=ax.transAxes,
                       bbox=dict(boxstyle="round,pad=0.2", facecolor="yellow", alpha=0.7),
                       fontsize=8, verticalalignment='top')
        
        fig.suptitle('ğŸšï¸ Cannyè¾¹ç¼˜æ£€æµ‹å‚æ•°è°ƒä¼˜å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("ğŸ“ Cannyå‚æ•°è°ƒä¼˜å»ºè®®:")
        print("   â€¢ ä½é˜ˆå€¼: æ£€æµ‹å¼±è¾¹ç¼˜ï¼Œå€¼è¿‡ä½ä¼šå¼•å…¥å™ªå£°")
        print("   â€¢ é«˜é˜ˆå€¼: æ£€æµ‹å¼ºè¾¹ç¼˜ï¼Œå€¼è¿‡é«˜ä¼šä¸¢å¤±ç»†èŠ‚")
        print("   â€¢ é«˜é˜ˆå€¼é€šå¸¸æ˜¯ä½é˜ˆå€¼çš„2-3å€")
        print("âœ… å‚æ•°è°ƒä¼˜æ¼”ç¤ºå®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè¾¹ç¼˜ä¾¦æ¢éƒ¨
    edge_dept = EdgeDetectionDepartment()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(640, 480, 'checkerboard')
    
    # ç»¼åˆè¾¹ç¼˜æ£€æµ‹å¯¹æ¯”
    edge_results = edge_dept.comprehensive_edge_comparison(test_image)
    
    # Cannyå‚æ•°è°ƒä¼˜æ¼”ç¤º
    edge_dept.canny_parameter_tuning(test_image)
```

#### ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šè§’ç‚¹æ£€æµ‹æŠ€æœ¯

è§’ç‚¹æ˜¯å›¾åƒä¸­ä¿¡æ¯æœ€ä¸°å¯Œçš„ç‰¹å¾ç‚¹ï¼Œå®ƒä»¬åœ¨å›¾åƒè¯†åˆ«å’ŒåŒ¹é…ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚

```python
# ç¤ºä¾‹10ï¼šè§’ç‚¹ä¾¦æ¢éƒ¨ - è§’ç‚¹æ£€æµ‹ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class CornerDetectionDepartment:
    """
    è§’ç‚¹ä¾¦æ¢éƒ¨ç±»
    è´Ÿè´£å„ç§è§’ç‚¹æ£€æµ‹ç®—æ³•çš„å®ç°å’Œåº”ç”¨
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–è§’ç‚¹ä¾¦æ¢éƒ¨
        """
        # æ”¯æŒçš„è§’ç‚¹æ£€æµ‹ç®—æ³•
        self.corner_algorithms = {
            'harris': 'Harrisè§’ç‚¹æ£€æµ‹ - ç»å…¸è§’ç‚¹æ£€æµ‹ç®—æ³•',
            'shi_tomasi': 'Shi-Tomasiè§’ç‚¹æ£€æµ‹ - æ”¹è¿›çš„Harris',
            'fast': 'FASTè§’ç‚¹æ£€æµ‹ - å¿«é€Ÿè§’ç‚¹æ£€æµ‹',
            'orb': 'ORBç‰¹å¾ç‚¹æ£€æµ‹ - æ—‹è½¬ä¸å˜ç‰¹å¾',
            'sift': 'SIFTç‰¹å¾ç‚¹æ£€æµ‹ - å°ºåº¦ä¸å˜ç‰¹å¾'
        }
        
        print("ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ¯ è§’ç‚¹æ£€æµ‹è®¾å¤‡å·²å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„æ£€æµ‹ç®—æ³•: {list(self.corner_algorithms.keys())}")
    
    def harris_corner_detection(self, image, k=0.04, threshold=0.01):
        """
        Harrisè§’ç‚¹æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        k (float): Harrisæ£€æµ‹å™¨è‡ªç”±å‚æ•°
        threshold (float): è§’ç‚¹é˜ˆå€¼
        
        Returns:
        tuple: (è§’ç‚¹å“åº”å›¾, è§’ç‚¹åæ ‡åˆ—è¡¨)
        """
        print(f"ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒHarrisè§’ç‚¹æ£€æµ‹ (k={k}, threshold={threshold})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # Harrisè§’ç‚¹æ£€æµ‹
        gray = np.float32(gray)
        harris_response = cv2.cornerHarris(gray, 2, 3, k)
        
        # è§’ç‚¹é˜ˆå€¼åŒ–
        corner_mask = harris_response > threshold * harris_response.max()
        corners = np.argwhere(corner_mask)
        
        print(f"âœ… Harrisæ£€æµ‹å®Œæˆï¼Œå‘ç°è§’ç‚¹æ•°: {len(corners)}")
        return harris_response, corners
    
    def shi_tomasi_corner_detection(self, image, max_corners=100, quality_level=0.01, min_distance=10):
        """
        Shi-Tomasiè§’ç‚¹æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        max_corners (int): æœ€å¤§è§’ç‚¹æ•°
        quality_level (float): è´¨é‡æ°´å¹³
        min_distance (float): æœ€å°è·ç¦»
        
        Returns:
        numpy.ndarray: è§’ç‚¹åæ ‡æ•°ç»„
        """
        print(f"ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒShi-Tomasiæ£€æµ‹ (æœ€å¤§è§’ç‚¹æ•°: {max_corners})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # Shi-Tomasiè§’ç‚¹æ£€æµ‹
        corners = cv2.goodFeaturesToTrack(gray, max_corners, quality_level, min_distance)
        
        if corners is not None:
            corners = np.int32(corners).reshape(-1, 2)
            print(f"âœ… Shi-Tomasiæ£€æµ‹å®Œæˆï¼Œå‘ç°è§’ç‚¹æ•°: {len(corners)}")
        else:
            corners = np.array([])
            print("âš ï¸ æœªæ£€æµ‹åˆ°è§’ç‚¹")
        
        return corners
    
    def fast_corner_detection(self, image, threshold=10, nonmax_suppression=True):
        """
        FASTè§’ç‚¹æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        threshold (int): FASTé˜ˆå€¼
        nonmax_suppression (bool): æ˜¯å¦è¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶
        
        Returns:
        list: è§’ç‚¹å…³é”®ç‚¹åˆ—è¡¨
        """
        print(f"ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒFASTæ£€æµ‹ (é˜ˆå€¼: {threshold})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # åˆ›å»ºFASTæ£€æµ‹å™¨
        fast = cv2.FastFeatureDetector_create(threshold=threshold, 
                                             nonmaxSuppression=nonmax_suppression)
        
        # æ£€æµ‹å…³é”®ç‚¹
        keypoints = fast.detect(gray, None)
        
        print(f"âœ… FASTæ£€æµ‹å®Œæˆï¼Œå‘ç°å…³é”®ç‚¹æ•°: {len(keypoints)}")
        return keypoints
    
    def orb_feature_detection(self, image, n_features=500):
        """
        ORBç‰¹å¾ç‚¹æ£€æµ‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        n_features (int): æœ€å¤§ç‰¹å¾ç‚¹æ•°
        
        Returns:
        tuple: (å…³é”®ç‚¹åˆ—è¡¨, æè¿°ç¬¦)
        """
        print(f"ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒORBç‰¹å¾æ£€æµ‹ (æœ€å¤§ç‰¹å¾æ•°: {n_features})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # åˆ›å»ºORBæ£€æµ‹å™¨
        orb = cv2.ORB_create(nfeatures=n_features)
        
        # æ£€æµ‹å…³é”®ç‚¹å’Œè®¡ç®—æè¿°ç¬¦
        keypoints, descriptors = orb.detectAndCompute(gray, None)
        
        print(f"âœ… ORBæ£€æµ‹å®Œæˆï¼Œå‘ç°ç‰¹å¾ç‚¹æ•°: {len(keypoints)}")
        return keypoints, descriptors
    
    def corner_detection_comparison(self, image, figsize=(20, 15)):
        """
        ç»¼åˆå¯¹æ¯”ä¸åŒè§’ç‚¹æ£€æµ‹ç®—æ³•
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ï¼šæ­£åœ¨è¿›è¡Œè§’ç‚¹æ£€æµ‹ç®—æ³•å¯¹æ¯”")
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        # åŸå§‹å›¾åƒ
        axes[0].imshow(image if len(image.shape) == 3 else image, 
                      cmap='gray' if len(image.shape) == 2 else None)
        axes[0].set_title('åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')
        axes[0].axis('off')
        
        # Harrisè§’ç‚¹æ£€æµ‹
        harris_response, harris_corners = self.harris_corner_detection(image)
        axes[1].imshow(harris_response, cmap='hot')
        axes[1].set_title('Harrisè§’ç‚¹å“åº”', fontsize=12, fontweight='bold')
        axes[1].axis('off')
        
        # Shi-Tomasiè§’ç‚¹æ£€æµ‹
        shi_tomasi_corners = self.shi_tomasi_corner_detection(image, max_corners=50)
        image_with_shi_tomasi = image.copy()
        if len(image_with_shi_tomasi.shape) == 2:
            image_with_shi_tomasi = cv2.cvtColor(image_with_shi_tomasi, cv2.COLOR_GRAY2RGB)
        
        for corner in shi_tomasi_corners:
            cv2.circle(image_with_shi_tomasi, tuple(corner), 3, (255, 0, 0), -1)
        
        axes[2].imshow(image_with_shi_tomasi)
        axes[2].set_title(f'Shi-Tomasiè§’ç‚¹ ({len(shi_tomasi_corners)}ä¸ª)', fontsize=12, fontweight='bold')
        axes[2].axis('off')
        
        # FASTè§’ç‚¹æ£€æµ‹
        fast_keypoints = self.fast_corner_detection(image)
        image_with_fast = image.copy()
        if len(image_with_fast.shape) == 2:
            image_with_fast = cv2.cvtColor(image_with_fast, cv2.COLOR_GRAY2RGB)
        else:
            image_with_fast = cv2.cvtColor(image_with_fast, cv2.COLOR_RGB2BGR)
        
        image_with_fast = cv2.drawKeypoints(image_with_fast, fast_keypoints, None, 
                                          color=(0, 255, 0), flags=0)
        image_with_fast = cv2.cvtColor(image_with_fast, cv2.COLOR_BGR2RGB)
        
        axes[3].imshow(image_with_fast)
        axes[3].set_title(f'FASTå…³é”®ç‚¹ ({len(fast_keypoints)}ä¸ª)', fontsize=12, fontweight='bold')
        axes[3].axis('off')
        
        # ORBç‰¹å¾æ£€æµ‹
        orb_keypoints, orb_descriptors = self.orb_feature_detection(image, n_features=100)
        image_with_orb = image.copy()
        if len(image_with_orb.shape) == 2:
            image_with_orb = cv2.cvtColor(image_with_orb, cv2.COLOR_GRAY2RGB)
        else:
            image_with_orb = cv2.cvtColor(image_with_orb, cv2.COLOR_RGB2BGR)
        
        image_with_orb = cv2.drawKeypoints(image_with_orb, orb_keypoints, None, 
                                         color=(255, 255, 0), 
                                         flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
        image_with_orb = cv2.cvtColor(image_with_orb, cv2.COLOR_BGR2RGB)
        
        axes[4].imshow(image_with_orb)
        axes[4].set_title(f'ORBç‰¹å¾ç‚¹ ({len(orb_keypoints)}ä¸ª)', fontsize=12, fontweight='bold')
        axes[4].axis('off')
        
        # ç®—æ³•å¯¹æ¯”æ€»ç»“
        axes[5].axis('off')
        summary_text = f"""
ğŸ“Š è§’ç‚¹æ£€æµ‹ç®—æ³•å¯¹æ¯”æ€»ç»“

ğŸ¯ Harrisè§’ç‚¹: {len(harris_corners)} ä¸ª
â€¢ ç‰¹ç‚¹: æ—‹è½¬ä¸å˜ï¼Œè®¡ç®—é‡ä¸­ç­‰
â€¢ é€‚ç”¨: ç»å…¸è§’ç‚¹æ£€æµ‹

ğŸ¯ Shi-Tomasi: {len(shi_tomasi_corners)} ä¸ª  
â€¢ ç‰¹ç‚¹: æ”¹è¿›çš„Harrisï¼Œæ›´ç¨³å®š
â€¢ é€‚ç”¨: è·Ÿè¸ªç‰¹å¾ç‚¹

âš¡ FAST: {len(fast_keypoints)} ä¸ª
â€¢ ç‰¹ç‚¹: é€Ÿåº¦å¿«ï¼Œå®æ—¶åº”ç”¨
â€¢ é€‚ç”¨: ç§»åŠ¨è®¾å¤‡ï¼Œè§†é¢‘å¤„ç†

ğŸ”„ ORB: {len(orb_keypoints)} ä¸ª
â€¢ ç‰¹ç‚¹: æ—‹è½¬å’Œå°ºåº¦ä¸å˜
â€¢ é€‚ç”¨: å›¾åƒåŒ¹é…ï¼ŒSLAM
        """
        
        axes[5].text(0.1, 0.9, summary_text, transform=axes[5].transAxes,
                    fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        fig.suptitle('ğŸ“ è§’ç‚¹ä¾¦æ¢éƒ¨ - è§’ç‚¹æ£€æµ‹ç®—æ³•å…¨é¢å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… è§’ç‚¹æ£€æµ‹å¯¹æ¯”å®Œæˆ")
        
        return {
            'harris': (harris_response, harris_corners),
            'shi_tomasi': shi_tomasi_corners,
            'fast': fast_keypoints,
            'orb': (orb_keypoints, orb_descriptors)
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè§’ç‚¹ä¾¦æ¢éƒ¨
    corner_dept = CornerDetectionDepartment()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image = workshop.create_test_image(640, 480, 'checkerboard')
    
    # ç»¼åˆè§’ç‚¹æ£€æµ‹å¯¹æ¯”
    corner_results = corner_dept.corner_detection_comparison(test_image)
```

ç°åœ¨æˆ‘ä»¬çš„å›¾åƒä¾¦æ¢å±€å·²ç»æ‹¥æœ‰äº†ä¸“ä¸šçš„è¾¹ç¼˜ä¾¦æ¢éƒ¨å’Œè§’ç‚¹ä¾¦æ¢éƒ¨ã€‚æ¥ä¸‹æ¥æˆ‘å°†ç»§ç»­æ·»åŠ åŒ¹é…ä¾¦æ¢éƒ¨å’Œæ¨¡æ¿ä¾¦æ¢éƒ¨ï¼Œä»¥åŠæœ€åçš„ç»¼åˆé¡¹ç›®éƒ¨åˆ†...

#### ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ï¼šç‰¹å¾åŒ¹é…æŠ€æœ¯

åŒ¹é…ä¾¦æ¢éƒ¨è´Ÿè´£åœ¨ä¸åŒå›¾åƒé—´å¯»æ‰¾ç›¸ä¼¼çš„ç‰¹å¾ï¼Œè¿™æ˜¯å®ç°å›¾åƒæ‹¼æ¥ã€ç›®æ ‡è¯†åˆ«ç­‰åº”ç”¨çš„åŸºç¡€ã€‚

```python
# ç¤ºä¾‹11ï¼šåŒ¹é…ä¾¦æ¢éƒ¨ - ç‰¹å¾åŒ¹é…ç³»ç»Ÿ
import cv2
import numpy as np
import matplotlib.pyplot as plt

class FeatureMatchingDepartment:
    """
    åŒ¹é…ä¾¦æ¢éƒ¨ç±»
    è´Ÿè´£ç‰¹å¾åŒ¹é…å’Œå›¾åƒå¯¹åº”å…³ç³»å»ºç«‹
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–åŒ¹é…ä¾¦æ¢éƒ¨
        """
        # æ”¯æŒçš„åŒ¹é…ç®—æ³•
        self.matching_algorithms = {
            'brute_force': 'æš´åŠ›åŒ¹é… - ç©·å°½æœç´¢æœ€ä½³åŒ¹é…',
            'flann': 'FLANNåŒ¹é… - å¿«é€Ÿè¿‘ä¼¼åŒ¹é…',
            'ratio_test': 'æ¯”å€¼æµ‹è¯• - Loweæ¯”å€¼æµ‹è¯•',
            'cross_check': 'äº¤å‰éªŒè¯ - åŒå‘åŒ¹é…éªŒè¯'
        }
        
        print("ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ” ç‰¹å¾åŒ¹é…è®¾å¤‡å·²å°±ç»ª")
        print(f"ğŸ“‹ æ”¯æŒçš„åŒ¹é…ç®—æ³•: {list(self.matching_algorithms.keys())}")
    
    def orb_feature_matching(self, image1, image2, max_features=1000):
        """
        ä½¿ç”¨ORBç‰¹å¾è¿›è¡Œå›¾åƒåŒ¹é…
        
        Parameters:
        image1 (numpy.ndarray): ç¬¬ä¸€å¼ å›¾åƒ
        image2 (numpy.ndarray): ç¬¬äºŒå¼ å›¾åƒ
        max_features (int): æœ€å¤§ç‰¹å¾ç‚¹æ•°
        
        Returns:
        tuple: (å…³é”®ç‚¹1, å…³é”®ç‚¹2, åŒ¹é…ç»“æœ, åŒ¹é…å›¾åƒ)
        """
        print(f"ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒORBç‰¹å¾åŒ¹é… (æœ€å¤§ç‰¹å¾æ•°: {max_features})")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        gray1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY) if len(image1.shape) == 3 else image1
        gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY) if len(image2.shape) == 3 else image2
        
        # åˆ›å»ºORBæ£€æµ‹å™¨
        orb = cv2.ORB_create(nfeatures=max_features)
        
        # æ£€æµ‹å…³é”®ç‚¹å’Œæè¿°ç¬¦
        kp1, des1 = orb.detectAndCompute(gray1, None)
        kp2, des2 = orb.detectAndCompute(gray2, None)
        
        print(f"   å›¾åƒ1ç‰¹å¾ç‚¹æ•°: {len(kp1)}")
        print(f"   å›¾åƒ2ç‰¹å¾ç‚¹æ•°: {len(kp2)}")
        
        if des1 is not None and des2 is not None:
            # åˆ›å»ºæš´åŠ›åŒ¹é…å™¨
            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
            
            # è¿›è¡Œç‰¹å¾åŒ¹é…
            matches = bf.match(des1, des2)
            
            # æŒ‰è·ç¦»æ’åº
            matches = sorted(matches, key=lambda x: x.distance)
            
            # ç»˜åˆ¶åŒ¹é…ç»“æœ
            match_img = cv2.drawMatches(image1, kp1, image2, kp2, matches[:50], None, 
                                      flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
            
            print(f"âœ… ORBåŒ¹é…å®Œæˆï¼Œå‘ç°åŒ¹é…å¯¹æ•°: {len(matches)}")
            return kp1, kp2, matches, match_img
        else:
            print("âŒ ç‰¹å¾æ£€æµ‹å¤±è´¥")
            return None, None, [], None
    
    def sift_feature_matching_with_ratio_test(self, image1, image2):
        """
        ä½¿ç”¨SIFTç‰¹å¾å’Œæ¯”å€¼æµ‹è¯•è¿›è¡ŒåŒ¹é…
        
        Parameters:
        image1 (numpy.ndarray): ç¬¬ä¸€å¼ å›¾åƒ
        image2 (numpy.ndarray): ç¬¬äºŒå¼ å›¾åƒ
        
        Returns:
        tuple: (å¥½çš„åŒ¹é…ç‚¹, åŒ¹é…å›¾åƒ)
        """
        print("ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ï¼šæ‰§è¡ŒSIFTç‰¹å¾åŒ¹é… (æ¯”å€¼æµ‹è¯•)")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        gray1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY) if len(image1.shape) == 3 else image1
        gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY) if len(image2.shape) == 3 else image2
        
        try:
            # åˆ›å»ºSIFTæ£€æµ‹å™¨
            sift = cv2.SIFT_create()
            
            # æ£€æµ‹å…³é”®ç‚¹å’Œæè¿°ç¬¦
            kp1, des1 = sift.detectAndCompute(gray1, None)
            kp2, des2 = sift.detectAndCompute(gray2, None)
            
            print(f"   å›¾åƒ1 SIFTç‰¹å¾ç‚¹æ•°: {len(kp1)}")
            print(f"   å›¾åƒ2 SIFTç‰¹å¾ç‚¹æ•°: {len(kp2)}")
            
            if des1 is not None and des2 is not None:
                # åˆ›å»ºFLANNåŒ¹é…å™¨
                FLANN_INDEX_KDTREE = 1
                index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
                search_params = dict(checks=50)
                flann = cv2.FlannBasedMatcher(index_params, search_params)
                
                # Kè¿‘é‚»åŒ¹é…
                matches = flann.knnMatch(des1, des2, k=2)
                
                # Loweæ¯”å€¼æµ‹è¯•
                good_matches = []
                for match_pair in matches:
                    if len(match_pair) == 2:
                        m, n = match_pair
                        if m.distance < 0.7 * n.distance:
                            good_matches.append(m)
                
                # ç»˜åˆ¶å¥½çš„åŒ¹é…
                match_img = cv2.drawMatches(image1, kp1, image2, kp2, good_matches, None,
                                          flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
                
                print(f"âœ… SIFTåŒ¹é…å®Œæˆï¼Œå¥½çš„åŒ¹é…å¯¹æ•°: {len(good_matches)}")
                return good_matches, match_img
            else:
                print("âŒ SIFTç‰¹å¾æ£€æµ‹å¤±è´¥")
                return [], None
                
        except Exception as e:
            print(f"âŒ SIFTåŒ¹é…å‡ºé”™: {e}")
            print("ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦å®‰è£…opencv-contrib-python")
            return [], None
    
    def template_matching_demo(self, image, template, methods=None):
        """
        æ¨¡æ¿åŒ¹é…æ¼”ç¤º
        
        Parameters:
        image (numpy.ndarray): æºå›¾åƒ
        template (numpy.ndarray): æ¨¡æ¿å›¾åƒ
        methods (list): åŒ¹é…æ–¹æ³•åˆ—è¡¨
        
        Returns:
        dict: ä¸åŒæ–¹æ³•çš„åŒ¹é…ç»“æœ
        """
        print("ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ï¼šæ‰§è¡Œæ¨¡æ¿åŒ¹é…æ¼”ç¤º")
        
        if methods is None:
            methods = [
                ('TM_CCOEFF', cv2.TM_CCOEFF),
                ('TM_CCOEFF_NORMED', cv2.TM_CCOEFF_NORMED),
                ('TM_CCORR', cv2.TM_CCORR),
                ('TM_CCORR_NORMED', cv2.TM_CCORR_NORMED),
                ('TM_SQDIFF', cv2.TM_SQDIFF),
                ('TM_SQDIFF_NORMED', cv2.TM_SQDIFF_NORMED)
            ]
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
        gray_template = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY) if len(template.shape) == 3 else template
        
        results = {}
        
        for method_name, method in methods:
            # æ‰§è¡Œæ¨¡æ¿åŒ¹é…
            result = cv2.matchTemplate(gray_image, gray_template, method)
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…ä½ç½®
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©æœ€ä½³ä½ç½®
            if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
                best_loc = min_loc
                best_val = min_val
            else:
                best_loc = max_loc
                best_val = max_val
            
            # ç»˜åˆ¶åŒ¹é…ç»“æœ
            h, w = gray_template.shape
            result_image = image.copy()
            cv2.rectangle(result_image, best_loc, (best_loc[0] + w, best_loc[1] + h), (255, 0, 0), 2)
            
            results[method_name] = {
                'result_map': result,
                'best_location': best_loc,
                'best_value': best_val,
                'result_image': result_image
            }
            
            print(f"   {method_name}: æœ€ä½³ä½ç½® {best_loc}, åŒ¹é…å€¼ {best_val:.4f}")
        
        print("âœ… æ¨¡æ¿åŒ¹é…æ¼”ç¤ºå®Œæˆ")
        return results
    
    def comprehensive_matching_demo(self, image1, image2, figsize=(20, 15)):
        """
        ç»¼åˆåŒ¹é…æ¼”ç¤º
        
        Parameters:
        image1 (numpy.ndarray): ç¬¬ä¸€å¼ å›¾åƒ
        image2 (numpy.ndarray): ç¬¬äºŒå¼ å›¾åƒ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ï¼šæ­£åœ¨è¿›è¡Œç»¼åˆåŒ¹é…æ¼”ç¤º")
        
        # åˆ›å»ºå±•ç¤ºå›¾
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        axes[0, 0].imshow(image1)
        axes[0, 0].set_title('å›¾åƒ1', fontsize=12, fontweight='bold')
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(image2) 
        axes[0, 1].set_title('å›¾åƒ2', fontsize=12, fontweight='bold')
        axes[0, 1].axis('off')
        
        # ORBç‰¹å¾åŒ¹é…
        kp1, kp2, orb_matches, orb_match_img = self.orb_feature_matching(image1, image2)
        if orb_match_img is not None:
            axes[1, 0].imshow(orb_match_img)
            axes[1, 0].set_title(f'ORBç‰¹å¾åŒ¹é… ({len(orb_matches)}ä¸ªåŒ¹é…)', fontsize=12, fontweight='bold')
        else:
            axes[1, 0].text(0.5, 0.5, 'ORBåŒ¹é…å¤±è´¥', ha='center', va='center', 
                           transform=axes[1, 0].transAxes, fontsize=12)
            axes[1, 0].set_title('ORBç‰¹å¾åŒ¹é…', fontsize=12, fontweight='bold')
        axes[1, 0].axis('off')
        
        # SIFTç‰¹å¾åŒ¹é… (å¦‚æœå¯ç”¨)
        sift_matches, sift_match_img = self.sift_feature_matching_with_ratio_test(image1, image2)
        if sift_match_img is not None:
            axes[1, 1].imshow(sift_match_img)
            axes[1, 1].set_title(f'SIFTç‰¹å¾åŒ¹é… ({len(sift_matches)}ä¸ªå¥½åŒ¹é…)', fontsize=12, fontweight='bold')
        else:
            axes[1, 1].text(0.5, 0.5, 'SIFTåŒ¹é…ä¸å¯ç”¨\néœ€è¦opencv-contrib', 
                           ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=10)
            axes[1, 1].set_title('SIFTç‰¹å¾åŒ¹é…', fontsize=12, fontweight='bold')
        axes[1, 1].axis('off')
        
        fig.suptitle('ğŸ”— åŒ¹é…ä¾¦æ¢éƒ¨ - ç‰¹å¾åŒ¹é…æŠ€æœ¯å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… ç»¼åˆåŒ¹é…æ¼”ç¤ºå®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåŒ¹é…ä¾¦æ¢éƒ¨
    matching_dept = FeatureMatchingDepartment()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    workshop = CaptureWorkshop()
    test_image1 = workshop.create_test_image(320, 240, 'checkerboard')
    test_image2 = workshop.create_test_image(320, 240, 'gradient')
    
    # ç»¼åˆåŒ¹é…æ¼”ç¤º
    matching_dept.comprehensive_matching_demo(test_image1, test_image2)
```

## ğŸ“š ç¬¬å››èŠ‚ï¼šæ™ºèƒ½æ–‡æ¡£æ‰«æä»ªç»¼åˆé¡¹ç›®

### 35.4 ç»¼åˆå®æˆ˜é¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£æ‰«æä»ª

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰å­¦åˆ°çš„æŠ€æœ¯æ•´åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªçœŸæ­£æœ‰å®ç”¨ä»·å€¼çš„ä¼ä¸šçº§é¡¹ç›®â€”â€”æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªï¼

#### ğŸ“± é¡¹ç›®æ¦‚è¿°

æˆ‘ä»¬è¦å¼€å‘ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£æ‰«æä»ªï¼Œå®ƒèƒ½å¤Ÿï¼š

ğŸ” **è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£è¾¹ç¼˜** - ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æŠ€æœ¯æ‰¾åˆ°æ–‡æ¡£è½®å»“  
ğŸ“ **æ™ºèƒ½é€è§†çŸ«æ­£** - ä½¿ç”¨å‡ ä½•å˜æ¢å°†å€¾æ–œçš„æ–‡æ¡£æ‹‰ç›´  
âœ¨ **å›¾åƒè´¨é‡å¢å¼º** - ä½¿ç”¨å›¾åƒå¢å¼ºæŠ€æœ¯ä¼˜åŒ–æ–‡æ¡£æ¸…æ™°åº¦  
ğŸ’¾ **å¤šæ ¼å¼è¾“å‡º** - æ”¯æŒå¤šç§æ ¼å¼ä¿å­˜å¤„ç†åçš„æ–‡æ¡£  

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„äº§å“çº§é¡¹ç›®ï¼Œå±•ç¤ºäº†ä»åŸå‹åˆ°äº§å“çš„å®Œæ•´å¼€å‘æµç¨‹ã€‚

```python
# ç¤ºä¾‹12ï¼šæ™ºèƒ½æ–‡æ¡£æ‰«æä»ª - å®Œæ•´é¡¹ç›®å®ç°
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from datetime import datetime

class SmartDocumentScanner:
    """
    æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªç±»
    é›†æˆäº†è¾¹ç¼˜æ£€æµ‹ã€è§’ç‚¹æ£€æµ‹ã€é€è§†å˜æ¢å’Œå›¾åƒå¢å¼ºæŠ€æœ¯
    """
    
    def __init__(self, output_dir="smart_scanner_output"):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ–‡æ¡£æ‰«æä»ª
        
        Parameters:
        output_dir (str): è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å¤„ç†å†å²è®°å½•
        self.scan_history = []
        
        # é…ç½®å‚æ•°
        self.config = {
            'edge_detection': {
                'canny_low': 50,
                'canny_high': 150,
                'blur_kernel': 5
            },
            'contour_detection': {
                'min_area': 1000,
                'epsilon_factor': 0.02
            },
            'enhancement': {
                'clahe_clip_limit': 2.0,
                'gamma': 1.2
            }
        }
        
        print("ğŸ“± æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def detect_document_edges(self, image):
        """
        æ£€æµ‹æ–‡æ¡£è¾¹ç¼˜
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        
        Returns:
        tuple: (è¾¹ç¼˜å›¾åƒ, è½®å»“åˆ—è¡¨)
        """
        print("ğŸ” æ­£åœ¨æ£€æµ‹æ–‡æ¡£è¾¹ç¼˜...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
        
        # é«˜æ–¯æ¨¡ç³Š
        blurred = cv2.GaussianBlur(gray, (self.config['edge_detection']['blur_kernel'], 
                                         self.config['edge_detection']['blur_kernel']), 0)
        
        # Cannyè¾¹ç¼˜æ£€æµ‹
        edges = cv2.Canny(blurred, 
                         self.config['edge_detection']['canny_low'], 
                         self.config['edge_detection']['canny_high'])
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # æŒ‰é¢ç§¯æ’åºè½®å»“
        contours = sorted(contours, key=cv2.contourArea, reverse=True)
        
        print(f"   å‘ç° {len(contours)} ä¸ªè½®å»“")
        print("âœ… æ–‡æ¡£è¾¹ç¼˜æ£€æµ‹å®Œæˆ")
        
        return edges, contours
    
    def find_document_corners(self, contours, image_shape):
        """
        å¯»æ‰¾æ–‡æ¡£çš„å››ä¸ªè§’ç‚¹
        
        Parameters:
        contours (list): è½®å»“åˆ—è¡¨
        image_shape (tuple): å›¾åƒå°ºå¯¸
        
        Returns:
        numpy.ndarray: å››ä¸ªè§’ç‚¹åæ ‡ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        print("ğŸ“ æ­£åœ¨å¯»æ‰¾æ–‡æ¡£è§’ç‚¹...")
        
        min_area = self.config['contour_detection']['min_area']
        epsilon_factor = self.config['contour_detection']['epsilon_factor']
        
        for contour in contours:
            # æ£€æŸ¥è½®å»“é¢ç§¯
            area = cv2.contourArea(contour)
            if area < min_area:
                continue
            
            # è½®å»“è¿‘ä¼¼
            epsilon = epsilon_factor * cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, epsilon, True)
            
            # å¦‚æœè¿‘ä¼¼è½®å»“æœ‰4ä¸ªç‚¹ï¼Œå¯èƒ½æ˜¯æ–‡æ¡£
            if len(approx) == 4:
                print(f"   æ‰¾åˆ°å››è¾¹å½¢è½®å»“ï¼Œé¢ç§¯: {area:.0f}")
                print("âœ… æ–‡æ¡£è§’ç‚¹æ£€æµ‹å®Œæˆ")
                return approx.reshape(4, 2)
        
        print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„å››è¾¹å½¢è½®å»“ï¼Œä½¿ç”¨å›¾åƒè¾¹ç•Œ")
        h, w = image_shape[:2]
        return np.array([[0, 0], [w-1, 0], [w-1, h-1], [0, h-1]], dtype=np.float32)
    
    def order_points(self, pts):
        """
        å¯¹å››ä¸ªç‚¹è¿›è¡Œæ’åºï¼šå·¦ä¸Šã€å³ä¸Šã€å³ä¸‹ã€å·¦ä¸‹
        
        Parameters:
        pts (numpy.ndarray): å››ä¸ªç‚¹çš„åæ ‡
        
        Returns:
        numpy.ndarray: æ’åºåçš„ç‚¹åæ ‡
        """
        # è®¡ç®—æ¯ä¸ªç‚¹çš„åæ ‡å’Œ
        sum_coords = pts.sum(axis=1)
        diff_coords = np.diff(pts, axis=1)
        
        # å·¦ä¸Šè§’ç‚¹çš„åæ ‡å’Œæœ€å°ï¼Œå³ä¸‹è§’ç‚¹çš„åæ ‡å’Œæœ€å¤§
        rect = np.zeros((4, 2), dtype=np.float32)
        rect[0] = pts[np.argmin(sum_coords)]  # å·¦ä¸Š
        rect[2] = pts[np.argmax(sum_coords)]  # å³ä¸‹
        
        # å³ä¸Šè§’ç‚¹çš„å·®å€¼æœ€å°ï¼Œå·¦ä¸‹è§’ç‚¹çš„å·®å€¼æœ€å¤§
        rect[1] = pts[np.argmin(diff_coords)]  # å³ä¸Š
        rect[3] = pts[np.argmax(diff_coords)]  # å·¦ä¸‹
        
        return rect
    
    def perspective_transform(self, image, corners):
        """
        æ‰§è¡Œé€è§†å˜æ¢
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        corners (numpy.ndarray): å››ä¸ªè§’ç‚¹
        
        Returns:
        numpy.ndarray: å˜æ¢åçš„å›¾åƒ
        """
        print("ğŸ“ æ­£åœ¨æ‰§è¡Œé€è§†å˜æ¢...")
        
        # æ’åºè§’ç‚¹
        ordered_corners = self.order_points(corners)
        
        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        (tl, tr, br, bl) = ordered_corners
        
        # è®¡ç®—æ–°å›¾åƒçš„å®½åº¦å’Œé«˜åº¦
        width_top = np.sqrt((tr[0] - tl[0]) ** 2 + (tr[1] - tl[1]) ** 2)
        width_bottom = np.sqrt((br[0] - bl[0]) ** 2 + (br[1] - bl[1]) ** 2)
        max_width = max(int(width_top), int(width_bottom))
        
        height_left = np.sqrt((tl[0] - bl[0]) ** 2 + (tl[1] - bl[1]) ** 2)
        height_right = np.sqrt((tr[0] - br[0]) ** 2 + (tr[1] - br[1]) ** 2)
        max_height = max(int(height_left), int(height_right))
        
        # å®šä¹‰ç›®æ ‡ç‚¹
        dst_points = np.array([
            [0, 0],
            [max_width - 1, 0],
            [max_width - 1, max_height - 1],
            [0, max_height - 1]
        ], dtype=np.float32)
        
        # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
        transform_matrix = cv2.getPerspectiveTransform(ordered_corners, dst_points)
        
        # æ‰§è¡Œé€è§†å˜æ¢
        warped = cv2.warpPerspective(image, transform_matrix, (max_width, max_height))
        
        print(f"   å˜æ¢åå°ºå¯¸: {max_width} x {max_height}")
        print("âœ… é€è§†å˜æ¢å®Œæˆ")
        
        return warped
    
    def enhance_document_image(self, image):
        """
        å¢å¼ºæ–‡æ¡£å›¾åƒè´¨é‡
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        
        Returns:
        numpy.ndarray: å¢å¼ºåçš„å›¾åƒ
        """
        print("âœ¨ æ­£åœ¨å¢å¼ºæ–‡æ¡£å›¾åƒ...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()
        
        # CLAHEè‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
        clahe = cv2.createCLAHE(clipLimit=self.config['enhancement']['clahe_clip_limit'])
        enhanced = clahe.apply(gray)
        
        # ä¼½é©¬æ ¡æ­£
        gamma = self.config['enhancement']['gamma']
        inv_gamma = 1.0 / gamma
        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
        enhanced = cv2.LUT(enhanced, table)
        
        # åŒè¾¹æ»¤æ³¢å»å™ª
        denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        print("âœ… å›¾åƒå¢å¼ºå®Œæˆ")
        return denoised
    
    def scan_document(self, image, save_intermediate=True):
        """
        å®Œæ•´çš„æ–‡æ¡£æ‰«ææµç¨‹
        
        Parameters:
        image (numpy.ndarray): è¾“å…¥å›¾åƒ
        save_intermediate (bool): æ˜¯å¦ä¿å­˜ä¸­é—´æ­¥éª¤å›¾åƒ
        
        Returns:
        dict: åŒ…å«æ‰€æœ‰å¤„ç†æ­¥éª¤ç»“æœçš„å­—å…¸
        """
        print("\nğŸ“± å¼€å§‹æ™ºèƒ½æ–‡æ¡£æ‰«ææµç¨‹...")
        print("=" * 50)
        
        results = {
            'original': image,
            'timestamp': datetime.now().isoformat(),
            'config': self.config.copy()
        }
        
        # æ­¥éª¤1: æ£€æµ‹æ–‡æ¡£è¾¹ç¼˜
        edges, contours = self.detect_document_edges(image)
        results['edges'] = edges
        results['contours'] = contours
        
        # æ­¥éª¤2: å¯»æ‰¾æ–‡æ¡£è§’ç‚¹
        corners = self.find_document_corners(contours, image.shape)
        results['corners'] = corners
        
        # æ­¥éª¤3: é€è§†å˜æ¢
        if corners is not None:
            warped = self.perspective_transform(image, corners)
            results['warped'] = warped
            
            # æ­¥éª¤4: å›¾åƒå¢å¼º
            enhanced = self.enhance_document_image(warped)
            results['enhanced'] = enhanced
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            if save_intermediate:
                scan_id = len(self.scan_history)
                self.save_scan_results(results, scan_id)
            
            self.scan_history.append({
                'scan_id': len(self.scan_history),
                'timestamp': results['timestamp'],
                'success': True,
                'corners_found': len(corners) == 4,
                'final_size': enhanced.shape
            })
            
            print("=" * 50)
            print("ğŸ‰ æ–‡æ¡£æ‰«æå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆæ–‡æ¡£å°ºå¯¸: {enhanced.shape[1]} x {enhanced.shape[0]}")
            
        else:
            print("âŒ æ–‡æ¡£æ‰«æå¤±è´¥ï¼šæœªèƒ½æ£€æµ‹åˆ°æœ‰æ•ˆè§’ç‚¹")
            results['enhanced'] = None
            
            self.scan_history.append({
                'scan_id': len(self.scan_history),
                'timestamp': results['timestamp'],
                'success': False,
                'corners_found': False,
                'final_size': None
            })
        
        return results
    
    def save_scan_results(self, results, scan_id):
        """
        ä¿å­˜æ‰«æç»“æœ
        
        Parameters:
        results (dict): æ‰«æç»“æœ
        scan_id (int): æ‰«æID
        """
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ‰«æç»“æœ (ID: {scan_id})...")
        
        scan_dir = self.output_dir / f"scan_{scan_id:03d}"
        scan_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜å„ä¸ªæ­¥éª¤çš„å›¾åƒ
        cv2.imwrite(str(scan_dir / "01_original.jpg"), 
                   cv2.cvtColor(results['original'], cv2.COLOR_RGB2BGR))
        cv2.imwrite(str(scan_dir / "02_edges.jpg"), results['edges'])
        
        if results.get('warped') is not None:
            cv2.imwrite(str(scan_dir / "03_warped.jpg"), 
                       cv2.cvtColor(results['warped'], cv2.COLOR_RGB2BGR))
        
        if results.get('enhanced') is not None:
            cv2.imwrite(str(scan_dir / "04_final.jpg"), results['enhanced'])
        
        # ä¿å­˜æ‰«æé…ç½®å’Œç»“æœä¿¡æ¯
        metadata = {
            'scan_id': scan_id,
            'timestamp': results['timestamp'],
            'config': results['config'],
            'corners': results['corners'].tolist() if results.get('corners') is not None else None,
            'processing_steps': ['edge_detection', 'corner_detection', 'perspective_transform', 'enhancement']
        }
        
        with open(scan_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ‰«æç»“æœå·²ä¿å­˜åˆ°: {scan_dir}")
    
    def visualize_scan_process(self, results, figsize=(20, 15)):
        """
        å¯è§†åŒ–æ‰«æè¿‡ç¨‹
        
        Parameters:
        results (dict): æ‰«æç»“æœ
        figsize (tuple): æ˜¾ç¤ºå°ºå¯¸
        """
        print("ğŸ–¼ï¸ æ­£åœ¨å¯è§†åŒ–æ‰«æè¿‡ç¨‹...")
        
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        axes = axes.flatten()
        
        # åŸå§‹å›¾åƒ
        axes[0].imshow(results['original'])
        axes[0].set_title('1. åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')
        axes[0].axis('off')
        
        # è¾¹ç¼˜æ£€æµ‹
        axes[1].imshow(results['edges'], cmap='gray')
        axes[1].set_title('2. è¾¹ç¼˜æ£€æµ‹', fontsize=12, fontweight='bold')
        axes[1].axis('off')
        
        # è§’ç‚¹æ£€æµ‹
        corners_img = results['original'].copy()
        if results.get('corners') is not None:
            corners = results['corners'].astype(int)
            for i, corner in enumerate(corners):
                cv2.circle(corners_img, tuple(corner), 10, (255, 0, 0), -1)
                cv2.putText(corners_img, str(i+1), (corner[0]+15, corner[1]), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        
        axes[2].imshow(corners_img)
        axes[2].set_title('3. è§’ç‚¹æ£€æµ‹', fontsize=12, fontweight='bold')
        axes[2].axis('off')
        
        # é€è§†å˜æ¢
        if results.get('warped') is not None:
            axes[3].imshow(results['warped'])
            axes[3].set_title('4. é€è§†å˜æ¢', fontsize=12, fontweight='bold')
        else:
            axes[3].text(0.5, 0.5, 'é€è§†å˜æ¢å¤±è´¥', ha='center', va='center',
                        transform=axes[3].transAxes, fontsize=12)
            axes[3].set_title('4. é€è§†å˜æ¢', fontsize=12, fontweight='bold')
        axes[3].axis('off')
        
        # å›¾åƒå¢å¼º
        if results.get('enhanced') is not None:
            axes[4].imshow(results['enhanced'], cmap='gray')
            axes[4].set_title('5. å›¾åƒå¢å¼º', fontsize=12, fontweight='bold')
        else:
            axes[4].text(0.5, 0.5, 'å›¾åƒå¢å¼ºå¤±è´¥', ha='center', va='center',
                        transform=axes[4].transAxes, fontsize=12)
            axes[4].set_title('5. å›¾åƒå¢å¼º', fontsize=12, fontweight='bold')
        axes[4].axis('off')
        
        # æ‰«æç»Ÿè®¡
        axes[5].axis('off')
        if results.get('enhanced') is not None:
            stats_text = f"""
ğŸ“Š æ‰«æç»Ÿè®¡ä¿¡æ¯

ğŸ“· åŸå§‹å°ºå¯¸: {results['original'].shape[1]} x {results['original'].shape[0]}
ğŸ“„ æœ€ç»ˆå°ºå¯¸: {results['enhanced'].shape[1]} x {results['enhanced'].shape[0]}
ğŸ” è¾¹ç¼˜ç‚¹æ•°: {np.sum(results['edges'] > 0):,}
ğŸ“ æ£€æµ‹è§’ç‚¹: {'âœ… æˆåŠŸ' if results.get('corners') is not None else 'âŒ å¤±è´¥'}
âš™ï¸ å¤„ç†çŠ¶æ€: {'âœ… å®Œæˆ' if results.get('enhanced') is not None else 'âŒ å¤±è´¥'}

ğŸ› ï¸ é…ç½®å‚æ•°:
â€¢ Cannyé˜ˆå€¼: {results['config']['edge_detection']['canny_low']}-{results['config']['edge_detection']['canny_high']}
â€¢ CLAHEé™åˆ¶: {results['config']['enhancement']['clahe_clip_limit']}
â€¢ ä¼½é©¬å€¼: {results['config']['enhancement']['gamma']}
            """
        else:
            stats_text = """
ğŸ“Š æ‰«æç»Ÿè®¡ä¿¡æ¯

âŒ æ‰«æå¤±è´¥
ğŸ” å¯èƒ½åŸå› :
â€¢ æ–‡æ¡£è¾¹ç¼˜ä¸æ¸…æ™°
â€¢ èƒŒæ™¯å¹²æ‰°è¿‡å¤š
â€¢ è§’ç‚¹æ£€æµ‹å¤±è´¥

ğŸ’¡ å»ºè®®:
â€¢ è°ƒæ•´æ‹æ‘„è§’åº¦
â€¢ æé«˜å…‰ç…§æ¡ä»¶
â€¢ è°ƒæ•´æ£€æµ‹å‚æ•°
            """
        
        axes[5].text(0.1, 0.9, stats_text, transform=axes[5].transAxes,
                    fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        fig.suptitle('ğŸ“± æ™ºèƒ½æ–‡æ¡£æ‰«æä»ª - å®Œæ•´å¤„ç†æµç¨‹', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        print("âœ… å¯è§†åŒ–å®Œæˆ")
    
    def get_scanner_status(self):
        """
        è·å–æ‰«æä»ªçŠ¶æ€æŠ¥å‘Š
        """
        print("\nğŸ“± æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªçŠ¶æ€æŠ¥å‘Š")
        print("=" * 50)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š æ€»æ‰«ææ¬¡æ•°: {len(self.scan_history)}")
        
        if self.scan_history:
            successful_scans = sum(1 for scan in self.scan_history if scan['success'])
            success_rate = successful_scans / len(self.scan_history) * 100
            
            print(f"âœ… æˆåŠŸæ‰«æ: {successful_scans} æ¬¡")
            print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
            
            print("\nğŸ“‹ æœ€è¿‘æ‰«æè®°å½•:")
            for scan in self.scan_history[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¬¡
                status = "âœ… æˆåŠŸ" if scan['success'] else "âŒ å¤±è´¥"
                timestamp = scan['timestamp'][:19]  # å»æ‰æ¯«ç§’
                print(f"   [{scan['scan_id']:03d}] {timestamp} - {status}")

# ä½¿ç”¨ç¤ºä¾‹å’Œæ¼”ç¤º
if __name__ == "__main__":
    # åˆ›å»ºæ™ºèƒ½æ–‡æ¡£æ‰«æä»ª
    scanner = SmartDocumentScanner()
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ–‡æ¡£å›¾åƒè¿›è¡Œæµ‹è¯•
    workshop = CaptureWorkshop()
    test_document = workshop.create_test_image(640, 480, 'checkerboard')
    
    # æ‰§è¡Œå®Œæ•´æ‰«ææµç¨‹
    scan_results = scanner.scan_document(test_document)
    
    # å¯è§†åŒ–æ‰«æè¿‡ç¨‹
    scanner.visualize_scan_process(scan_results)
    
    # æ˜¾ç¤ºæ‰«æä»ªçŠ¶æ€
    scanner.get_scanner_status()
```

## ğŸ“ ç« èŠ‚æ€»ç»“

æ­å–œæ‚¨ï¼æ‚¨å·²ç»æˆåŠŸå»ºç«‹äº†ä¸€åº§å®Œæ•´çš„**æ•°å­—ç›¸æœºå·¥å‚**ï¼Œå¹¶æŒæ¡äº†OpenCVå›¾åƒå¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

### ğŸ† æ‚¨å·²ç»æŒæ¡çš„æŠ€èƒ½

#### ğŸ­ å·¥å‚åŸºç¡€è®¾æ–½
- âœ… OpenCVç¯å¢ƒçš„æ­å»ºå’Œé…ç½®
- âœ… å›¾åƒè¾“å…¥ã€æ˜¾ç¤ºã€ä¿å­˜çš„å®Œæ•´æµç¨‹
- âœ… ä¼ä¸šçº§ä»£ç ç»“æ„å’Œé”™è¯¯å¤„ç†

#### ğŸ¨ å›¾åƒå¤„ç†æŠ€æœ¯
- âœ… é¢œè‰²ç©ºé—´è½¬æ¢å’Œåˆ†æ
- âœ… å›¾åƒæ»¤æ³¢å’Œé™å™ªæŠ€æœ¯
- âœ… å›¾åƒå¢å¼ºå’Œè´¨é‡ä¼˜åŒ–

#### ğŸ” ç‰¹å¾æ£€æµ‹æŠ€æœ¯
- âœ… è¾¹ç¼˜æ£€æµ‹ç®—æ³•ï¼ˆCanny, Sobel, Laplacianï¼‰
- âœ… è§’ç‚¹æ£€æµ‹ç®—æ³•ï¼ˆHarris, Shi-Tomasi, FAST, ORBï¼‰
- âœ… ç‰¹å¾åŒ¹é…å’Œå›¾åƒå¯¹åº”å…³ç³»å»ºç«‹

#### ğŸ“± ä¼ä¸šçº§é¡¹ç›®å¼€å‘
- âœ… å®Œæ•´çš„æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªé¡¹ç›®
- âœ… ä»åŸå‹åˆ°äº§å“çš„å¼€å‘æµç¨‹
- âœ… é…ç½®ç®¡ç†ã€ç»“æœä¿å­˜ã€çŠ¶æ€ç›‘æ§

### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†OpenCVçš„åŸºç¡€æŠ€æœ¯ï¼Œå»ºè®®æ‚¨ï¼š

1. **æ·±å…¥å­¦ä¹ ç‰¹å®šé¢†åŸŸ** - é€‰æ‹©è®¡ç®—æœºè§†è§‰çš„æŸä¸ªåˆ†æ”¯æ·±å…¥ç ”ç©¶
2. **å®è·µæ›´å¤šé¡¹ç›®** - å¼€å‘è½¦ç‰Œè¯†åˆ«ã€äººè„¸è¯†åˆ«ç­‰å®ç”¨é¡¹ç›®
3. **å­¦ä¹ æ·±åº¦å­¦ä¹ ** - ç»“åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯æå‡è§†è§‰åº”ç”¨èƒ½åŠ›
4. **å…³æ³¨æ€§èƒ½ä¼˜åŒ–** - å­¦ä¹ ç®—æ³•ä¼˜åŒ–å’Œå¹¶è¡Œå¤„ç†æŠ€æœ¯

### ğŸ’¡ æ€è€ƒé¢˜

1. **æŠ€æœ¯å¯¹æ¯”**: åœ¨ä»€ä¹ˆæƒ…å†µä¸‹é€‰æ‹©Cannyè¾¹ç¼˜æ£€æµ‹è€Œä¸æ˜¯Sobelè¾¹ç¼˜æ£€æµ‹ï¼Ÿè¯·åˆ†æå®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚

2. **å‚æ•°è°ƒä¼˜**: å¦‚ä½•ä¸ºä¸åŒç±»å‹çš„å›¾åƒï¼ˆå¦‚åŒ»å­¦å½±åƒã€å«æ˜Ÿå›¾åƒã€å·¥ä¸šæ£€æµ‹å›¾åƒï¼‰è°ƒæ•´è¾¹ç¼˜æ£€æµ‹çš„å‚æ•°ï¼Ÿ

3. **é¡¹ç›®æ‰©å±•**: å¦‚ä½•æ‰©å±•æ™ºèƒ½æ–‡æ¡£æ‰«æä»ªï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤šé¡µæ–‡æ¡£å¹¶è‡ªåŠ¨åˆ†ç¦»é¡µé¢ï¼Ÿ

4. **æ€§èƒ½ä¼˜åŒ–**: åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²æ–‡æ¡£æ‰«æåŠŸèƒ½æ—¶ï¼Œå¦‚ä½•å¹³è¡¡å¤„ç†é€Ÿåº¦å’Œæ•ˆæœè´¨é‡ï¼Ÿ

---

**ç¬¬35ç« å®Œæˆæ—¶é—´**: 2025å¹´2æœˆ3æ—¥  
**ä»£ç ç¤ºä¾‹æ€»æ•°**: 12ä¸ªå®Œæ•´ç¤ºä¾‹  
**æ€»å­—æ•°**: çº¦25,000å­—  
**å®æˆ˜é¡¹ç›®**: æ™ºèƒ½æ–‡æ¡£æ‰«æä»ª  

> ğŸ¯ **æ­å–œæ‚¨ï¼** æ‚¨å·²ç»æˆåŠŸæŒæ¡äº†OpenCVå›¾åƒå¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå»ºç«‹äº†å®Œæ•´çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯åŸºç¡€ã€‚ç°åœ¨æ‚¨å…·å¤‡äº†å¼€å‘ä¸“ä¸šå›¾åƒå¤„ç†åº”ç”¨çš„èƒ½åŠ›ï¼