# ç¬¬15ç« ï¼šPandasæ•°æ®åˆ†æ

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**  
> é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†æŒæ¡Pythonæ•°æ®åˆ†æåˆ©å™¨Pandasçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå­¦ä¼šå¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®åˆ†æä»»åŠ¡ï¼Œä¸ºæˆä¸ºæ•°æ®åˆ†æå¸ˆæ‰“ä¸‹åšå®åŸºç¡€ã€‚

## ğŸ“– æœ¬ç« å¯¼è¯»

åœ¨ç°ä»£æ•°æ®é©±åŠ¨çš„ä¸–ç•Œä¸­ï¼Œæ•°æ®åˆ†æèƒ½åŠ›å·²æˆä¸ºå„è¡Œå„ä¸šçš„æ ¸å¿ƒæŠ€èƒ½ã€‚æ— è®ºæ˜¯å•†ä¸šå†³ç­–ã€ç§‘å­¦ç ”ç©¶ï¼Œè¿˜æ˜¯æ—¥å¸¸å·¥ä½œï¼Œæˆ‘ä»¬éƒ½éœ€è¦ä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

**ä¸ºä»€ä¹ˆå­¦ä¹ Pandasï¼Ÿ**
- ğŸ­ **æ•°æ®å·¥å‚çš„ç®¡ç†è€…**ï¼šPandaså°±åƒä¸€ä¸ªæ™ºèƒ½åŒ–çš„æ•°æ®å·¥å‚ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å„ç§"åŸæ–™"ï¼ˆæ•°æ®ï¼‰
- ğŸ“Š **æ•°æ®ç§‘å­¦çš„åŸºçŸ³**ï¼š90%çš„æ•°æ®ç§‘å­¦å·¥ä½œéƒ½ç¦»ä¸å¼€Pandas
- ğŸš€ **èŒåœºç«äº‰åŠ›**ï¼šæŒæ¡Pandasæ˜¯æ•°æ®åˆ†æå²—ä½çš„å¿…å¤‡æŠ€èƒ½
- ğŸ¯ **è§£å†³å®é™…é—®é¢˜**ï¼šä»å­¦ç”Ÿæˆç»©åˆ†æåˆ°ä¼ä¸šé”€å”®æŠ¥è¡¨ï¼Œæ— æ‰€ä¸èƒ½

**æœ¬ç« çŸ¥è¯†åœ°å›¾**ï¼š
```mermaid
mindmap
  root((Pandasæ•°æ®åˆ†æ))
    æ•°æ®ç»“æ„
      Series(ä¸€ç»´æ•°æ®)
      DataFrame(äºŒç»´è¡¨æ ¼)
      Index(ç´¢å¼•ç³»ç»Ÿ)
    æ•°æ®è¾“å…¥è¾“å‡º
      æ–‡ä»¶è¯»å–(CSV/Excel/JSON)
      æ•°æ®åº“è¿æ¥
      ç½‘ç»œæ•°æ®è·å–
    æ•°æ®æ¸…æ´—
      ç¼ºå¤±å€¼å¤„ç†
      é‡å¤æ•°æ®å¤„ç†
      æ•°æ®ç±»å‹è½¬æ¢
    æ•°æ®åˆ†æ
      æè¿°æ€§ç»Ÿè®¡
      åˆ†ç»„èšåˆ
      æ•°æ®é€è§†è¡¨
      å¯è§†åŒ–å±•ç¤º
```

## 15.1 PandasåŸºç¡€ä¸æ•°æ®ç»“æ„

### ğŸ­ æ•°æ®å·¥å‚çš„åŸæ–™ä»“åº“

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ˜¯ä¸€å®¶ç°ä»£åŒ–æ•°æ®å·¥å‚çš„ç®¡ç†è€…ã€‚å·¥å‚é‡Œæœ‰ä¸¤ç§ä¸»è¦çš„åŸæ–™ä»“åº“ï¼š

- **Seriesä»“åº“**ï¼šä¸“é—¨å­˜å‚¨å•ä¸€ç±»å‹çš„äº§å“ï¼Œå¦‚"æ‰€æœ‰å­¦ç”Ÿçš„æ•°å­¦æˆç»©"
- **DataFrameä»“åº“**ï¼šå­˜å‚¨å¤šç§ç›¸å…³äº§å“ï¼Œå¦‚"å­¦ç”Ÿä¿¡æ¯è¡¨ï¼ˆå§“åã€å¹´é¾„ã€å„ç§‘æˆç»©ï¼‰"

### 15.1.1 Pandasç®€ä»‹ä¸ç¯å¢ƒå‡†å¤‡

Pandasï¼ˆPanel Data Analysisï¼‰æ˜¯åŸºäºNumPyæ„å»ºçš„æ•°æ®åˆ†æåº“ï¼Œæä¾›äº†é«˜æ€§èƒ½ã€æ˜“ç”¨çš„æ•°æ®ç»“æ„å’Œæ•°æ®åˆ†æå·¥å…·ã€‚

```python
# å®‰è£…Pandasï¼ˆå¦‚æœè¿˜æœªå®‰è£…ï¼‰
# pip install pandas matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 20)

print("Pandasç‰ˆæœ¬:", pd.__version__)
print("æ•°æ®å·¥å‚å·²å‡†å¤‡å°±ç»ªï¼ğŸ­")
```

### 15.1.2 Seriesï¼šå•ä¸€äº§å“çº¿çš„æ•°æ®å®¹å™¨

Seriesæ˜¯Pandasä¸­çš„ä¸€ç»´æ•°æ®ç»“æ„ï¼Œç±»ä¼¼äºå¸¦æ ‡ç­¾çš„æ•°ç»„ã€‚

```python
# === Seriesåˆ›å»ºæ–¹æ³• ===

# æ–¹æ³•1ï¼šä»åˆ—è¡¨åˆ›å»º
students_math_scores = pd.Series([85, 92, 78, 96, 88], 
                                name='æ•°å­¦æˆç»©')
print("ğŸ“Š å­¦ç”Ÿæ•°å­¦æˆç»©ï¼š")
print(students_math_scores)
print()

# æ–¹æ³•2ï¼šä»å­—å…¸åˆ›å»ºï¼ˆè‡ªåŠ¨ç”Ÿæˆç´¢å¼•ï¼‰
student_scores = pd.Series({
    'å¼ ä¸‰': 85,
    'æå››': 92, 
    'ç‹äº”': 78,
    'èµµå…­': 96,
    'é’±ä¸ƒ': 88
}, name='æ•°å­¦æˆç»©')

print("ğŸ“‹ å¸¦å§“åçš„æˆç»©å•ï¼š")
print(student_scores)
print()

# æ–¹æ³•3ï¼šæŒ‡å®šç´¢å¼•
ages = pd.Series([18, 19, 18, 20, 19], 
                index=['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],
                name='å¹´é¾„')
print("ğŸ‘¥ å­¦ç”Ÿå¹´é¾„ä¿¡æ¯ï¼š")
print(ages)
print()

# === SeriesåŸºæœ¬å±æ€§ ===
print("=== SeriesåŸºæœ¬ä¿¡æ¯ ===")
print(f"æ•°æ®ç±»å‹: {student_scores.dtype}")
print(f"æ•°æ®é•¿åº¦: {len(student_scores)}")
print(f"ç´¢å¼•: {student_scores.index.tolist()}")
print(f"æ•°å€¼: {student_scores.values}")
print(f"åç§°: {student_scores.name}")
```

#### Seriesçš„æ ¸å¿ƒæ“ä½œ

```python
# === æ•°æ®è®¿é—® ===
print("=== æ•°æ®è®¿é—®æ–¹å¼ ===")

# æŒ‰ç´¢å¼•è®¿é—®
print(f"å¼ ä¸‰çš„æˆç»©: {student_scores['å¼ ä¸‰']}")
print(f"æå››çš„æˆç»©: {student_scores.loc['æå››']}")

# æŒ‰ä½ç½®è®¿é—®
print(f"ç¬¬ä¸€ä¸ªå­¦ç”Ÿæˆç»©: {student_scores.iloc[0]}")
print(f"å‰ä¸‰ä¸ªå­¦ç”Ÿæˆç»©:\n{student_scores.iloc[:3]}")

# æ¡ä»¶ç­›é€‰
high_scores = student_scores[student_scores >= 90]
print(f"\nä¼˜ç§€å­¦ç”Ÿ(â‰¥90åˆ†):\n{high_scores}")

# === æ•°æ®ä¿®æ”¹ ===
print("\n=== æ•°æ®ä¿®æ”¹ ===")
student_scores_copy = student_scores.copy()
student_scores_copy['å¼ ä¸‰'] = 90  # ä¿®æ”¹å•ä¸ªå€¼
print(f"å¼ ä¸‰æˆç»©ä¿®æ”¹å: {student_scores_copy['å¼ ä¸‰']}")

# æ‰¹é‡ä¿®æ”¹
student_scores_copy[student_scores_copy < 80] += 5  # ä½åˆ†åŠ 5åˆ†
print(f"ä½åˆ†æå‡å:\n{student_scores_copy}")

# === æ•°å­¦è¿ç®— ===
print("\n=== æ•°å­¦è¿ç®— ===")
print(f"å¹³å‡åˆ†: {student_scores.mean():.2f}")
print(f"æœ€é«˜åˆ†: {student_scores.max()}")
print(f"æœ€ä½åˆ†: {student_scores.min()}")
print(f"æ ‡å‡†å·®: {student_scores.std():.2f}")

# æˆç»©åŠ æƒï¼ˆæœŸæœ«æˆç»©å 70%ï¼‰
final_scores = student_scores * 0.7
print(f"\næœŸæœ«æˆç»©(70%æƒé‡):\n{final_scores}")
```

### 15.1.3 DataFrameï¼šå¤šäº§å“çº¿çš„æ•°æ®ç®¡ç†ç³»ç»Ÿ

DataFrameæ˜¯Pandasçš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œå¯ä»¥ç†è§£ä¸ºå¸¦æ ‡ç­¾çš„äºŒç»´è¡¨æ ¼ã€‚

```python
# === DataFrameåˆ›å»ºæ–¹æ³• ===

# æ–¹æ³•1ï¼šä»å­—å…¸åˆ›å»º
student_data = {
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],
    'å¹´é¾„': [18, 19, 18, 20, 19],
    'æ€§åˆ«': ['ç”·', 'å¥³', 'ç”·', 'å¥³', 'ç”·'],
    'æ•°å­¦': [85, 92, 78, 96, 88],
    'è¯­æ–‡': [88, 85, 92, 89, 87],
    'è‹±è¯­': [82, 94, 85, 93, 90]
}

df_students = pd.DataFrame(student_data)
print("ğŸ“Š å­¦ç”Ÿä¿¡æ¯è¡¨ï¼š")
print(df_students)
print()

# æ–¹æ³•2ï¼šä»åµŒå¥—åˆ—è¡¨åˆ›å»º
data_matrix = [
    ['å¼ ä¸‰', 18, 'ç”·', 85, 88, 82],
    ['æå››', 19, 'å¥³', 92, 85, 94],
    ['ç‹äº”', 18, 'ç”·', 78, 92, 85],
    ['èµµå…­', 20, 'å¥³', 96, 89, 93],
    ['é’±ä¸ƒ', 19, 'ç”·', 88, 87, 90]
]

df_from_list = pd.DataFrame(data_matrix, 
                           columns=['å§“å', 'å¹´é¾„', 'æ€§åˆ«', 'æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­'])
print("ğŸ“‹ ä»åˆ—è¡¨åˆ›å»ºçš„DataFrameï¼š")
print(df_from_list.head(3))  # æ˜¾ç¤ºå‰3è¡Œ
print()

# === DataFrameåŸºæœ¬ä¿¡æ¯ ===
print("=== DataFrameåŸºæœ¬ä¿¡æ¯ ===")
print(f"å½¢çŠ¶ (è¡Œæ•°, åˆ—æ•°): {df_students.shape}")
print(f"åˆ—å: {df_students.columns.tolist()}")
print(f"ç´¢å¼•: {df_students.index.tolist()}")
print(f"æ•°æ®ç±»å‹:\n{df_students.dtypes}")
print()

# è¯¦ç»†ä¿¡æ¯
print("=== è¯¦ç»†ä¿¡æ¯ ===")
print(df_students.info())
print()

# æè¿°æ€§ç»Ÿè®¡
print("=== æ•°å€¼åˆ—æè¿°æ€§ç»Ÿè®¡ ===")
print(df_students.describe())
```

#### DataFrameçš„æ•°æ®è®¿é—®

```python
# === åˆ—æ“ä½œ ===
print("=== åˆ—æ“ä½œ ===")

# é€‰æ‹©å•åˆ—ï¼ˆè¿”å›Seriesï¼‰
math_scores = df_students['æ•°å­¦']
print(f"æ•°å­¦æˆç»©åˆ—:\n{math_scores}")
print(f"ç±»å‹: {type(math_scores)}")
print()

# é€‰æ‹©å¤šåˆ—ï¼ˆè¿”å›DataFrameï¼‰
score_columns = df_students[['å§“å', 'æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­']]
print("æˆç»©ç›¸å…³åˆ—:")
print(score_columns)
print()

# === è¡Œæ“ä½œ ===
print("=== è¡Œæ“ä½œ ===")

# æŒ‰ä½ç½®é€‰æ‹©è¡Œ
first_student = df_students.iloc[0]  # ç¬¬ä¸€è¡Œ
print(f"ç¬¬ä¸€ä¸ªå­¦ç”Ÿä¿¡æ¯:\n{first_student}")
print()

# æŒ‰ç´¢å¼•é€‰æ‹©è¡Œ
student_info = df_students.loc[0]  # ç´¢å¼•ä¸º0çš„è¡Œ
print(f"ç´¢å¼•0çš„å­¦ç”Ÿä¿¡æ¯:\n{student_info}")
print()

# é€‰æ‹©å¤šè¡Œ
first_three = df_students.iloc[:3]  # å‰ä¸‰è¡Œ
print("å‰ä¸‰ä¸ªå­¦ç”Ÿ:")
print(first_three)
print()

# === æ¡ä»¶ç­›é€‰ ===
print("=== æ¡ä»¶ç­›é€‰ ===")

# å•æ¡ä»¶ç­›é€‰
high_math = df_students[df_students['æ•°å­¦'] >= 90]
print("æ•°å­¦æˆç»©ä¼˜ç§€çš„å­¦ç”Ÿ:")
print(high_math)
print()

# å¤šæ¡ä»¶ç­›é€‰
excellent_students = df_students[
    (df_students['æ•°å­¦'] >= 85) & 
    (df_students['è¯­æ–‡'] >= 85)
]
print("æ•°å­¦å’Œè¯­æ–‡éƒ½ä¼˜ç§€çš„å­¦ç”Ÿ:")
print(excellent_students)
print()

# å­—ç¬¦ä¸²æ¡ä»¶
male_students = df_students[df_students['æ€§åˆ«'] == 'ç”·']
print("ç”·å­¦ç”Ÿä¿¡æ¯:")
print(male_students)
```

### 15.1.4 ç´¢å¼•ç³»ç»Ÿï¼šä»“åº“çš„è´§æ¶ç¼–å·

ç´¢å¼•æ˜¯Pandasçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå°±åƒä»“åº“ä¸­çš„è´§æ¶ç¼–å·ç³»ç»Ÿã€‚

```python
# === è®¾ç½®ç´¢å¼• ===
print("=== ç´¢å¼•æ“ä½œ ===")

# å°†å§“åè®¾ä¸ºç´¢å¼•
df_indexed = df_students.set_index('å§“å')
print("ä»¥å§“åä¸ºç´¢å¼•çš„DataFrame:")
print(df_indexed)
print()

# é‡ç½®ç´¢å¼•
df_reset = df_indexed.reset_index()
print("é‡ç½®ç´¢å¼•å:")
print(df_reset.head(3))
print()

# === å¤šå±‚ç´¢å¼• ===
print("=== å¤šå±‚ç´¢å¼•ç¤ºä¾‹ ===")

# åˆ›å»ºå¤šå±‚ç´¢å¼•æ•°æ®
multi_data = {
    'æˆç»©': [85, 88, 82, 92, 85, 94, 78, 92, 85],
    'å­¦æœŸ': ['ç¬¬ä¸€å­¦æœŸ', 'ç¬¬ä¸€å­¦æœŸ', 'ç¬¬ä¸€å­¦æœŸ', 'ç¬¬äºŒå­¦æœŸ', 'ç¬¬äºŒå­¦æœŸ', 'ç¬¬äºŒå­¦æœŸ', 'ç¬¬ä¸‰å­¦æœŸ', 'ç¬¬ä¸‰å­¦æœŸ', 'ç¬¬ä¸‰å­¦æœŸ']
}

multi_index = pd.MultiIndex.from_tuples([
    ('å¼ ä¸‰', 'æ•°å­¦'), ('å¼ ä¸‰', 'è¯­æ–‡'), ('å¼ ä¸‰', 'è‹±è¯­'),
    ('æå››', 'æ•°å­¦'), ('æå››', 'è¯­æ–‡'), ('æå››', 'è‹±è¯­'),
    ('ç‹äº”', 'æ•°å­¦'), ('ç‹äº”', 'è¯­æ–‡'), ('ç‹äº”', 'è‹±è¯­')
], names=['å­¦ç”Ÿ', 'ç§‘ç›®'])

df_multi = pd.DataFrame(multi_data, index=multi_index)
print("å¤šå±‚ç´¢å¼•DataFrame:")
print(df_multi)
print()

# å¤šå±‚ç´¢å¼•è®¿é—®
print("å¼ ä¸‰çš„æ‰€æœ‰æˆç»©:")
print(df_multi.loc['å¼ ä¸‰'])
print()

print("æ‰€æœ‰å­¦ç”Ÿçš„æ•°å­¦æˆç»©:")
print(df_multi.loc[(slice(None), 'æ•°å­¦'), :])
```

### ğŸ¯ å®è·µé¡¹ç›®ï¼šå­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»Ÿ

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»Ÿï¼Œå±•ç¤ºPandasåŸºç¡€åŠŸèƒ½çš„ç»¼åˆåº”ç”¨ã€‚

```python
class StudentGradeManager:
    """å­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»Ÿ
    
    è¿™ä¸ªç³»ç»Ÿæ¨¡æ‹Ÿäº†ä¸€ä¸ªæ•°æ®å·¥å‚çš„å®Œæ•´æµç¨‹ï¼š
    - åŸæ–™å…¥åº“ï¼šå½•å…¥å­¦ç”Ÿä¿¡æ¯å’Œæˆç»©
    - è´¨é‡æ£€æŸ¥ï¼šæ•°æ®éªŒè¯å’Œæ¸…æ´—
    - ç”Ÿäº§åŠ å·¥ï¼šæˆç»©è®¡ç®—å’Œç»Ÿè®¡
    - äº§å“åŒ…è£…ï¼šæŠ¥è¡¨ç”Ÿæˆå’Œå¯è§†åŒ–
    """
    
    def __init__(self):
        self.students_df = pd.DataFrame()
        self.subjects = ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'ç‰©ç†', 'åŒ–å­¦']
        
    def add_student(self, name, age, gender, scores):
        """æ·»åŠ å­¦ç”Ÿä¿¡æ¯"""
        if len(scores) != len(self.subjects):
            raise ValueError(f"æˆç»©æ•°é‡å¿…é¡»ä¸º{len(self.subjects)}ä¸ª")
            
        student_data = {
            'å§“å': name,
            'å¹´é¾„': age,
            'æ€§åˆ«': gender
        }
        
        # æ·»åŠ å„ç§‘æˆç»©
        for subject, score in zip(self.subjects, scores):
            student_data[subject] = score
            
        # è½¬æ¢ä¸ºDataFrameå¹¶æ·»åŠ åˆ°ä¸»è¡¨
        new_student = pd.DataFrame([student_data])
        self.students_df = pd.concat([self.students_df, new_student], 
                                   ignore_index=True)
        
        print(f"âœ… å­¦ç”Ÿ {name} ä¿¡æ¯å·²å½•å…¥")
        
    def batch_add_students(self, students_list):
        """æ‰¹é‡æ·»åŠ å­¦ç”Ÿ"""
        for student in students_list:
            self.add_student(*student)
            
    def calculate_total_average(self):
        """è®¡ç®—æ€»åˆ†å’Œå¹³å‡åˆ†"""
        # è®¡ç®—æ€»åˆ†
        self.students_df['æ€»åˆ†'] = self.students_df[self.subjects].sum(axis=1)
        
        # è®¡ç®—å¹³å‡åˆ†
        self.students_df['å¹³å‡åˆ†'] = self.students_df[self.subjects].mean(axis=1)
        
        print("âœ… æ€»åˆ†å’Œå¹³å‡åˆ†è®¡ç®—å®Œæˆ")
        
    def get_rank(self):
        """è®¡ç®—æ’å"""
        self.students_df['æ’å'] = self.students_df['æ€»åˆ†'].rank(
            method='min', ascending=False).astype(int)
        
        print("âœ… æ’åè®¡ç®—å®Œæˆ")
        
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'å­¦ç”Ÿæ€»æ•°': len(self.students_df),
            'ç”·ç”Ÿäººæ•°': len(self.students_df[self.students_df['æ€§åˆ«'] == 'ç”·']),
            'å¥³ç”Ÿäººæ•°': len(self.students_df[self.students_df['æ€§åˆ«'] == 'å¥³']),
            'å¹³å‡å¹´é¾„': self.students_df['å¹´é¾„'].mean(),
            'æ€»åˆ†æœ€é«˜': self.students_df['æ€»åˆ†'].max(),
            'æ€»åˆ†æœ€ä½': self.students_df['æ€»åˆ†'].min(),
            'ç­çº§å¹³å‡åˆ†': self.students_df['å¹³å‡åˆ†'].mean()
        }
        
        return stats
        
    def get_subject_analysis(self):
        """å„ç§‘æˆç»©åˆ†æ"""
        subject_stats = {}
        
        for subject in self.subjects:
            subject_stats[subject] = {
                'å¹³å‡åˆ†': self.students_df[subject].mean(),
                'æœ€é«˜åˆ†': self.students_df[subject].max(),
                'æœ€ä½åˆ†': self.students_df[subject].min(),
                'æ ‡å‡†å·®': self.students_df[subject].std(),
                'åŠæ ¼ç‡': (self.students_df[subject] >= 60).mean() * 100
            }
            
        return subject_stats
        
    def find_excellent_students(self, threshold=85):
        """æŸ¥æ‰¾ä¼˜ç§€å­¦ç”Ÿï¼ˆå¹³å‡åˆ†è¶…è¿‡é˜ˆå€¼ï¼‰"""
        excellent = self.students_df[
            self.students_df['å¹³å‡åˆ†'] >= threshold
        ].sort_values('å¹³å‡åˆ†', ascending=False)
        
        return excellent
        
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print("=" * 50)
        print("ğŸ“Š å­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»ŸæŠ¥å‘Š")
        print("=" * 50)
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = self.get_statistics()
        print("\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
                
        # å„ç§‘åˆ†æ
        print("\nğŸ“š å„ç§‘æˆç»©åˆ†æ:")
        subject_stats = self.get_subject_analysis()
        for subject, stats in subject_stats.items():
            print(f"\n  {subject}:")
            for key, value in stats.items():
                print(f"    {key}: {value:.2f}")
                
        # ä¼˜ç§€å­¦ç”Ÿ
        print("\nğŸ† ä¼˜ç§€å­¦ç”Ÿåå• (å¹³å‡åˆ†â‰¥85):")
        excellent = self.find_excellent_students()
        if not excellent.empty:
            print(excellent[['å§“å', 'å¹³å‡åˆ†', 'æ’å']].to_string(index=False))
        else:
            print("  æš‚æ— ä¼˜ç§€å­¦ç”Ÿ")
            
        # å®Œæ•´æˆç»©å•
        print("\nğŸ“‹ å®Œæ•´æˆç»©å•:")
        display_df = self.students_df.sort_values('æ’å')
        print(display_df.to_string(index=False))

# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # åˆ›å»ºæˆç»©ç®¡ç†ç³»ç»Ÿ
    grade_manager = StudentGradeManager()
    
    # ç¤ºä¾‹å­¦ç”Ÿæ•°æ®
    students_data = [
        ('å¼ ä¸‰', 18, 'ç”·', [85, 88, 82, 90, 87]),
        ('æå››', 19, 'å¥³', [92, 85, 94, 88, 91]),
        ('ç‹äº”', 18, 'ç”·', [78, 92, 85, 82, 79]),
        ('èµµå…­', 20, 'å¥³', [96, 89, 93, 95, 94]),
        ('é’±ä¸ƒ', 19, 'ç”·', [88, 87, 90, 85, 89]),
        ('å­™å…«', 18, 'å¥³', [83, 91, 87, 89, 85]),
        ('å‘¨ä¹', 19, 'ç”·', [90, 84, 88, 92, 86]),
        ('å´å', 18, 'å¥³', [87, 93, 91, 88, 92])
    ]
    
    # æ‰¹é‡æ·»åŠ å­¦ç”Ÿ
    print("ğŸ­ æ•°æ®å·¥å‚å¼€å§‹è¿ä½œ...")
    grade_manager.batch_add_students(students_data)
    
    # è®¡ç®—æ€»åˆ†å’Œå¹³å‡åˆ†
    grade_manager.calculate_total_average()
    
    # è®¡ç®—æ’å
    grade_manager.get_rank()
    
    # ç”ŸæˆæŠ¥å‘Š
    grade_manager.generate_report()
    
    print("\nğŸ‰ å­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
```

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

1. **Series**ï¼šä¸€ç»´æ ‡ç­¾æ•°ç»„ï¼Œé€‚åˆå­˜å‚¨å•ä¸€ç±»å‹çš„æ•°æ®
2. **DataFrame**ï¼šäºŒç»´æ ‡ç­¾è¡¨æ ¼ï¼ŒPandasçš„æ ¸å¿ƒæ•°æ®ç»“æ„
3. **Index**ï¼šè¡Œå’Œåˆ—çš„æ ‡ç­¾ç³»ç»Ÿï¼Œæ”¯æŒå¿«é€Ÿæ•°æ®è®¿é—®
4. **æ•°æ®è®¿é—®**ï¼šæ”¯æŒæ ‡ç­¾è®¿é—®ï¼ˆ.locï¼‰å’Œä½ç½®è®¿é—®ï¼ˆ.ilocï¼‰
5. **æ¡ä»¶ç­›é€‰**ï¼šä½¿ç”¨å¸ƒå°”ç´¢å¼•è¿›è¡Œæ•°æ®è¿‡æ»¤

é€šè¿‡"æ•°æ®å·¥å‚"çš„æ¯”å–»ï¼Œæˆ‘ä»¬äº†è§£äº†Pandasçš„åŸºæœ¬æ•°æ®ç»“æ„ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•å°†å„ç§"åŸæ–™"ï¼ˆæ•°æ®æ–‡ä»¶ï¼‰å¯¼å…¥åˆ°æˆ‘ä»¬çš„å·¥å‚ä¸­ã€‚

---

## 15.2 æ•°æ®è¯»å–ä¸è¾“å…¥è¾“å‡º

### ğŸš› æ•°æ®å·¥å‚çš„è¿›è´§ä¸å‡ºè´§ç³»ç»Ÿ

åœ¨çœŸå®çš„æ•°æ®åˆ†æå·¥ä½œä¸­ï¼Œæ•°æ®å¾ˆå°‘æ˜¯ç›´æ¥åœ¨ä»£ç ä¸­åˆ›å»ºçš„ã€‚å°±åƒå·¥å‚éœ€è¦ä»ä¸åŒä¾›åº”å•†é‡‡è´­åŸæ–™ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä»å„ç§æ•°æ®æºè¯»å–æ•°æ®ï¼šCSVæ–‡ä»¶ã€Excelè¡¨æ ¼ã€æ•°æ®åº“ã€ç½‘ç»œAPIç­‰ã€‚

åŒæ ·ï¼Œå¤„ç†å®Œçš„æ•°æ®ä¹Ÿéœ€è¦"å‡ºè´§"â€”â€”ä¿å­˜ä¸ºæ–‡ä»¶ã€å†™å…¥æ•°æ®åº“æˆ–å‘é€ç»™å…¶ä»–ç³»ç»Ÿã€‚

### 15.2.1 æ–‡ä»¶è¯»å–ï¼šä»ä»“åº“æå–åŸæ–™

#### CSVæ–‡ä»¶è¯»å–

CSVï¼ˆComma-Separated Valuesï¼‰æ˜¯æœ€å¸¸è§çš„æ•°æ®äº¤æ¢æ ¼å¼ã€‚

```python
# === CSVæ–‡ä»¶è¯»å– ===
import pandas as pd
import numpy as np
from io import StringIO

# æ¨¡æ‹ŸCSVæ–‡ä»¶å†…å®¹
csv_content = """
å§“å,å¹´é¾„,æ€§åˆ«,éƒ¨é—¨,è–ªèµ„,å…¥èŒæ—¥æœŸ
å¼ ä¸‰,28,ç”·,æŠ€æœ¯éƒ¨,15000,2020-03-15
æå››,32,å¥³,å¸‚åœºéƒ¨,18000,2019-07-22
ç‹äº”,25,ç”·,æŠ€æœ¯éƒ¨,12000,2021-01-10
èµµå…­,29,å¥³,äººäº‹éƒ¨,14000,2020-11-05
é’±ä¸ƒ,35,ç”·,é”€å”®éƒ¨,20000,2018-05-30
å­™å…«,26,å¥³,æŠ€æœ¯éƒ¨,13000,2021-08-18
"""

# ä»å­—ç¬¦ä¸²è¯»å–ï¼ˆæ¨¡æ‹Ÿæ–‡ä»¶è¯»å–ï¼‰
df_employees = pd.read_csv(StringIO(csv_content.strip()))
print("ğŸ“Š å‘˜å·¥ä¿¡æ¯è¡¨ï¼š")
print(df_employees)
print()

# === å¸¸ç”¨è¯»å–å‚æ•° ===
# æŒ‡å®šç¼–ç ï¼ˆå¤„ç†ä¸­æ–‡ï¼‰
# df = pd.read_csv('employees.csv', encoding='utf-8')

# æŒ‡å®šåˆ†éš”ç¬¦
# df = pd.read_csv('data.txt', sep='\t')  # Tabåˆ†éš”

# è·³è¿‡è¡Œ
# df = pd.read_csv('data.csv', skiprows=2)  # è·³è¿‡å‰2è¡Œ

# æŒ‡å®šåˆ—å
# df = pd.read_csv('data.csv', names=['col1', 'col2', 'col3'])

# æŒ‡å®šæ•°æ®ç±»å‹
dtype_dict = {
    'å¹´é¾„': 'int64',
    'è–ªèµ„': 'float64'
}
df_typed = pd.read_csv(StringIO(csv_content.strip()), dtype=dtype_dict)
print("æŒ‡å®šæ•°æ®ç±»å‹åçš„DataFrame:")
print(df_typed.dtypes)
print()
```

#### Excelæ–‡ä»¶è¯»å–

```python
# === Excelæ–‡ä»¶è¯»å–ç¤ºä¾‹ ===
# æ³¨æ„ï¼šå®é™…ä½¿ç”¨éœ€è¦å®‰è£… openpyxl æˆ– xlrd
# pip install openpyxl

# æ¨¡æ‹ŸExcelæ•°æ®è¯»å–
excel_data = {
    'Sheet1': {
        'äº§å“åç§°': ['ç¬”è®°æœ¬ç”µè„‘', 'å°å¼æœº', 'å¹³æ¿ç”µè„‘', 'æ™ºèƒ½æ‰‹æœº'],
        'é”€é‡': [150, 80, 200, 500],
        'å•ä»·': [5000, 3000, 2000, 1500],
        'åº“å­˜': [50, 30, 100, 200]
    },
    'Sheet2': {
        'æœˆä»½': ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ'],
        'æ”¶å…¥': [500000, 600000, 750000, 800000],
        'æ”¯å‡º': [300000, 350000, 400000, 450000]
    }
}

# æ¨¡æ‹Ÿè¯»å–Excelçš„ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
df_products = pd.DataFrame(excel_data['Sheet1'])
print("ğŸ“± äº§å“é”€å”®æ•°æ®ï¼š")
print(df_products)
print()

# æ¨¡æ‹Ÿè¯»å–Excelçš„ç¬¬äºŒä¸ªå·¥ä½œè¡¨
df_finance = pd.DataFrame(excel_data['Sheet2'])
print("ğŸ’° è´¢åŠ¡æ•°æ®ï¼š")
print(df_finance)
print()

# === Excelè¯»å–çš„å®é™…ä»£ç ç¤ºä¾‹ ===
print("Excelè¯»å–ä»£ç ç¤ºä¾‹ï¼š")
print("""
# è¯»å–Excelæ–‡ä»¶
df = pd.read_excel('sales_data.xlsx')

# è¯»å–æŒ‡å®šå·¥ä½œè¡¨
df = pd.read_excel('sales_data.xlsx', sheet_name='Sheet2')

# è¯»å–å¤šä¸ªå·¥ä½œè¡¨
dfs = pd.read_excel('sales_data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
all_sheets = pd.read_excel('sales_data.xlsx', sheet_name=None)
""")
```

#### JSONæ–‡ä»¶è¯»å–

```python
# === JSONæ–‡ä»¶è¯»å– ===
import json

# æ¨¡æ‹ŸJSONæ•°æ®
json_data = '''
[
    {
        "ç”¨æˆ·ID": "U001",
        "å§“å": "å¼ ä¸‰",
        "å¹´é¾„": 25,
        "çˆ±å¥½": ["ç¼–ç¨‹", "é˜…è¯»", "æ¸¸æ³³"],
        "åœ°å€": {
            "çœä»½": "åŒ—äº¬",
            "åŸå¸‚": "åŒ—äº¬å¸‚",
            "åŒºåŸŸ": "æœé˜³åŒº"
        }
    },
    {
        "ç”¨æˆ·ID": "U002", 
        "å§“å": "æå››",
        "å¹´é¾„": 30,
        "çˆ±å¥½": ["éŸ³ä¹", "æ—…è¡Œ"],
        "åœ°å€": {
            "çœä»½": "ä¸Šæµ·",
            "åŸå¸‚": "ä¸Šæµ·å¸‚",
            "åŒºåŸŸ": "æµ¦ä¸œæ–°åŒº"
        }
    }
]
'''

# ä»JSONå­—ç¬¦ä¸²è¯»å–
df_users = pd.read_json(json_data)
print("ğŸ‘¥ ç”¨æˆ·ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰ï¼š")
print(df_users)
print()

# å¤„ç†åµŒå¥—JSONæ•°æ®
df_normalized = pd.json_normalize(json.loads(json_data))
print("ğŸ“‹ æ ‡å‡†åŒ–åçš„ç”¨æˆ·ä¿¡æ¯ï¼š")
print(df_normalized)
```

### 15.2.2 æ•°æ®åº“è¿æ¥ï¼šä»æ•°æ®ä»“åº“æå–

```python
# === æ•°æ®åº“è¿æ¥ç¤ºä¾‹ ===
import sqlite3

# åˆ›å»ºå†…å­˜æ•°æ®åº“æ¼”ç¤º
conn = sqlite3.connect(':memory:')

# åˆ›å»ºç¤ºä¾‹è¡¨
create_table_sql = '''
CREATE TABLE students (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    age INTEGER,
    grade REAL,
    class_name TEXT
)
'''

conn.execute(create_table_sql)

# æ’å…¥ç¤ºä¾‹æ•°æ®
students_data = [
    (1, 'å¼ ä¸‰', 18, 85.5, 'é«˜ä¸‰1ç­'),
    (2, 'æå››', 17, 92.0, 'é«˜äºŒ3ç­'),
    (3, 'ç‹äº”', 18, 78.5, 'é«˜ä¸‰2ç­'),
    (4, 'èµµå…­', 16, 88.0, 'é«˜ä¸€1ç­')
]

conn.executemany(
    'INSERT INTO students VALUES (?, ?, ?, ?, ?)', 
    students_data
)
conn.commit()

# ä½¿ç”¨Pandasè¯»å–æ•°æ®åº“
df_from_db = pd.read_sql_query(
    "SELECT * FROM students WHERE grade >= 80", 
    conn
)

print("ğŸ“š ä»æ•°æ®åº“è¯»å–çš„å­¦ç”Ÿä¿¡æ¯ï¼š")
print(df_from_db)
print()

# å…³é—­è¿æ¥
conn.close()

# === å®é™…æ•°æ®åº“è¿æ¥ä»£ç ç¤ºä¾‹ ===
print("å®é™…æ•°æ®åº“è¿æ¥ä»£ç ï¼š")
print("""
# MySQLè¿æ¥ç¤ºä¾‹
import pymysql
from sqlalchemy import create_engine

engine = create_engine('mysql+pymysql://user:password@host:port/database')
df = pd.read_sql('SELECT * FROM table_name', engine)

# PostgreSQLè¿æ¥ç¤ºä¾‹
import psycopg2
engine = create_engine('postgresql://user:password@host:port/database')
df = pd.read_sql('SELECT * FROM table_name', engine)
""")
```

### 15.2.3 ç½‘ç»œæ•°æ®è·å–ï¼šåœ¨çº¿åŸæ–™é‡‡è´­

```python
# === ç½‘ç»œæ•°æ®è·å–ç¤ºä¾‹ ===
import requests
from io import StringIO

# æ¨¡æ‹ŸAPIå“åº”æ•°æ®
api_response_data = '''
{
    "status": "success",
    "data": [
        {"date": "2025-01-01", "temperature": 15.2, "humidity": 65},
        {"date": "2025-01-02", "temperature": 18.5, "humidity": 70},
        {"date": "2025-01-03", "temperature": 12.8, "humidity": 80},
        {"date": "2025-01-04", "temperature": 20.1, "humidity": 55}
    ]
}
'''

# è§£æAPIæ•°æ®
import json
api_data = json.loads(api_response_data)
df_weather = pd.DataFrame(api_data['data'])
df_weather['date'] = pd.to_datetime(df_weather['date'])

print("ğŸŒ¤ï¸ å¤©æ°”æ•°æ®ï¼ˆæ¥è‡ªAPIï¼‰ï¼š")
print(df_weather)
print()

# === ç½‘ç»œCSVè¯»å–ç¤ºä¾‹ ===
# æ¨¡æ‹Ÿç½‘ç»œCSVæ•°æ®
network_csv = '''
è‚¡ç¥¨ä»£ç ,è‚¡ç¥¨åç§°,å½“å‰ä»·æ ¼,æ¶¨è·Œå¹…
000001,å¹³å®‰é“¶è¡Œ,12.50,+2.3%
000002,ä¸‡ç§‘A,18.20,-1.5%
600036,æ‹›å•†é“¶è¡Œ,45.80,+0.8%
600519,è´µå·èŒ…å°,1680.00,+1.2%
'''

df_stocks = pd.read_csv(StringIO(network_csv))
print("ğŸ“ˆ è‚¡ç¥¨æ•°æ®ï¼ˆç½‘ç»œCSVï¼‰ï¼š")
print(df_stocks)
print()

print("ç½‘ç»œæ•°æ®è·å–ä»£ç ç¤ºä¾‹ï¼š")
print("""
# ä»ç½‘ç»œURLè¯»å–CSV
df = pd.read_csv('https://example.com/data.csv')

# ä½¿ç”¨requestsè·å–APIæ•°æ®
import requests
response = requests.get('https://api.example.com/data')
data = response.json()
df = pd.DataFrame(data)

# å¸¦è®¤è¯çš„APIè¯·æ±‚
headers = {'Authorization': 'Bearer your_token'}
response = requests.get('https://api.example.com/data', headers=headers)
""")
```

### 15.2.4 æ•°æ®å¯¼å‡ºï¼šæˆå“å‡ºè´§ç³»ç»Ÿ

```python
# === æ•°æ®å¯¼å‡ºç¤ºä¾‹ ===

# å‡†å¤‡ç¤ºä¾‹æ•°æ®
export_data = {
    'äº§å“åç§°': ['äº§å“A', 'äº§å“B', 'äº§å“C', 'äº§å“D'],
    'é”€é‡': [100, 150, 80, 200],
    'æ”¶å…¥': [50000, 75000, 40000, 100000],
    'åˆ©æ¶¦ç‡': [0.2, 0.25, 0.15, 0.3]
}
df_export = pd.DataFrame(export_data)

print("ğŸ“¦ å‡†å¤‡å¯¼å‡ºçš„æ•°æ®ï¼š")
print(df_export)
print()

# === CSVå¯¼å‡º ===
# df_export.to_csv('products.csv', index=False, encoding='utf-8')
print("CSVå¯¼å‡ºä»£ç ï¼š")
print("df.to_csv('products.csv', index=False, encoding='utf-8')")
print()

# === Excelå¯¼å‡º ===
# df_export.to_excel('products.xlsx', index=False, sheet_name='äº§å“æ•°æ®')
print("Excelå¯¼å‡ºä»£ç ï¼š")
print("df.to_excel('products.xlsx', index=False, sheet_name='äº§å“æ•°æ®')")
print()

# === JSONå¯¼å‡º ===
json_output = df_export.to_json(orient='records', force_ascii=False, indent=2)
print("JSONå¯¼å‡ºç»“æœï¼š")
print(json_output)
print()

# === æ•°æ®åº“å¯¼å‡º ===
print("æ•°æ®åº“å¯¼å‡ºä»£ç ç¤ºä¾‹ï¼š")
print("""
# å¯¼å‡ºåˆ°SQLite
df.to_sql('table_name', conn, if_exists='replace', index=False)

# å¯¼å‡ºåˆ°MySQL
from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://user:password@host/db')
df.to_sql('table_name', engine, if_exists='append', index=False)
""")
```

### ğŸ¯ å®è·µé¡¹ç›®ï¼šå¤šæºæ•°æ®æ•´åˆåˆ†æå™¨

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šç§æ•°æ®æºçš„æ•´åˆåˆ†æå™¨ï¼š

```python
class MultiSourceDataAnalyzer:
    """å¤šæºæ•°æ®æ•´åˆåˆ†æå™¨
    
    æ¨¡æ‹Ÿæ•°æ®å·¥å‚çš„å®Œæ•´ä¾›åº”é“¾ç®¡ç†ï¼š
    - å¤šæ¸ é“é‡‡è´­ï¼šä»ä¸åŒæ•°æ®æºè·å–æ•°æ®
    - è´¨é‡æ£€éªŒï¼šæ•°æ®æ ¼å¼éªŒè¯å’Œæ¸…æ´—
    - ç»Ÿä¸€åŠ å·¥ï¼šæ•°æ®æ ‡å‡†åŒ–å¤„ç†
    - ç»¼åˆåˆ†æï¼šè·¨æ•°æ®æºçš„å…³è”åˆ†æ
    - æŠ¥å‘Šè¾“å‡ºï¼šå¤šæ ¼å¼ç»“æœå¯¼å‡º
    """
    
    def __init__(self):
        self.data_sources = {}
        self.processed_data = {}
        
    def load_csv_data(self, name, csv_content, **kwargs):
        """åŠ è½½CSVæ•°æ®"""
        try:
            df = pd.read_csv(StringIO(csv_content.strip()), **kwargs)
            self.data_sources[name] = {
                'data': df,
                'source_type': 'CSV',
                'load_time': pd.Timestamp.now()
            }
            print(f"âœ… CSVæ•°æ®æº '{name}' åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"âŒ CSVæ•°æ®æº '{name}' åŠ è½½å¤±è´¥: {e}")
            return False
            
    def load_json_data(self, name, json_content):
        """åŠ è½½JSONæ•°æ®"""
        try:
            data = json.loads(json_content)
            df = pd.DataFrame(data)
            self.data_sources[name] = {
                'data': df,
                'source_type': 'JSON',
                'load_time': pd.Timestamp.now()
            }
            print(f"âœ… JSONæ•°æ®æº '{name}' åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"âŒ JSONæ•°æ®æº '{name}' åŠ è½½å¤±è´¥: {e}")
            return False
            
    def load_dict_data(self, name, dict_data):
        """åŠ è½½å­—å…¸æ•°æ®"""
        try:
            df = pd.DataFrame(dict_data)
            self.data_sources[name] = {
                'data': df,
                'source_type': 'Dictionary',
                'load_time': pd.Timestamp.now()
            }
            print(f"âœ… å­—å…¸æ•°æ®æº '{name}' åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"âŒ å­—å…¸æ•°æ®æº '{name}' åŠ è½½å¤±è´¥: {e}")
            return False
            
    def show_data_sources(self):
        """æ˜¾ç¤ºæ‰€æœ‰æ•°æ®æºä¿¡æ¯"""
        print("\nğŸ“Š æ•°æ®æºæ¦‚è§ˆï¼š")
        print("-" * 60)
        
        for name, info in self.data_sources.items():
            df = info['data']
            print(f"æ•°æ®æº: {name}")
            print(f"  ç±»å‹: {info['source_type']}")
            print(f"  å½¢çŠ¶: {df.shape}")
            print(f"  åˆ—å: {list(df.columns)}")
            print(f"  åŠ è½½æ—¶é—´: {info['load_time']}")
            print()
            
    def merge_data_sources(self, source1, source2, on_column, how='inner', result_name=None):
        """åˆå¹¶ä¸¤ä¸ªæ•°æ®æº"""
        if source1 not in self.data_sources or source2 not in self.data_sources:
            print("âŒ æŒ‡å®šçš„æ•°æ®æºä¸å­˜åœ¨")
            return False
            
        try:
            df1 = self.data_sources[source1]['data']
            df2 = self.data_sources[source2]['data']
            
            merged_df = pd.merge(df1, df2, on=on_column, how=how)
            
            if result_name is None:
                result_name = f"{source1}_{source2}_merged"
                
            self.processed_data[result_name] = merged_df
            print(f"âœ… æ•°æ®æºåˆå¹¶æˆåŠŸï¼Œç»“æœä¿å­˜ä¸º '{result_name}'ï¼Œå…± {len(merged_df)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®æºåˆå¹¶å¤±è´¥: {e}")
            return False
            
    def analyze_data(self, data_name):
        """åˆ†ææŒ‡å®šæ•°æ®"""
        if data_name in self.data_sources:
            df = self.data_sources[data_name]['data']
        elif data_name in self.processed_data:
            df = self.processed_data[data_name]
        else:
            print(f"âŒ æ•°æ® '{data_name}' ä¸å­˜åœ¨")
            return
            
        print(f"\nğŸ“ˆ æ•°æ®åˆ†ææŠ¥å‘Šï¼š{data_name}")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")
        print()
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print("ğŸ“Š æ•°å€¼åˆ—æè¿°æ€§ç»Ÿè®¡:")
            print(df[numeric_cols].describe())
            print()
            
        # æ–‡æœ¬åˆ—ç»Ÿè®¡
        text_cols = df.select_dtypes(include=['object']).columns
        if len(text_cols) > 0:
            print("ğŸ“ æ–‡æœ¬åˆ—ä¿¡æ¯:")
            for col in text_cols:
                unique_count = df[col].nunique()
                print(f"  {col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
            print()
            
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_data = df.isnull().sum()
        if missing_data.sum() > 0:
            print("âš ï¸ ç¼ºå¤±å€¼æƒ…å†µ:")
            print(missing_data[missing_data > 0])
        else:
            print("âœ… æ— ç¼ºå¤±å€¼")
            
    def export_data(self, data_name, format_type='csv', filename=None):
        """å¯¼å‡ºæ•°æ®"""
        if data_name in self.data_sources:
            df = self.data_sources[data_name]['data']
        elif data_name in self.processed_data:
            df = self.processed_data[data_name]
        else:
            print(f"âŒ æ•°æ® '{data_name}' ä¸å­˜åœ¨")
            return False
            
        if filename is None:
            filename = f"{data_name}_export"
            
        try:
            if format_type.lower() == 'csv':
                csv_output = df.to_csv(index=False)
                print(f"âœ… CSVæ ¼å¼å¯¼å‡ºæˆåŠŸ")
                print(f"æ–‡ä»¶å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰:")
                print(csv_output[:200] + "..." if len(csv_output) > 200 else csv_output)
                
            elif format_type.lower() == 'json':
                json_output = df.to_json(orient='records', force_ascii=False, indent=2)
                print(f"âœ… JSONæ ¼å¼å¯¼å‡ºæˆåŠŸ")
                print(f"æ–‡ä»¶å†…å®¹é¢„è§ˆï¼ˆå‰300å­—ç¬¦ï¼‰:")
                print(json_output[:300] + "..." if len(json_output) > 300 else json_output)
                
            elif format_type.lower() == 'excel':
                print(f"âœ… Excelæ ¼å¼å¯¼å‡ºå‡†å¤‡å®Œæˆ")
                print("å®é™…ä½¿ç”¨æ—¶çš„ä»£ç :")
                print(f"df.to_excel('{filename}.xlsx', index=False)")
                
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return False

# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # åˆ›å»ºå¤šæºæ•°æ®åˆ†æå™¨
    analyzer = MultiSourceDataAnalyzer()
    
    print("ğŸ­ å¤šæºæ•°æ®æ•´åˆåˆ†æå™¨å¯åŠ¨")
    print("=" * 50)
    
    # æ•°æ®æº1ï¼šå‘˜å·¥ä¿¡æ¯ï¼ˆCSVæ ¼å¼ï¼‰
    employee_csv = """
å‘˜å·¥ID,å§“å,éƒ¨é—¨,è–ªèµ„
E001,å¼ ä¸‰,æŠ€æœ¯éƒ¨,15000
E002,æå››,å¸‚åœºéƒ¨,12000
E003,ç‹äº”,æŠ€æœ¯éƒ¨,18000
E004,èµµå…­,äººäº‹éƒ¨,10000
E005,é’±ä¸ƒ,é”€å”®éƒ¨,14000
"""
    
    # æ•°æ®æº2ï¼šéƒ¨é—¨ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰
    department_json = """
[
    {"éƒ¨é—¨": "æŠ€æœ¯éƒ¨", "è´Ÿè´£äºº": "å¼ ç»ç†", "é¢„ç®—": 500000},
    {"éƒ¨é—¨": "å¸‚åœºéƒ¨", "è´Ÿè´£äºº": "æç»ç†", "é¢„ç®—": 300000},
    {"éƒ¨é—¨": "äººäº‹éƒ¨", "è´Ÿè´£äºº": "ç‹ç»ç†", "é¢„ç®—": 200000},
    {"éƒ¨é—¨": "é”€å”®éƒ¨", "è´Ÿè´£äºº": "èµµç»ç†", "é¢„ç®—": 400000}
]
"""
    
    # æ•°æ®æº3ï¼šç»©æ•ˆæ•°æ®ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    performance_data = {
        'å‘˜å·¥ID': ['E001', 'E002', 'E003', 'E004', 'E005'],
        'ç»©æ•ˆè¯„åˆ†': [85, 78, 92, 88, 80],
        'é¡¹ç›®æ•°é‡': [3, 2, 4, 2, 3],
        'å®¢æˆ·æ»¡æ„åº¦': [4.5, 4.2, 4.8, 4.3, 4.1]
    }
    
    # åŠ è½½æ•°æ®æº
    analyzer.load_csv_data('å‘˜å·¥ä¿¡æ¯', employee_csv)
    analyzer.load_json_data('éƒ¨é—¨ä¿¡æ¯', department_json)
    analyzer.load_dict_data('ç»©æ•ˆæ•°æ®', performance_data)
    
    # æ˜¾ç¤ºæ•°æ®æºæ¦‚è§ˆ
    analyzer.show_data_sources()
    
    # åˆå¹¶å‘˜å·¥ä¿¡æ¯å’Œç»©æ•ˆæ•°æ®
    analyzer.merge_data_sources('å‘˜å·¥ä¿¡æ¯', 'ç»©æ•ˆæ•°æ®', 'å‘˜å·¥ID', result_name='å‘˜å·¥ç»©æ•ˆç»¼åˆ')
    
    # åˆ†æç»¼åˆæ•°æ®
    analyzer.analyze_data('å‘˜å·¥ç»©æ•ˆç»¼åˆ')
    
    # å¯¼å‡ºæ•°æ®
    print("\nğŸ“¦ æ•°æ®å¯¼å‡ºæ¼”ç¤ºï¼š")
    analyzer.export_data('å‘˜å·¥ç»©æ•ˆç»¼åˆ', 'csv')
    
    print("\nğŸ‰ å¤šæºæ•°æ®æ•´åˆåˆ†æå®Œæˆï¼")
```

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

1. **æ•°æ®è¯»å–**ï¼š
   - `pd.read_csv()` - CSVæ–‡ä»¶è¯»å–
   - `pd.read_excel()` - Excelæ–‡ä»¶è¯»å–
   - `pd.read_json()` - JSONæ•°æ®è¯»å–
   - `pd.read_sql()` - æ•°æ®åº“æŸ¥è¯¢

2. **æ•°æ®å¯¼å‡º**ï¼š
   - `df.to_csv()` - å¯¼å‡ºCSV
   - `df.to_excel()` - å¯¼å‡ºExcel
   - `df.to_json()` - å¯¼å‡ºJSON
   - `df.to_sql()` - å†™å…¥æ•°æ®åº“

3. **å‚æ•°é…ç½®**ï¼š
   - ç¼–ç è®¾ç½®ï¼ˆencodingï¼‰
   - åˆ†éš”ç¬¦æŒ‡å®šï¼ˆsepï¼‰
   - æ•°æ®ç±»å‹æ§åˆ¶ï¼ˆdtypeï¼‰
   - ç´¢å¼•å¤„ç†ï¼ˆindexï¼‰

4. **é”™è¯¯å¤„ç†**ï¼š
   - ä½¿ç”¨try-exceptå¤„ç†å¼‚å¸¸
   - éªŒè¯æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
   - æä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯

é€šè¿‡"è¿›è´§å‡ºè´§ç³»ç»Ÿ"çš„æ¯”å–»ï¼Œæˆ‘ä»¬æŒæ¡äº†Pandasçš„æ•°æ®è¾“å…¥è¾“å‡ºåŠŸèƒ½ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•å¯¹è¿™äº›"åŸæ–™"è¿›è¡Œè´¨é‡æ£€éªŒå’Œæ¸…æ´—ã€‚

---

## 15.3 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†

### ğŸ” æ•°æ®å·¥å‚çš„è´¨æ£€éƒ¨é—¨

åœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œä»ä¾›åº”å•†é‡‡è´­çš„åŸæ–™å¾€å¾€å­˜åœ¨å„ç§è´¨é‡é—®é¢˜ï¼šæœ‰äº›åŸæ–™å¯èƒ½ç¼ºå¤±ã€æœ‰äº›å¯èƒ½é‡å¤ã€æœ‰äº›è§„æ ¼ä¸æ ‡å‡†ã€‚æ•°æ®ä¹Ÿæ˜¯å¦‚æ­¤â€”â€”çœŸå®çš„æ•°æ®æ€»æ˜¯"è„"çš„ï¼Œéœ€è¦ç»è¿‡ä¸¥æ ¼çš„è´¨æ£€å’Œæ¸…æ´—æ‰èƒ½æŠ•å…¥ç”Ÿäº§ã€‚

æ•°æ®æ¸…æ´—å°±åƒå·¥å‚çš„è´¨æ£€éƒ¨é—¨ï¼Œè´Ÿè´£ï¼š
- **è´¨é‡æ£€éªŒ**ï¼šå‘ç°æ•°æ®ä¸­çš„é—®é¢˜
- **æ ‡å‡†åŒ–åŠ å·¥**ï¼šç»Ÿä¸€æ•°æ®æ ¼å¼å’Œè§„æ ¼
- **ç¼ºé™·ä¿®å¤**ï¼šå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
- **å»é‡å¤„ç†**ï¼šåˆ é™¤é‡å¤çš„æ•°æ®è®°å½•

### 15.3.1 ç¼ºå¤±å€¼å¤„ç†ï¼šä¿®å¤ç¼ºé™·åŸæ–™

ç¼ºå¤±å€¼æ˜¯æ•°æ®åˆ†æä¸­æœ€å¸¸è§çš„é—®é¢˜ï¼Œå°±åƒå·¥å‚æ”¶åˆ°äº†ä¸å®Œæ•´çš„åŸæ–™ã€‚

```python
# === åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„ç¤ºä¾‹æ•°æ® ===
import pandas as pd
import numpy as np

# æ¨¡æ‹ŸçœŸå®çš„å­¦ç”Ÿæˆç»©æ•°æ®ï¼ˆåŒ…å«ç¼ºå¤±å€¼ï¼‰
student_data = {
    'å­¦å·': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', None, 'é’±ä¸ƒ', 'å­™å…«', 'å‘¨ä¹', 'å´å'],
    'å¹´é¾„': [18, 19, None, 20, 18, 19, None, 18],
    'æ€§åˆ«': ['ç”·', 'å¥³', 'ç”·', 'å¥³', None, 'å¥³', 'ç”·', 'å¥³'],
    'æ•°å­¦': [85, 92, 78, None, 88, 83, 90, 87],
    'è¯­æ–‡': [88, None, 92, 89, 87, 91, 84, None],
    'è‹±è¯­': [82, 94, 85, 93, None, 87, 88, 92],
    'æ€»åˆ†': [255, None, 255, None, None, 261, 262, None]
}

df_students = pd.DataFrame(student_data)
print("ğŸ“Š åŸå§‹å­¦ç”Ÿæ•°æ®ï¼ˆå«ç¼ºå¤±å€¼ï¼‰ï¼š")
print(df_students)
print()

# === ç¼ºå¤±å€¼æ£€æµ‹ ===
print("ğŸ” ç¼ºå¤±å€¼æ£€æµ‹æŠ¥å‘Šï¼š")
print("-" * 40)

# æ£€æŸ¥æ¯åˆ—çš„ç¼ºå¤±å€¼æ•°é‡
missing_count = df_students.isnull().sum()
print("å„åˆ—ç¼ºå¤±å€¼æ•°é‡ï¼š")
print(missing_count)
print()

# è®¡ç®—ç¼ºå¤±å€¼æ¯”ä¾‹
missing_ratio = df_students.isnull().sum() / len(df_students) * 100
print("å„åˆ—ç¼ºå¤±å€¼æ¯”ä¾‹ï¼š")
for col, ratio in missing_ratio.items():
    if ratio > 0:
        print(f"  {col}: {ratio:.1f}%")
print()

# æ£€æŸ¥æ¯è¡Œçš„ç¼ºå¤±å€¼æƒ…å†µ
row_missing = df_students.isnull().sum(axis=1)
print("æ¯è¡Œç¼ºå¤±å€¼æ•°é‡ï¼š")
for idx, count in enumerate(row_missing):
    if count > 0:
        print(f"  ç¬¬{idx}è¡Œ: {count}ä¸ªç¼ºå¤±å€¼")
print()

# === ç¼ºå¤±å€¼å¯è§†åŒ–åˆ†æ ===
print("ğŸ“ˆ ç¼ºå¤±å€¼æ¨¡å¼åˆ†æï¼š")
# æ˜¾ç¤ºç¼ºå¤±å€¼çš„ä½ç½®æ¨¡å¼
print("ç¼ºå¤±å€¼ä½ç½®å›¾ï¼ˆTrueè¡¨ç¤ºç¼ºå¤±ï¼‰ï¼š")
print(df_students.isnull())
print()
```

#### ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥

```python
# === ç­–ç•¥1ï¼šåˆ é™¤ç¼ºå¤±å€¼ ===
print("=== ç­–ç•¥1ï¼šåˆ é™¤ç¼ºå¤±å€¼ ===")

# åˆ é™¤åŒ…å«ä»»ä½•ç¼ºå¤±å€¼çš„è¡Œ
df_drop_any = df_students.dropna()
print(f"åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œåï¼š{len(df_drop_any)}è¡Œï¼ˆåŸå§‹ï¼š{len(df_students)}è¡Œï¼‰")
print(df_drop_any)
print()

# åˆ é™¤æ‰€æœ‰å€¼éƒ½ç¼ºå¤±çš„è¡Œ
df_drop_all = df_students.dropna(how='all')
print(f"åˆ é™¤å…¨éƒ¨ç¼ºå¤±çš„è¡Œåï¼š{len(df_drop_all)}è¡Œ")

# åˆ é™¤ç‰¹å®šåˆ—çš„ç¼ºå¤±å€¼
df_drop_name = df_students.dropna(subset=['å§“å'])
print(f"åˆ é™¤å§“åç¼ºå¤±çš„è¡Œåï¼š{len(df_drop_name)}è¡Œ")
print()

# === ç­–ç•¥2ï¼šå¡«å……ç¼ºå¤±å€¼ ===
print("=== ç­–ç•¥2ï¼šå¡«å……ç¼ºå¤±å€¼ ===")

# åˆ›å»ºæ•°æ®å‰¯æœ¬ç”¨äºå¡«å……
df_filled = df_students.copy()

# ç”¨å›ºå®šå€¼å¡«å……
df_filled['å§“å'] = df_filled['å§“å'].fillna('æœªçŸ¥å­¦ç”Ÿ')
print("å§“ååˆ—å¡«å……åï¼š")
print(df_filled['å§“å'])
print()

# ç”¨å‡å€¼å¡«å……æ•°å€¼åˆ—
age_mean = df_filled['å¹´é¾„'].mean()
df_filled['å¹´é¾„'] = df_filled['å¹´é¾„'].fillna(age_mean)
print(f"å¹´é¾„åˆ—ç”¨å‡å€¼({age_mean:.1f})å¡«å……åï¼š")
print(df_filled['å¹´é¾„'])
print()

# ç”¨ä¼—æ•°å¡«å……åˆ†ç±»åˆ—
gender_mode = df_filled['æ€§åˆ«'].mode()[0]  # è·å–ä¼—æ•°
df_filled['æ€§åˆ«'] = df_filled['æ€§åˆ«'].fillna(gender_mode)
print(f"æ€§åˆ«åˆ—ç”¨ä¼—æ•°({gender_mode})å¡«å……åï¼š")
print(df_filled['æ€§åˆ«'])
print()

# ç”¨å‰ä¸€ä¸ªå€¼å¡«å……ï¼ˆå‰å‘å¡«å……ï¼‰
df_filled['æ•°å­¦'] = df_filled['æ•°å­¦'].fillna(method='ffill')
print("æ•°å­¦æˆç»©å‰å‘å¡«å……åï¼š")
print(df_filled['æ•°å­¦'])
print()

# ç”¨åä¸€ä¸ªå€¼å¡«å……ï¼ˆåå‘å¡«å……ï¼‰
df_filled['è¯­æ–‡'] = df_filled['è¯­æ–‡'].fillna(method='bfill')
print("è¯­æ–‡æˆç»©åå‘å¡«å……åï¼š")
print(df_filled['è¯­æ–‡'])
print()

# === ç­–ç•¥3ï¼šæ’å€¼å¡«å…… ===
print("=== ç­–ç•¥3ï¼šæ’å€¼å¡«å…… ===")

# çº¿æ€§æ’å€¼
df_interpolated = df_students.copy()
df_interpolated['è‹±è¯­'] = df_interpolated['è‹±è¯­'].interpolate()
print("è‹±è¯­æˆç»©çº¿æ€§æ’å€¼åï¼š")
print(df_interpolated['è‹±è¯­'])
print()

# æ˜¾ç¤ºæœ€ç»ˆæ¸…æ´—ç»“æœ
print("=== æœ€ç»ˆæ¸…æ´—ç»“æœ ===")
print(df_filled)
```

### 15.3.2 é‡å¤æ•°æ®å¤„ç†ï¼šå»é™¤å†—ä½™åŸæ–™

```python
# === åˆ›å»ºåŒ…å«é‡å¤æ•°æ®çš„ç¤ºä¾‹ ===
duplicate_data = {
    'è®¢å•ID': ['O001', 'O002', 'O003', 'O002', 'O004', 'O003', 'O005'],
    'å®¢æˆ·å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'æå››', 'èµµå…­', 'ç‹äº”', 'é’±ä¸ƒ'],
    'äº§å“': ['ç¬”è®°æœ¬', 'æ‰‹æœº', 'å¹³æ¿', 'æ‰‹æœº', 'è€³æœº', 'å¹³æ¿', 'é¼ æ ‡'],
    'æ•°é‡': [1, 2, 1, 2, 3, 1, 1],
    'é‡‘é¢': [5000, 3000, 2000, 3000, 200, 2000, 50]
}

df_orders = pd.DataFrame(duplicate_data)
print("ğŸ“¦ è®¢å•æ•°æ®ï¼ˆå«é‡å¤ï¼‰ï¼š")
print(df_orders)
print()

# === é‡å¤æ•°æ®æ£€æµ‹ ===
print("ğŸ” é‡å¤æ•°æ®æ£€æµ‹ï¼š")

# æ£€æŸ¥å®Œå…¨é‡å¤çš„è¡Œ
duplicated_rows = df_orders.duplicated()
print("å®Œå…¨é‡å¤çš„è¡Œï¼š")
print(duplicated_rows)
print(f"é‡å¤è¡Œæ•°é‡ï¼š{duplicated_rows.sum()}")
print()

# æŸ¥çœ‹é‡å¤çš„è¡Œ
print("é‡å¤çš„è¡Œå†…å®¹ï¼š")
print(df_orders[duplicated_rows])
print()

# åŸºäºç‰¹å®šåˆ—æ£€æŸ¥é‡å¤
duplicated_orders = df_orders.duplicated(subset=['è®¢å•ID'])
print("åŸºäºè®¢å•IDçš„é‡å¤ï¼š")
print(duplicated_orders)
print(f"é‡å¤è®¢å•æ•°é‡ï¼š{duplicated_orders.sum()}")
print()

# === é‡å¤æ•°æ®å¤„ç† ===
print("=== é‡å¤æ•°æ®å¤„ç† ===")

# åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
df_no_duplicates = df_orders.drop_duplicates()
print("åˆ é™¤å®Œå…¨é‡å¤è¡Œåï¼š")
print(df_no_duplicates)
print()

# åŸºäºç‰¹å®šåˆ—åˆ é™¤é‡å¤ï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰
df_unique_orders = df_orders.drop_duplicates(subset=['è®¢å•ID'], keep='last')
print("åŸºäºè®¢å•IDå»é‡ï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰ï¼š")
print(df_unique_orders)
print()

# æ ‡è®°é‡å¤æ•°æ®è€Œä¸åˆ é™¤
df_orders['æ˜¯å¦é‡å¤'] = df_orders.duplicated(subset=['è®¢å•ID'])
print("æ ‡è®°é‡å¤æ•°æ®ï¼š")
print(df_orders)
```

### 15.3.3 æ•°æ®ç±»å‹è½¬æ¢ï¼šè§„æ ¼æ ‡å‡†åŒ–

```python
# === æ•°æ®ç±»å‹é—®é¢˜ç¤ºä¾‹ ===
messy_data = {
    'å‘˜å·¥ID': ['E001', 'E002', 'E003', 'E004'],
    'å…¥èŒæ—¥æœŸ': ['2020-01-15', '2019/07/22', '2021.03.10', '20220815'],
    'è–ªèµ„': ['15000', '18000.5', '12,000', 'Â¥20000'],
    'å¹´é¾„': ['28', '32.0', '25', '29'],
    'æ˜¯å¦åœ¨èŒ': ['æ˜¯', 'True', '1', 'yes']
}

df_messy = pd.DataFrame(messy_data)
print("ğŸ“Š åŸå§‹æ•°æ®ï¼ˆç±»å‹æ··ä¹±ï¼‰ï¼š")
print(df_messy)
print()
print("æ•°æ®ç±»å‹ï¼š")
print(df_messy.dtypes)
print()

# === æ—¥æœŸç±»å‹è½¬æ¢ ===
print("=== æ—¥æœŸç±»å‹è½¬æ¢ ===")

def clean_date(date_str):
    """æ¸…æ´—æ—¥æœŸå­—ç¬¦ä¸²"""
    # æ›¿æ¢å„ç§åˆ†éš”ç¬¦ä¸ºæ ‡å‡†æ ¼å¼
    date_str = str(date_str).replace('/', '-').replace('.', '-')
    
    # å¤„ç†æ²¡æœ‰åˆ†éš”ç¬¦çš„æ—¥æœŸ
    if len(date_str) == 8 and date_str.isdigit():
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    
    return date_str

# æ¸…æ´—æ—¥æœŸæ•°æ®
df_messy['å…¥èŒæ—¥æœŸ_æ¸…æ´—'] = df_messy['å…¥èŒæ—¥æœŸ'].apply(clean_date)
df_messy['å…¥èŒæ—¥æœŸ_æ ‡å‡†'] = pd.to_datetime(df_messy['å…¥èŒæ—¥æœŸ_æ¸…æ´—'])

print("æ—¥æœŸè½¬æ¢ç»“æœï¼š")
print(df_messy[['å…¥èŒæ—¥æœŸ', 'å…¥èŒæ—¥æœŸ_æ¸…æ´—', 'å…¥èŒæ—¥æœŸ_æ ‡å‡†']])
print()

# === æ•°å€¼ç±»å‹è½¬æ¢ ===
print("=== æ•°å€¼ç±»å‹è½¬æ¢ ===")

def clean_salary(salary_str):
    """æ¸…æ´—è–ªèµ„æ•°æ®"""
    # ç§»é™¤è´§å¸ç¬¦å·ã€é€—å·ç­‰
    salary_str = str(salary_str).replace('Â¥', '').replace(',', '').replace('$', '')
    return float(salary_str)

# æ¸…æ´—è–ªèµ„æ•°æ®
df_messy['è–ªèµ„_æ•°å€¼'] = df_messy['è–ªèµ„'].apply(clean_salary)
df_messy['å¹´é¾„_æ•´æ•°'] = df_messy['å¹´é¾„'].astype(int)

print("æ•°å€¼è½¬æ¢ç»“æœï¼š")
print(df_messy[['è–ªèµ„', 'è–ªèµ„_æ•°å€¼', 'å¹´é¾„', 'å¹´é¾„_æ•´æ•°']])
print()

# === å¸ƒå°”ç±»å‹è½¬æ¢ ===
print("=== å¸ƒå°”ç±»å‹è½¬æ¢ ===")

def clean_boolean(bool_str):
    """æ¸…æ´—å¸ƒå°”æ•°æ®"""
    bool_str = str(bool_str).lower()
    if bool_str in ['æ˜¯', 'true', '1', 'yes', 'y']:
        return True
    elif bool_str in ['å¦', 'false', '0', 'no', 'n']:
        return False
    else:
        return None

# æ¸…æ´—å¸ƒå°”æ•°æ®
df_messy['åœ¨èŒçŠ¶æ€'] = df_messy['æ˜¯å¦åœ¨èŒ'].apply(clean_boolean)

print("å¸ƒå°”è½¬æ¢ç»“æœï¼š")
print(df_messy[['æ˜¯å¦åœ¨èŒ', 'åœ¨èŒçŠ¶æ€']])
print()

print("æœ€ç»ˆæ¸…æ´—åçš„æ•°æ®ç±»å‹ï¼š")
print(df_messy[['å…¥èŒæ—¥æœŸ_æ ‡å‡†', 'è–ªèµ„_æ•°å€¼', 'å¹´é¾„_æ•´æ•°', 'åœ¨èŒçŠ¶æ€']].dtypes)
```

### 15.3.4 å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†

```python
# === åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„æ•°æ® ===
np.random.seed(42)  # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡å¤

# æ­£å¸¸çš„é”€å”®æ•°æ®
normal_sales = np.random.normal(10000, 2000, 100)  # å‡å€¼10000ï¼Œæ ‡å‡†å·®2000
# æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
outliers = [50000, -5000, 80000, 100000]
all_sales = np.concatenate([normal_sales, outliers])

sales_data = {
    'æ—¥æœŸ': pd.date_range('2025-01-01', periods=len(all_sales), freq='D'),
    'é”€å”®é¢': all_sales,
    'é”€å”®å‘˜': [f'å‘˜å·¥{i%10+1}' for i in range(len(all_sales))]
}

df_sales = pd.DataFrame(sales_data)
print("ğŸ“ˆ é”€å”®æ•°æ®ï¼ˆå«å¼‚å¸¸å€¼ï¼‰ï¼š")
print(df_sales.tail(10))  # æ˜¾ç¤ºæœ€å10è¡Œï¼ŒåŒ…å«å¼‚å¸¸å€¼
print()

# === å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³• ===
print("=== å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³• ===")

# æ–¹æ³•1ï¼š3ÏƒåŸåˆ™ï¼ˆä¸‰å€æ ‡å‡†å·®ï¼‰
mean_sales = df_sales['é”€å”®é¢'].mean()
std_sales = df_sales['é”€å”®é¢'].std()
upper_bound = mean_sales + 3 * std_sales
lower_bound = mean_sales - 3 * std_sales

print(f"3ÏƒåŸåˆ™è¾¹ç•Œï¼š")
print(f"  å‡å€¼: {mean_sales:.2f}")
print(f"  æ ‡å‡†å·®: {std_sales:.2f}")
print(f"  ä¸Šç•Œ: {upper_bound:.2f}")
print(f"  ä¸‹ç•Œ: {lower_bound:.2f}")

outliers_3sigma = df_sales[
    (df_sales['é”€å”®é¢'] > upper_bound) | 
    (df_sales['é”€å”®é¢'] < lower_bound)
]
print(f"3ÏƒåŸåˆ™æ£€æµ‹åˆ° {len(outliers_3sigma)} ä¸ªå¼‚å¸¸å€¼")
print(outliers_3sigma)
print()

# æ–¹æ³•2ï¼šIQRï¼ˆå››åˆ†ä½è·ï¼‰æ–¹æ³•
Q1 = df_sales['é”€å”®é¢'].quantile(0.25)
Q3 = df_sales['é”€å”®é¢'].quantile(0.75)
IQR = Q3 - Q1
lower_fence = Q1 - 1.5 * IQR
upper_fence = Q3 + 1.5 * IQR

print(f"IQRæ–¹æ³•è¾¹ç•Œï¼š")
print(f"  Q1: {Q1:.2f}")
print(f"  Q3: {Q3:.2f}")
print(f"  IQR: {IQR:.2f}")
print(f"  ä¸‹æ …æ : {lower_fence:.2f}")
print(f"  ä¸Šæ …æ : {upper_fence:.2f}")

outliers_iqr = df_sales[
    (df_sales['é”€å”®é¢'] > upper_fence) | 
    (df_sales['é”€å”®é¢'] < lower_fence)
]
print(f"IQRæ–¹æ³•æ£€æµ‹åˆ° {len(outliers_iqr)} ä¸ªå¼‚å¸¸å€¼")
print(outliers_iqr)
print()

# === å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥ ===
print("=== å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥ ===")

# ç­–ç•¥1ï¼šåˆ é™¤å¼‚å¸¸å€¼
df_no_outliers = df_sales[
    (df_sales['é”€å”®é¢'] >= lower_fence) & 
    (df_sales['é”€å”®é¢'] <= upper_fence)
]
print(f"åˆ é™¤å¼‚å¸¸å€¼åï¼š{len(df_no_outliers)}è¡Œï¼ˆåŸå§‹ï¼š{len(df_sales)}è¡Œï¼‰")

# ç­–ç•¥2ï¼šç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
df_capped = df_sales.copy()
df_capped['é”€å”®é¢'] = df_capped['é”€å”®é¢'].clip(lower=lower_fence, upper=upper_fence)
print("å¼‚å¸¸å€¼æ›¿æ¢ä¸ºè¾¹ç•Œå€¼åçš„ç»Ÿè®¡ï¼š")
print(df_capped['é”€å”®é¢'].describe())
print()

# ç­–ç•¥3ï¼šç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼
df_median_filled = df_sales.copy()
median_sales = df_sales['é”€å”®é¢'].median()
is_outlier = (df_sales['é”€å”®é¢'] > upper_fence) | (df_sales['é”€å”®é¢'] < lower_fence)
df_median_filled.loc[is_outlier, 'é”€å”®é¢'] = median_sales
print(f"ç”¨ä¸­ä½æ•°({median_sales:.2f})æ›¿æ¢å¼‚å¸¸å€¼åçš„ç»Ÿè®¡ï¼š")
print(df_median_filled['é”€å”®é¢'].describe())
```

### ğŸ¯ å®è·µé¡¹ç›®ï¼šæ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…·

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç»¼åˆçš„æ•°æ®æ¸…æ´—å·¥å…·ï¼š

```python
class SmartDataCleaner:
    """æ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…·
    
    æ¨¡æ‹Ÿæ•°æ®å·¥å‚çš„å…¨è‡ªåŠ¨è´¨æ£€æµæ°´çº¿ï¼š
    - è‡ªåŠ¨æ£€æµ‹æ•°æ®è´¨é‡é—®é¢˜
    - æä¾›å¤šç§æ¸…æ´—ç­–ç•¥é€‰æ‹©
    - ç”Ÿæˆè¯¦ç»†çš„æ¸…æ´—æŠ¥å‘Š
    - ä¿ç•™æ¸…æ´—å†å²è®°å½•
    """
    
    def __init__(self):
        self.original_data = None
        self.cleaned_data = None
        self.cleaning_log = []
        
    def load_data(self, df):
        """åŠ è½½å¾…æ¸…æ´—çš„æ•°æ®"""
        self.original_data = df.copy()
        self.cleaned_data = df.copy()
        self.cleaning_log = []
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} è¡Œ {len(df.columns)} åˆ—")
        
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        if self.cleaned_data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return
            
        print("\nğŸ” æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®å½¢çŠ¶: {self.cleaned_data.shape}")
        print(f"å†…å­˜ä½¿ç”¨: {self.cleaned_data.memory_usage(deep=True).sum() / 1024:.2f} KB")
        print()
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_info = self.cleaned_data.isnull().sum()
        if missing_info.sum() > 0:
            print("âš ï¸ ç¼ºå¤±å€¼æƒ…å†µ:")
            for col, count in missing_info.items():
                if count > 0:
                    ratio = count / len(self.cleaned_data) * 100
                    print(f"  {col}: {count}ä¸ª ({ratio:.1f}%)")
        else:
            print("âœ… æ— ç¼ºå¤±å€¼")
        print()
        
        # é‡å¤å€¼åˆ†æ
        duplicate_count = self.cleaned_data.duplicated().sum()
        if duplicate_count > 0:
            print(f"âš ï¸ å‘ç° {duplicate_count} è¡Œé‡å¤æ•°æ®")
        else:
            print("âœ… æ— é‡å¤æ•°æ®")
        print()
        
        # æ•°æ®ç±»å‹åˆ†æ
        print("ğŸ“Š æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        type_counts = self.cleaned_data.dtypes.value_counts()
        for dtype, count in type_counts.items():
            print(f"  {dtype}: {count}åˆ—")
        print()
        
        # å¼‚å¸¸å€¼åˆ†æï¼ˆä»…é’ˆå¯¹æ•°å€¼åˆ—ï¼‰
        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print("ğŸ“ˆ æ•°å€¼åˆ—å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRæ–¹æ³•ï¼‰:")
            for col in numeric_cols:
                Q1 = self.cleaned_data[col].quantile(0.25)
                Q3 = self.cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_fence = Q1 - 1.5 * IQR
                upper_fence = Q3 + 1.5 * IQR
                
                outliers = self.cleaned_data[
                    (self.cleaned_data[col] < lower_fence) | 
                    (self.cleaned_data[col] > upper_fence)
                ]
                
                if len(outliers) > 0:
                    print(f"  {col}: {len(outliers)}ä¸ªå¼‚å¸¸å€¼")
                else:
                    print(f"  {col}: æ— å¼‚å¸¸å€¼")
        print()
        
    def handle_missing_values(self, strategy='auto', columns=None):
        """å¤„ç†ç¼ºå¤±å€¼"""
        if columns is None:
            columns = self.cleaned_data.columns
            
        for col in columns:
            if self.cleaned_data[col].isnull().sum() == 0:
                continue
                
            if strategy == 'auto':
                # è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
                if self.cleaned_data[col].dtype in ['int64', 'float64']:
                    # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……
                    fill_value = self.cleaned_data[col].median()
                    self.cleaned_data[col].fillna(fill_value, inplace=True)
                    self.cleaning_log.append(f"åˆ— '{col}' ç¼ºå¤±å€¼ç”¨ä¸­ä½æ•° {fill_value:.2f} å¡«å……")
                else:
                    # åˆ†ç±»åˆ—ç”¨ä¼—æ•°å¡«å……
                    mode_values = self.cleaned_data[col].mode()
                    if len(mode_values) > 0:
                        fill_value = mode_values[0]
                        self.cleaned_data[col].fillna(fill_value, inplace=True)
                        self.cleaning_log.append(f"åˆ— '{col}' ç¼ºå¤±å€¼ç”¨ä¼—æ•° '{fill_value}' å¡«å……")
                        
            elif strategy == 'drop':
                # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
                before_count = len(self.cleaned_data)
                self.cleaned_data.dropna(subset=[col], inplace=True)
                after_count = len(self.cleaned_data)
                self.cleaning_log.append(f"åˆ é™¤åˆ— '{col}' çš„ç¼ºå¤±å€¼ï¼Œå‡å°‘ {before_count - after_count} è¡Œ")
                
        print(f"âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼ˆç­–ç•¥ï¼š{strategy}ï¼‰")
        
    def handle_duplicates(self, subset=None, keep='first'):
        """å¤„ç†é‡å¤å€¼"""
        before_count = len(self.cleaned_data)
        self.cleaned_data.drop_duplicates(subset=subset, keep=keep, inplace=True)
        after_count = len(self.cleaned_data)
        
        removed_count = before_count - after_count
        if removed_count > 0:
            self.cleaning_log.append(f"åˆ é™¤ {removed_count} è¡Œé‡å¤æ•°æ®")
            print(f"âœ… åˆ é™¤äº† {removed_count} è¡Œé‡å¤æ•°æ®")
        else:
            print("âœ… æœªå‘ç°é‡å¤æ•°æ®")
            
    def handle_outliers(self, columns=None, method='iqr', action='cap'):
        """å¤„ç†å¼‚å¸¸å€¼"""
        if columns is None:
            columns = self.cleaned_data.select_dtypes(include=[np.number]).columns
            
        for col in columns:
            if method == 'iqr':
                Q1 = self.cleaned_data[col].quantile(0.25)
                Q3 = self.cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
            else:  # 3sigma
                mean_val = self.cleaned_data[col].mean()
                std_val = self.cleaned_data[col].std()
                lower_bound = mean_val - 3 * std_val
                upper_bound = mean_val + 3 * std_val
                
            # æ£€æµ‹å¼‚å¸¸å€¼
            outliers_mask = (
                (self.cleaned_data[col] < lower_bound) | 
                (self.cleaned_data[col] > upper_bound)
            )
            outliers_count = outliers_mask.sum()
            
            if outliers_count > 0:
                if action == 'cap':
                    # ç”¨è¾¹ç•Œå€¼æ›¿æ¢
                    self.cleaned_data[col] = self.cleaned_data[col].clip(
                        lower=lower_bound, upper=upper_bound
                    )
                    self.cleaning_log.append(
                        f"åˆ— '{col}' çš„ {outliers_count} ä¸ªå¼‚å¸¸å€¼è¢«è¾¹ç•Œå€¼æ›¿æ¢"
                    )
                elif action == 'remove':
                    # åˆ é™¤å¼‚å¸¸å€¼
                    self.cleaned_data = self.cleaned_data[~outliers_mask]
                    self.cleaning_log.append(
                        f"åˆ é™¤åˆ— '{col}' çš„ {outliers_count} ä¸ªå¼‚å¸¸å€¼"
                    )
                    
        print(f"âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼ˆæ–¹æ³•ï¼š{method}ï¼Œæ“ä½œï¼š{action}ï¼‰")
        
    def convert_data_types(self, type_mapping):
        """è½¬æ¢æ•°æ®ç±»å‹"""
        for col, target_type in type_mapping.items():
            if col in self.cleaned_data.columns:
                try:
                    if target_type == 'datetime':
                        self.cleaned_data[col] = pd.to_datetime(self.cleaned_data[col])
                    else:
                        self.cleaned_data[col] = self.cleaned_data[col].astype(target_type)
                    self.cleaning_log.append(f"åˆ— '{col}' è½¬æ¢ä¸º {target_type} ç±»å‹")
                except Exception as e:
                    print(f"âŒ åˆ— '{col}' ç±»å‹è½¬æ¢å¤±è´¥: {e}")
                    
        print("âœ… æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ")
        
    def generate_cleaning_report(self):
        """ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š"""
        print("\nğŸ“‹ æ•°æ®æ¸…æ´—æŠ¥å‘Š")
        print("=" * 50)
        
        print(f"åŸå§‹æ•°æ®: {self.original_data.shape}")
        print(f"æ¸…æ´—åæ•°æ®: {self.cleaned_data.shape}")
        
        if len(self.cleaning_log) > 0:
            print("\nğŸ”§ æ‰§è¡Œçš„æ¸…æ´—æ“ä½œ:")
            for i, operation in enumerate(self.cleaning_log, 1):
                print(f"  {i}. {operation}")
        else:
            print("\nâœ… æ•°æ®æ— éœ€æ¸…æ´—")
            
        print("\nğŸ“Š æ¸…æ´—å‰åå¯¹æ¯”:")
        print("ç¼ºå¤±å€¼æ•°é‡:")
        print(f"  æ¸…æ´—å‰: {self.original_data.isnull().sum().sum()}")
        print(f"  æ¸…æ´—å: {self.cleaned_data.isnull().sum().sum()}")
        
        print("é‡å¤è¡Œæ•°é‡:")
        print(f"  æ¸…æ´—å‰: {self.original_data.duplicated().sum()}")
        print(f"  æ¸…æ´—å: {self.cleaned_data.duplicated().sum()}")
        
    def get_cleaned_data(self):
        """è·å–æ¸…æ´—åçš„æ•°æ®"""
        return self.cleaned_data.copy()

# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        'å‘˜å·¥ID': ['E001', 'E002', 'E003', 'E002', 'E004', 'E005'],  # åŒ…å«é‡å¤
        'å§“å': ['å¼ ä¸‰', 'æå››', None, 'æå››', 'èµµå…­', 'é’±ä¸ƒ'],        # åŒ…å«ç¼ºå¤±å€¼
        'å¹´é¾„': [25, 30, 28, 30, 150, 22],                         # åŒ…å«å¼‚å¸¸å€¼
        'è–ªèµ„': [5000, 8000, 6000, 8000, None, 7000],             # åŒ…å«ç¼ºå¤±å€¼
        'å…¥èŒæ—¥æœŸ': ['2020-01-01', '2019-06-15', '2021-03-10', '2019-06-15', '2022-08-20', '2023-01-15']
    }
    
    df_test = pd.DataFrame(test_data)
    
    # åˆ›å»ºæ¸…æ´—å·¥å…·
    cleaner = SmartDataCleaner()
    
    print("ğŸ­ æ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…·å¯åŠ¨")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    cleaner.load_data(df_test)
    
    # åˆ†ææ•°æ®è´¨é‡
    cleaner.analyze_data_quality()
    
    # æ‰§è¡Œæ¸…æ´—æ“ä½œ
    cleaner.handle_missing_values(strategy='auto')
    cleaner.handle_duplicates()
    cleaner.handle_outliers(method='iqr', action='cap')
    
    # ç±»å‹è½¬æ¢
    type_mapping = {
        'å…¥èŒæ—¥æœŸ': 'datetime',
        'å¹´é¾„': 'int',
        'è–ªèµ„': 'float'
    }
    cleaner.convert_data_types(type_mapping)
    
    # ç”ŸæˆæŠ¥å‘Š
    cleaner.generate_cleaning_report()
    
    # è·å–æ¸…æ´—åçš„æ•°æ®
    cleaned_df = cleaner.get_cleaned_data()
    print("\nğŸ“Š æœ€ç»ˆæ¸…æ´—ç»“æœ:")
    print(cleaned_df)
    print("\næ•°æ®ç±»å‹:")
    print(cleaned_df.dtypes)
    
    print("\nğŸ‰ æ•°æ®æ¸…æ´—å®Œæˆï¼")
```

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

1. **ç¼ºå¤±å€¼å¤„ç†**ï¼š
   - æ£€æµ‹ï¼š`df.isnull()`, `df.info()`
   - åˆ é™¤ï¼š`df.dropna()`
   - å¡«å……ï¼š`df.fillna()`, `df.interpolate()`

2. **é‡å¤æ•°æ®å¤„ç†**ï¼š
   - æ£€æµ‹ï¼š`df.duplicated()`
   - åˆ é™¤ï¼š`df.drop_duplicates()`

3. **æ•°æ®ç±»å‹è½¬æ¢**ï¼š
   - åŸºæœ¬è½¬æ¢ï¼š`df.astype()`
   - æ—¥æœŸè½¬æ¢ï¼š`pd.to_datetime()`
   - æ•°å€¼è½¬æ¢ï¼š`pd.to_numeric()`

4. **å¼‚å¸¸å€¼å¤„ç†**ï¼š
   - 3ÏƒåŸåˆ™ï¼šå‡å€¼Â±3å€æ ‡å‡†å·®
   - IQRæ–¹æ³•ï¼šQ1-1.5Ã—IQR åˆ° Q3+1.5Ã—IQR
   - å¤„ç†ç­–ç•¥ï¼šåˆ é™¤ã€æ›¿æ¢ã€æ ‡è®°

é€šè¿‡"è´¨æ£€éƒ¨é—¨"çš„æ¯”å–»ï¼Œæˆ‘ä»¬æŒæ¡äº†æ•°æ®æ¸…æ´—çš„æ ¸å¿ƒæŠ€èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•å¯¹æ¸…æ´—åçš„æ•°æ®è¿›è¡Œæ·±å…¥åˆ†æå’Œå¯è§†åŒ–å±•ç¤ºã€‚

---

## 15.4 æ•°æ®åˆ†æä¸å¯è§†åŒ–

### ğŸ“Š æ•°æ®å·¥å‚çš„åˆ†ææŠ¥å‘Šéƒ¨é—¨

ç»è¿‡è´¨æ£€éƒ¨é—¨çš„ä¸¥æ ¼æŠŠå…³ï¼Œæˆ‘ä»¬çš„æ•°æ®å·²ç»å˜å¾—å¹²å‡€ã€æ ‡å‡†ã€‚ç°åœ¨è½®åˆ°åˆ†ææŠ¥å‘Šéƒ¨é—¨ç™»åœºäº†ï¼è¿™ä¸ªéƒ¨é—¨è´Ÿè´£ï¼š
- **ç»Ÿè®¡åˆ†æ**ï¼šä»æ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡å’Œè¶‹åŠ¿
- **åˆ†ç»„èšåˆ**ï¼šæŒ‰ä¸åŒç»´åº¦æ±‡æ€»æ•°æ®
- **é€è§†åˆ†æ**ï¼šå¤šè§’åº¦å®¡è§†æ•°æ®
- **å¯è§†åŒ–å±•ç¤º**ï¼šåˆ¶ä½œå›¾è¡¨è®©æ•°æ®"è¯´è¯"

### 15.4.1 æè¿°æ€§ç»Ÿè®¡åˆ†æï¼šæ•°æ®çš„åŸºæœ¬ç”»åƒ

```python
# === åˆ›å»ºç»¼åˆåˆ†ææ•°æ®é›† ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# åˆ›å»ºç”µå•†é”€å”®æ•°æ®
np.random.seed(42)
n_records = 1000

sales_data = {
    'è®¢å•ID': [f'ORD{i:05d}' for i in range(1, n_records + 1)],
    'å®¢æˆ·ID': [f'C{np.random.randint(1, 201):04d}' for _ in range(n_records)],
    'äº§å“ç±»åˆ«': np.random.choice(['ç”µå­äº§å“', 'æœè£…', 'å®¶å±…', 'å›¾ä¹¦', 'è¿åŠ¨'], n_records, 
                              p=[0.3, 0.25, 0.2, 0.15, 0.1]),
    'é”€å”®é¢': np.random.lognormal(mean=7, sigma=0.8, size=n_records).round(2),
    'æ•°é‡': np.random.poisson(lam=2, size=n_records) + 1,
    'é”€å”®æ—¥æœŸ': pd.date_range('2023-01-01', periods=n_records, freq='H'),
    'é”€å”®æ¸ é“': np.random.choice(['çº¿ä¸Š', 'çº¿ä¸‹'], n_records, p=[0.7, 0.3]),
    'å®¢æˆ·å¹´é¾„': np.random.normal(35, 12, n_records).astype(int).clip(18, 70),
    'å®¢æˆ·æ€§åˆ«': np.random.choice(['ç”·', 'å¥³'], n_records, p=[0.45, 0.55])
}

df_sales = pd.DataFrame(sales_data)
df_sales['æœˆä»½'] = df_sales['é”€å”®æ—¥æœŸ'].dt.month
df_sales['æ˜ŸæœŸ'] = df_sales['é”€å”®æ—¥æœŸ'].dt.day_name()

print("ğŸ“Š ç”µå•†é”€å”®æ•°æ®æ¦‚è§ˆï¼š")
print(df_sales.head())
print()
print(f"æ•°æ®é›†å¤§å°ï¼š{df_sales.shape}")
print()

# === åŸºç¡€æè¿°æ€§ç»Ÿè®¡ ===
print("=== åŸºç¡€æè¿°æ€§ç»Ÿè®¡ ===")

# æ•°å€¼åˆ—ç»Ÿè®¡
print("ğŸ“ˆ æ•°å€¼åˆ—æè¿°æ€§ç»Ÿè®¡ï¼š")
numeric_stats = df_sales[['é”€å”®é¢', 'æ•°é‡', 'å®¢æˆ·å¹´é¾„']].describe()
print(numeric_stats)
print()

# åˆ†ç±»åˆ—ç»Ÿè®¡
print("ğŸ“ åˆ†ç±»åˆ—ç»Ÿè®¡ï¼š")
categorical_cols = ['äº§å“ç±»åˆ«', 'é”€å”®æ¸ é“', 'å®¢æˆ·æ€§åˆ«']
for col in categorical_cols:
    print(f"\n{col} åˆ†å¸ƒï¼š")
    value_counts = df_sales[col].value_counts()
    percentages = df_sales[col].value_counts(normalize=True) * 100
    
    for value, count in value_counts.items():
        percentage = percentages[value]
        print(f"  {value}: {count}ä¸ª ({percentage:.1f}%)")

# === é«˜çº§ç»Ÿè®¡åˆ†æ ===
print("\n=== é«˜çº§ç»Ÿè®¡åˆ†æ ===")

# é”€å”®é¢åˆ†æ
print("ğŸ’° é”€å”®é¢æ·±åº¦åˆ†æï¼š")
sales_amount = df_sales['é”€å”®é¢']

print(f"  æ€»é”€å”®é¢: Â¥{sales_amount.sum():,.2f}")
print(f"  å¹³å‡é”€å”®é¢: Â¥{sales_amount.mean():.2f}")
print(f"  ä¸­ä½æ•°é”€å”®é¢: Â¥{sales_amount.median():.2f}")
print(f"  é”€å”®é¢æ ‡å‡†å·®: Â¥{sales_amount.std():.2f}")
print(f"  é”€å”®é¢å˜å¼‚ç³»æ•°: {sales_amount.std() / sales_amount.mean():.3f}")

# åˆ†ä½æ•°åˆ†æ
quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
print(f"\n  é”€å”®é¢åˆ†ä½æ•°åˆ†æï¼š")
for q in quantiles:
    value = sales_amount.quantile(q)
    print(f"    {q*100:.0f}%åˆ†ä½æ•°: Â¥{value:.2f}")

# ååº¦å’Œå³°åº¦
from scipy import stats
skewness = stats.skew(sales_amount)
kurtosis = stats.kurtosis(sales_amount)
print(f"\n  ååº¦ (Skewness): {skewness:.3f}")
print(f"  å³°åº¦ (Kurtosis): {kurtosis:.3f}")
```

### 15.4.2 åˆ†ç»„èšåˆæ“ä½œï¼šå¤šç»´åº¦æ•°æ®æ±‡æ€»

```python
# === åˆ†ç»„èšåˆåˆ†æ ===
print("\n=== åˆ†ç»„èšåˆåˆ†æ ===")

# æŒ‰äº§å“ç±»åˆ«åˆ†ç»„
print("ğŸ“¦ æŒ‰äº§å“ç±»åˆ«åˆ†ç»„åˆ†æï¼š")
category_analysis = df_sales.groupby('äº§å“ç±»åˆ«').agg({
    'é”€å”®é¢': ['sum', 'mean', 'count'],
    'æ•°é‡': ['sum', 'mean'],
    'å®¢æˆ·å¹´é¾„': 'mean'
}).round(2)

# æ‰å¹³åŒ–åˆ—å
category_analysis.columns = ['_'.join(col).strip() for col in category_analysis.columns]
print(category_analysis)
print()

# æŒ‰é”€å”®æ¸ é“åˆ†ç»„
print("ğŸ›’ æŒ‰é”€å”®æ¸ é“åˆ†ç»„åˆ†æï¼š")
channel_analysis = df_sales.groupby('é”€å”®æ¸ é“').agg({
    'é”€å”®é¢': ['sum', 'mean', 'std'],
    'æ•°é‡': 'sum',
    'å®¢æˆ·å¹´é¾„': 'mean'
}).round(2)

channel_analysis.columns = ['_'.join(col).strip() for col in channel_analysis.columns]
print(channel_analysis)
print()

# å¤šå±‚åˆ†ç»„
print("ğŸ¯ å¤šç»´åº¦åˆ†ç»„åˆ†æï¼ˆäº§å“ç±»åˆ« Ã— é”€å”®æ¸ é“ï¼‰ï¼š")
multi_group = df_sales.groupby(['äº§å“ç±»åˆ«', 'é”€å”®æ¸ é“']).agg({
    'é”€å”®é¢': ['sum', 'mean'],
    'æ•°é‡': 'sum'
}).round(2)

print(multi_group)
print()

# æ—¶é—´åºåˆ—åˆ†ç»„
print("ğŸ“… æŒ‰æœˆä»½åˆ†ç»„åˆ†æï¼š")
monthly_analysis = df_sales.groupby('æœˆä»½').agg({
    'é”€å”®é¢': ['sum', 'mean'],
    'æ•°é‡': 'sum',
    'è®¢å•ID': 'count'  # è®¢å•æ•°é‡
}).round(2)

monthly_analysis.columns = ['_'.join(col).strip() for col in monthly_analysis.columns]
print(monthly_analysis)
print()

# è‡ªå®šä¹‰èšåˆå‡½æ•°
print("ğŸ”§ è‡ªå®šä¹‰èšåˆåˆ†æï¼š")
def coefficient_of_variation(x):
    """å˜å¼‚ç³»æ•°"""
    return x.std() / x.mean() if x.mean() != 0 else 0

def sales_range(x):
    """é”€å”®é¢èŒƒå›´"""
    return x.max() - x.min()

custom_agg = df_sales.groupby('äº§å“ç±»åˆ«')['é”€å”®é¢'].agg([
    ('å¹³å‡å€¼', 'mean'),
    ('æ ‡å‡†å·®', 'std'),
    ('å˜å¼‚ç³»æ•°', coefficient_of_variation),
    ('é”€å”®èŒƒå›´', sales_range),
    ('æœ€å¤§å€¼', 'max'),
    ('æœ€å°å€¼', 'min')
]).round(3)

print(custom_agg)
```

### 15.4.3 æ•°æ®é€è§†è¡¨ï¼šå¤šè§’åº¦æ•°æ®å®¡è§†

```python
# === æ•°æ®é€è§†è¡¨åˆ†æ ===
print("\n=== æ•°æ®é€è§†è¡¨åˆ†æ ===")

# åŸºç¡€é€è§†è¡¨
print("ğŸ“Š åŸºç¡€é€è§†è¡¨ï¼ˆäº§å“ç±»åˆ« vs é”€å”®æ¸ é“ï¼‰ï¼š")
pivot_basic = pd.pivot_table(
    df_sales,
    values='é”€å”®é¢',
    index='äº§å“ç±»åˆ«',
    columns='é”€å”®æ¸ é“',
    aggfunc='sum',
    fill_value=0,
    margins=True  # æ·»åŠ æ€»è®¡è¡Œå’Œåˆ—
)
print(pivot_basic)
print()

# å¤šå€¼é€è§†è¡¨
print("ğŸ“ˆ å¤šæŒ‡æ ‡é€è§†è¡¨ï¼š")
pivot_multi = pd.pivot_table(
    df_sales,
    values=['é”€å”®é¢', 'æ•°é‡'],
    index='äº§å“ç±»åˆ«',
    columns='é”€å”®æ¸ é“',
    aggfunc={'é”€å”®é¢': 'sum', 'æ•°é‡': 'mean'},
    fill_value=0
)
print(pivot_multi)
print()

# å¤šå±‚ç´¢å¼•é€è§†è¡¨
print("ğŸ¯ å¤šå±‚ç´¢å¼•é€è§†è¡¨ï¼ˆæŒ‰æœˆä»½å’Œæ€§åˆ«ï¼‰ï¼š")
pivot_complex = pd.pivot_table(
    df_sales,
    values='é”€å”®é¢',
    index=['äº§å“ç±»åˆ«', 'å®¢æˆ·æ€§åˆ«'],
    columns='æœˆä»½',
    aggfunc='mean',
    fill_value=0
)
print(pivot_complex.head(10))
print()

# é€è§†è¡¨ç™¾åˆ†æ¯”åˆ†æ
print("ğŸ“Š é€è§†è¡¨ç™¾åˆ†æ¯”åˆ†æï¼š")
pivot_pct = pd.pivot_table(
    df_sales,
    values='é”€å”®é¢',
    index='äº§å“ç±»åˆ«',
    columns='é”€å”®æ¸ é“',
    aggfunc='sum',
    fill_value=0
)

# è®¡ç®—è¡Œç™¾åˆ†æ¯”ï¼ˆæ¯ä¸ªäº§å“ç±»åˆ«åœ¨ä¸åŒæ¸ é“çš„å æ¯”ï¼‰
pivot_pct_row = pivot_pct.div(pivot_pct.sum(axis=1), axis=0) * 100
print("å„äº§å“ç±»åˆ«åœ¨ä¸åŒæ¸ é“çš„é”€å”®å æ¯”ï¼ˆ%ï¼‰ï¼š")
print(pivot_pct_row.round(1))
print()

# è®¡ç®—åˆ—ç™¾åˆ†æ¯”ï¼ˆæ¯ä¸ªæ¸ é“ä¸­ä¸åŒäº§å“çš„å æ¯”ï¼‰
pivot_pct_col = pivot_pct.div(pivot_pct.sum(axis=0), axis=1) * 100
print("å„æ¸ é“ä¸­ä¸åŒäº§å“ç±»åˆ«çš„é”€å”®å æ¯”ï¼ˆ%ï¼‰ï¼š")
print(pivot_pct_col.round(1))
```

### 15.4.4 æ•°æ®å¯è§†åŒ–ï¼šè®©æ•°æ®è¯´è¯

```python
# === æ•°æ®å¯è§†åŒ– ===
print("\n=== æ•°æ®å¯è§†åŒ–å±•ç¤º ===")

# åˆ›å»ºå›¾è¡¨ç”»å¸ƒ
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('ç”µå•†é”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿', fontsize=16, fontweight='bold')

# 1. é”€å”®é¢åˆ†å¸ƒç›´æ–¹å›¾
axes[0, 0].hist(df_sales['é”€å”®é¢'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
axes[0, 0].set_title('é”€å”®é¢åˆ†å¸ƒ')
axes[0, 0].set_xlabel('é”€å”®é¢ (Â¥)')
axes[0, 0].set_ylabel('é¢‘æ¬¡')
axes[0, 0].axvline(df_sales['é”€å”®é¢'].mean(), color='red', linestyle='--', 
                   label=f'å¹³å‡å€¼: Â¥{df_sales["é”€å”®é¢"].mean():.2f}')
axes[0, 0].legend()

# 2. äº§å“ç±»åˆ«é”€å”®é¢æŸ±çŠ¶å›¾
category_sales = df_sales.groupby('äº§å“ç±»åˆ«')['é”€å”®é¢'].sum().sort_values(ascending=False)
bars = axes[0, 1].bar(range(len(category_sales)), category_sales.values, 
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])
axes[0, 1].set_title('å„äº§å“ç±»åˆ«é”€å”®é¢')
axes[0, 1].set_xlabel('äº§å“ç±»åˆ«')
axes[0, 1].set_ylabel('é”€å”®é¢ (Â¥)')
axes[0, 1].set_xticks(range(len(category_sales)))
axes[0, 1].set_xticklabels(category_sales.index, rotation=45)

# åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars):
    height = bar.get_height()
    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,
                    f'Â¥{height:,.0f}', ha='center', va='bottom', fontsize=9)

# 3. é”€å”®æ¸ é“é¥¼å›¾
channel_counts = df_sales['é”€å”®æ¸ é“'].value_counts()
colors = ['#FF9999', '#66B2FF']
wedges, texts, autotexts = axes[0, 2].pie(channel_counts.values, labels=channel_counts.index, 
                                          autopct='%1.1f%%', colors=colors, startangle=90)
axes[0, 2].set_title('é”€å”®æ¸ é“åˆ†å¸ƒ')

# 4. æœˆåº¦é”€å”®è¶‹åŠ¿çº¿å›¾
monthly_sales = df_sales.groupby('æœˆä»½')['é”€å”®é¢'].sum()
axes[1, 0].plot(monthly_sales.index, monthly_sales.values, marker='o', linewidth=2, 
                markersize=8, color='#2E86AB')
axes[1, 0].set_title('æœˆåº¦é”€å”®è¶‹åŠ¿')
axes[1, 0].set_xlabel('æœˆä»½')
axes[1, 0].set_ylabel('é”€å”®é¢ (Â¥)')
axes[1, 0].grid(True, alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for x, y in zip(monthly_sales.index, monthly_sales.values):
    axes[1, 0].annotate(f'Â¥{y:,.0f}', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontsize=8)

# 5. å®¢æˆ·å¹´é¾„ä¸é”€å”®é¢æ•£ç‚¹å›¾
scatter = axes[1, 1].scatter(df_sales['å®¢æˆ·å¹´é¾„'], df_sales['é”€å”®é¢'], 
                            alpha=0.6, c=df_sales['é”€å”®é¢'], cmap='viridis', s=30)
axes[1, 1].set_title('å®¢æˆ·å¹´é¾„ vs é”€å”®é¢')
axes[1, 1].set_xlabel('å®¢æˆ·å¹´é¾„')
axes[1, 1].set_ylabel('é”€å”®é¢ (Â¥)')

# æ·»åŠ è¶‹åŠ¿çº¿
z = np.polyfit(df_sales['å®¢æˆ·å¹´é¾„'], df_sales['é”€å”®é¢'], 1)
p = np.poly1d(z)
axes[1, 1].plot(df_sales['å®¢æˆ·å¹´é¾„'].sort_values(), 
                p(df_sales['å®¢æˆ·å¹´é¾„'].sort_values()), "r--", alpha=0.8)

# 6. çƒ­åŠ›å›¾ï¼šäº§å“ç±»åˆ« vs å®¢æˆ·æ€§åˆ«
heatmap_data = pd.pivot_table(df_sales, values='é”€å”®é¢', 
                             index='äº§å“ç±»åˆ«', columns='å®¢æˆ·æ€§åˆ«', 
                             aggfunc='mean', fill_value=0)
im = axes[1, 2].imshow(heatmap_data.values, cmap='YlOrRd', aspect='auto')
axes[1, 2].set_title('äº§å“ç±»åˆ« vs å®¢æˆ·æ€§åˆ«çƒ­åŠ›å›¾')
axes[1, 2].set_xticks(range(len(heatmap_data.columns)))
axes[1, 2].set_xticklabels(heatmap_data.columns)
axes[1, 2].set_yticks(range(len(heatmap_data.index)))
axes[1, 2].set_yticklabels(heatmap_data.index)

# åœ¨çƒ­åŠ›å›¾ä¸Šæ·»åŠ æ•°å€¼
for i in range(len(heatmap_data.index)):
    for j in range(len(heatmap_data.columns)):
        text = axes[1, 2].text(j, i, f'{heatmap_data.iloc[i, j]:.0f}',
                              ha="center", va="center", color="black", fontsize=8)

plt.tight_layout()
plt.show()

print("ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆï¼")
print()

# === ç›¸å…³æ€§åˆ†æ ===
print("=== ç›¸å…³æ€§åˆ†æ ===")
correlation_data = df_sales[['é”€å”®é¢', 'æ•°é‡', 'å®¢æˆ·å¹´é¾„']].corr()
print("æ•°å€¼å˜é‡ç›¸å…³æ€§çŸ©é˜µï¼š")
print(correlation_data.round(3))
print()

# è§£é‡Šç›¸å…³æ€§
print("ğŸ“ˆ ç›¸å…³æ€§è§£è¯»ï¼š")
for i in range(len(correlation_data.columns)):
    for j in range(i+1, len(correlation_data.columns)):
        var1 = correlation_data.columns[i]
        var2 = correlation_data.columns[j]
        corr_value = correlation_data.iloc[i, j]
        
        if abs(corr_value) > 0.7:
            strength = "å¼º"
        elif abs(corr_value) > 0.3:
            strength = "ä¸­ç­‰"
        else:
            strength = "å¼±"
            
        direction = "æ­£" if corr_value > 0 else "è´Ÿ"
        print(f"  {var1} ä¸ {var2}: {direction}ç›¸å…³ï¼Œå¼ºåº¦{strength} (r={corr_value:.3f})")
```

### ğŸ¯ å®è·µé¡¹ç›®ï¼šé”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„é”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿ï¼š

```python
class SalesAnalyticsDashboard:
    """é”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿
    
    æ¨¡æ‹Ÿæ•°æ®å·¥å‚çš„æ™ºèƒ½åˆ†ææŠ¥å‘Šç³»ç»Ÿï¼š
    - å…¨æ–¹ä½æ•°æ®åˆ†æï¼šæè¿°æ€§ç»Ÿè®¡ã€è¶‹åŠ¿åˆ†æã€å…³è”åˆ†æ
    - å¤šç»´åº¦æ•°æ®é€è§†ï¼šæ—¶é—´ã€äº§å“ã€å®¢æˆ·ã€æ¸ é“ç­‰ç»´åº¦
    - æ™ºèƒ½å¯è§†åŒ–ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³å›¾è¡¨ç±»å‹
    - æ´å¯ŸæŠ¥å‘Šç”Ÿæˆï¼šè‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„å…³é”®æ´å¯Ÿ
    """
    
    def __init__(self, data):
        self.data = data.copy()
        self.insights = []
        
    def basic_analysis(self):
        """åŸºç¡€æ•°æ®åˆ†æ"""
        print("ğŸ“Š åŸºç¡€æ•°æ®åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        # æ•°æ®æ¦‚è§ˆ
        print(f"æ•°æ®é›†å¤§å°: {self.data.shape[0]} è¡Œ Ã— {self.data.shape[1]} åˆ—")
        print(f"æ—¶é—´èŒƒå›´: {self.data['é”€å”®æ—¥æœŸ'].min()} è‡³ {self.data['é”€å”®æ—¥æœŸ'].max()}")
        print()
        
        # å…³é”®æŒ‡æ ‡
        total_sales = self.data['é”€å”®é¢'].sum()
        total_orders = len(self.data)
        avg_order_value = self.data['é”€å”®é¢'].mean()
        unique_customers = self.data['å®¢æˆ·ID'].nunique()
        
        print("ğŸ¯ å…³é”®ä¸šåŠ¡æŒ‡æ ‡:")
        print(f"  æ€»é”€å”®é¢: Â¥{total_sales:,.2f}")
        print(f"  æ€»è®¢å•æ•°: {total_orders:,}")
        print(f"  å¹³å‡è®¢å•ä»·å€¼: Â¥{avg_order_value:.2f}")
        print(f"  ç‹¬ç«‹å®¢æˆ·æ•°: {unique_customers:,}")
        print(f"  å®¢æˆ·å¹³å‡è®¢å•æ•°: {total_orders / unique_customers:.1f}")
        print()
        
        # äº§å“åˆ†æ
        print("ğŸ“¦ äº§å“ç±»åˆ«åˆ†æ:")
        category_stats = self.data.groupby('äº§å“ç±»åˆ«').agg({
            'é”€å”®é¢': ['sum', 'mean'],
            'è®¢å•ID': 'count'
        }).round(2)
        
        category_stats.columns = ['æ€»é”€å”®é¢', 'å¹³å‡é”€å”®é¢', 'è®¢å•æ•°']
        category_stats['é”€å”®é¢å æ¯”'] = (category_stats['æ€»é”€å”®é¢'] / total_sales * 100).round(1)
        category_stats = category_stats.sort_values('æ€»é”€å”®é¢', ascending=False)
        
        print(category_stats)
        print()
        
        # è‡ªåŠ¨æ´å¯Ÿå‘ç°
        self._discover_insights()
        
    def time_series_analysis(self):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        print("ğŸ“… æ—¶é—´åºåˆ—åˆ†æ")
        print("=" * 50)
        
        # æŒ‰æ—¥æœŸèšåˆ
        daily_sales = self.data.groupby(self.data['é”€å”®æ—¥æœŸ'].dt.date).agg({
            'é”€å”®é¢': 'sum',
            'è®¢å•ID': 'count'
        }).reset_index()
        daily_sales.columns = ['æ—¥æœŸ', 'é”€å”®é¢', 'è®¢å•æ•°']
        
        print("ğŸ“ˆ æ¯æ—¥é”€å”®è¶‹åŠ¿:")
        print(f"  æœ€é«˜æ—¥é”€å”®é¢: Â¥{daily_sales['é”€å”®é¢'].max():,.2f}")
        print(f"  æœ€ä½æ—¥é”€å”®é¢: Â¥{daily_sales['é”€å”®é¢'].min():,.2f}")
        print(f"  å¹³å‡æ—¥é”€å”®é¢: Â¥{daily_sales['é”€å”®é¢'].mean():.2f}")
        print(f"  é”€å”®é¢æ ‡å‡†å·®: Â¥{daily_sales['é”€å”®é¢'].std():.2f}")
        print()
        
        # æŒ‰æœˆä»½åˆ†æ
        monthly_stats = self.data.groupby('æœˆä»½').agg({
            'é”€å”®é¢': ['sum', 'mean'],
            'è®¢å•ID': 'count'
        }).round(2)
        monthly_stats.columns = ['æœˆé”€å”®é¢', 'å¹³å‡è®¢å•ä»·å€¼', 'è®¢å•æ•°']
        
        print("ğŸ“Š æœˆåº¦é”€å”®åˆ†æ:")
        print(monthly_stats)
        print()
        
        # å¢é•¿ç‡åˆ†æ
        monthly_growth = monthly_stats['æœˆé”€å”®é¢'].pct_change() * 100
        print("ğŸ“ˆ æœˆåº¦å¢é•¿ç‡:")
        for month, growth in monthly_growth.items():
            if not pd.isna(growth):
                direction = "ğŸ“ˆ" if growth > 0 else "ğŸ“‰"
                print(f"  {month}æœˆ: {direction} {growth:.1f}%")
        print()
        
    def customer_analysis(self):
        """å®¢æˆ·åˆ†æ"""
        print("ğŸ‘¥ å®¢æˆ·è¡Œä¸ºåˆ†æ")
        print("=" * 50)
        
        # å®¢æˆ·ä»·å€¼åˆ†æ
        customer_stats = self.data.groupby('å®¢æˆ·ID').agg({
            'é”€å”®é¢': ['sum', 'mean', 'count'],
            'é”€å”®æ—¥æœŸ': ['min', 'max']
        })
        customer_stats.columns = ['æ€»æ¶ˆè´¹', 'å¹³å‡è®¢å•ä»·å€¼', 'è®¢å•æ•°', 'é¦–æ¬¡è´­ä¹°', 'æœ€åè´­ä¹°']
        
        # å®¢æˆ·ç”Ÿå‘½å‘¨æœŸï¼ˆå¤©æ•°ï¼‰
        customer_stats['ç”Ÿå‘½å‘¨æœŸå¤©æ•°'] = (
            customer_stats['æœ€åè´­ä¹°'] - customer_stats['é¦–æ¬¡è´­ä¹°']
        ).dt.days + 1
        
        print("ğŸ† å®¢æˆ·ä»·å€¼åˆ†æ:")
        print(f"  å¹³å‡å®¢æˆ·æ€»æ¶ˆè´¹: Â¥{customer_stats['æ€»æ¶ˆè´¹'].mean():.2f}")
        print(f"  å¹³å‡å®¢æˆ·è®¢å•æ•°: {customer_stats['è®¢å•æ•°'].mean():.1f}")
        print(f"  å¹³å‡å®¢æˆ·ç”Ÿå‘½å‘¨æœŸ: {customer_stats['ç”Ÿå‘½å‘¨æœŸå¤©æ•°'].mean():.1f} å¤©")
        print()
        
        # å®¢æˆ·åˆ†å±‚
        # ä½¿ç”¨RFMæ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬
        customer_stats['æ¶ˆè´¹ç­‰çº§'] = pd.qcut(customer_stats['æ€»æ¶ˆè´¹'], 
                                        q=3, labels=['ä½ä»·å€¼', 'ä¸­ä»·å€¼', 'é«˜ä»·å€¼'])
        customer_stats['é¢‘æ¬¡ç­‰çº§'] = pd.qcut(customer_stats['è®¢å•æ•°'], 
                                        q=3, labels=['ä½é¢‘', 'ä¸­é¢‘', 'é«˜é¢‘'])
        
        # å®¢æˆ·åˆ†å±‚ç»Ÿè®¡
        customer_segments = customer_stats.groupby(['æ¶ˆè´¹ç­‰çº§', 'é¢‘æ¬¡ç­‰çº§']).size().unstack(fill_value=0)
        print("ğŸ“Š å®¢æˆ·åˆ†å±‚çŸ©é˜µ:")
        print(customer_segments)
        print()
        
        # æ€§åˆ«å’Œå¹´é¾„åˆ†æ
        demo_analysis = self.data.groupby(['å®¢æˆ·æ€§åˆ«', pd.cut(self.data['å®¢æˆ·å¹´é¾„'], 
                                                           bins=[0, 25, 35, 45, 55, 100], 
                                                           labels=['18-25', '26-35', '36-45', '46-55', '55+'])]).agg({
            'é”€å”®é¢': ['sum', 'mean'],
            'è®¢å•ID': 'count'
        }).round(2)
        
        print("ğŸ‘« å®¢æˆ·äººå£ç»Ÿè®¡åˆ†æ:")
        print(demo_analysis)
        print()
        
    def product_performance_analysis(self):
        """äº§å“ç»©æ•ˆåˆ†æ"""
        print("ğŸ“¦ äº§å“ç»©æ•ˆåˆ†æ")
        print("=" * 50)
        
        # äº§å“ç±»åˆ«ç»©æ•ˆ
        category_performance = self.data.groupby('äº§å“ç±»åˆ«').agg({
            'é”€å”®é¢': ['sum', 'mean', 'std'],
            'æ•°é‡': ['sum', 'mean'],
            'è®¢å•ID': 'count'
        }).round(2)
        
        category_performance.columns = ['æ€»é”€å”®é¢', 'å¹³å‡é”€å”®é¢', 'é”€å”®é¢æ ‡å‡†å·®', 
                                      'æ€»é”€é‡', 'å¹³å‡é”€é‡', 'è®¢å•æ•°']
        
        # è®¡ç®—å˜å¼‚ç³»æ•°
        category_performance['å˜å¼‚ç³»æ•°'] = (
            category_performance['é”€å”®é¢æ ‡å‡†å·®'] / category_performance['å¹³å‡é”€å”®é¢']
        ).round(3)
        
        # æŒ‰æ€»é”€å”®é¢æ’åº
        category_performance = category_performance.sort_values('æ€»é”€å”®é¢', ascending=False)
        
        print("ğŸ† äº§å“ç±»åˆ«ç»©æ•ˆæ’è¡Œ:")
        print(category_performance)
        print()
        
        # æ¸ é“æ•ˆæœåˆ†æ
        channel_analysis = self.data.groupby(['äº§å“ç±»åˆ«', 'é”€å”®æ¸ é“']).agg({
            'é”€å”®é¢': 'sum',
            'è®¢å•ID': 'count'
        }).unstack(fill_value=0)
        
        print("ğŸ›’ æ¸ é“é”€å”®æ•ˆæœåˆ†æ:")
        print(channel_analysis)
        print()
        
    def generate_insights_report(self):
        """ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š"""
        print("ğŸ’¡ æ•°æ®æ´å¯ŸæŠ¥å‘Š")
        print("=" * 50)
        
        if self.insights:
            for i, insight in enumerate(self.insights, 1):
                print(f"{i}. {insight}")
        else:
            print("æœªå‘ç°ç‰¹æ®Šæ´å¯Ÿ")
        print()
        
    def _discover_insights(self):
        """è‡ªåŠ¨å‘ç°æ•°æ®æ´å¯Ÿ"""
        # æ´å¯Ÿ1ï¼šæœ€å—æ¬¢è¿çš„äº§å“ç±»åˆ«
        top_category = self.data.groupby('äº§å“ç±»åˆ«')['é”€å”®é¢'].sum().idxmax()
        top_category_sales = self.data.groupby('äº§å“ç±»åˆ«')['é”€å”®é¢'].sum().max()
        total_sales = self.data['é”€å”®é¢'].sum()
        top_category_pct = (top_category_sales / total_sales) * 100
        
        self.insights.append(
            f"ğŸ“Š {top_category}æ˜¯æœ€å—æ¬¢è¿çš„äº§å“ç±»åˆ«ï¼Œå æ€»é”€å”®é¢çš„{top_category_pct:.1f}%"
        )
        
        # æ´å¯Ÿ2ï¼šé”€å”®æ¸ é“æ•ˆæœ
        channel_comparison = self.data.groupby('é”€å”®æ¸ é“')['é”€å”®é¢'].mean()
        if channel_comparison['çº¿ä¸Š'] > channel_comparison['çº¿ä¸‹']:
            better_channel = 'çº¿ä¸Š'
            diff_pct = ((channel_comparison['çº¿ä¸Š'] - channel_comparison['çº¿ä¸‹']) / 
                       channel_comparison['çº¿ä¸‹']) * 100
        else:
            better_channel = 'çº¿ä¸‹'
            diff_pct = ((channel_comparison['çº¿ä¸‹'] - channel_comparison['çº¿ä¸Š']) / 
                       channel_comparison['çº¿ä¸Š']) * 100
            
        self.insights.append(
            f"ğŸ›’ {better_channel}æ¸ é“çš„å¹³å‡è®¢å•ä»·å€¼æ¯”å¦ä¸€æ¸ é“é«˜{diff_pct:.1f}%"
        )
        
        # æ´å¯Ÿ3ï¼šå®¢æˆ·å¹´é¾„ä¸æ¶ˆè´¹å…³ç³»
        age_corr = self.data['å®¢æˆ·å¹´é¾„'].corr(self.data['é”€å”®é¢'])
        if abs(age_corr) > 0.1:
            direction = "æ­£ç›¸å…³" if age_corr > 0 else "è´Ÿç›¸å…³"
            self.insights.append(
                f"ğŸ‘¥ å®¢æˆ·å¹´é¾„ä¸æ¶ˆè´¹é‡‘é¢å‘ˆ{direction}å…³ç³» (ç›¸å…³ç³»æ•°: {age_corr:.3f})"
            )
            
        # æ´å¯Ÿ4ï¼šæ€§åˆ«æ¶ˆè´¹å·®å¼‚
        gender_avg = self.data.groupby('å®¢æˆ·æ€§åˆ«')['é”€å”®é¢'].mean()
        if abs(gender_avg['ç”·'] - gender_avg['å¥³']) > 50:
            higher_gender = 'ç”·æ€§' if gender_avg['ç”·'] > gender_avg['å¥³'] else 'å¥³æ€§'
            diff_amount = abs(gender_avg['ç”·'] - gender_avg['å¥³'])
            self.insights.append(
                f"ğŸ‘« {higher_gender}å®¢æˆ·çš„å¹³å‡æ¶ˆè´¹æ¯”å¦ä¸€æ€§åˆ«é«˜Â¥{diff_amount:.2f}"
            )

# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    print("ğŸ­ é”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿å¯åŠ¨")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æä»ªè¡¨æ¿
    dashboard = SalesAnalyticsDashboard(df_sales)
    
    # æ‰§è¡Œå…¨é¢åˆ†æ
    dashboard.basic_analysis()
    dashboard.time_series_analysis()
    dashboard.customer_analysis()
    dashboard.product_performance_analysis()
    dashboard.generate_insights_report()
    
    print("ğŸ‰ é”€å”®æ•°æ®åˆ†æå®Œæˆï¼")
```

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

1. **æè¿°æ€§ç»Ÿè®¡**ï¼š
   - åŸºæœ¬ç»Ÿè®¡ï¼š`df.describe()`, `df.mean()`, `df.std()`
   - åˆ†ä½æ•°åˆ†æï¼š`df.quantile()`
   - åˆ†å¸ƒç‰¹å¾ï¼šååº¦ã€å³°åº¦

2. **åˆ†ç»„èšåˆ**ï¼š
   - å•ç»´åˆ†ç»„ï¼š`df.groupby('column')`
   - å¤šç»´åˆ†ç»„ï¼š`df.groupby(['col1', 'col2'])`
   - èšåˆå‡½æ•°ï¼š`sum`, `mean`, `count`, `std`

3. **æ•°æ®é€è§†**ï¼š
   - é€è§†è¡¨ï¼š`pd.pivot_table()`
   - äº¤å‰è¡¨ï¼š`pd.crosstab()`
   - å¤šå±‚ç´¢å¼•å¤„ç†

4. **æ•°æ®å¯è§†åŒ–**ï¼š
   - ç›´æ–¹å›¾ï¼šåˆ†å¸ƒåˆ†æ
   - æŸ±çŠ¶å›¾ï¼šåˆ†ç±»æ¯”è¾ƒ
   - çº¿å›¾ï¼šè¶‹åŠ¿åˆ†æ
   - æ•£ç‚¹å›¾ï¼šå…³ç³»åˆ†æ
   - çƒ­åŠ›å›¾ï¼šç›¸å…³æ€§å±•ç¤º

é€šè¿‡"åˆ†ææŠ¥å‘Šéƒ¨é—¨"çš„æ¯”å–»ï¼Œæˆ‘ä»¬æŒæ¡äº†Pandasæ•°æ®åˆ†æçš„æ ¸å¿ƒæŠ€èƒ½ã€‚æ•°æ®å·¥å‚çš„å®Œæ•´æµç¨‹â€”â€”ä»åŸæ–™å…¥åº“åˆ°è´¨æ£€æ¸…æ´—ï¼Œå†åˆ°æ·±åº¦åˆ†æå’Œå¯è§†åŒ–æŠ¥å‘Šâ€”â€”ç°åœ¨ä½ éƒ½èƒ½ç†Ÿç»ƒæ“ä½œäº†ï¼

---

## ğŸ“š æœ¬ç« æ€»ç»“

### ğŸ¯ æ ¸å¿ƒçŸ¥è¯†å›é¡¾

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å®Œæ•´åœ°ä½“éªŒäº†"æ•°æ®å·¥å‚"çš„è¿ä½œæµç¨‹ï¼š

1. **åŸæ–™ä»“åº“ç®¡ç†ï¼ˆæ•°æ®ç»“æ„ï¼‰**
   - Seriesï¼šä¸€ç»´æ•°æ®çš„æ ‡ç­¾æ•°ç»„
   - DataFrameï¼šäºŒç»´æ•°æ®çš„æ ‡ç­¾è¡¨æ ¼
   - Indexï¼šé«˜æ•ˆçš„æ ‡ç­¾ç´¢å¼•ç³»ç»Ÿ

2. **è¿›è´§å‡ºè´§ç³»ç»Ÿï¼ˆæ•°æ®è¾“å…¥è¾“å‡ºï¼‰**
   - å¤šæ ¼å¼æ•°æ®è¯»å–ï¼šCSVã€Excelã€JSONã€æ•°æ®åº“
   - çµæ´»çš„æ•°æ®å¯¼å‡ºï¼šæ”¯æŒå¤šç§æ ¼å¼å’Œå‚æ•°é…ç½®
   - ç½‘ç»œæ•°æ®è·å–ï¼šAPIæ¥å£å’Œåœ¨çº¿æ•°æ®æº

3. **è´¨æ£€éƒ¨é—¨ï¼ˆæ•°æ®æ¸…æ´—ï¼‰**
   - ç¼ºå¤±å€¼å¤„ç†ï¼šæ£€æµ‹ã€åˆ é™¤ã€å¡«å……ã€æ’å€¼
   - é‡å¤æ•°æ®å¤„ç†ï¼šè¯†åˆ«å’Œå»é‡ç­–ç•¥
   - æ•°æ®ç±»å‹è½¬æ¢ï¼šæ ‡å‡†åŒ–æ•°æ®æ ¼å¼
   - å¼‚å¸¸å€¼å¤„ç†ï¼šæ£€æµ‹æ–¹æ³•å’Œå¤„ç†ç­–ç•¥

4. **åˆ†ææŠ¥å‘Šéƒ¨é—¨ï¼ˆæ•°æ®åˆ†æä¸å¯è§†åŒ–ï¼‰**
   - æè¿°æ€§ç»Ÿè®¡ï¼šå…¨é¢çš„æ•°æ®ç”»åƒ
   - åˆ†ç»„èšåˆï¼šå¤šç»´åº¦æ•°æ®æ±‡æ€»
   - æ•°æ®é€è§†ï¼šçµæ´»çš„æ•°æ®é‡ç»„
   - å¯è§†åŒ–å±•ç¤ºï¼šè®©æ•°æ®è¯´è¯

### ğŸ“Š æŠ€èƒ½æŒæ¡è¯„ä¼°

| æŠ€èƒ½é¢†åŸŸ | æŒæ¡ç¨‹åº¦ | æ ¸å¿ƒæ–¹æ³• | å®é™…åº”ç”¨ |
|---------|---------|----------|----------|
| æ•°æ®ç»“æ„ | â­â­â­â­â­ | Series, DataFrame, Index | å­¦ç”Ÿæˆç»©ç®¡ç†ç³»ç»Ÿ |
| æ•°æ®IO | â­â­â­â­â­ | read_csv, to_excel, read_sql | å¤šæºæ•°æ®æ•´åˆåˆ†æå™¨ |
| æ•°æ®æ¸…æ´— | â­â­â­â­â­ | dropna, fillna, drop_duplicates | æ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…· |
| æ•°æ®åˆ†æ | â­â­â­â­â­ | groupby, pivot_table, describe | é”€å”®æ•°æ®åˆ†æä»ªè¡¨æ¿ |
| æ•°æ®å¯è§†åŒ– | â­â­â­â­â­ | matplotlib, seaborn | å¤šç»´åº¦å›¾è¡¨å±•ç¤º |

### ğŸš€ å­¦ä¹ æˆæœå±•ç¤º

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å·²ç»å…·å¤‡äº†ï¼š

1. **ä¼ä¸šçº§æ•°æ®å¤„ç†èƒ½åŠ›**
   - å¤„ç†çœŸå®ä¸–ç•Œçš„"è„"æ•°æ®
   - è®¾è®¡å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
   - æ„å»ºå¯é‡ç”¨çš„æ•°æ®åˆ†æå·¥å…·

2. **æ•°æ®åˆ†ææ€ç»´**
   - ä»ä¸šåŠ¡è§’åº¦ç†è§£æ•°æ®
   - å¤šç»´åº¦å®¡è§†æ•°æ®é—®é¢˜
   - è‡ªåŠ¨å‘ç°æ•°æ®æ´å¯Ÿ

3. **é¡¹ç›®å®æˆ˜ç»éªŒ**
   - å®Œæˆäº†5ä¸ªå®Œæ•´çš„ä¼ä¸šçº§é¡¹ç›®
   - ç¼–å†™äº†1800+è¡Œé«˜è´¨é‡ä»£ç 
   - æŒæ¡äº†æ•°æ®åˆ†æçš„å®Œæ•´å·¥ä½œæµ

### ğŸ¯ è¿›é˜¶å­¦ä¹ æ–¹å‘

1. **é«˜çº§æ•°æ®åˆ†æ**
   - æ—¶é—´åºåˆ—åˆ†æ
   - ç»Ÿè®¡å‡è®¾æ£€éªŒ
   - æœºå™¨å­¦ä¹ é¢„å¤„ç†

2. **å¤§æ•°æ®å¤„ç†**
   - Daskå¹¶è¡Œè®¡ç®—
   - æ•°æ®åº“ä¼˜åŒ–
   - å†…å­˜ç®¡ç†æŠ€å·§

3. **å¯è§†åŒ–è¿›é˜¶**
   - äº¤äº’å¼å›¾è¡¨ï¼ˆPlotlyï¼‰
   - åœ°ç†æ•°æ®å¯è§†åŒ–
   - åŠ¨æ€ä»ªè¡¨æ¿

4. **ä¸šåŠ¡åº”ç”¨**
   - ç”¨æˆ·è¡Œä¸ºåˆ†æ
   - è´¢åŠ¡æ•°æ®åˆ†æ
   - è¿è¥æ•°æ®åˆ†æ

### ğŸ’¼ èŒåœºåº”ç”¨ä»·å€¼

æŒæ¡Pandasæ•°æ®åˆ†ææŠ€èƒ½ï¼Œä½ å¯ä»¥èƒœä»»ï¼š

- **æ•°æ®åˆ†æå¸ˆ**ï¼šå¤„ç†ä¸šåŠ¡æ•°æ®ï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
- **ä¸šåŠ¡åˆ†æå¸ˆ**ï¼šæ”¯æŒå•†ä¸šå†³ç­–ï¼Œä¼˜åŒ–ä¸šåŠ¡æµç¨‹  
- **äº§å“ç»ç†**ï¼šåˆ†æç”¨æˆ·è¡Œä¸ºï¼ŒæŒ‡å¯¼äº§å“è¿­ä»£
- **è¿è¥ä¸“å‘˜**ï¼šç›‘æ§è¿è¥æŒ‡æ ‡ï¼Œä¼˜åŒ–è¿è¥ç­–ç•¥
- **ç ”ç©¶å‘˜**ï¼šå¤„ç†ç ”ç©¶æ•°æ®ï¼ŒéªŒè¯ç ”ç©¶å‡è®¾

### ğŸŒŸ æœ€ä½³å®è·µæ€»ç»“

1. **æ•°æ®å¤„ç†åŸåˆ™**
   - å§‹ç»ˆä¿ç•™åŸå§‹æ•°æ®å‰¯æœ¬
   - è®°å½•æ¯æ­¥æ•°æ®å¤„ç†æ“ä½œ
   - éªŒè¯æ•°æ®å¤„ç†ç»“æœçš„åˆç†æ€§

2. **ä»£ç ç»„ç»‡å»ºè®®**
   - ä½¿ç”¨ç±»å°è£…å¤æ‚çš„æ•°æ®å¤„ç†é€»è¾‘
   - ç¼–å†™å¯é‡ç”¨çš„å·¥å…·å‡½æ•°
   - æ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£

3. **æ€§èƒ½ä¼˜åŒ–æŠ€å·§**
   - åˆç†é€‰æ‹©æ•°æ®ç±»å‹
   - é¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶
   - ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯

4. **å¯è§†åŒ–è®¾è®¡åŸåˆ™**
   - é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹
   - ä¿æŒå›¾è¡¨ç®€æ´æ¸…æ™°
   - æ·»åŠ å¿…è¦çš„æ ‡ç­¾å’Œè¯´æ˜

æ­å–œä½ ï¼ä½ å·²ç»æˆåŠŸæŒæ¡äº†Pandasæ•°æ®åˆ†æçš„æ ¸å¿ƒæŠ€èƒ½ã€‚è¿™äº›æŠ€èƒ½å°†æˆä¸ºä½ æ•°æ®ç§‘å­¦ä¹‹è·¯çš„é‡è¦åŸºçŸ³ï¼Œä¸ºåç»­å­¦ä¹ æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯æ‰“ä¸‹åšå®åŸºç¡€ã€‚

è®°ä½ï¼šæ•°æ®åˆ†æä¸ä»…æ˜¯æŠ€æœ¯æ´»ï¼Œæ›´æ˜¯ä¸€é—¨è‰ºæœ¯ã€‚ç»§ç»­å®è·µï¼Œä¸æ–­æ¢ç´¢ï¼Œä½ å°†åœ¨æ•°æ®çš„æµ·æ´‹ä¸­å‘ç°æ›´å¤šå®è—ï¼ğŸ‰ 