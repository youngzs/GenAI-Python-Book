## 8.2 HTTPåè®®ä¸WebåŸºç¡€ - ä¸‡ç»´ç½‘çš„è¯­è¨€

> **æ ¸å¿ƒæ€æƒ³**ï¼šHTTPåè®®å°±åƒé¤å…çš„ç‚¹é¤ç³»ç»Ÿï¼Œå®¢æˆ·ï¼ˆæµè§ˆå™¨ï¼‰å‘æœåŠ¡å‘˜ï¼ˆæœåŠ¡å™¨ï¼‰å‘å‡ºè¯·æ±‚ï¼ŒæœåŠ¡å‘˜è¿”å›ç›¸åº”çš„æœåŠ¡ã€‚

### 8.2.1 HTTPåè®®æ·±åº¦è§£æ

æƒ³è±¡ä½ åœ¨é¤å…ç”¨é¤ï¼šä½ çœ‹èœå•ï¼ˆæµè§ˆç½‘é¡µï¼‰ï¼Œå‘æœåŠ¡å‘˜ç‚¹é¤ï¼ˆå‘é€HTTPè¯·æ±‚ï¼‰ï¼ŒæœåŠ¡å‘˜ç¡®è®¤è®¢å•å¹¶ä¸Šèœï¼ˆè¿”å›HTTPå“åº”ï¼‰ã€‚HTTPåè®®å°±æ˜¯è¿™æ ·ä¸€å¥—æ ‡å‡†åŒ–çš„"æœåŠ¡æµç¨‹"ã€‚

#### ğŸ½ï¸ HTTPè¯·æ±‚å“åº”æ¨¡å‹

```python
"""
HTTPé€šä¿¡å°±åƒé¤å…æœåŠ¡æµç¨‹ï¼š

å®¢æˆ·ç«¯ï¼ˆé£Ÿå®¢ï¼‰    â†’    æœåŠ¡å™¨ï¼ˆé¤å…ï¼‰
     â†“                    â†“
1. çœ‹èœå•              1. å‡†å¤‡èœå•
2. ç‚¹é¤ï¼ˆè¯·æ±‚ï¼‰         2. æ¥æ”¶è®¢å•
3. ç­‰å¾…ä¸Šèœ            3. å¤„ç†è®¢å•
4. äº«ç”¨ç¾é£Ÿï¼ˆå“åº”ï¼‰      4. æä¾›æœåŠ¡
"""

import urllib.request
import urllib.parse
import json
from typing import Dict, Optional, Any

class HTTPMessage:
    """HTTPæ¶ˆæ¯çš„åŸºç¡€ç±» - å°±åƒé¤å…çš„è®¢å•"""
    
    def __init__(self):
        self.headers = {}  # æ¶ˆæ¯å¤´ = è®¢å•è¯¦æƒ…
        self.body = ""     # æ¶ˆæ¯ä½“ = å…·ä½“å†…å®¹
    
    def add_header(self, name: str, value: str):
        """æ·»åŠ æ¶ˆæ¯å¤´"""
        self.headers[name] = value
        print(f"ğŸ“‹ æ·»åŠ å¤´éƒ¨ä¿¡æ¯: {name} = {value}")
    
    def set_body(self, content: str):
        """è®¾ç½®æ¶ˆæ¯ä½“"""
        self.body = content
        print(f"ğŸ“ è®¾ç½®æ¶ˆæ¯å†…å®¹: {len(content)} å­—ç¬¦")

class HTTPRequest(HTTPMessage):
    """HTTPè¯·æ±‚ - å°±åƒå®¢æˆ·çš„ç‚¹é¤å•"""
    
    def __init__(self, method: str, url: str):
        super().__init__()
        self.method = method.upper()  # GET, POST, PUT, DELETE
        self.url = url
        self.path = ""
        self.query_params = {}
        
        # è§£æURL
        self._parse_url()
    
    def _parse_url(self):
        """è§£æURLè·å–è·¯å¾„å’Œå‚æ•°"""
        from urllib.parse import urlparse, parse_qs
        
        parsed = urlparse(self.url)
        self.path = parsed.path
        self.query_params = parse_qs(parsed.query)
        
        print(f"ğŸ¯ è¯·æ±‚ç›®æ ‡: {self.method} {self.path}")
        if self.query_params:
            print(f"ğŸ“Š æŸ¥è¯¢å‚æ•°: {self.query_params}")
    
    def add_query_param(self, name: str, value: str):
        """æ·»åŠ æŸ¥è¯¢å‚æ•°"""
        self.query_params[name] = [value]
        print(f"ğŸ” æ·»åŠ æŸ¥è¯¢å‚æ•°: {name} = {value}")
    
    def to_string(self) -> str:
        """è½¬æ¢ä¸ºHTTPè¯·æ±‚å­—ç¬¦ä¸²"""
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        query_string = ""
        if self.query_params:
            params = []
            for name, values in self.query_params.items():
                for value in values:
                    params.append(f"{name}={urllib.parse.quote(str(value))}")
            query_string = "?" + "&".join(params)
        
        # æ„å»ºè¯·æ±‚è¡Œ
        request_line = f"{self.method} {self.path}{query_string} HTTP/1.1"
        
        # æ„å»ºè¯·æ±‚å¤´
        header_lines = []
        for name, value in self.headers.items():
            header_lines.append(f"{name}: {value}")
        
        # ç»„è£…å®Œæ•´è¯·æ±‚
        request_parts = [request_line] + header_lines + ["", self.body]
        return "\n".join(request_parts)

class HTTPResponse(HTTPMessage):
    """HTTPå“åº” - å°±åƒé¤å…çš„ä¸Šèœ"""
    
    def __init__(self, status_code: int, reason_phrase: str = ""):
        super().__init__()
        self.status_code = status_code
        self.reason_phrase = reason_phrase or self._get_reason_phrase(status_code)
    
    def _get_reason_phrase(self, code: int) -> str:
        """æ ¹æ®çŠ¶æ€ç è·å–åŸå› çŸ­è¯­"""
        status_messages = {
            200: "OK", 201: "Created", 204: "No Content",
            301: "Moved Permanently", 302: "Found", 304: "Not Modified",
            400: "Bad Request", 401: "Unauthorized", 403: "Forbidden", 404: "Not Found",
            500: "Internal Server Error", 502: "Bad Gateway", 503: "Service Unavailable"
        }
        return status_messages.get(code, "Unknown")
    
    def to_string(self) -> str:
        """è½¬æ¢ä¸ºHTTPå“åº”å­—ç¬¦ä¸²"""
        # çŠ¶æ€è¡Œ
        status_line = f"HTTP/1.1 {self.status_code} {self.reason_phrase}"
        
        # å“åº”å¤´
        header_lines = []
        for name, value in self.headers.items():
            header_lines.append(f"{name}: {value}")
        
        # ç»„è£…å®Œæ•´å“åº”
        response_parts = [status_line] + header_lines + ["", self.body]
        return "\n".join(response_parts)

# HTTPçŠ¶æ€ç è¯¦è§£
class HTTPStatusCode:
    """HTTPçŠ¶æ€ç  - å°±åƒé¤å…çš„æœåŠ¡çŠ¶æ€"""
    
    @classmethod
    def explain_status(cls, code: int) -> str:
        """è§£é‡ŠçŠ¶æ€ç å«ä¹‰"""
        status_map = {
            200: "âœ… OK - è®¢å•æˆåŠŸå¤„ç†",
            201: "âœ… Created - æ–°èœå“å·²æ·»åŠ åˆ°èœå•",
            204: "âœ… No Content - è®¢å•å¤„ç†å®Œæˆï¼Œæ— éœ€è¿”å›å†…å®¹",
            301: "ğŸ”„ Moved Permanently - é¤å…æ°¸ä¹…æ¬è¿",
            302: "ğŸ”„ Found - ä¸´æ—¶æ¢åˆ°å…¶ä»–é¤å…",
            304: "ğŸ”„ Not Modified - èœå•æœªæ›´æ–°ï¼Œä½¿ç”¨ç¼“å­˜",
            400: "âŒ Bad Request - è®¢å•æ ¼å¼é”™è¯¯",
            401: "âŒ Unauthorized - éœ€è¦ä¼šå‘˜èº«ä»½",
            403: "âŒ Forbidden - ç¦æ­¢ç‚¹è¿™é“èœ",
            404: "âŒ Not Found - èœå•ä¸Šæ²¡æœ‰è¿™é“èœ",
            500: "ğŸ’¥ Internal Server Error - å¨æˆ¿è®¾å¤‡æ•…éšœ",
            502: "ğŸ’¥ Bad Gateway - ä¾›åº”å•†å‡ºé—®é¢˜",
            503: "ğŸ’¥ Service Unavailable - é¤å…æš‚åœè¥ä¸š"
        }
        
        return status_map.get(code, f"æœªçŸ¥çŠ¶æ€ç : {code}")

# æ¼”ç¤ºHTTPæ¶ˆæ¯æ„å»º
def http_message_demo():
    """HTTPæ¶ˆæ¯æ„å»ºæ¼”ç¤º"""
    print("=== HTTPæ¶ˆæ¯æ„å»ºæ¼”ç¤º ===\n")
    
    # åˆ›å»ºHTTPè¯·æ±‚
    print("ğŸ½ï¸ æ¨¡æ‹Ÿé¤å…ç‚¹é¤ï¼ˆHTTPè¯·æ±‚ï¼‰:")
    request = HTTPRequest("GET", "https://restaurant.com/menu?category=main&spicy=true")
    request.add_header("Host", "restaurant.com")
    request.add_header("User-Agent", "Hungry-Customer/1.0")
    request.add_header("Accept", "application/json")
    
    print("\nğŸ“‹ å®Œæ•´HTTPè¯·æ±‚:")
    print(request.to_string())
    
    # åˆ›å»ºHTTPå“åº”
    print("\n" + "="*50)
    print("ğŸœ æ¨¡æ‹Ÿé¤å…ä¸Šèœï¼ˆHTTPå“åº”ï¼‰:")
    response = HTTPResponse(200)
    response.add_header("Content-Type", "application/json")
    response.add_header("Server", "Restaurant-Server/2.0")
    response.set_body('{"dishes": ["å®«ä¿é¸¡ä¸", "éº»å©†è±†è…"], "status": "available"}')
    
    print("\nğŸ“¦ å®Œæ•´HTTPå“åº”:")
    print(response.to_string())
    
    # çŠ¶æ€ç æ¼”ç¤º
    print("\n" + "="*50)
    print("ğŸ“Š HTTPçŠ¶æ€ç è§£é‡Š:")
    test_codes = [200, 404, 500, 302]
    for code in test_codes:
        print(f"{code}: {HTTPStatusCode.explain_status(code)}")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    http_message_demo()
```

### 8.2.2 Python HTTPç¼–ç¨‹

#### ğŸ ä½¿ç”¨urllibåº“ - Pythonçš„å†…ç½®HTTPå·¥å…·

```python
import urllib.request
import urllib.parse
import urllib.error
import json
from typing import Dict, Any, Optional

class SimpleHTTPClient:
    """ç®€å•çš„HTTPå®¢æˆ·ç«¯ - å°±åƒä¸€ä¸ªä¼šç‚¹é¤çš„æœºå™¨äºº"""
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
        self.session_headers = {
            "User-Agent": "Python-HTTP-Client/1.0"
        }
    
    def get(self, url: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        """å‘é€GETè¯·æ±‚ - å°±åƒæŸ¥çœ‹èœå•"""
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            if params:
                query_string = urllib.parse.urlencode(params)
                url = f"{url}?{query_string}"
            
            print(f"ğŸ” å‘é€GETè¯·æ±‚: {url}")
            
            # åˆ›å»ºè¯·æ±‚å¯¹è±¡
            request = urllib.request.Request(url)
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # å‘é€è¯·æ±‚
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                # è¯»å–å“åº”
                status_code = response.getcode()
                headers = dict(response.headers)
                content = response.read().decode('utf-8')
                
                print(f"âœ… å“åº”çŠ¶æ€: {status_code}")
                print(f"ğŸ“¦ å“åº”å¤§å°: {len(content)} å­—ç¬¦")
                
                return {
                    "status_code": status_code,
                    "headers": headers,
                    "content": content,
                    "json": self._try_parse_json(content)
                }
        
        except urllib.error.HTTPError as e:
            print(f"âŒ HTTPé”™è¯¯: {e.code} {e.reason}")
            return {
                "status_code": e.code,
                "error": f"HTTP {e.code}: {e.reason}",
                "content": ""
            }
        except urllib.error.URLError as e:
            print(f"âŒ URLé”™è¯¯: {e.reason}")
            return {
                "error": f"URLé”™è¯¯: {e.reason}",
                "content": ""
            }
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return {
                "error": str(e),
                "content": ""
            }
    
    def post(self, url: str, data: Dict[str, Any] = None, json_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """å‘é€POSTè¯·æ±‚ - å°±åƒæäº¤è®¢å•"""
        try:
            print(f"ğŸ“¤ å‘é€POSTè¯·æ±‚: {url}")
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            request_data = None
            content_type = "application/x-www-form-urlencoded"
            
            if json_data:
                # JSONæ•°æ®
                request_data = json.dumps(json_data).encode('utf-8')
                content_type = "application/json"
                print(f"ğŸ“ JSONæ•°æ®: {json_data}")
            elif data:
                # è¡¨å•æ•°æ®
                request_data = urllib.parse.urlencode(data).encode('utf-8')
                print(f"ğŸ“ è¡¨å•æ•°æ®: {data}")
            
            # åˆ›å»ºè¯·æ±‚å¯¹è±¡
            request = urllib.request.Request(url, data=request_data, method='POST')
            request.add_header('Content-Type', content_type)
            
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # å‘é€è¯·æ±‚
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                status_code = response.getcode()
                headers = dict(response.headers)
                content = response.read().decode('utf-8')
                
                print(f"âœ… å“åº”çŠ¶æ€: {status_code}")
                
                return {
                    "status_code": status_code,
                    "headers": headers,
                    "content": content,
                    "json": self._try_parse_json(content)
                }
        
        except Exception as e:
            print(f"âŒ POSTè¯·æ±‚å¤±è´¥: {e}")
            return {
                "error": str(e),
                "content": ""
            }
    
    def _try_parse_json(self, content: str) -> Optional[Dict[str, Any]]:
        """å°è¯•è§£æJSONå“åº”"""
        try:
            return json.loads(content)
        except:
            return None
    
    def add_header(self, name: str, value: str):
        """æ·»åŠ ä¼šè¯å¤´éƒ¨"""
        self.session_headers[name] = value
        print(f"ğŸ“‹ æ·»åŠ ä¼šè¯å¤´éƒ¨: {name} = {value}")

# HTTPå®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹
def http_client_demo():
    """HTTPå®¢æˆ·ç«¯ä½¿ç”¨æ¼”ç¤º"""
    print("=== HTTPå®¢æˆ·ç«¯ç¼–ç¨‹æ¼”ç¤º ===\n")
    
    # ä½¿ç”¨ç®€å•å®¢æˆ·ç«¯
    print("ğŸ¤– ç®€å•HTTPå®¢æˆ·ç«¯æ¼”ç¤º:")
    client = SimpleHTTPClient()
    
    # æ¨¡æ‹ŸAPIè°ƒç”¨
    print("\n1. GETè¯·æ±‚æ¼”ç¤º:")
    response = client.get("https://httpbin.org/get", {
        "name": "å¼ ä¸‰",
        "age": "25"
    })
    
    print("\n2. POSTè¯·æ±‚æ¼”ç¤º:")
    response = client.post("https://httpbin.org/post", json_data={
        "username": "testuser",
        "message": "Hello from Python!"
    })
    
    print("HTTPå®¢æˆ·ç«¯ä»£ç å·²å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    http_client_demo()
```

### 8.2.3 Webçˆ¬è™«åŸºç¡€

#### ğŸ•·ï¸ ç½‘é¡µå†…å®¹æŠ“å– - åƒèœ˜è››ä¸€æ ·çˆ¬å–ä¿¡æ¯

```python
import urllib.request
import urllib.parse
import re
import time
import random
from typing import List, Dict, Any
from html.parser import HTMLParser

class SimpleWebScraper:
    """ç®€å•çš„ç½‘é¡µçˆ¬è™« - å°±åƒä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ä¿¡æ¯æ”¶é›†å‘˜"""
    
    def __init__(self, delay: float = 1.0):
        self.delay = delay  # è¯·æ±‚é—´éš”ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
        self.session_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    def fetch_page(self, url: str) -> Dict[str, Any]:
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–: {url}")
            
            # åˆ›å»ºè¯·æ±‚
            request = urllib.request.Request(url)
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # å‘é€è¯·æ±‚
            with urllib.request.urlopen(request, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                print(f"âœ… æˆåŠŸè·å– {len(content)} å­—ç¬¦")
                
                return {
                    "url": url,
                    "content": content,
                    "status": "success"
                }
        
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            return {
                "url": url,
                "content": "",
                "status": "failed",
                "error": str(e)
            }
    
    def extract_links(self, html_content: str, base_url: str = "") -> List[str]:
        """æå–ç½‘é¡µä¸­çš„é“¾æ¥"""
        link_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>'
        links = re.findall(link_pattern, html_content, re.IGNORECASE)
        
        # å¤„ç†ç›¸å¯¹é“¾æ¥
        absolute_links = []
        for link in links:
            if link.startswith('http'):
                absolute_links.append(link)
            elif base_url and link.startswith('/'):
                absolute_links.append(base_url.rstrip('/') + link)
        
        print(f"ğŸ”— æ‰¾åˆ° {len(absolute_links)} ä¸ªé“¾æ¥")
        return absolute_links
    
    def extract_text(self, html_content: str) -> str:
        """æå–ç½‘é¡µçº¯æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text_content = re.sub(r'<[^>]+>', '', clean_content)
        
        # æ¸…ç†ç©ºç™½å­—ç¬¦
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        print(f"ğŸ“ æå–æ–‡æœ¬ {len(text_content)} å­—ç¬¦")
        return text_content
    
    def crawl_website(self, start_url: str, max_pages: int = 5) -> List[Dict[str, Any]]:
        """çˆ¬å–ç½‘ç«™çš„å¤šä¸ªé¡µé¢"""
        visited_urls = set()
        urls_to_visit = [start_url]
        crawled_data = []
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ç½‘ç«™: {start_url}")
        print(f"ğŸ“Š æœ€å¤§é¡µé¢æ•°: {max_pages}")
        
        while urls_to_visit and len(crawled_data) < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
            
            visited_urls.add(current_url)
            
            # è·å–é¡µé¢å†…å®¹
            page_data = self.fetch_page(current_url)
            
            if page_data["status"] == "success":
                # æå–æ–‡æœ¬å†…å®¹
                text_content = self.extract_text(page_data["content"])
                
                # æå–é“¾æ¥
                links = self.extract_links(page_data["content"], start_url)
                
                # æ·»åŠ æ–°é“¾æ¥åˆ°å¾…è®¿é—®åˆ—è¡¨
                for link in links[:3]:  # é™åˆ¶æ¯é¡µæœ€å¤š3ä¸ªæ–°é“¾æ¥
                    if link not in visited_urls:
                        urls_to_visit.append(link)
                
                # ä¿å­˜æ•°æ®
                crawled_data.append({
                    "url": current_url,
                    "title": self._extract_title(page_data["content"]),
                    "text": text_content[:500],  # åªä¿å­˜å‰500å­—ç¬¦
                    "links_count": len(links)
                })
                
                print(f"ğŸ“‹ å·²çˆ¬å– {len(crawled_data)}/{max_pages} é¡µ")
            
            # å»¶è¿Ÿï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
            time.sleep(self.delay + random.uniform(0, 0.5))
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(crawled_data)} é¡µæ•°æ®")
        return crawled_data
    
    def _extract_title(self, html_content: str) -> str:
        """æå–ç½‘é¡µæ ‡é¢˜"""
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        return "æ— æ ‡é¢˜"

# åçˆ¬è™«æŠ€æœ¯åº”å¯¹
class AdvancedWebScraper(SimpleWebScraper):
    """é«˜çº§ç½‘é¡µçˆ¬è™« - å…·å¤‡åçˆ¬è™«åº”å¯¹èƒ½åŠ›"""
    
    def __init__(self, delay: float = 2.0):
        super().__init__(delay)
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
        ]
        self.session_cookies = {}
    
    def fetch_page(self, url: str) -> Dict[str, Any]:
        """å¸¦åçˆ¬è™«åº”å¯¹çš„é¡µé¢è·å–"""
        try:
            print(f"ğŸ•·ï¸ æ™ºèƒ½çˆ¬å–: {url}")
            
            # éšæœºé€‰æ‹©User-Agent
            user_agent = random.choice(self.user_agents)
            
            # åˆ›å»ºè¯·æ±‚
            request = urllib.request.Request(url)
            request.add_header('User-Agent', user_agent)
            request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
            request.add_header('Accept-Language', 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3')
            request.add_header('Accept-Encoding', 'gzip, deflate')
            request.add_header('Connection', 'keep-alive')
            
            # æ·»åŠ Cookie
            if self.session_cookies:
                cookie_string = '; '.join([f"{k}={v}" for k, v in self.session_cookies.items()])
                request.add_header('Cookie', cookie_string)
            
            # éšæœºå»¶è¿Ÿ
            delay_time = self.delay + random.uniform(0, 1.0)
            time.sleep(delay_time)
            
            # å‘é€è¯·æ±‚
            with urllib.request.urlopen(request, timeout=15) as response:
                # å¤„ç†Cookie
                set_cookie = response.headers.get('Set-Cookie')
                if set_cookie:
                    self._update_cookies(set_cookie)
                
                content = response.read()
                
                # å¤„ç†gzipå‹ç¼©
                if response.headers.get('Content-Encoding') == 'gzip':
                    import gzip
                    content = gzip.decompress(content)
                
                content = content.decode('utf-8', errors='ignore')
                
                print(f"âœ… æ™ºèƒ½è·å–æˆåŠŸ {len(content)} å­—ç¬¦")
                
                return {
                    "url": url,
                    "content": content,
                    "status": "success"
                }
        
        except Exception as e:
            print(f"âŒ æ™ºèƒ½çˆ¬å–å¤±è´¥: {e}")
            return {
                "url": url,
                "content": "",
                "status": "failed",
                "error": str(e)
            }
    
    def _update_cookies(self, set_cookie: str):
        """æ›´æ–°Cookie"""
        for cookie_part in set_cookie.split(','):
            if '=' in cookie_part:
                name, value = cookie_part.split('=', 1)
                self.session_cookies[name.strip()] = value.split(';')[0].strip()

# çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹
def web_scraper_demo():
    """ç½‘é¡µçˆ¬è™«æ¼”ç¤º"""
    print("=== ç½‘é¡µçˆ¬è™«æ¼”ç¤º ===\n")
    
    # ç®€å•çˆ¬è™«æ¼”ç¤º
    print("ğŸ•·ï¸ ç®€å•çˆ¬è™«æ¼”ç¤º:")
    simple_scraper = SimpleWebScraper(delay=0.5)
    
    # çˆ¬å–ç¤ºä¾‹ç½‘ç«™
    test_url = "https://httpbin.org/html"
    page_data = simple_scraper.fetch_page(test_url)
    
    if page_data["status"] == "success":
        text_content = simple_scraper.extract_text(page_data["content"])
        links = simple_scraper.extract_links(page_data["content"])
        
        print(f"ğŸ“„ é¡µé¢å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
        print(f"ğŸ”— æ‰¾åˆ°é“¾æ¥æ•°é‡: {len(links)}")
    
    print("\n" + "="*50)
    print("ğŸ¤– é«˜çº§çˆ¬è™«æ¼”ç¤º:")
    
    advanced_scraper = AdvancedWebScraper(delay=1.0)
    
    # æ¼”ç¤ºåçˆ¬è™«æŠ€æœ¯
    print("ğŸ›¡ï¸ ä½¿ç”¨åçˆ¬è™«æŠ€æœ¯:")
    print("- éšæœºUser-Agent")
    print("- æ™ºèƒ½å»¶è¿Ÿ")
    print("- Cookieç®¡ç†")
    print("- è¯·æ±‚å¤´ä¼ªè£…")
    
    print("\nç½‘é¡µçˆ¬è™«ä»£ç å·²å‡†å¤‡å°±ç»ªï¼")

if __name__ == "__main__":
    web_scraper_demo()
```

---

## ğŸ¯ ç¬¬8.2èŠ‚æ€»ç»“

### ğŸ“š æ ¸å¿ƒæ¦‚å¿µå›é¡¾
1. **HTTPåè®®**ï¼šåŸºäºè¯·æ±‚-å“åº”æ¨¡å‹çš„åº”ç”¨å±‚åè®®
2. **çŠ¶æ€ç ç³»ç»Ÿ**ï¼šç”¨æ•°å­—è¡¨ç¤ºè¯·æ±‚å¤„ç†ç»“æœçš„æ ‡å‡†
3. **å¤´éƒ¨å­—æ®µ**ï¼šHTTPæ¶ˆæ¯çš„å…ƒæ•°æ®ä¿¡æ¯
4. **Webçˆ¬è™«**ï¼šè‡ªåŠ¨åŒ–çš„ç½‘é¡µä¿¡æ¯æ”¶é›†å·¥å…·

### ğŸ’¡ å…³é”®æŠ€èƒ½æŒæ¡
- âœ… ç†è§£HTTPè¯·æ±‚å“åº”çš„å®Œæ•´æµç¨‹
- âœ… æŒæ¡Python urllibåº“çš„ä½¿ç”¨æ–¹æ³•
- âœ… èƒ½å¤Ÿè§£æå’Œæ„å»ºHTTPæ¶ˆæ¯
- âœ… å…·å¤‡åŸºç¡€çš„ç½‘é¡µçˆ¬è™«å¼€å‘èƒ½åŠ›

### ğŸ”— ç”Ÿæ´»åŒ–ç†è§£
- **HTTPåè®®** = é¤å…çš„ç‚¹é¤ç³»ç»Ÿ
- **çŠ¶æ€ç ** = é¤å…çš„æœåŠ¡çŠ¶æ€åé¦ˆ
- **è¯·æ±‚å¤´** = è®¢å•çš„è¯¦ç»†è¯´æ˜
- **Webçˆ¬è™«** = è‡ªåŠ¨åŒ–çš„ä¿¡æ¯æ”¶é›†å‘˜

### ğŸ“ ç»ƒä¹ å»ºè®®
1. å®ç°ä¸€ä¸ªæ–°é—»ç½‘ç«™çš„å†…å®¹çˆ¬è™«
2. å¼€å‘ä¸€ä¸ªAPIçŠ¶æ€ç›‘æ§å·¥å…·
3. åˆ›å»ºä¸€ä¸ªç®€å•çš„HTTPå‹åŠ›æµ‹è¯•å·¥å…·

---

*ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ HTTPæœåŠ¡å™¨å¼€å‘ï¼Œä»å®¢æˆ·ç«¯è½¬å‘æœåŠ¡ç«¯...* 