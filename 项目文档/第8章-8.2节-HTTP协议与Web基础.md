## 8.2 HTTP协议与Web基础 - 万维网的语言

> **核心思想**：HTTP协议就像餐厅的点餐系统，客户（浏览器）向服务员（服务器）发出请求，服务员返回相应的服务。

### 8.2.1 HTTP协议深度解析

想象你在餐厅用餐：你看菜单（浏览网页），向服务员点餐（发送HTTP请求），服务员确认订单并上菜（返回HTTP响应）。HTTP协议就是这样一套标准化的"服务流程"。

#### 🍽️ HTTP请求响应模型

```python
"""
HTTP通信就像餐厅服务流程：

客户端（食客）    →    服务器（餐厅）
     ↓                    ↓
1. 看菜单              1. 准备菜单
2. 点餐（请求）         2. 接收订单
3. 等待上菜            3. 处理订单
4. 享用美食（响应）      4. 提供服务
"""

import urllib.request
import urllib.parse
import json
from typing import Dict, Optional, Any

class HTTPMessage:
    """HTTP消息的基础类 - 就像餐厅的订单"""
    
    def __init__(self):
        self.headers = {}  # 消息头 = 订单详情
        self.body = ""     # 消息体 = 具体内容
    
    def add_header(self, name: str, value: str):
        """添加消息头"""
        self.headers[name] = value
        print(f"📋 添加头部信息: {name} = {value}")
    
    def set_body(self, content: str):
        """设置消息体"""
        self.body = content
        print(f"📝 设置消息内容: {len(content)} 字符")

class HTTPRequest(HTTPMessage):
    """HTTP请求 - 就像客户的点餐单"""
    
    def __init__(self, method: str, url: str):
        super().__init__()
        self.method = method.upper()  # GET, POST, PUT, DELETE
        self.url = url
        self.path = ""
        self.query_params = {}
        
        # 解析URL
        self._parse_url()
    
    def _parse_url(self):
        """解析URL获取路径和参数"""
        from urllib.parse import urlparse, parse_qs
        
        parsed = urlparse(self.url)
        self.path = parsed.path
        self.query_params = parse_qs(parsed.query)
        
        print(f"🎯 请求目标: {self.method} {self.path}")
        if self.query_params:
            print(f"📊 查询参数: {self.query_params}")
    
    def add_query_param(self, name: str, value: str):
        """添加查询参数"""
        self.query_params[name] = [value]
        print(f"🔍 添加查询参数: {name} = {value}")
    
    def to_string(self) -> str:
        """转换为HTTP请求字符串"""
        # 构建查询字符串
        query_string = ""
        if self.query_params:
            params = []
            for name, values in self.query_params.items():
                for value in values:
                    params.append(f"{name}={urllib.parse.quote(str(value))}")
            query_string = "?" + "&".join(params)
        
        # 构建请求行
        request_line = f"{self.method} {self.path}{query_string} HTTP/1.1"
        
        # 构建请求头
        header_lines = []
        for name, value in self.headers.items():
            header_lines.append(f"{name}: {value}")
        
        # 组装完整请求
        request_parts = [request_line] + header_lines + ["", self.body]
        return "\n".join(request_parts)

class HTTPResponse(HTTPMessage):
    """HTTP响应 - 就像餐厅的上菜"""
    
    def __init__(self, status_code: int, reason_phrase: str = ""):
        super().__init__()
        self.status_code = status_code
        self.reason_phrase = reason_phrase or self._get_reason_phrase(status_code)
    
    def _get_reason_phrase(self, code: int) -> str:
        """根据状态码获取原因短语"""
        status_messages = {
            200: "OK", 201: "Created", 204: "No Content",
            301: "Moved Permanently", 302: "Found", 304: "Not Modified",
            400: "Bad Request", 401: "Unauthorized", 403: "Forbidden", 404: "Not Found",
            500: "Internal Server Error", 502: "Bad Gateway", 503: "Service Unavailable"
        }
        return status_messages.get(code, "Unknown")
    
    def to_string(self) -> str:
        """转换为HTTP响应字符串"""
        # 状态行
        status_line = f"HTTP/1.1 {self.status_code} {self.reason_phrase}"
        
        # 响应头
        header_lines = []
        for name, value in self.headers.items():
            header_lines.append(f"{name}: {value}")
        
        # 组装完整响应
        response_parts = [status_line] + header_lines + ["", self.body]
        return "\n".join(response_parts)

# HTTP状态码详解
class HTTPStatusCode:
    """HTTP状态码 - 就像餐厅的服务状态"""
    
    @classmethod
    def explain_status(cls, code: int) -> str:
        """解释状态码含义"""
        status_map = {
            200: "✅ OK - 订单成功处理",
            201: "✅ Created - 新菜品已添加到菜单",
            204: "✅ No Content - 订单处理完成，无需返回内容",
            301: "🔄 Moved Permanently - 餐厅永久搬迁",
            302: "🔄 Found - 临时换到其他餐厅",
            304: "🔄 Not Modified - 菜单未更新，使用缓存",
            400: "❌ Bad Request - 订单格式错误",
            401: "❌ Unauthorized - 需要会员身份",
            403: "❌ Forbidden - 禁止点这道菜",
            404: "❌ Not Found - 菜单上没有这道菜",
            500: "💥 Internal Server Error - 厨房设备故障",
            502: "💥 Bad Gateway - 供应商出问题",
            503: "💥 Service Unavailable - 餐厅暂停营业"
        }
        
        return status_map.get(code, f"未知状态码: {code}")

# 演示HTTP消息构建
def http_message_demo():
    """HTTP消息构建演示"""
    print("=== HTTP消息构建演示 ===\n")
    
    # 创建HTTP请求
    print("🍽️ 模拟餐厅点餐（HTTP请求）:")
    request = HTTPRequest("GET", "https://restaurant.com/menu?category=main&spicy=true")
    request.add_header("Host", "restaurant.com")
    request.add_header("User-Agent", "Hungry-Customer/1.0")
    request.add_header("Accept", "application/json")
    
    print("\n📋 完整HTTP请求:")
    print(request.to_string())
    
    # 创建HTTP响应
    print("\n" + "="*50)
    print("🍜 模拟餐厅上菜（HTTP响应）:")
    response = HTTPResponse(200)
    response.add_header("Content-Type", "application/json")
    response.add_header("Server", "Restaurant-Server/2.0")
    response.set_body('{"dishes": ["宫保鸡丁", "麻婆豆腐"], "status": "available"}')
    
    print("\n📦 完整HTTP响应:")
    print(response.to_string())
    
    # 状态码演示
    print("\n" + "="*50)
    print("📊 HTTP状态码解释:")
    test_codes = [200, 404, 500, 302]
    for code in test_codes:
        print(f"{code}: {HTTPStatusCode.explain_status(code)}")

# 运行演示
if __name__ == "__main__":
    http_message_demo()
```

### 8.2.2 Python HTTP编程

#### 🐍 使用urllib库 - Python的内置HTTP工具

```python
import urllib.request
import urllib.parse
import urllib.error
import json
from typing import Dict, Any, Optional

class SimpleHTTPClient:
    """简单的HTTP客户端 - 就像一个会点餐的机器人"""
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
        self.session_headers = {
            "User-Agent": "Python-HTTP-Client/1.0"
        }
    
    def get(self, url: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        """发送GET请求 - 就像查看菜单"""
        try:
            # 构建查询参数
            if params:
                query_string = urllib.parse.urlencode(params)
                url = f"{url}?{query_string}"
            
            print(f"🔍 发送GET请求: {url}")
            
            # 创建请求对象
            request = urllib.request.Request(url)
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # 发送请求
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                # 读取响应
                status_code = response.getcode()
                headers = dict(response.headers)
                content = response.read().decode('utf-8')
                
                print(f"✅ 响应状态: {status_code}")
                print(f"📦 响应大小: {len(content)} 字符")
                
                return {
                    "status_code": status_code,
                    "headers": headers,
                    "content": content,
                    "json": self._try_parse_json(content)
                }
        
        except urllib.error.HTTPError as e:
            print(f"❌ HTTP错误: {e.code} {e.reason}")
            return {
                "status_code": e.code,
                "error": f"HTTP {e.code}: {e.reason}",
                "content": ""
            }
        except urllib.error.URLError as e:
            print(f"❌ URL错误: {e.reason}")
            return {
                "error": f"URL错误: {e.reason}",
                "content": ""
            }
        except Exception as e:
            print(f"❌ 请求失败: {e}")
            return {
                "error": str(e),
                "content": ""
            }
    
    def post(self, url: str, data: Dict[str, Any] = None, json_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """发送POST请求 - 就像提交订单"""
        try:
            print(f"📤 发送POST请求: {url}")
            
            # 准备请求数据
            request_data = None
            content_type = "application/x-www-form-urlencoded"
            
            if json_data:
                # JSON数据
                request_data = json.dumps(json_data).encode('utf-8')
                content_type = "application/json"
                print(f"📝 JSON数据: {json_data}")
            elif data:
                # 表单数据
                request_data = urllib.parse.urlencode(data).encode('utf-8')
                print(f"📝 表单数据: {data}")
            
            # 创建请求对象
            request = urllib.request.Request(url, data=request_data, method='POST')
            request.add_header('Content-Type', content_type)
            
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # 发送请求
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                status_code = response.getcode()
                headers = dict(response.headers)
                content = response.read().decode('utf-8')
                
                print(f"✅ 响应状态: {status_code}")
                
                return {
                    "status_code": status_code,
                    "headers": headers,
                    "content": content,
                    "json": self._try_parse_json(content)
                }
        
        except Exception as e:
            print(f"❌ POST请求失败: {e}")
            return {
                "error": str(e),
                "content": ""
            }
    
    def _try_parse_json(self, content: str) -> Optional[Dict[str, Any]]:
        """尝试解析JSON响应"""
        try:
            return json.loads(content)
        except:
            return None
    
    def add_header(self, name: str, value: str):
        """添加会话头部"""
        self.session_headers[name] = value
        print(f"📋 添加会话头部: {name} = {value}")

# HTTP客户端使用示例
def http_client_demo():
    """HTTP客户端使用演示"""
    print("=== HTTP客户端编程演示 ===\n")
    
    # 使用简单客户端
    print("🤖 简单HTTP客户端演示:")
    client = SimpleHTTPClient()
    
    # 模拟API调用
    print("\n1. GET请求演示:")
    response = client.get("https://httpbin.org/get", {
        "name": "张三",
        "age": "25"
    })
    
    print("\n2. POST请求演示:")
    response = client.post("https://httpbin.org/post", json_data={
        "username": "testuser",
        "message": "Hello from Python!"
    })
    
    print("HTTP客户端代码已准备就绪！")

if __name__ == "__main__":
    http_client_demo()
```

### 8.2.3 Web爬虫基础

#### 🕷️ 网页内容抓取 - 像蜘蛛一样爬取信息

```python
import urllib.request
import urllib.parse
import re
import time
import random
from typing import List, Dict, Any
from html.parser import HTMLParser

class SimpleWebScraper:
    """简单的网页爬虫 - 就像一个自动化的信息收集员"""
    
    def __init__(self, delay: float = 1.0):
        self.delay = delay  # 请求间隔，避免给服务器造成压力
        self.session_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    def fetch_page(self, url: str) -> Dict[str, Any]:
        """获取网页内容"""
        try:
            print(f"🕷️ 正在爬取: {url}")
            
            # 创建请求
            request = urllib.request.Request(url)
            for name, value in self.session_headers.items():
                request.add_header(name, value)
            
            # 发送请求
            with urllib.request.urlopen(request, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                print(f"✅ 成功获取 {len(content)} 字符")
                
                return {
                    "url": url,
                    "content": content,
                    "status": "success"
                }
        
        except Exception as e:
            print(f"❌ 爬取失败: {e}")
            return {
                "url": url,
                "content": "",
                "status": "failed",
                "error": str(e)
            }
    
    def extract_links(self, html_content: str, base_url: str = "") -> List[str]:
        """提取网页中的链接"""
        link_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>'
        links = re.findall(link_pattern, html_content, re.IGNORECASE)
        
        # 处理相对链接
        absolute_links = []
        for link in links:
            if link.startswith('http'):
                absolute_links.append(link)
            elif base_url and link.startswith('/'):
                absolute_links.append(base_url.rstrip('/') + link)
        
        print(f"🔗 找到 {len(absolute_links)} 个链接")
        return absolute_links
    
    def extract_text(self, html_content: str) -> str:
        """提取网页纯文本内容"""
        # 移除脚本和样式
        clean_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        clean_content = re.sub(r'<style[^>]*>.*?</style>', '', clean_content, flags=re.DOTALL | re.IGNORECASE)
        
        # 移除HTML标签
        text_content = re.sub(r'<[^>]+>', '', clean_content)
        
        # 清理空白字符
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        print(f"📝 提取文本 {len(text_content)} 字符")
        return text_content
    
    def crawl_website(self, start_url: str, max_pages: int = 5) -> List[Dict[str, Any]]:
        """爬取网站的多个页面"""
        visited_urls = set()
        urls_to_visit = [start_url]
        crawled_data = []
        
        print(f"🚀 开始爬取网站: {start_url}")
        print(f"📊 最大页面数: {max_pages}")
        
        while urls_to_visit and len(crawled_data) < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
            
            visited_urls.add(current_url)
            
            # 获取页面内容
            page_data = self.fetch_page(current_url)
            
            if page_data["status"] == "success":
                # 提取文本内容
                text_content = self.extract_text(page_data["content"])
                
                # 提取链接
                links = self.extract_links(page_data["content"], start_url)
                
                # 添加新链接到待访问列表
                for link in links[:3]:  # 限制每页最多3个新链接
                    if link not in visited_urls:
                        urls_to_visit.append(link)
                
                # 保存数据
                crawled_data.append({
                    "url": current_url,
                    "title": self._extract_title(page_data["content"]),
                    "text": text_content[:500],  # 只保存前500字符
                    "links_count": len(links)
                })
                
                print(f"📋 已爬取 {len(crawled_data)}/{max_pages} 页")
            
            # 延迟，避免给服务器造成压力
            time.sleep(self.delay + random.uniform(0, 0.5))
        
        print(f"🎉 爬取完成，共获取 {len(crawled_data)} 页数据")
        return crawled_data
    
    def _extract_title(self, html_content: str) -> str:
        """提取网页标题"""
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        return "无标题"

# 反爬虫技术应对
class AdvancedWebScraper(SimpleWebScraper):
    """高级网页爬虫 - 具备反爬虫应对能力"""
    
    def __init__(self, delay: float = 2.0):
        super().__init__(delay)
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
        ]
        self.session_cookies = {}
    
    def fetch_page(self, url: str) -> Dict[str, Any]:
        """带反爬虫应对的页面获取"""
        try:
            print(f"🕷️ 智能爬取: {url}")
            
            # 随机选择User-Agent
            user_agent = random.choice(self.user_agents)
            
            # 创建请求
            request = urllib.request.Request(url)
            request.add_header('User-Agent', user_agent)
            request.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
            request.add_header('Accept-Language', 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3')
            request.add_header('Accept-Encoding', 'gzip, deflate')
            request.add_header('Connection', 'keep-alive')
            
            # 添加Cookie
            if self.session_cookies:
                cookie_string = '; '.join([f"{k}={v}" for k, v in self.session_cookies.items()])
                request.add_header('Cookie', cookie_string)
            
            # 随机延迟
            delay_time = self.delay + random.uniform(0, 1.0)
            time.sleep(delay_time)
            
            # 发送请求
            with urllib.request.urlopen(request, timeout=15) as response:
                # 处理Cookie
                set_cookie = response.headers.get('Set-Cookie')
                if set_cookie:
                    self._update_cookies(set_cookie)
                
                content = response.read()
                
                # 处理gzip压缩
                if response.headers.get('Content-Encoding') == 'gzip':
                    import gzip
                    content = gzip.decompress(content)
                
                content = content.decode('utf-8', errors='ignore')
                
                print(f"✅ 智能获取成功 {len(content)} 字符")
                
                return {
                    "url": url,
                    "content": content,
                    "status": "success"
                }
        
        except Exception as e:
            print(f"❌ 智能爬取失败: {e}")
            return {
                "url": url,
                "content": "",
                "status": "failed",
                "error": str(e)
            }
    
    def _update_cookies(self, set_cookie: str):
        """更新Cookie"""
        for cookie_part in set_cookie.split(','):
            if '=' in cookie_part:
                name, value = cookie_part.split('=', 1)
                self.session_cookies[name.strip()] = value.split(';')[0].strip()

# 爬虫使用示例
def web_scraper_demo():
    """网页爬虫演示"""
    print("=== 网页爬虫演示 ===\n")
    
    # 简单爬虫演示
    print("🕷️ 简单爬虫演示:")
    simple_scraper = SimpleWebScraper(delay=0.5)
    
    # 爬取示例网站
    test_url = "https://httpbin.org/html"
    page_data = simple_scraper.fetch_page(test_url)
    
    if page_data["status"] == "success":
        text_content = simple_scraper.extract_text(page_data["content"])
        links = simple_scraper.extract_links(page_data["content"])
        
        print(f"📄 页面内容预览: {text_content[:200]}...")
        print(f"🔗 找到链接数量: {len(links)}")
    
    print("\n" + "="*50)
    print("🤖 高级爬虫演示:")
    
    advanced_scraper = AdvancedWebScraper(delay=1.0)
    
    # 演示反爬虫技术
    print("🛡️ 使用反爬虫技术:")
    print("- 随机User-Agent")
    print("- 智能延迟")
    print("- Cookie管理")
    print("- 请求头伪装")
    
    print("\n网页爬虫代码已准备就绪！")

if __name__ == "__main__":
    web_scraper_demo()
```

---

## 🎯 第8.2节总结

### 📚 核心概念回顾
1. **HTTP协议**：基于请求-响应模型的应用层协议
2. **状态码系统**：用数字表示请求处理结果的标准
3. **头部字段**：HTTP消息的元数据信息
4. **Web爬虫**：自动化的网页信息收集工具

### 💡 关键技能掌握
- ✅ 理解HTTP请求响应的完整流程
- ✅ 掌握Python urllib库的使用方法
- ✅ 能够解析和构建HTTP消息
- ✅ 具备基础的网页爬虫开发能力

### 🔗 生活化理解
- **HTTP协议** = 餐厅的点餐系统
- **状态码** = 餐厅的服务状态反馈
- **请求头** = 订单的详细说明
- **Web爬虫** = 自动化的信息收集员

### 📝 练习建议
1. 实现一个新闻网站的内容爬虫
2. 开发一个API状态监控工具
3. 创建一个简单的HTTP压力测试工具

---

*下一节我们将学习HTTP服务器开发，从客户端转向服务端...* 