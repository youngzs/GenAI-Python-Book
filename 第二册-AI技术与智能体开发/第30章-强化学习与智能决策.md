# ç¬¬30ç«  å¼ºåŒ–å­¦ä¹ ä¸æ™ºèƒ½å†³ç­–

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- **æ·±å…¥ç†è§£å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ**ï¼šæŒæ¡æ™ºèƒ½ä½“ã€ç¯å¢ƒã€çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±çš„åŸºæœ¬åŸç†
- **æŒæ¡ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼šç†Ÿç»ƒè¿ç”¨Q-Learningã€ç­–ç•¥æ¢¯åº¦ã€Actor-Criticç­‰æ ¸å¿ƒç®—æ³•
- **ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸç†**ï¼šäº†è§£DQNã€A3Cã€PPOç­‰ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•
- **è®¤è¯†å¼ºåŒ–å­¦ä¹ åº”ç”¨åœºæ™¯**ï¼šç†è§£RLåœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡
- **æ„å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ**ï¼šèƒ½å¤Ÿä½¿ç”¨Gymç¯å¢ƒå’Œè‡ªå®šä¹‰ç¯å¢ƒè¿›è¡ŒRLå®éªŒ
- **å®ç°ç»å…¸RLç®—æ³•**ï¼šä»é›¶å®ç°Q-Learningã€SARSAã€ç­–ç•¥æ¢¯åº¦ç­‰ç®—æ³•
- **å¼€å‘æ™ºèƒ½æ¸¸æˆAI**ï¼šæ„å»ºèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ å’Œå†³ç­–çš„æ¸¸æˆæ™ºèƒ½ä½“
- **åº”ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨TensorFlow/PyTorchå®ç°æ·±åº¦RLç®—æ³•

### ğŸ§  ç´ å…»ç›®æ ‡
- **åŸ¹å…»æ™ºèƒ½å†³ç­–æ€ç»´**ï¼šç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼åˆ†æå’Œè§£å†³åºè´¯å†³ç­–é—®é¢˜
- **å»ºç«‹è¯•é”™å­¦ä¹ æ„è¯†**ï¼šç†è§£é€šè¿‡ç¯å¢ƒåé¦ˆä¸æ–­ä¼˜åŒ–ç­–ç•¥çš„å­¦ä¹ æ¨¡å¼
- **å½¢æˆç³»ç»Ÿä¼˜åŒ–èƒ½åŠ›**ï¼šæŒæ¡åœ¨å¤æ‚ç¯å¢ƒä¸­å¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„æ–¹æ³•è®º
- **æ ‘ç«‹AIä¼¦ç†è§‚å¿µ**ï¼šè®¤è¯†å¼ºåŒ–å­¦ä¹ åœ¨è‡ªä¸»å†³ç­–ä¸­çš„è´£ä»»å’Œé£é™©

---

## ğŸ® 30.1 æ¬¢è¿æ¥åˆ°æ™ºèƒ½å†³ç­–å­¦é™¢ï¼

### ğŸ›ï¸ ä»çŸ¥è¯†æ£€ç´¢åˆ°æ™ºèƒ½å†³ç­–çš„å‡çº§

è¿˜è®°å¾—ç¬¬29ç« æˆ‘ä»¬å»ºç«‹çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒå—ï¼Ÿåœ¨é‚£é‡Œï¼Œæˆ‘ä»¬å­¦ä¼šäº†å¦‚ä½•ä»æµ·é‡ä¿¡æ¯ä¸­æ£€ç´¢å’Œç”ŸæˆçŸ¥è¯†ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è¦å°†è¿™ä¸ªä¸­å¿ƒå‡çº§ä¸ºä¸€ä¸ªæ›´åŠ æ™ºèƒ½çš„**æ™ºèƒ½å†³ç­–å­¦é™¢**ï¼

å¦‚æœè¯´çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒè§£å†³çš„æ˜¯"å¦‚ä½•è·å–ä¿¡æ¯"çš„é—®é¢˜ï¼Œé‚£ä¹ˆæ™ºèƒ½å†³ç­–å­¦é™¢è¦è§£å†³çš„å°±æ˜¯"å¦‚ä½•åŸºäºä¿¡æ¯åšå‡ºæœ€ä¼˜å†³ç­–"çš„é—®é¢˜ã€‚

### ğŸ¯ æ™ºèƒ½å†³ç­–å­¦é™¢çš„ç»„ç»‡æ¶æ„

```mermaid
graph TD
    A["ğŸ›ï¸ æ™ºèƒ½å†³ç­–å­¦é™¢"] --> B["ğŸ® æ¸¸æˆAIç ”ç©¶æ‰€"]
    A --> C["ğŸ¤– è‡ªä¸»å­¦ä¹ å®éªŒå®¤"]
    A --> D["ğŸ“Š ç­–ç•¥ä¼˜åŒ–ä¸­å¿ƒ"]
    A --> E["ğŸ¯ å†³ç­–è¯„ä¼°éƒ¨é—¨"]
    
    B --> B1["Q-Learningç®—æ³•"]
    B --> B2["ç­–ç•¥æ¢¯åº¦æ–¹æ³•"]
    
    C --> C1["ç¯å¢ƒäº¤äº’"]
    C --> C2["ç»éªŒå›æ”¾"]
    
    D --> D1["ç­–ç•¥æ”¹è¿›"]
    D --> D2["ä»·å€¼ä¼°è®¡"]
    
    E --> E1["æ€§èƒ½è¯„ä¼°"]
    E --> E2["æ”¶æ•›åˆ†æ"]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e3f2fd
```

### ğŸ¤– ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ

**å¼ºåŒ–å­¦ä¹ **å°±åƒæ˜¯åŸ¹å…»ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ å’Œå†³ç­–çš„æ™ºèƒ½ä½“ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦æ•™ä¸€ä¸ªæœºå™¨äººå­¦ä¼šç©æ¸¸æˆï¼š

1. **ğŸ® æœºå™¨äººè§‚å¯Ÿæ¸¸æˆçŠ¶æ€**ï¼ˆæ„ŸçŸ¥ç¯å¢ƒï¼‰
2. **ğŸ¯ æœºå™¨äººé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ**ï¼ˆåšå‡ºå†³ç­–ï¼‰
3. **ğŸ† æ¸¸æˆç»™å‡ºåˆ†æ•°å¥–åŠ±**ï¼ˆè·å¾—åé¦ˆï¼‰
4. **ğŸ§  æœºå™¨äººæ ¹æ®å¥–åŠ±è°ƒæ•´ç­–ç•¥**ï¼ˆå­¦ä¹ ä¼˜åŒ–ï¼‰
5. **ğŸ”„ é‡å¤è¿™ä¸ªè¿‡ç¨‹ç›´åˆ°æŒæ¡æ¸¸æˆ**ï¼ˆæŒç»­æ”¹è¿›ï¼‰

è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ï¼

```python
# ğŸ­ å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µæ¼”ç¤º
import numpy as np
import matplotlib.pyplot as plt

class RLBasicConcepts:
    """å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µæ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.concepts = {
            "æ™ºèƒ½ä½“(Agent)": {
                "å®šä¹‰": "åšå‡ºå†³ç­–çš„å­¦ä¹ è€…",
                "æ¯”å–»": "ğŸ¤– æ¸¸æˆç©å®¶",
                "èŒè´£": "è§‚å¯Ÿç¯å¢ƒã€é€‰æ‹©åŠ¨ä½œã€å­¦ä¹ ç­–ç•¥"
            },
            "ç¯å¢ƒ(Environment)": {
                "å®šä¹‰": "æ™ºèƒ½ä½“äº¤äº’çš„å¤–éƒ¨ä¸–ç•Œ",
                "æ¯”å–»": "ğŸ® æ¸¸æˆä¸–ç•Œ",
                "èŒè´£": "æä¾›çŠ¶æ€ã€æ¥æ”¶åŠ¨ä½œã€ç»™å‡ºå¥–åŠ±"
            },
            "çŠ¶æ€(State)": {
                "å®šä¹‰": "ç¯å¢ƒçš„å½“å‰æƒ…å†µæè¿°",
                "æ¯”å–»": "ğŸ“Š æ¸¸æˆç”»é¢",
                "ç‰¹ç‚¹": "åŒ…å«å†³ç­–æ‰€éœ€çš„å…³é”®ä¿¡æ¯"
            },
            "åŠ¨ä½œ(Action)": {
                "å®šä¹‰": "æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ",
                "æ¯”å–»": "ğŸ•¹ï¸ æŒ‰é”®æ“ä½œ",
                "ç±»å‹": "ç¦»æ•£åŠ¨ä½œ vs è¿ç»­åŠ¨ä½œ"
            },
            "å¥–åŠ±(Reward)": {
                "å®šä¹‰": "ç¯å¢ƒå¯¹åŠ¨ä½œçš„å³æ—¶åé¦ˆ",
                "æ¯”å–»": "ğŸ† æ¸¸æˆå¾—åˆ†",
                "ä½œç”¨": "æŒ‡å¯¼æ™ºèƒ½ä½“å­¦ä¹ æ–¹å‘"
            },
            "ç­–ç•¥(Policy)": {
                "å®šä¹‰": "ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„è§„åˆ™",
                "æ¯”å–»": "ğŸ¯ æ¸¸æˆç­–ç•¥",
                "ç›®æ ‡": "æœ€å¤§åŒ–é•¿æœŸç´¯ç§¯å¥–åŠ±"
            }
        }
    
    def explain_concepts(self):
        """è§£é‡Šå¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ"""
        print("ğŸ“ å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µè§£æ")
        print("=" * 50)
        
        for concept, info in self.concepts.items():
            print(f"\nğŸ” {concept}")
            print(f"   ğŸ“– å®šä¹‰ï¼š{info['å®šä¹‰']}")
            print(f"   ğŸ­ æ¯”å–»ï¼š{info['æ¯”å–»']}")
            if 'èŒè´£' in info:
                print(f"   ğŸ’¼ èŒè´£ï¼š{info['èŒè´£']}")
            elif 'ç‰¹ç‚¹' in info:
                print(f"   âœ¨ ç‰¹ç‚¹ï¼š{info['ç‰¹ç‚¹']}")
            elif 'ç±»å‹' in info:
                print(f"   ğŸ“‚ ç±»å‹ï¼š{info['ç±»å‹']}")
            elif 'ä½œç”¨' in info:
                print(f"   ğŸ¯ ä½œç”¨ï¼š{info['ä½œç”¨']}")
            elif 'ç›®æ ‡' in info:
                print(f"   ğŸ¯ ç›®æ ‡ï¼š{info['ç›®æ ‡']}")
    
    def visualize_rl_loop(self):
        """å¯è§†åŒ–å¼ºåŒ–å­¦ä¹ äº¤äº’å¾ªç¯"""
        print("\nğŸ”„ å¼ºåŒ–å­¦ä¹ äº¤äº’å¾ªç¯")
        print("=" * 30)
        
        steps = [
            "1. ğŸ¤– æ™ºèƒ½ä½“è§‚å¯Ÿå½“å‰çŠ¶æ€",
            "2. ğŸ§  åŸºäºç­–ç•¥é€‰æ‹©åŠ¨ä½œ",
            "3. ğŸ® ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œå¹¶è½¬æ¢çŠ¶æ€",
            "4. ğŸ† ç¯å¢ƒè¿”å›å¥–åŠ±ä¿¡å·",
            "5. ğŸ“š æ™ºèƒ½ä½“æ›´æ–°ç­–ç•¥",
            "6. ğŸ”„ é‡å¤ç›´åˆ°ä»»åŠ¡å®Œæˆ"
        ]
        
        for step in steps:
            print(f"   {step}")
        
        print("\nğŸ’¡ è¿™ä¸ªå¾ªç¯ä½“ç°äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ï¼š")
        print("   é€šè¿‡è¯•é”™å’Œåé¦ˆä¸æ–­ä¼˜åŒ–å†³ç­–ç­–ç•¥ï¼")

# åˆ›å»ºå¹¶è¿è¡Œæ¦‚å¿µæ¼”ç¤º
rl_concepts = RLBasicConcepts()
rl_concepts.explain_concepts()
rl_concepts.visualize_rl_loop()
```

### ğŸ² å¼ºåŒ–å­¦ä¹  vs å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¯¹æ¯”è¡¨æ¥ç†è§£å¼ºåŒ–å­¦ä¹ çš„ç‹¬ç‰¹ä¹‹å¤„ï¼š

```python
# ğŸ¯ æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”åˆ†æ
def compare_ml_methods():
    """å¯¹æ¯”ä¸åŒæœºå™¨å­¦ä¹ æ–¹æ³•çš„ç‰¹ç‚¹"""
    
    comparison = {
        "å­¦ä¹ æ–¹å¼": {
            "ç›‘ç£å­¦ä¹ ": "ğŸ§‘â€ğŸ« è€å¸ˆæä¾›æ ‡å‡†ç­”æ¡ˆ",
            "æ— ç›‘ç£å­¦ä¹ ": "ğŸ•µï¸ è‡ªå·±å‘ç°æ•°æ®è§„å¾‹",
            "å¼ºåŒ–å­¦ä¹ ": "ğŸ® é€šè¿‡è¯•é”™è·å¾—ç»éªŒ"
        },
        "æ•°æ®ç‰¹ç‚¹": {
            "ç›‘ç£å­¦ä¹ ": "ğŸ“š æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®",
            "æ— ç›‘ç£å­¦ä¹ ": "ğŸ“Š æ— æ ‡ç­¾çš„åŸå§‹æ•°æ®",
            "å¼ºåŒ–å­¦ä¹ ": "ğŸ¯ ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„ç»éªŒ"
        },
        "å­¦ä¹ ç›®æ ‡": {
            "ç›‘ç£å­¦ä¹ ": "ğŸ¯ æ‹Ÿåˆè¾“å…¥è¾“å‡ºæ˜ å°„",
            "æ— ç›‘ç£å­¦ä¹ ": "ğŸ” å‘ç°æ•°æ®å†…åœ¨ç»“æ„",
            "å¼ºåŒ–å­¦ä¹ ": "ğŸ† æœ€å¤§åŒ–é•¿æœŸç´¯ç§¯å¥–åŠ±"
        },
        "åº”ç”¨åœºæ™¯": {
            "ç›‘ç£å­¦ä¹ ": "ğŸ“ åˆ†ç±»ã€å›å½’é¢„æµ‹",
            "æ— ç›‘ç£å­¦ä¹ ": "ğŸ¨ èšç±»ã€é™ç»´ã€å¼‚å¸¸æ£€æµ‹",
            "å¼ºåŒ–å­¦ä¹ ": "ğŸ¤– æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿ"
        },
        "å­¦ä¹ è¿‡ç¨‹": {
            "ç›‘ç£å­¦ä¹ ": "ğŸ“– æ‰¹é‡å­¦ä¹ ï¼Œä¸€æ¬¡æ€§è®­ç»ƒ",
            "æ— ç›‘ç£å­¦ä¹ ": "ğŸ”¬ æ¨¡å¼å‘ç°ï¼Œç»“æ„åˆ†æ",
            "å¼ºåŒ–å­¦ä¹ ": "ğŸ”„ åœ¨çº¿å­¦ä¹ ï¼ŒæŒç»­ä¼˜åŒ–"
        }
    }
    
    print("ğŸ” æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    for aspect, methods in comparison.items():
        print(f"\nğŸ“‹ {aspect}:")
        for method, description in methods.items():
            print(f"   â€¢ {method}: {description}")
    
    print("\nğŸ’¡ å¼ºåŒ–å­¦ä¹ çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼š")
    print("   ğŸ¯ èƒ½å¤Ÿåœ¨æœªçŸ¥ç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ ")
    print("   ğŸ”„ é€šè¿‡è¯•é”™ä¸æ–­ä¼˜åŒ–ç­–ç•¥")
    print("   ğŸ† è¿½æ±‚é•¿æœŸæœ€ä¼˜è€ŒéçŸ­æœŸæœ€ä¼˜")
    print("   ğŸ¤– é€‚åˆåºè´¯å†³ç­–é—®é¢˜")

# è¿è¡Œå¯¹æ¯”åˆ†æ
compare_ml_methods()
```

---

## ğŸ® 30.2 å¼ºåŒ–å­¦ä¹ åŸºç¡€ç†è®º

### ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)

å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€æ˜¯**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)**ã€‚è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„æ¸¸æˆæ¥ç†è§£è¿™ä¸ªæ¦‚å¿µã€‚

#### ğŸ² ä»€ä¹ˆæ˜¯é©¬å°”å¯å¤«æ€§è´¨ï¼Ÿ

**é©¬å°”å¯å¤«æ€§è´¨**ï¼šæœªæ¥åªä¾èµ–äºç°åœ¨ï¼Œè€Œä¸ä¾èµ–äºè¿‡å»ã€‚

```python
# ğŸ² é©¬å°”å¯å¤«æ€§è´¨æ¼”ç¤º
import random
import numpy as np

class MarkovPropertyDemo:
    """é©¬å°”å¯å¤«æ€§è´¨æ¼”ç¤º"""
    
    def __init__(self):
        self.weather_states = ["â˜€ï¸æ™´å¤©", "ğŸŒ§ï¸é›¨å¤©", "â˜ï¸é˜´å¤©"]
        # è½¬ç§»æ¦‚ç‡çŸ©é˜µ
        self.transition_matrix = {
            "â˜€ï¸æ™´å¤©": {"â˜€ï¸æ™´å¤©": 0.7, "ğŸŒ§ï¸é›¨å¤©": 0.2, "â˜ï¸é˜´å¤©": 0.1},
            "ğŸŒ§ï¸é›¨å¤©": {"â˜€ï¸æ™´å¤©": 0.3, "ğŸŒ§ï¸é›¨å¤©": 0.4, "â˜ï¸é˜´å¤©": 0.3},
            "â˜ï¸é˜´å¤©": {"â˜€ï¸æ™´å¤©": 0.4, "ğŸŒ§ï¸é›¨å¤©": 0.3, "â˜ï¸é˜´å¤©": 0.3}
        }
    
    def predict_next_weather(self, current_weather):
        """åŸºäºå½“å‰å¤©æ°”é¢„æµ‹æ˜å¤©å¤©æ°”ï¼ˆé©¬å°”å¯å¤«æ€§è´¨ï¼‰"""
        probabilities = self.transition_matrix[current_weather]
        next_weather = random.choices(
            list(probabilities.keys()),
            weights=list(probabilities.values())
        )[0]
        return next_weather
    
    def simulate_weather_sequence(self, initial_weather, days=7):
        """æ¨¡æ‹Ÿå¤©æ°”å˜åŒ–åºåˆ—"""
        print(f"ğŸŒ¤ï¸ å¤©æ°”å˜åŒ–æ¨¡æ‹Ÿï¼ˆé©¬å°”å¯å¤«è¿‡ç¨‹ï¼‰")
        print("=" * 40)
        
        weather_sequence = [initial_weather]
        current_weather = initial_weather
        
        print(f"ç¬¬0å¤©: {current_weather}")
        
        for day in range(1, days):
            next_weather = self.predict_next_weather(current_weather)
            weather_sequence.append(next_weather)
            print(f"ç¬¬{day}å¤©: {next_weather}")
            current_weather = next_weather
        
        print(f"\nğŸ’¡ é©¬å°”å¯å¤«æ€§è´¨ä½“ç°ï¼š")
        print(f"   æ¯å¤©çš„å¤©æ°”åªä¾èµ–äºå‰ä¸€å¤©ï¼Œä¸è€ƒè™‘æ›´æ—©çš„å†å²")
        
        return weather_sequence

# è¿è¡Œé©¬å°”å¯å¤«æ€§è´¨æ¼”ç¤º
weather_demo = MarkovPropertyDemo()
sequence = weather_demo.simulate_weather_sequence("â˜€ï¸æ™´å¤©", 7)
```

#### ğŸ¯ MDPçš„äº”è¦ç´ 

ä¸€ä¸ªå®Œæ•´çš„MDPç”±äº”ä¸ªè¦ç´ ç»„æˆï¼š

```python
# ğŸ¯ MDPäº”è¦ç´ è¯¦è§£
class MDPComponents:
    """é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹äº”è¦ç´ æ¼”ç¤º"""
    
    def __init__(self):
        self.components = {
            "çŠ¶æ€ç©ºé—´(S)": {
                "å®šä¹‰": "æ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆ",
                "ä¾‹å­": "æ¸¸æˆä¸­çš„æ‰€æœ‰å¯èƒ½åœºæ™¯",
                "ç¬¦å·": "S = {sâ‚, sâ‚‚, sâ‚ƒ, ...}",
                "ç‰¹ç‚¹": "æœ‰é™æˆ–æ— é™é›†åˆ"
            },
            "åŠ¨ä½œç©ºé—´(A)": {
                "å®šä¹‰": "æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ‰€æœ‰åŠ¨ä½œ",
                "ä¾‹å­": "ä¸Šä¸‹å·¦å³ç§»åŠ¨ã€æ”»å‡»ã€é˜²å¾¡",
                "ç¬¦å·": "A = {aâ‚, aâ‚‚, aâ‚ƒ, ...}",
                "ç‰¹ç‚¹": "å¯ä»¥æ˜¯ç¦»æ•£æˆ–è¿ç»­çš„"
            },
            "è½¬ç§»æ¦‚ç‡(P)": {
                "å®šä¹‰": "æ‰§è¡ŒåŠ¨ä½œåçŠ¶æ€è½¬ç§»çš„æ¦‚ç‡",
                "ä¾‹å­": "å‘å³ç§»åŠ¨æˆåŠŸçš„æ¦‚ç‡",
                "ç¬¦å·": "P(s'|s,a)",
                "å«ä¹‰": "åœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaåè½¬ç§»åˆ°s'çš„æ¦‚ç‡"
            },
            "å¥–åŠ±å‡½æ•°(R)": {
                "å®šä¹‰": "ç¯å¢ƒå¯¹åŠ¨ä½œçš„å³æ—¶åé¦ˆ",
                "ä¾‹å­": "å¾—åˆ†+10ã€æ‰£åˆ†-5ã€æ— å˜åŒ–0",
                "ç¬¦å·": "R(s,a,s')",
                "ä½œç”¨": "æŒ‡å¯¼æ™ºèƒ½ä½“å­¦ä¹ æ–¹å‘"
            },
            "æŠ˜æ‰£å› å­(Î³)": {
                "å®šä¹‰": "æœªæ¥å¥–åŠ±çš„é‡è¦æ€§æƒé‡",
                "ä¾‹å­": "Î³=0.9è¡¨ç¤ºæœªæ¥å¥–åŠ±æ‰“9æŠ˜",
                "ç¬¦å·": "Î³ âˆˆ [0,1]",
                "æ„ä¹‰": "å¹³è¡¡å³æ—¶å¥–åŠ±å’Œé•¿æœŸå¥–åŠ±"
            }
        }
    
    def explain_components(self):
        """è¯¦ç»†è§£é‡ŠMDPäº”è¦ç´ """
        print("ğŸ¯ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)äº”è¦ç´ ")
        print("=" * 50)
        
        for i, (component, info) in enumerate(self.components.items(), 1):
            print(f"\n{i}. ğŸ“‹ {component}")
            print(f"   ğŸ“– å®šä¹‰ï¼š{info['å®šä¹‰']}")
            print(f"   ğŸ® ä¾‹å­ï¼š{info['ä¾‹å­']}")
            print(f"   ğŸ”¢ ç¬¦å·ï¼š{info['ç¬¦å·']}")
            
            if 'ç‰¹ç‚¹' in info:
                print(f"   âœ¨ ç‰¹ç‚¹ï¼š{info['ç‰¹ç‚¹']}")
            elif 'å«ä¹‰' in info:
                print(f"   ğŸ’¡ å«ä¹‰ï¼š{info['å«ä¹‰']}")
            elif 'ä½œç”¨' in info:
                print(f"   ğŸ¯ ä½œç”¨ï¼š{info['ä½œç”¨']}")
            elif 'æ„ä¹‰' in info:
                print(f"   ğŸŒŸ æ„ä¹‰ï¼š{info['æ„ä¹‰']}")
    
    def create_simple_mdp_example(self):
        """åˆ›å»ºä¸€ä¸ªç®€å•çš„MDPç¤ºä¾‹"""
        print(f"\nğŸ® ç®€å•MDPç¤ºä¾‹ï¼šå¯»å®æ¸¸æˆ")
        print("=" * 30)
        
        # 3x3ç½‘æ ¼ä¸–ç•Œ
        grid_world = """
        ğŸ [ ] [ğŸ’]
        [ ] [ğŸ•³ï¸] [ ]
        [ğŸ¤–] [ ] [ ]
        """
        
        print("ğŸ—ºï¸ æ¸¸æˆåœ°å›¾ï¼š")
        print(grid_world)
        
        print("ğŸ“‹ MDPè¦ç´ ï¼š")
        print("   ğŸ¯ çŠ¶æ€ç©ºé—´S: 9ä¸ªç½‘æ ¼ä½ç½®")
        print("   ğŸ•¹ï¸ åŠ¨ä½œç©ºé—´A: {ä¸Š, ä¸‹, å·¦, å³}")
        print("   ğŸ“Š è½¬ç§»æ¦‚ç‡P: ç§»åŠ¨æˆåŠŸç‡90%ï¼Œå¤±è´¥æ—¶ç•™åœ¨åŸåœ°")
        print("   ğŸ† å¥–åŠ±å‡½æ•°R:")
        print("      â€¢ åˆ°è¾¾å®çŸ³ğŸ’: +100")
        print("      â€¢ æ‰å…¥é™·é˜±ğŸ•³ï¸: -100")
        print("      â€¢ å…¶ä»–ç§»åŠ¨: -1")
        print("   â° æŠ˜æ‰£å› å­Î³: 0.9")
        
        print("\nğŸ¯ ç›®æ ‡ï¼šä»ğŸ¤–ä½ç½®å‡ºå‘ï¼Œæ‰¾åˆ°ğŸ’å®çŸ³ï¼Œé¿å¼€ğŸ•³ï¸é™·é˜±")

# è¿è¡ŒMDPç»„ä»¶æ¼”ç¤º
mdp_demo = MDPComponents()
mdp_demo.explain_components()
mdp_demo.create_simple_mdp_example()
```

### ğŸ“Š ä»·å€¼å‡½æ•°ä¸ç­–ç•¥

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°çŠ¶æ€çš„å¥½åå’ŒåŠ¨ä½œçš„ä¼˜åŠ£ï¼Œè¿™å°±éœ€è¦**ä»·å€¼å‡½æ•°**çš„æ¦‚å¿µã€‚

#### ğŸ¯ çŠ¶æ€ä»·å€¼å‡½æ•° V(s)

```python
# ğŸ“Š ä»·å€¼å‡½æ•°æ¦‚å¿µæ¼”ç¤º
class ValueFunctionDemo:
    """ä»·å€¼å‡½æ•°æ¦‚å¿µæ¼”ç¤º"""
    
    def __init__(self):
        # ç®€å•çš„3x3ç½‘æ ¼ä¸–ç•Œ
        self.grid_size = 3
        self.states = [(i, j) for i in range(3) for j in range(3)]
        self.treasure = (0, 2)  # å®çŸ³ä½ç½®
        self.trap = (1, 1)      # é™·é˜±ä½ç½®
        self.start = (2, 0)     # èµ·å§‹ä½ç½®
        
        # å¥–åŠ±è®¾ç½®
        self.rewards = {
            self.treasure: 100,
            self.trap: -100
        }
        
        self.gamma = 0.9  # æŠ˜æ‰£å› å­
    
    def calculate_state_values(self):
        """è®¡ç®—çŠ¶æ€ä»·å€¼å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ“Š çŠ¶æ€ä»·å€¼å‡½æ•°V(s)è®¡ç®—")
        print("=" * 35)
        
        # ç®€åŒ–çš„ä»·å€¼è®¡ç®—ï¼ˆåŸºäºè·ç¦»å’Œå¥–åŠ±ï¼‰
        state_values = {}
        
        for state in self.states:
            if state == self.treasure:
                value = 100
            elif state == self.trap:
                value = -100
            else:
                # åŸºäºåˆ°å®çŸ³çš„æ›¼å“ˆé¡¿è·ç¦»è®¡ç®—ä»·å€¼
                distance_to_treasure = abs(state[0] - self.treasure[0]) + abs(state[1] - self.treasure[1])
                distance_to_trap = abs(state[0] - self.trap[0]) + abs(state[1] - self.trap[1])
                
                # ç®€åŒ–çš„ä»·å€¼è®¡ç®—
                value = (50 / (distance_to_treasure + 1)) - (30 / (distance_to_trap + 1))
            
            state_values[state] = round(value, 2)
        
        # å¯è§†åŒ–çŠ¶æ€ä»·å€¼
        print("ğŸ—ºï¸ çŠ¶æ€ä»·å€¼åˆ†å¸ƒï¼š")
        for i in range(3):
            row = ""
            for j in range(3):
                state = (i, j)
                value = state_values[state]
                if state == self.treasure:
                    row += f"[ğŸ’{value:>6}] "
                elif state == self.trap:
                    row += f"[ğŸ•³ï¸{value:>6}] "
                elif state == self.start:
                    row += f"[ğŸ¤–{value:>6}] "
                else:
                    row += f"[  {value:>6}] "
            print(f"   {row}")
        
        print("\nğŸ’¡ ä»·å€¼å‡½æ•°å«ä¹‰ï¼š")
        print("   â€¢ æ­£å€¼è¶Šå¤§ï¼šè¯¥çŠ¶æ€è¶Šæœ‰åˆ©")
        print("   â€¢ è´Ÿå€¼è¶Šå°ï¼šè¯¥çŠ¶æ€è¶Šå±é™©")
        print("   â€¢ ä»·å€¼æŒ‡å¯¼æ™ºèƒ½ä½“é€‰æ‹©æ›´å¥½çš„è·¯å¾„")
        
        return state_values
    
    def explain_action_value_function(self):
        """è§£é‡ŠåŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)"""
        print(f"\nğŸ¯ åŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)")
        print("=" * 30)
        
        print("ğŸ“– å®šä¹‰ï¼šåœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaçš„æœŸæœ›ç´¯ç§¯å¥–åŠ±")
        print("ğŸ”¢ å…¬å¼ï¼šQ(s,a) = E[Rt+1 + Î³Rt+2 + Î³Â²Rt+3 + ... | St=s, At=a]")
        
        print("\nğŸ® å®é™…å«ä¹‰ï¼š")
        print("   â€¢ Q(å½“å‰ä½ç½®, å‘ä¸Š) = å‘ä¸Šç§»åŠ¨çš„é•¿æœŸä»·å€¼")
        print("   â€¢ Q(å½“å‰ä½ç½®, å‘å³) = å‘å³ç§»åŠ¨çš„é•¿æœŸä»·å€¼")
        print("   â€¢ Q(å½“å‰ä½ç½®, å‘ä¸‹) = å‘ä¸‹ç§»åŠ¨çš„é•¿æœŸä»·å€¼")
        print("   â€¢ Q(å½“å‰ä½ç½®, å‘å·¦) = å‘å·¦ç§»åŠ¨çš„é•¿æœŸä»·å€¼")
        
        print("\nğŸ§  æ™ºèƒ½ä½“å†³ç­–ï¼š")
        print("   é€‰æ‹©ä½¿Q(s,a)æœ€å¤§çš„åŠ¨ä½œa*")
        print("   a* = argmax Q(s,a)")

# è¿è¡Œä»·å€¼å‡½æ•°æ¼”ç¤º
value_demo = ValueFunctionDemo()
state_values = value_demo.calculate_state_values()
value_demo.explain_action_value_function()
```

#### ğŸ¯ ç­–ç•¥çš„æ¦‚å¿µ

```python
# ğŸ¯ ç­–ç•¥æ¦‚å¿µæ¼”ç¤º
class PolicyDemo:
    """ç­–ç•¥æ¦‚å¿µæ¼”ç¤º"""
    
    def __init__(self):
        self.actions = ["â¬†ï¸", "â¬‡ï¸", "â¬…ï¸", "â¡ï¸"]
        self.action_names = ["ä¸Š", "ä¸‹", "å·¦", "å³"]
    
    def demonstrate_policy_types(self):
        """æ¼”ç¤ºä¸åŒç±»å‹çš„ç­–ç•¥"""
        print("ğŸ¯ ç­–ç•¥(Policy)ç±»å‹æ¼”ç¤º")
        print("=" * 35)
        
        print("1. ğŸ“‹ ç¡®å®šæ€§ç­–ç•¥(Deterministic Policy)")
        print("   å®šä¹‰ï¼šÏ€(s) = aï¼Œåœ¨çŠ¶æ€sæ€»æ˜¯é€‰æ‹©åŠ¨ä½œa")
        print("   ä¾‹å­ï¼šåœ¨æ¯ä¸ªä½ç½®éƒ½é€‰æ‹©å›ºå®šçš„ç§»åŠ¨æ–¹å‘")
        
        # æ¼”ç¤ºç¡®å®šæ€§ç­–ç•¥
        deterministic_policy = {
            (0,0): "â¡ï¸", (0,1): "â¡ï¸", (0,2): "ğŸ’",
            (1,0): "â¬†ï¸", (1,1): "ğŸ•³ï¸", (1,2): "â¬‡ï¸",
            (2,0): "â¬†ï¸", (2,1): "â¬†ï¸", (2,2): "â¬†ï¸"
        }
        
        print("\n   ğŸ—ºï¸ ç¡®å®šæ€§ç­–ç•¥ç¤ºä¾‹ï¼š")
        for i in range(3):
            row = "   "
            for j in range(3):
                action = deterministic_policy[(i,j)]
                row += f"[{action}] "
            print(row)
        
        print("\n2. ğŸ² éšæœºæ€§ç­–ç•¥(Stochastic Policy)")
        print("   å®šä¹‰ï¼šÏ€(a|s) = P(At=a|St=s)ï¼Œç»™å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ")
        print("   ä¾‹å­ï¼šåœ¨æŸä¸ªä½ç½®ï¼Œ30%å‘ä¸Šï¼Œ70%å‘å³")
        
        # æ¼”ç¤ºéšæœºæ€§ç­–ç•¥
        print("\n   ğŸ“Š éšæœºç­–ç•¥ç¤ºä¾‹(ä½ç½®(1,0))ï¼š")
        stochastic_example = {
            "â¬†ï¸ä¸Š": 0.4,
            "â¡ï¸å³": 0.3,
            "â¬‡ï¸ä¸‹": 0.2,
            "â¬…ï¸å·¦": 0.1
        }
        
        for action, prob in stochastic_example.items():
            print(f"      {action}: {prob*100}%")
        
        print("\nğŸ’¡ ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ï¼š")
        print("   æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥Ï€*ï¼Œä½¿æœŸæœ›ç´¯ç§¯å¥–åŠ±æœ€å¤§")
        print("   Ï€* = argmax E[âˆ‘Î³áµ—Rt | Ï€]")
    
    def demonstrate_policy_evaluation(self):
        """æ¼”ç¤ºç­–ç•¥è¯„ä¼°è¿‡ç¨‹"""
        print(f"\nğŸ“ˆ ç­–ç•¥è¯„ä¼°ä¸æ”¹è¿›")
        print("=" * 25)
        
        print("ğŸ”„ ç­–ç•¥è¿­ä»£ç®—æ³•æµç¨‹ï¼š")
        steps = [
            "1. ğŸ¯ åˆå§‹åŒ–éšæœºç­–ç•¥Ï€â‚€",
            "2. ğŸ“Š ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—Váµ–(s)",
            "3. ğŸ”§ ç­–ç•¥æ”¹è¿›ï¼šÏ€' = greedy(Váµ–)",
            "4. âœ… æ£€æŸ¥æ”¶æ•›ï¼šÏ€' = Ï€?",
            "5. ğŸ”„ å¦‚æœæœªæ”¶æ•›ï¼Œè¿”å›æ­¥éª¤2"
        ]
        
        for step in steps:
            print(f"   {step}")
        
        print("\nğŸ’¡ æ ¸å¿ƒæ€æƒ³ï¼š")
        print("   â€¢ è¯„ä¼°å½“å‰ç­–ç•¥çš„ä»·å€¼")
        print("   â€¢ åŸºäºä»·å€¼è´ªå¿ƒåœ°æ”¹è¿›ç­–ç•¥")
        print("   â€¢ é‡å¤ç›´åˆ°ç­–ç•¥ä¸å†å˜åŒ–")

# è¿è¡Œç­–ç•¥æ¼”ç¤º
policy_demo = PolicyDemo()
policy_demo.demonstrate_policy_types()
policy_demo.demonstrate_policy_evaluation()

---

## ğŸ§  30.3 Q-Learningç®—æ³•è¯¦è§£

### ğŸ¯ Q-Learningï¼šæ— æ¨¡å‹çš„ä»·å€¼å­¦ä¹ 

**Q-Learning**æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€ç»å…¸çš„ç®—æ³•ä¹‹ä¸€ï¼Œå®ƒä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜çš„åŠ¨ä½œä»·å€¼å‡½æ•°ã€‚

#### ğŸ”¬ Q-Learningç®—æ³•åŸç†

Q-Learningçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨**æ—¶åºå·®åˆ†(Temporal Difference, TD)**æ–¹æ³•æ¥æ›´æ–°Qå€¼ï¼š

```python
# ğŸ§  Q-Learningç®—æ³•å®ç°
import numpy as np
import random
import matplotlib.pyplot as plt
from collections import defaultdict

class QLearningAgent:
    """Q-Learningæ™ºèƒ½ä½“å®ç°"""
    
    def __init__(self, actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):
        """
        åˆå§‹åŒ–Q-Learningæ™ºèƒ½ä½“
        
        Args:
            actions: å¯æ‰§è¡Œçš„åŠ¨ä½œåˆ—è¡¨
            learning_rate: å­¦ä¹ ç‡Î±
            discount_factor: æŠ˜æ‰£å› å­Î³
            epsilon: Îµ-è´ªå¿ƒç­–ç•¥çš„æ¢ç´¢ç‡
        """
        self.actions = actions
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        
        # Qè¡¨ï¼šå­˜å‚¨çŠ¶æ€-åŠ¨ä½œä»·å€¼
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'episodes': [],
            'rewards': [],
            'steps': []
        }
    
    def get_action(self, state, training=True):
        """
        æ ¹æ®Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.choice(self.actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            q_values = [self.q_table[state][action] for action in self.actions]
            max_q = max(q_values)
            
            # å¦‚æœæœ‰å¤šä¸ªæœ€å¤§å€¼ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
            max_actions = [action for action, q in zip(self.actions, q_values) if q == max_q]
            return random.choice(max_actions)
    
    def update_q_table(self, state, action, reward, next_state, done):
        """
        ä½¿ç”¨Q-Learningæ›´æ–°è§„åˆ™æ›´æ–°Qè¡¨
        
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
        """
        current_q = self.q_table[state][action]
        
        if done:
            # ç»ˆæ­¢çŠ¶æ€ï¼Œæ²¡æœ‰ä¸‹ä¸€æ­¥
            target_q = reward
        else:
            # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
            next_q_values = [self.q_table[next_state][a] for a in self.actions]
            max_next_q = max(next_q_values) if next_q_values else 0
            target_q = reward + self.gamma * max_next_q
        
        # Q-Learningæ›´æ–°è§„åˆ™
        self.q_table[state][action] = current_q + self.lr * (target_q - current_q)
    
    def train_episode(self, env, max_steps=1000):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action = self.get_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = env.step(action)
            
            # æ›´æ–°Qè¡¨
            self.update_q_table(state, action, reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        return total_reward, steps
    
    def print_q_table(self):
        """æ‰“å°Qè¡¨ï¼ˆé€‚ç”¨äºå°è§„æ¨¡é—®é¢˜ï¼‰"""
        print("ğŸ“Š Qè¡¨å†…å®¹ï¼š")
        print("=" * 40)
        
        if not self.q_table:
            print("   Qè¡¨ä¸ºç©º")
            return
        
        # è·å–æ‰€æœ‰çŠ¶æ€
        states = sorted(self.q_table.keys())
        
        # æ‰“å°è¡¨å¤´
        header = "çŠ¶æ€\\åŠ¨ä½œ"
        for action in self.actions:
            header += f"\t{action}"
        print(header)
        print("-" * len(header.expandtabs()))
        
        # æ‰“å°æ¯ä¸ªçŠ¶æ€çš„Qå€¼
        for state in states:
            row = f"{state}"
            for action in self.actions:
                q_value = self.q_table[state][action]
                row += f"\t{q_value:.2f}"
            print(row)

class GridWorldEnvironment:
    """ç®€å•çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ"""
    
    def __init__(self, size=4):
        """
        åˆ›å»ºç½‘æ ¼ä¸–ç•Œ
        
        Args:
            size: ç½‘æ ¼å¤§å°
        """
        self.size = size
        self.start_pos = (0, 0)
        self.goal_pos = (size-1, size-1)
        self.trap_pos = (1, 1)  # é™·é˜±ä½ç½®
        
        self.current_pos = self.start_pos
        self.actions = ['up', 'down', 'left', 'right']
        
        # åŠ¨ä½œåˆ°åæ ‡å˜åŒ–çš„æ˜ å°„
        self.action_map = {
            'up': (-1, 0),
            'down': (1, 0),
            'left': (0, -1),
            'right': (0, 1)
        }
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€"""
        self.current_pos = self.start_pos
        return self.current_pos
    
    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        Returns:
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            reward: å¥–åŠ±
            done: æ˜¯å¦ç»“æŸ
        """
        # è®¡ç®—æ–°ä½ç½®
        delta = self.action_map[action]
        new_pos = (
            self.current_pos[0] + delta[0],
            self.current_pos[1] + delta[1]
        )
        
        # æ£€æŸ¥è¾¹ç•Œ
        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size):
            self.current_pos = new_pos
        # å¦‚æœè¶…å‡ºè¾¹ç•Œï¼Œä¿æŒåŸä½ç½®
        
        # è®¡ç®—å¥–åŠ±
        if self.current_pos == self.goal_pos:
            reward = 100  # åˆ°è¾¾ç›®æ ‡
            done = True
        elif self.current_pos == self.trap_pos:
            reward = -100  # æ‰å…¥é™·é˜±
            done = True
        else:
            reward = -1  # æ¯æ­¥çš„å°æƒ©ç½š
            done = False
        
        return self.current_pos, reward, done
    
    def render(self):
        """å¯è§†åŒ–å½“å‰ç¯å¢ƒçŠ¶æ€"""
        print("\nğŸ—ºï¸ å½“å‰ç¯å¢ƒçŠ¶æ€ï¼š")
        for i in range(self.size):
            row = ""
            for j in range(self.size):
                pos = (i, j)
                if pos == self.current_pos:
                    row += "ğŸ¤– "
                elif pos == self.goal_pos:
                    row += "ğŸ¯ "
                elif pos == self.trap_pos:
                    row += "ğŸ•³ï¸ "
                else:
                    row += "â¬œ "
            print(f"   {row}")

def demonstrate_q_learning():
    """æ¼”ç¤ºQ-Learningç®—æ³•"""
    print("ğŸ§  Q-Learningç®—æ³•æ¼”ç¤º")
    print("=" * 40)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = GridWorldEnvironment(size=4)
    agent = QLearningAgent(
        actions=env.actions,
        learning_rate=0.1,
        discount_factor=0.9,
        epsilon=0.1
    )
    
    print("ğŸ® ç¯å¢ƒè®¾ç½®ï¼š")
    print("   â€¢ 4x4ç½‘æ ¼ä¸–ç•Œ")
    print("   â€¢ ğŸ¤– èµ·ç‚¹ï¼š(0,0)")
    print("   â€¢ ğŸ¯ ç›®æ ‡ï¼š(3,3)")
    print("   â€¢ ğŸ•³ï¸ é™·é˜±ï¼š(1,1)")
    print("   â€¢ å¥–åŠ±ï¼šç›®æ ‡+100ï¼Œé™·é˜±-100ï¼Œç§»åŠ¨-1")
    
    env.render()
    
    # è®­ç»ƒè¿‡ç¨‹
    print("\nğŸ“ å¼€å§‹è®­ç»ƒ...")
    episodes = 500
    
    for episode in range(episodes):
        total_reward, steps = agent.train_episode(env)
        agent.learning_stats['episodes'].append(episode)
        agent.learning_stats['rewards'].append(total_reward)
        agent.learning_stats['steps'].append(steps)
        
        # æ¯100ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(agent.learning_stats['rewards'][-100:])
            avg_steps = np.mean(agent.learning_stats['steps'][-100:])
            print(f"   Episode {episode+1}: å¹³å‡å¥–åŠ±={avg_reward:.2f}, å¹³å‡æ­¥æ•°={avg_steps:.2f}")
    
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    
    # æ‰“å°å­¦ä¹ åˆ°çš„Qè¡¨
    agent.print_q_table()
    
    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    print(f"\nğŸ¯ æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ï¼š")
    test_episodes = 5
    
    for test_ep in range(test_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        path = [state]
        
        print(f"\n   æµ‹è¯•Episode {test_ep+1}:")
        
        for step in range(20):  # æœ€å¤š20æ­¥
            action = agent.get_action(state, training=False)
            next_state, reward, done = env.step(action)
            
            path.append(next_state)
            total_reward += reward
            steps += 1
            
            print(f"      æ­¥éª¤{step+1}: {state} --{action}--> {next_state}, å¥–åŠ±={reward}")
            
            state = next_state
            if done:
                break
        
        print(f"      ç»“æœ: æ€»å¥–åŠ±={total_reward}, æ€»æ­¥æ•°={steps}")
        print(f"      è·¯å¾„: {' -> '.join(map(str, path))}")

# è¿è¡ŒQ-Learningæ¼”ç¤º
demonstrate_q_learning()
```

#### ğŸ” Q-Learningç®—æ³•åˆ†æ

```python
# ğŸ” Q-Learningç®—æ³•ç‰¹ç‚¹åˆ†æ
class QLearningAnalysis:
    """Q-Learningç®—æ³•ç‰¹ç‚¹åˆ†æ"""
    
    def __init__(self):
        self.characteristics = {
            "ç®—æ³•ç±»å‹": {
                "åˆ†ç±»": "æ— æ¨¡å‹(Model-free)å¼ºåŒ–å­¦ä¹ ",
                "ç‰¹ç‚¹": "ä¸éœ€è¦ç¯å¢ƒè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°",
                "ä¼˜åŠ¿": "é€‚ç”¨äºæœªçŸ¥ç¯å¢ƒ"
            },
            "å­¦ä¹ æ–¹å¼": {
                "åˆ†ç±»": "ç¦»ç­–ç•¥(Off-policy)å­¦ä¹ ",
                "ç‰¹ç‚¹": "å­¦ä¹ çš„ç­–ç•¥ä¸è¡Œä¸ºç­–ç•¥å¯ä»¥ä¸åŒ",
                "ä¼˜åŠ¿": "å¯ä»¥ä»ä»»æ„ç­–ç•¥çš„ç»éªŒä¸­å­¦ä¹ "
            },
            "æ”¶æ•›æ€§": {
                "æ¡ä»¶": "æ»¡è¶³ä¸€å®šæ¡ä»¶ä¸‹ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜",
                "è¦æ±‚": "æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹è¢«æ— é™æ¬¡è®¿é—®",
                "å®é™…": "åœ¨å®è·µä¸­é€šå¸¸èƒ½æ‰¾åˆ°å¾ˆå¥½çš„ç­–ç•¥"
            },
            "æ¢ç´¢ç­–ç•¥": {
                "å¸¸ç”¨": "Îµ-è´ªå¿ƒç­–ç•¥",
                "å¹³è¡¡": "æ¢ç´¢(exploration) vs åˆ©ç”¨(exploitation)",
                "è°ƒèŠ‚": "Îµå€¼å¯ä»¥éšè®­ç»ƒè¿‡ç¨‹è¡°å‡"
            }
        }
    
    def analyze_algorithm(self):
        """åˆ†æQ-Learningç®—æ³•ç‰¹ç‚¹"""
        print("ğŸ” Q-Learningç®—æ³•æ·±åº¦åˆ†æ")
        print("=" * 45)
        
        for aspect, info in self.characteristics.items():
            print(f"\nğŸ“‹ {aspect}:")
            for key, value in info.items():
                print(f"   â€¢ {key}: {value}")
    
    def compare_update_rules(self):
        """æ¯”è¾ƒä¸åŒçš„æ›´æ–°è§„åˆ™"""
        print(f"\nğŸ“Š æ›´æ–°è§„åˆ™å¯¹æ¯”")
        print("=" * 25)
        
        update_rules = {
            "Q-Learning": {
                "å…¬å¼": "Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]",
                "ç‰¹ç‚¹": "ä½¿ç”¨ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼",
                "ç­–ç•¥": "ç¦»ç­–ç•¥å­¦ä¹ "
            },
            "SARSA": {
                "å…¬å¼": "Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]",
                "ç‰¹ç‚¹": "ä½¿ç”¨å®é™…é€‰æ‹©çš„ä¸‹ä¸€åŠ¨ä½œçš„Qå€¼",
                "ç­–ç•¥": "åœ¨ç­–ç•¥å­¦ä¹ "
            },
            "Expected SARSA": {
                "å…¬å¼": "Q(s,a) â† Q(s,a) + Î±[r + Î³ E[Q(s',a')] - Q(s,a)]",
                "ç‰¹ç‚¹": "ä½¿ç”¨ä¸‹ä¸€çŠ¶æ€Qå€¼çš„æœŸæœ›",
                "ç­–ç•¥": "ä»‹äºä¸¤è€…ä¹‹é—´"
            }
        }
        
        for method, info in update_rules.items():
            print(f"\nğŸ§® {method}:")
            print(f"   ğŸ“ å…¬å¼: {info['å…¬å¼']}")
            print(f"   âœ¨ ç‰¹ç‚¹: {info['ç‰¹ç‚¹']}")
            print(f"   ğŸ¯ ç­–ç•¥: {info['ç­–ç•¥']}")
    
    def discuss_hyperparameters(self):
        """è®¨è®ºè¶…å‚æ•°çš„å½±å“"""
        print(f"\nâš™ï¸ è¶…å‚æ•°è°ƒä¼˜æŒ‡å—")
        print("=" * 25)
        
        hyperparams = {
            "å­¦ä¹ ç‡(Î±)": {
                "èŒƒå›´": "0 < Î± â‰¤ 1",
                "ä½œç”¨": "æ§åˆ¶å­¦ä¹ é€Ÿåº¦",
                "è°ƒä¼˜": "é€šå¸¸ä»0.1å¼€å§‹ï¼Œå¯ä»¥è¡°å‡",
                "å½±å“": "å¤ªå¤§ä¸ç¨³å®šï¼Œå¤ªå°æ”¶æ•›æ…¢"
            },
            "æŠ˜æ‰£å› å­(Î³)": {
                "èŒƒå›´": "0 â‰¤ Î³ < 1",
                "ä½œç”¨": "å¹³è¡¡å³æ—¶ä¸é•¿æœŸå¥–åŠ±",
                "è°ƒä¼˜": "é€šå¸¸è®¾ä¸º0.9-0.99",
                "å½±å“": "æ¥è¿‘1é‡è§†é•¿æœŸï¼Œæ¥è¿‘0é‡è§†å³æ—¶"
            },
            "æ¢ç´¢ç‡(Îµ)": {
                "èŒƒå›´": "0 â‰¤ Îµ â‰¤ 1",
                "ä½œç”¨": "æ§åˆ¶æ¢ç´¢ç¨‹åº¦",
                "è°ƒä¼˜": "ä»0.1å¼€å§‹ï¼Œå¯ä»¥è¡°å‡åˆ°0.01",
                "å½±å“": "å¤ªå¤§è¿‡åº¦æ¢ç´¢ï¼Œå¤ªå°é™·å…¥å±€éƒ¨æœ€ä¼˜"
            }
        }
        
        for param, info in hyperparams.items():
            print(f"\nğŸ›ï¸ {param}:")
            for key, value in info.items():
                print(f"   â€¢ {key}: {value}")

# è¿è¡ŒQ-Learningåˆ†æ
analysis = QLearningAnalysis()
analysis.analyze_algorithm()
analysis.compare_update_rules()
analysis.discuss_hyperparameters()
```

---

## ğŸ¯ 30.4 ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### ğŸ¨ ä»ä»·å€¼å‡½æ•°åˆ°ç­–ç•¥ä¼˜åŒ–

è™½ç„¶Q-Learningé€šè¿‡å­¦ä¹ ä»·å€¼å‡½æ•°æ¥é—´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥**ç›´æ¥ä¼˜åŒ–ç­–ç•¥**ã€‚è¿™å°±æ˜¯**ç­–ç•¥æ¢¯åº¦æ–¹æ³•**çš„æ ¸å¿ƒæ€æƒ³ã€‚

#### ğŸ§® ç­–ç•¥æ¢¯åº¦çš„æ•°å­¦åŸç†

ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥å‚æ•°åŒ–ç­–ç•¥å‡½æ•°ï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼š

```python
# ğŸ¯ ç­–ç•¥æ¢¯åº¦æ–¹æ³•å®ç°
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼šè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    
    def __init__(self, state_size, action_size, hidden_size=128):
        """
        åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            hidden_size: éšè—å±‚å¤§å°
        """
        super(PolicyNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼šè®¡ç®—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class REINFORCEAgent:
    """REINFORCEç®—æ³•å®ç°ï¼ˆç­–ç•¥æ¢¯åº¦çš„åŸºç¡€ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, state_size, action_size, learning_rate=0.01, gamma=0.99):
        """
        åˆå§‹åŒ–REINFORCEæ™ºèƒ½ä½“
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        self.policy_net = PolicyNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # å­˜å‚¨ä¸€ä¸ªepisodeçš„ç»éªŒ
        self.episode_states = []
        self.episode_actions = []
        self.episode_rewards = []
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'episodes': [],
            'rewards': [],
            'policy_losses': []
        }
    
    def get_action(self, state):
        """
        æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy_net(state_tensor)
        
        # åˆ›å»ºæ¦‚ç‡åˆ†å¸ƒå¹¶é‡‡æ ·åŠ¨ä½œ
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob
    
    def store_experience(self, state, action, reward):
        """å­˜å‚¨ä¸€æ­¥çš„ç»éªŒ"""
        self.episode_states.append(state)
        self.episode_actions.append(action)
        self.episode_rewards.append(reward)
    
    def calculate_returns(self):
        """è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„ç´¯ç§¯å›æŠ¥"""
        returns = []
        G = 0
        
        # ä»åå¾€å‰è®¡ç®—ç´¯ç§¯å›æŠ¥
        for reward in reversed(self.episode_rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        
        # æ ‡å‡†åŒ–å›æŠ¥ï¼ˆå¯é€‰ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§ï¼‰
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return returns
    
    def update_policy(self):
        """ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
        if len(self.episode_rewards) == 0:
            return
        
        # è®¡ç®—ç´¯ç§¯å›æŠ¥
        returns = self.calculate_returns()
        
        # è®¡ç®—ç­–ç•¥æŸå¤±
        policy_losses = []
        
        for i in range(len(self.episode_states)):
            state = torch.FloatTensor(self.episode_states[i]).unsqueeze(0)
            action_probs = self.policy_net(state)
            dist = Categorical(action_probs)
            
            action = self.episode_actions[i]
            log_prob = dist.log_prob(torch.tensor(action))
            
            # ç­–ç•¥æ¢¯åº¦ï¼šlog Ï€(a|s) * G
            policy_loss = -log_prob * returns[i]
            policy_losses.append(policy_loss)
        
        # è®¡ç®—æ€»æŸå¤±
        total_loss = torch.stack(policy_losses).sum()
        
        # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        # è®°å½•æŸå¤±
        self.learning_stats['policy_losses'].append(total_loss.item())
        
        # æ¸…ç©ºepisodeç»éªŒ
        self.episode_states = []
        self.episode_actions = []
        self.episode_rewards = []
        
        return total_loss.item()

class CartPoleEnvironment:
    """ç®€åŒ–çš„CartPoleç¯å¢ƒï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¯å¢ƒ"""
        self.state_size = 4  # [ä½ç½®, é€Ÿåº¦, è§’åº¦, è§’é€Ÿåº¦]
        self.action_size = 2  # [å‘å·¦, å‘å³]
        
        # ç¯å¢ƒå‚æ•°
        self.gravity = 9.8
        self.masscart = 1.0
        self.masspole = 0.1
        self.total_mass = (self.masspole + self.masscart)
        self.length = 0.5
        self.polemass_length = (self.masspole * self.length)
        self.force_mag = 10.0
        self.tau = 0.02  # seconds between state updates
        
        # ç»ˆæ­¢æ¡ä»¶
        self.theta_threshold_radians = 12 * 2 * np.pi / 360
        self.x_threshold = 2.4
        
        self.reset()
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))
        self.steps_beyond_done = None
        return self.state.copy()
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        x, x_dot, theta, theta_dot = self.state
        force = self.force_mag if action == 1 else -self.force_mag
        
        costheta = np.cos(theta)
        sintheta = np.sin(theta)
        
        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
        thetaacc = (self.gravity * sintheta - costheta * temp) / \
                   (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))
        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
        
        x = x + self.tau * x_dot
        x_dot = x_dot + self.tau * xacc
        theta = theta + self.tau * theta_dot
        theta_dot = theta_dot + self.tau * thetaacc
        
        self.state = np.array([x, x_dot, theta, theta_dot])
        
        done = bool(
            x < -self.x_threshold
            or x > self.x_threshold
            or theta < -self.theta_threshold_radians
            or theta > self.theta_threshold_radians
        )
        
        if not done:
            reward = 1.0
        elif self.steps_beyond_done is None:
            self.steps_beyond_done = 0
            reward = 1.0
        else:
            if self.steps_beyond_done == 0:
                print("You are calling 'step()' even though this environment has already returned done = True.")
            self.steps_beyond_done += 1
            reward = 0.0
        
        return self.state.copy(), reward, done
    
    def render(self):
        """å¯è§†åŒ–å½“å‰ç¯å¢ƒçŠ¶æ€"""
        print("\nğŸ—ºï¸ å½“å‰ç¯å¢ƒçŠ¶æ€ï¼š")
        for i in range(self.size):
            row = ""
            for j in range(self.size):
                pos = (i, j)
                if pos == self.current_pos:
                    row += "ğŸ¤– "
                elif pos == self.goal_pos:
                    row += "ğŸ¯ "
                elif pos == self.trap_pos:
                    row += "ğŸ•³ï¸ "
                else:
                    row += "â¬œ "
            print(f"   {row}")

def demonstrate_policy_gradient():
    """æ¼”ç¤ºç­–ç•¥æ¢¯åº¦æ–¹æ³•"""
    print("ğŸ¯ ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¼”ç¤º")
    print("=" * 40)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = CartPoleEnvironment()
    agent = REINFORCEAgent(
        state_size=env.state_size,
        action_size=env.action_size,
        learning_rate=0.01,
        gamma=0.99
    )
    
    print("ğŸ® ç¯å¢ƒè®¾ç½®ï¼š")
    print("   â€¢ CartPoleå¹³è¡¡æ†ç¯å¢ƒ")
    print("   â€¢ çŠ¶æ€ç©ºé—´ï¼š4ç»´è¿ç»­ç©ºé—´")
    print("   â€¢ åŠ¨ä½œç©ºé—´ï¼š2ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆå·¦/å³ï¼‰")
    print("   â€¢ ç›®æ ‡ï¼šä¿æŒæ†å­å¹³è¡¡å°½å¯èƒ½é•¿æ—¶é—´")
    
    # è®­ç»ƒè¿‡ç¨‹
    print("\nğŸ“ å¼€å§‹è®­ç»ƒ...")
    episodes = 1000
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        
        # è¿è¡Œä¸€ä¸ªepisode
        for step in range(500):  # æœ€å¤š500æ­¥
            action, log_prob = agent.get_action(state)
            next_state, reward, done = env.step(action)
            
            agent.store_experience(state, action, reward)
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # æ›´æ–°ç­–ç•¥
        policy_loss = agent.update_policy()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        agent.learning_stats['episodes'].append(episode)
        agent.learning_stats['rewards'].append(total_reward)
        
        # æ¯100ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(agent.learning_stats['rewards'][-100:])
            print(f"   Episode {episode+1}: å¹³å‡å¥–åŠ±={avg_reward:.2f}")
    
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    print(f"\nğŸ¯ æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ï¼š")
    test_episodes = 5
    
    for test_ep in range(test_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for step in range(500):
            action, _ = agent.get_action(state)
            next_state, reward, done = env.step(action)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        print(f"   æµ‹è¯•Episode {test_ep+1}: æ€»å¥–åŠ±={total_reward}, æ€»æ­¥æ•°={steps}")

# è¿è¡Œç­–ç•¥æ¢¯åº¦æ¼”ç¤º
demonstrate_policy_gradient()
```

#### ğŸ” ç­–ç•¥æ¢¯åº¦æ–¹æ³•åˆ†æ

```python
# ğŸ” ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ·±åº¦åˆ†æ
class PolicyGradientAnalysis:
    """ç­–ç•¥æ¢¯åº¦æ–¹æ³•åˆ†æ"""
    
    def __init__(self):
        self.advantages = {
            "ç›´æ¥ä¼˜åŒ–": "ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä¸éœ€è¦ä»·å€¼å‡½æ•°ä½œä¸ºä¸­ä»‹",
            "è¿ç»­åŠ¨ä½œ": "å¤©ç„¶æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´",
            "éšæœºç­–ç•¥": "å¯ä»¥å­¦ä¹ éšæœºç­–ç•¥ï¼Œé€‚åˆéƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒ",
            "æ”¶æ•›ä¿è¯": "åœ¨ä¸€å®šæ¡ä»¶ä¸‹ä¿è¯æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜"
        }
        
        self.challenges = {
            "é«˜æ–¹å·®": "ç­–ç•¥æ¢¯åº¦ä¼°è®¡æ–¹å·®è¾ƒå¤§ï¼Œå­¦ä¹ ä¸ç¨³å®š",
            "æ ·æœ¬æ•ˆç‡": "é€šå¸¸éœ€è¦å¤§é‡æ ·æœ¬æ‰èƒ½æ”¶æ•›",
            "å±€éƒ¨æœ€ä¼˜": "å¯èƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è€Œéå…¨å±€æœ€ä¼˜",
            "è¶…å‚æ•°æ•æ„Ÿ": "å¯¹å­¦ä¹ ç‡ç­‰è¶…å‚æ•°æ¯”è¾ƒæ•æ„Ÿ"
        }
        
        self.improvements = {
            "åŸºçº¿æ–¹æ³•": "ä½¿ç”¨åŸºçº¿å‡å°‘æ–¹å·®",
            "Actor-Critic": "ç»“åˆä»·å€¼å‡½æ•°ä¼°è®¡",
            "è‡ªç„¶ç­–ç•¥æ¢¯åº¦": "ä½¿ç”¨è‡ªç„¶æ¢¯åº¦æ”¹è¿›æ”¶æ•›",
            "ä¿¡ä»»åŒºåŸŸ": "é™åˆ¶ç­–ç•¥æ›´æ–°æ­¥é•¿"
        }
    
    def analyze_method(self):
        """åˆ†æç­–ç•¥æ¢¯åº¦æ–¹æ³•"""
        print("ğŸ” ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ·±åº¦åˆ†æ")
        print("=" * 45)
        
        print("âœ… ä¸»è¦ä¼˜åŠ¿ï¼š")
        for advantage, description in self.advantages.items():
            print(f"   â€¢ {advantage}: {description}")
        
        print("\nâš ï¸ ä¸»è¦æŒ‘æˆ˜ï¼š")
        for challenge, description in self.challenges.items():
            print(f"   â€¢ {challenge}: {description}")
        
        print("\nğŸš€ æ”¹è¿›æ–¹å‘ï¼š")
        for improvement, description in self.improvements.items():
            print(f"   â€¢ {improvement}: {description}")
    
    def explain_policy_gradient_theorem(self):
        """è§£é‡Šç­–ç•¥æ¢¯åº¦å®šç†"""
        print(f"\nğŸ“ ç­–ç•¥æ¢¯åº¦å®šç†")
        print("=" * 25)
        
        print("ğŸ§® æ ¸å¿ƒå…¬å¼ï¼š")
        print("   âˆ‡J(Î¸) = E[âˆ‡log Ï€(a|s,Î¸) * Q(s,a)]")
        
        print("\nğŸ“ å…¬å¼è§£é‡Šï¼š")
        print("   â€¢ J(Î¸): ç­–ç•¥çš„æœŸæœ›å›æŠ¥")
        print("   â€¢ âˆ‡J(Î¸): ç­–ç•¥æ¢¯åº¦")
        print("   â€¢ Ï€(a|s,Î¸): å‚æ•°åŒ–çš„ç­–ç•¥å‡½æ•°")
        print("   â€¢ Q(s,a): åŠ¨ä½œä»·å€¼å‡½æ•°")
        
        print("\nğŸ’¡ ç›´è§‚ç†è§£ï¼š")
        print("   â€¢ å¦‚æœQ(s,a)>0ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼Œå¢åŠ Ï€(a|s)çš„æ¦‚ç‡")
        print("   â€¢ å¦‚æœQ(s,a)<0ï¼ˆååŠ¨ä½œï¼‰ï¼Œå‡å°‘Ï€(a|s)çš„æ¦‚ç‡")
        print("   â€¢ æ¢¯åº¦æ–¹å‘æŒ‡å‘æœŸæœ›å›æŠ¥å¢åŠ çš„æ–¹å‘")
    
    def compare_algorithms(self):
        """æ¯”è¾ƒä¸åŒçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•"""
        print(f"\nğŸ“Š ç­–ç•¥æ¢¯åº¦ç®—æ³•å¯¹æ¯”")
        print("=" * 30)
        
        algorithms = {
            "REINFORCE": {
                "ç‰¹ç‚¹": "æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•",
                "ä¼˜åŠ¿": "ç®€å•æ˜“å®ç°",
                "åŠ£åŠ¿": "é«˜æ–¹å·®ï¼Œæ”¶æ•›æ…¢"
            },
            "Actor-Critic": {
                "ç‰¹ç‚¹": "ç»“åˆç­–ç•¥å’Œä»·å€¼å‡½æ•°",
                "ä¼˜åŠ¿": "é™ä½æ–¹å·®ï¼Œæé«˜æ ·æœ¬æ•ˆç‡",
                "åŠ£åŠ¿": "éœ€è¦åŒæ—¶è®­ç»ƒä¸¤ä¸ªç½‘ç»œ"
            },
            "A3C": {
                "ç‰¹ç‚¹": "å¼‚æ­¥å¹¶è¡Œè®­ç»ƒ",
                "ä¼˜åŠ¿": "è®­ç»ƒé€Ÿåº¦å¿«ï¼Œç¨³å®šæ€§å¥½",
                "åŠ£åŠ¿": "å®ç°å¤æ‚åº¦è¾ƒé«˜"
            },
            "PPO": {
                "ç‰¹ç‚¹": "è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–",
                "ä¼˜åŠ¿": "è®­ç»ƒç¨³å®šï¼Œæ•ˆæœå¥½",
                "åŠ£åŠ¿": "è¶…å‚æ•°è°ƒèŠ‚é‡è¦"
            }
        }
        
        for algo, info in algorithms.items():
            print(f"\nğŸ¯ {algo}:")
            print(f"   ğŸ“‹ ç‰¹ç‚¹: {info['ç‰¹ç‚¹']}")
            print(f"   âœ… ä¼˜åŠ¿: {info['ä¼˜åŠ¿']}")
            print(f"   âš ï¸ åŠ£åŠ¿: {info['åŠ£åŠ¿']}")

# è¿è¡Œç­–ç•¥æ¢¯åº¦åˆ†æ
pg_analysis = PolicyGradientAnalysis()
pg_analysis.analyze_method()
pg_analysis.explain_policy_gradient_theorem()
pg_analysis.compare_algorithms()
```

---

## ğŸ¤– 30.5 æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºç¡€

### ğŸ§  ç¥ç»ç½‘ç»œä¸å¼ºåŒ–å­¦ä¹ çš„ç»“åˆ

å½“çŠ¶æ€ç©ºé—´æˆ–åŠ¨ä½œç©ºé—´å˜å¾—éå¸¸å¤§æ—¶ï¼Œä¼ ç»Ÿçš„è¡¨æ ¼æ–¹æ³•ï¼ˆå¦‚Q-tableï¼‰å°±ä¸å†é€‚ç”¨äº†ã€‚è¿™æ—¶æˆ‘ä»¬éœ€è¦ä½¿ç”¨**å‡½æ•°é€¼è¿‘**ï¼Œè€Œæ·±åº¦ç¥ç»ç½‘ç»œæ˜¯æœ€å¼ºå¤§çš„å‡½æ•°é€¼è¿‘å™¨ä¹‹ä¸€ã€‚

#### ğŸ¯ æ·±åº¦Qç½‘ç»œ(DQN)

**DQN(Deep Q-Network)**æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸå°†æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆçš„ç®—æ³•ï¼š

```python
# ğŸ¤– æ·±åº¦Qç½‘ç»œ(DQN)å®ç°
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque

class DQNNetwork(nn.Module):
    """æ·±åº¦Qç½‘ç»œ"""
    
    def __init__(self, state_size, action_size, hidden_sizes=[128, 128]):
        """
        åˆå§‹åŒ–DQNç½‘ç»œ
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            hidden_sizes: éšè—å±‚å¤§å°åˆ—è¡¨
        """
        super(DQNNetwork, self).__init__()
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        input_size = state_size
        
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(input_size, hidden_size))
            layers.append(nn.ReLU())
            input_size = hidden_size
        
        layers.append(nn.Linear(input_size, action_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼šè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼"""
        return self.network(state)

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity=10000):
        """
        åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        experience = (state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    
    def __init__(self, state_size, action_size, learning_rate=0.001, 
                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        """
        åˆå§‹åŒ–DQNæ™ºèƒ½ä½“
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: åˆå§‹æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQNNetwork(state_size, action_size)
        self.target_network = DQNNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(capacity=10000)
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 32
        self.update_target_freq = 100  # æ¯100æ­¥æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.train_freq = 4  # æ¯4æ­¥è®­ç»ƒä¸€æ¬¡
        self.step_count = 0
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'episodes': [],
            'rewards': [],
            'losses': [],
            'epsilon_values': []
        }
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
    
    def get_action(self, state, training=True):
        """
        æ ¹æ®Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randrange(self.action_size)
        
        # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return q_values.argmax().item()
    
    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def train(self):
        """è®­ç»ƒDQNç½‘ç»œ"""
        if len(self.replay_buffer) < self.batch_size:
            return
        
        # ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # è®°å½•æŸå¤±
        self.learning_stats['losses'].append(loss.item())
        
        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return loss.item()
    
    def train_episode(self, env, max_steps=1000):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = env.reset()
        total_reward = 0
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action = self.get_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            self.store_experience(state, action, reward, next_state, done)
            
            # è®­ç»ƒç½‘ç»œ
            if self.step_count % self.train_freq == 0:
                self.train()
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if self.step_count % self.update_target_freq == 0:
                self.update_target_network()
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.learning_stats['epsilon_values'].append(self.epsilon)
        
        return total_reward

def demonstrate_dqn():
    """æ¼”ç¤ºDQNç®—æ³•"""
    print("ğŸ¤– æ·±åº¦Qç½‘ç»œ(DQN)æ¼”ç¤º")
    print("=" * 40)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = CartPoleEnvironment()
    agent = DQNAgent(
        state_size=env.state_size,
        action_size=env.action_size,
        learning_rate=0.001,
        gamma=0.99,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01
    )
    
    print("ğŸ® ç¯å¢ƒè®¾ç½®ï¼š")
    print("   â€¢ CartPoleå¹³è¡¡æ†ç¯å¢ƒ")
    print("   â€¢ çŠ¶æ€ç©ºé—´ï¼š4ç»´è¿ç»­ç©ºé—´")
    print("   â€¢ åŠ¨ä½œç©ºé—´ï¼š2ä¸ªç¦»æ•£åŠ¨ä½œ")
    print("   â€¢ ç¥ç»ç½‘ç»œï¼š2å±‚éšè—å±‚ï¼Œæ¯å±‚128ä¸ªç¥ç»å…ƒ")
    
    # è®­ç»ƒè¿‡ç¨‹
    print("\nğŸ“ å¼€å§‹è®­ç»ƒ...")
    episodes = 500
    
    for episode in range(episodes):
        total_reward = agent.train_episode(env)
        agent.learning_stats['episodes'].append(episode)
        agent.learning_stats['rewards'].append(total_reward)
        
        # æ¯50ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(agent.learning_stats['rewards'][-50:])
            current_epsilon = agent.epsilon
            print(f"   Episode {episode+1}: å¹³å‡å¥–åŠ±={avg_reward:.2f}, Îµ={current_epsilon:.3f}")
    
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    print(f"\nğŸ¯ æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ï¼š")
    test_episodes = 5
    
    for test_ep in range(test_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for step in range(500):
            action = agent.get_action(state, training=False)
            next_state, reward, done = env.step(action)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        print(f"   æµ‹è¯•Episode {test_ep+1}: æ€»å¥–åŠ±={total_reward}, æ€»æ­¥æ•°={steps}")

# è¿è¡ŒDQNæ¼”ç¤º
demonstrate_dqn()
```

#### ğŸ” DQNçš„å…³é”®åˆ›æ–°

```python
# ğŸ” DQNå…³é”®æŠ€æœ¯åˆ†æ
class DQNInnovationAnalysis:
    """DQNå…³é”®æŠ€æœ¯åˆ›æ–°åˆ†æ"""
    
    def __init__(self):
        self.innovations = {
            "ç»éªŒå›æ”¾(Experience Replay)": {
                "é—®é¢˜": "å¼ºåŒ–å­¦ä¹ æ ·æœ¬é—´ç›¸å…³æ€§å¼ºï¼Œè¿åç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾",
                "è§£å†³": "å°†ç»éªŒå­˜å‚¨åœ¨ç¼“å†²åŒºï¼Œéšæœºé‡‡æ ·è®­ç»ƒ",
                "ä¼˜åŠ¿": "æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡"
            },
            "ç›®æ ‡ç½‘ç»œ(Target Network)": {
                "é—®é¢˜": "Qå­¦ä¹ ä¸­ç›®æ ‡å€¼å’Œå½“å‰å€¼ä½¿ç”¨åŒä¸€ç½‘ç»œï¼Œä¸ç¨³å®š",
                "è§£å†³": "ä½¿ç”¨ç‹¬ç«‹çš„ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡å€¼",
                "ä¼˜åŠ¿": "ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…ç›®æ ‡å€¼å‰§çƒˆå˜åŒ–"
            },
            "æ·±åº¦ç¥ç»ç½‘ç»œ": {
                "é—®é¢˜": "çŠ¶æ€ç©ºé—´è¿‡å¤§ï¼Œæ— æ³•ä½¿ç”¨è¡¨æ ¼æ–¹æ³•",
                "è§£å†³": "ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°",
                "ä¼˜åŠ¿": "å¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´ï¼Œå¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›"
            },
            "Îµ-è´ªå¿ƒæ¢ç´¢": {
                "é—®é¢˜": "éœ€è¦å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨",
                "è§£å†³": "ä½¿ç”¨è¡°å‡çš„Îµ-è´ªå¿ƒç­–ç•¥",
                "ä¼˜åŠ¿": "åˆæœŸå……åˆ†æ¢ç´¢ï¼ŒåæœŸä¸“æ³¨åˆ©ç”¨"
            }
        }
    
    def analyze_innovations(self):
        """åˆ†æDQNçš„å…³é”®åˆ›æ–°"""
        print("ğŸ” DQNå…³é”®æŠ€æœ¯åˆ›æ–°åˆ†æ")
        print("=" * 45)
        
        for innovation, details in self.innovations.items():
            print(f"\nğŸ’¡ {innovation}:")
            print(f"   â“ é—®é¢˜: {details['é—®é¢˜']}")
            print(f"   ğŸ’Š è§£å†³: {details['è§£å†³']}")
            print(f"   âœ… ä¼˜åŠ¿: {details['ä¼˜åŠ¿']}")
    
    def explain_training_process(self):
        """è§£é‡ŠDQNè®­ç»ƒè¿‡ç¨‹"""
        print(f"\nğŸ“ DQNè®­ç»ƒè¿‡ç¨‹è¯¦è§£")
        print("=" * 30)
        
        steps = [
            "1. ğŸ® æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒ(s,a,r,s')",
            "2. ğŸ’¾ å°†ç»éªŒå­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒº",
            "3. ğŸ² ä»ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ",
            "4. ğŸ§® ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡Qå€¼",
            "5. ğŸ“‰ è®¡ç®—æŸå¤±å¹¶æ›´æ–°ä¸»ç½‘ç»œ",
            "6. ğŸ”„ å®šæœŸå°†ä¸»ç½‘ç»œæƒé‡å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ",
            "7. ğŸ“ˆ è¡°å‡æ¢ç´¢ç‡Îµ",
            "8. ğŸ” é‡å¤ç›´åˆ°æ”¶æ•›"
        ]
        
        for step in steps:
            print(f"   {step}")
        
        print("\nğŸ’¡ å…³é”®ç‚¹ï¼š")
        print("   â€¢ ç»éªŒå›æ”¾æé«˜æ ·æœ¬æ•ˆç‡")
        print("   â€¢ ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹")
        print("   â€¢ æ¢ç´¢ç‡è¡°å‡å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨")
    
    def compare_improvements(self):
        """æ¯”è¾ƒDQNçš„æ”¹è¿›ç‰ˆæœ¬"""
        print(f"\nğŸš€ DQNæ”¹è¿›ç‰ˆæœ¬å¯¹æ¯”")
        print("=" * 25)
        
        improvements = {
            "Double DQN": {
                "æ”¹è¿›": "è§£å†³Qå€¼è¿‡ä¼°è®¡é—®é¢˜",
                "æ–¹æ³•": "ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼",
                "æ•ˆæœ": "æ›´å‡†ç¡®çš„Qå€¼ä¼°è®¡"
            },
            "Dueling DQN": {
                "æ”¹è¿›": "åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿",
                "æ–¹æ³•": "ç½‘ç»œè¾“å‡ºV(s)å’ŒA(s,a)ï¼Œå†åˆå¹¶ä¸ºQ(s,a)",
                "æ•ˆæœ": "æ›´å¥½çš„ä»·å€¼å‡½æ•°å­¦ä¹ "
            },
            "Prioritized Experience Replay": {
                "æ”¹è¿›": "ä¼˜å…ˆé‡è¦ç»éªŒçš„å­¦ä¹ ",
                "æ–¹æ³•": "æ ¹æ®TDè¯¯å·®ç¡®å®šé‡‡æ ·ä¼˜å…ˆçº§",
                "æ•ˆæœ": "æé«˜å­¦ä¹ æ•ˆç‡"
            },
            "Rainbow DQN": {
                "æ”¹è¿›": "é›†æˆå¤šç§æ”¹è¿›æŠ€æœ¯",
                "æ–¹æ³•": "ç»“åˆæ‰€æœ‰ä¸»è¦DQNæ”¹è¿›",
                "æ•ˆæœ": "è¾¾åˆ°æœ€ä½³æ€§èƒ½"
            }
        }
        
        for version, info in improvements.items():
            print(f"\nğŸŒŸ {version}:")
            print(f"   ğŸ¯ æ”¹è¿›: {info['æ”¹è¿›']}")
            print(f"   ğŸ”§ æ–¹æ³•: {info['æ–¹æ³•']}")
            print(f"   ğŸ“ˆ æ•ˆæœ: {info['æ•ˆæœ']}")

# è¿è¡ŒDQNåˆ›æ–°åˆ†æ
dqn_analysis = DQNInnovationAnalysis()
dqn_analysis.analyze_innovations()
dqn_analysis.explain_training_process()
dqn_analysis.compare_improvements()
```

---

## ğŸ­ 30.6 Actor-Criticæ–¹æ³•

### ğŸª æ¼”å‘˜ä¸è¯„è®ºå®¶çš„åä½œ

**Actor-Criticæ–¹æ³•**ç»“åˆäº†ä»·å€¼å‡½æ•°æ–¹æ³•å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œä½¿ç”¨ä¸¤ä¸ªç¥ç»ç½‘ç»œï¼š
- **Actorï¼ˆæ¼”å‘˜ï¼‰**ï¼šå­¦ä¹ ç­–ç•¥å‡½æ•°Ï€(a|s,Î¸)
- **Criticï¼ˆè¯„è®ºå®¶ï¼‰**ï¼šå­¦ä¹ ä»·å€¼å‡½æ•°V(s,Ï†)

#### ğŸ¬ Actor-Criticç®—æ³•å®ç°

```python
# ğŸ­ Actor-Criticæ–¹æ³•å®ç°
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np

class ActorNetwork(nn.Module):
    """Actorç½‘ç»œï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    
    def __init__(self, state_size, action_size, hidden_size=128):
        """
        åˆå§‹åŒ–Actorç½‘ç»œ
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            hidden_size: éšè—å±‚å¤§å°
        """
        super(ActorNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class CriticNetwork(nn.Module):
    """Criticç½‘ç»œï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼"""
    
    def __init__(self, state_size, hidden_size=128):
        """
        åˆå§‹åŒ–Criticç½‘ç»œ
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            hidden_size: éšè—å±‚å¤§å°
        """
        super(CriticNetwork, self).__init__()
        
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼"""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        value = self.fc3(x)
        return value

class ActorCriticAgent:
    """Actor-Criticæ™ºèƒ½ä½“"""
    
    def __init__(self, state_size, action_size, actor_lr=0.001, critic_lr=0.005, gamma=0.99):
        """
        åˆå§‹åŒ–Actor-Criticæ™ºèƒ½ä½“
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            actor_lr: Actorå­¦ä¹ ç‡
            critic_lr: Criticå­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        
        # åˆ›å»ºActorå’ŒCriticç½‘ç»œ
        self.actor = ActorNetwork(state_size, action_size)
        self.critic = CriticNetwork(state_size)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'episodes': [],
            'rewards': [],
            'actor_losses': [],
            'critic_losses': []
        }
    
    def get_action(self, state):
        """
        æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_probs = self.actor(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        # è·å–çŠ¶æ€ä»·å€¼
        value = self.critic(state_tensor)
        
        return action.item(), log_prob, value
    
    def update(self, log_prob, value, reward, next_value, done):
        """
        æ›´æ–°Actorå’ŒCriticç½‘ç»œ
        
        Args:
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: å½“å‰çŠ¶æ€ä»·å€¼
            reward: å¥–åŠ±
            next_value: ä¸‹ä¸€çŠ¶æ€ä»·å€¼
            done: æ˜¯å¦ç»“æŸ
        """
        # è®¡ç®—TDè¯¯å·®ï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰
        if done:
            target_value = reward
        else:
            target_value = reward + self.gamma * next_value.item()
        
        advantage = target_value - value.item()
        
        # æ›´æ–°Criticï¼ˆä»·å€¼å‡½æ•°ï¼‰
        critic_loss = F.mse_loss(value, torch.tensor([[target_value]], dtype=torch.float32))
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        self.critic_optimizer.step()
        
        # æ›´æ–°Actorï¼ˆç­–ç•¥å‡½æ•°ï¼‰
        actor_loss = -log_prob * advantage
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # è®°å½•æŸå¤±
        self.learning_stats['actor_losses'].append(actor_loss.item())
        self.learning_stats['critic_losses'].append(critic_loss.item())
        
        return actor_loss.item(), critic_loss.item()
    
    def train_episode(self, env, max_steps=1000):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = env.reset()
        total_reward = 0
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, value = self.get_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = env.step(action)
            
            # è·å–ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼
            if not done:
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                next_value = self.critic(next_state_tensor)
            else:
                next_value = torch.tensor([[0.0]])
            
            # æ›´æ–°ç½‘ç»œ
            self.update(log_prob, value, reward, next_value, done)
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        return total_reward

def demonstrate_actor_critic():
    """æ¼”ç¤ºActor-Criticç®—æ³•"""
    print("ğŸ­ Actor-Criticæ–¹æ³•æ¼”ç¤º")
    print("=" * 40)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = CartPoleEnvironment()
    agent = ActorCriticAgent(
        state_size=env.state_size,
        action_size=env.action_size,
        actor_lr=0.001,
        critic_lr=0.005,
        gamma=0.99
    )
    
    print("ğŸ® ç¯å¢ƒè®¾ç½®ï¼š")
    print("   â€¢ CartPoleå¹³è¡¡æ†ç¯å¢ƒ")
    print("   â€¢ Actorç½‘ç»œï¼šå­¦ä¹ ç­–ç•¥Ï€(a|s)")
    print("   â€¢ Criticç½‘ç»œï¼šå­¦ä¹ ä»·å€¼V(s)")
    print("   â€¢ ä¼˜åŠ¿å‡½æ•°ï¼šA(s,a) = r + Î³V(s') - V(s)")
    
    # è®­ç»ƒè¿‡ç¨‹
    print("\nğŸ“ å¼€å§‹è®­ç»ƒ...")
    episodes = 1000
    
    for episode in range(episodes):
        total_reward = agent.train_episode(env)
        agent.learning_stats['episodes'].append(episode)
        agent.learning_stats['rewards'].append(total_reward)
        
        # æ¯100ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(agent.learning_stats['rewards'][-100:])
            avg_actor_loss = np.mean(agent.learning_stats['actor_losses'][-100:]) if agent.learning_stats['actor_losses'] else 0
            avg_critic_loss = np.mean(agent.learning_stats['critic_losses'][-100:]) if agent.learning_stats['critic_losses'] else 0
            print(f"   Episode {episode+1}: å¥–åŠ±={avg_reward:.2f}, ActoræŸå¤±={avg_actor_loss:.4f}, CriticæŸå¤±={avg_critic_loss:.4f}")
    
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    print(f"\nğŸ¯ æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ï¼š")
    test_episodes = 5
    
    for test_ep in range(test_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for step in range(500):
            action, _, _ = agent.get_action(state)
            next_state, reward, done = env.step(action)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        print(f"   æµ‹è¯•Episode {test_ep+1}: æ€»å¥–åŠ±={total_reward}, æ€»æ­¥æ•°={steps}")

# è¿è¡ŒActor-Criticæ¼”ç¤º
demonstrate_actor_critic()
```

#### ğŸ” Actor-Criticæ–¹æ³•åˆ†æ

```python
# ğŸ” Actor-Criticæ–¹æ³•æ·±åº¦åˆ†æ
class ActorCriticAnalysis:
    """Actor-Criticæ–¹æ³•åˆ†æ"""
    
    def __init__(self):
        self.advantages = {
            "é™ä½æ–¹å·®": "Criticæä¾›åŸºçº¿ï¼Œå‡å°‘ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®",
            "åœ¨çº¿å­¦ä¹ ": "å¯ä»¥è¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œä¸éœ€è¦ç­‰å¾…episodeç»“æŸ",
            "æ ·æœ¬æ•ˆç‡": "æ¯”çº¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ›´é«˜æ•ˆ",
            "ç¨³å®šæ€§": "æ¯”çº¯ä»·å€¼å‡½æ•°æ–¹æ³•æ›´ç¨³å®š"
        }
        
        self.components = {
            "Actorï¼ˆæ¼”å‘˜ï¼‰": {
                "åŠŸèƒ½": "å­¦ä¹ ç­–ç•¥å‡½æ•°Ï€(a|s,Î¸)",
                "ç›®æ ‡": "æœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±",
                "æ›´æ–°": "ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•"
            },
            "Criticï¼ˆè¯„è®ºå®¶ï¼‰": {
                "åŠŸèƒ½": "å­¦ä¹ ä»·å€¼å‡½æ•°V(s,Ï†)",
                "ç›®æ ‡": "å‡†ç¡®ä¼°è®¡çŠ¶æ€ä»·å€¼",
                "æ›´æ–°": "ä½¿ç”¨æ—¶åºå·®åˆ†æ–¹æ³•"
            },
            "ä¼˜åŠ¿å‡½æ•°": {
                "åŠŸèƒ½": "A(s,a) = Q(s,a) - V(s)",
                "ç›®æ ‡": "è¡¡é‡åŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„å¥½å",
                "æ›´æ–°": "æŒ‡å¯¼Actorçš„ç­–ç•¥æ›´æ–°"
            }
        }
    
    def analyze_method(self):
        """åˆ†æActor-Criticæ–¹æ³•"""
        print("ğŸ” Actor-Criticæ–¹æ³•æ·±åº¦åˆ†æ")
        print("=" * 45)
        
        print("âœ… ä¸»è¦ä¼˜åŠ¿ï¼š")
        for advantage, description in self.advantages.items():
            print(f"   â€¢ {advantage}: {description}")
        
        print(f"\nğŸ­ æ ¸å¿ƒç»„ä»¶ï¼š")
        for component, details in self.components.items():
            print(f"\nğŸ“‹ {component}:")
            print(f"   ğŸ¯ åŠŸèƒ½: {details['åŠŸèƒ½']}")
            print(f"   ğŸ¯ ç›®æ ‡: {details['ç›®æ ‡']}")
            print(f"   ğŸ”„ æ›´æ–°: {details['æ›´æ–°']}")
    
    def explain_advantage_function(self):
        """è§£é‡Šä¼˜åŠ¿å‡½æ•°çš„ä½œç”¨"""
        print(f"\nğŸ¯ ä¼˜åŠ¿å‡½æ•°è¯¦è§£")
        print("=" * 25)
        
        print("ğŸ“ æ•°å­¦å®šä¹‰ï¼š")
        print("   A(s,a) = Q(s,a) - V(s)")
        print("   æˆ–è€…ï¼šA(s,a) = r + Î³V(s') - V(s)  (TDè¯¯å·®)")
        
        print("\nğŸ’¡ ç›´è§‚ç†è§£ï¼š")
        print("   â€¢ A(s,a) > 0ï¼šåŠ¨ä½œaæ¯”å¹³å‡æ°´å¹³å¥½ï¼Œå¢åŠ é€‰æ‹©æ¦‚ç‡")
        print("   â€¢ A(s,a) < 0ï¼šåŠ¨ä½œaæ¯”å¹³å‡æ°´å¹³å·®ï¼Œå‡å°‘é€‰æ‹©æ¦‚ç‡")
        print("   â€¢ A(s,a) = 0ï¼šåŠ¨ä½œaå¤„äºå¹³å‡æ°´å¹³ï¼Œä¸æ”¹å˜æ¦‚ç‡")
        
        print("\nğŸ¯ ä½œç”¨æœºåˆ¶ï¼š")
        print("   1. ğŸ­ Actorä½¿ç”¨ä¼˜åŠ¿å‡½æ•°æŒ‡å¯¼ç­–ç•¥æ›´æ–°")
        print("   2. ğŸ“Š Criticé€šè¿‡TDè¯¯å·®å­¦ä¹ ä»·å€¼å‡½æ•°")
        print("   3. ğŸ”„ ä¸¤è€…ç›¸äº’é…åˆï¼Œå…±åŒä¼˜åŒ–")
    
    def compare_variants(self):
        """æ¯”è¾ƒActor-Criticçš„å˜ä½“"""
        print(f"\nğŸš€ Actor-Criticç®—æ³•å˜ä½“")
        print("=" * 25)
        
        variants = {
            "A2C (Advantage Actor-Critic)": {
                "ç‰¹ç‚¹": "åŒæ­¥ç‰ˆæœ¬çš„Actor-Critic",
                "ä¼˜åŠ¿": "è®­ç»ƒç¨³å®šï¼Œæ˜“äºå®ç°",
                "é€‚ç”¨": "å•æœºè®­ç»ƒåœºæ™¯"
            },
            "A3C (Asynchronous Advantage Actor-Critic)": {
                "ç‰¹ç‚¹": "å¼‚æ­¥å¹¶è¡Œè®­ç»ƒ",
                "ä¼˜åŠ¿": "è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ¢ç´¢å……åˆ†",
                "é€‚ç”¨": "å¤šæ ¸CPUè®­ç»ƒ"
            },
            "PPO (Proximal Policy Optimization)": {
                "ç‰¹ç‚¹": "é™åˆ¶ç­–ç•¥æ›´æ–°æ­¥é•¿",
                "ä¼˜åŠ¿": "è®­ç»ƒç¨³å®šï¼Œæ€§èƒ½ä¼˜ç§€",
                "é€‚ç”¨": "å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ä»»åŠ¡"
            },
            "SAC (Soft Actor-Critic)": {
                "ç‰¹ç‚¹": "æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ",
                "ä¼˜åŠ¿": "æ¢ç´¢èƒ½åŠ›å¼ºï¼Œæ ·æœ¬æ•ˆç‡é«˜",
                "é€‚ç”¨": "è¿ç»­æ§åˆ¶ä»»åŠ¡"
            }
        }
        
        for variant, info in variants.items():
            print(f"\nğŸŒŸ {variant}:")
            print(f"   ğŸ¯ ç‰¹ç‚¹: {info['ç‰¹ç‚¹']}")
            print(f"   âœ… ä¼˜åŠ¿: {info['ä¼˜åŠ¿']}")
            print(f"   ğŸ¯ é€‚ç”¨: {info['é€‚ç”¨']}")

# è¿è¡ŒActor-Criticåˆ†æ
ac_analysis = ActorCriticAnalysis()
ac_analysis.analyze_method()
ac_analysis.explain_advantage_function()
ac_analysis.compare_variants()
```

---

## ğŸ® 30.7 ä¼ä¸šçº§å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿå®æˆ˜

### ğŸ¢ æ™ºèƒ½äº¤æ˜“å†³ç­–ç³»ç»Ÿ

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼š**æ™ºèƒ½è‚¡ç¥¨äº¤æ˜“å†³ç­–ç³»ç»Ÿ**ã€‚è¿™ä¸ªç³»ç»Ÿå°†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ æœ€ä¼˜çš„äº¤æ˜“ç­–ç•¥ã€‚

#### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

```mermaid
graph TD
    A["ğŸ“Š å¸‚åœºæ•°æ®æº"] --> B["ğŸ”„ æ•°æ®é¢„å¤„ç†å™¨"]
    B --> C["ğŸ“ˆ ç‰¹å¾å·¥ç¨‹å™¨"]
    C --> D["ğŸ§  RLäº¤æ˜“æ™ºèƒ½ä½“"]
    
    D --> E["ğŸ’¼ æŠ•èµ„ç»„åˆç®¡ç†å™¨"]
    D --> F["âš ï¸ é£é™©æ§åˆ¶å™¨"]
    D --> G["ğŸ“‹ äº¤æ˜“æ‰§è¡Œå™¨"]
    
    E --> H["ğŸ’° æ”¶ç›Šè®¡ç®—å™¨"]
    F --> I["ğŸ›¡ï¸ æ­¢æŸæœºåˆ¶"]
    G --> J["ğŸ“ äº¤æ˜“è®°å½•å™¨"]
    
    H --> K["ğŸ“Š æ€§èƒ½è¯„ä¼°å™¨"]
    I --> K
    J --> K
    
    K --> L["ğŸ“ˆ å¯è§†åŒ–æŠ¥å‘Š"]
    K --> M["ğŸ”§ ç­–ç•¥ä¼˜åŒ–å™¨"]
    
    M --> D
    
    style A fill:#e1f5fe
    style D fill:#e8f5e8
    style K fill:#fff3e0
    style L fill:#f3e5f5
```

#### ğŸ’¼ äº¤æ˜“ç¯å¢ƒå®ç°

```python
# ğŸ® æ™ºèƒ½äº¤æ˜“ç¯å¢ƒå®ç°
import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

class TradingEnvironment:
    """è‚¡ç¥¨äº¤æ˜“å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, symbol='AAPL', start_date='2020-01-01', end_date='2023-12-31', 
                 initial_capital=100000, transaction_cost=0.001):
        """
        åˆå§‹åŒ–äº¤æ˜“ç¯å¢ƒ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            initial_capital: åˆå§‹èµ„é‡‘
            transaction_cost: äº¤æ˜“æˆæœ¬ç‡
        """
        self.symbol = symbol
        self.initial_capital = initial_capital
        self.transaction_cost = transaction_cost
        
        # è·å–è‚¡ç¥¨æ•°æ®
        self.data = self._fetch_stock_data(symbol, start_date, end_date)
        self.data_length = len(self.data)
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.cash = initial_capital
        self.stock_owned = 0
        self.total_value = initial_capital
        
        # åŠ¨ä½œç©ºé—´ï¼š0-æŒæœ‰ï¼Œ1-ä¹°å…¥ï¼Œ2-å–å‡º
        self.action_space = 3
        
        # çŠ¶æ€ç©ºé—´ï¼šä»·æ ¼ç‰¹å¾ + æŠ€æœ¯æŒ‡æ ‡ + æŒä»“ä¿¡æ¯
        self.state_size = 10
        
        # äº¤æ˜“å†å²
        self.trading_history = []
        
    def _fetch_stock_data(self, symbol, start_date, end_date):
        """è·å–è‚¡ç¥¨æ•°æ®å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
        try:
            # ä¸‹è½½è‚¡ç¥¨æ•°æ®
            stock_data = yf.download(symbol, start=start_date, end=end_date)
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            stock_data['Returns'] = stock_data['Close'].pct_change()
            stock_data['MA_5'] = stock_data['Close'].rolling(window=5).mean()
            stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()
            stock_data['RSI'] = self._calculate_rsi(stock_data['Close'])
            stock_data['MACD'] = self._calculate_macd(stock_data['Close'])
            
            # åˆ é™¤åŒ…å«NaNçš„è¡Œ
            stock_data = stock_data.dropna()
            
            return stock_data
            
        except Exception as e:
            print(f"è·å–æ•°æ®å¤±è´¥: {e}")
            # å¦‚æœè·å–çœŸå®æ•°æ®å¤±è´¥ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            return self._generate_synthetic_data(start_date, end_date)
    
    def _generate_synthetic_data(self, start_date, end_date):
        """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®"""
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        np.random.seed(42)
        
        # ç”Ÿæˆä»·æ ¼æ•°æ®ï¼ˆå‡ ä½•å¸ƒæœ—è¿åŠ¨ï¼‰
        n_days = len(dates)
        returns = np.random.normal(0.001, 0.02, n_days)  # æ—¥æ”¶ç›Šç‡
        prices = [100]  # åˆå§‹ä»·æ ¼
        
        for i in range(1, n_days):
            prices.append(prices[-1] * (1 + returns[i]))
        
        # åˆ›å»ºDataFrame
        data = pd.DataFrame({
            'Open': prices,
            'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'Close': prices,
            'Volume': np.random.randint(1000000, 10000000, n_days)
        }, index=dates)
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        data['Returns'] = data['Close'].pct_change()
        data['MA_5'] = data['Close'].rolling(window=5).mean()
        data['MA_20'] = data['Close'].rolling(window=20).mean()
        data['RSI'] = self._calculate_rsi(data['Close'])
        data['MACD'] = self._calculate_macd(data['Close'])
        
        return data.dropna()
    
    def _calculate_rsi(self, prices, window=14):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices, fast=12, slow=26):
        """è®¡ç®—MACDæŒ‡æ ‡"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        return macd
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.cash = self.initial_capital
        self.stock_owned = 0
        self.total_value = self.initial_capital
        self.trading_history = []
        
        return self._get_state()
    
    def _get_state(self):
        """è·å–å½“å‰çŠ¶æ€"""
        if self.current_step >= self.data_length:
            return np.zeros(self.state_size)
        
        row = self.data.iloc[self.current_step]
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ï¼‰
        current_price = row['Close']
        price_change = row['Returns'] if not np.isnan(row['Returns']) else 0
        
        # æŠ€æœ¯æŒ‡æ ‡ï¼ˆæ ‡å‡†åŒ–ï¼‰
        ma5_ratio = (current_price / row['MA_5'] - 1) if not np.isnan(row['MA_5']) else 0
        ma20_ratio = (current_price / row['MA_20'] - 1) if not np.isnan(row['MA_20']) else 0
        rsi = (row['RSI'] / 100 - 0.5) if not np.isnan(row['RSI']) else 0
        macd = row['MACD'] / current_price if not np.isnan(row['MACD']) else 0
        
        # æŒä»“ä¿¡æ¯
        cash_ratio = self.cash / self.initial_capital
        stock_ratio = (self.stock_owned * current_price) / self.initial_capital
        total_value_ratio = self.total_value / self.initial_capital
        portfolio_return = (self.total_value / self.initial_capital - 1)
        
        state = np.array([
            price_change,      # ä»·æ ¼å˜åŒ–
            ma5_ratio,         # 5æ—¥å‡çº¿æ¯”ç‡
            ma20_ratio,        # 20æ—¥å‡çº¿æ¯”ç‡
            rsi,               # RSIæŒ‡æ ‡
            macd,              # MACDæŒ‡æ ‡
            cash_ratio,        # ç°é‡‘æ¯”ä¾‹
            stock_ratio,       # è‚¡ç¥¨æ¯”ä¾‹
            total_value_ratio, # æ€»ä»·å€¼æ¯”ä¾‹
            portfolio_return,  # ç»„åˆæ”¶ç›Šç‡
            self.current_step / self.data_length  # æ—¶é—´è¿›åº¦
        ])
        
        return state
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.current_step >= self.data_length - 1:
            return self._get_state(), 0, True
        
        current_price = self.data.iloc[self.current_step]['Close']
        
        # æ‰§è¡Œäº¤æ˜“åŠ¨ä½œ
        reward = 0
        transaction_cost = 0
        
        if action == 1:  # ä¹°å…¥
            if self.cash > current_price:
                # è®¡ç®—å¯ä¹°å…¥è‚¡æ•°
                max_shares = int(self.cash / current_price)
                shares_to_buy = max_shares
                
                if shares_to_buy > 0:
                    cost = shares_to_buy * current_price
                    transaction_cost = cost * self.transaction_cost
                    
                    self.cash -= (cost + transaction_cost)
                    self.stock_owned += shares_to_buy
                    
                    self.trading_history.append({
                        'step': self.current_step,
                        'action': 'BUY',
                        'shares': shares_to_buy,
                        'price': current_price,
                        'cost': cost + transaction_cost
                    })
        
        elif action == 2:  # å–å‡º
            if self.stock_owned > 0:
                shares_to_sell = self.stock_owned
                revenue = shares_to_sell * current_price
                transaction_cost = revenue * self.transaction_cost
                
                self.cash += (revenue - transaction_cost)
                self.stock_owned = 0
                
                self.trading_history.append({
                    'step': self.current_step,
                    'action': 'SELL',
                    'shares': shares_to_sell,
                    'price': current_price,
                    'revenue': revenue - transaction_cost
                })
        
        # æ›´æ–°æ€»ä»·å€¼
        self.total_value = self.cash + self.stock_owned * current_price
        
        # è®¡ç®—å¥–åŠ±
        if self.current_step > 0:
            previous_value = self.initial_capital if self.current_step == 1 else getattr(self, '_previous_value', self.initial_capital)
            value_change = (self.total_value - previous_value) / previous_value
            
            # åŸºå‡†æ”¶ç›Šï¼ˆä¹°å…¥å¹¶æŒæœ‰ï¼‰
            previous_price = self.data.iloc[self.current_step - 1]['Close']
            benchmark_return = (current_price - previous_price) / previous_price
            
            # è¶…é¢æ”¶ç›Šä½œä¸ºå¥–åŠ±
            reward = (value_change - benchmark_return) * 100
            
            # æƒ©ç½šé¢‘ç¹äº¤æ˜“
            if action != 0:  # å¦‚æœæœ‰äº¤æ˜“è¡Œä¸º
                reward -= 0.1
        
        self._previous_value = self.total_value
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= self.data_length - 1
        
        return self._get_state(), reward, done
    
    def get_portfolio_stats(self):
        """è·å–æŠ•èµ„ç»„åˆç»Ÿè®¡ä¿¡æ¯"""
        if self.current_step == 0:
            return {}
        
        total_return = (self.total_value / self.initial_capital - 1) * 100
        
        # åŸºå‡†æ”¶ç›Šï¼ˆä¹°å…¥å¹¶æŒæœ‰ï¼‰
        start_price = self.data.iloc[0]['Close']
        end_price = self.data.iloc[self.current_step - 1]['Close']
        benchmark_return = (end_price / start_price - 1) * 100
        
        # è®¡ç®—å¤æ™®æ¯”ç‡ç­‰æŒ‡æ ‡
        returns = []
        for i in range(1, self.current_step):
            if i < len(self.data):
                price_change = (self.data.iloc[i]['Close'] / self.data.iloc[i-1]['Close'] - 1)
                returns.append(price_change)
        
        if returns:
            volatility = np.std(returns) * np.sqrt(252) * 100  # å¹´åŒ–æ³¢åŠ¨ç‡
            sharpe_ratio = (total_return - 2) / volatility if volatility > 0 else 0  # å‡è®¾æ— é£é™©åˆ©ç‡2%
        else:
            volatility = 0
            sharpe_ratio = 0
        
        return {
            'total_return': total_return,
            'benchmark_return': benchmark_return,
            'excess_return': total_return - benchmark_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'total_trades': len(self.trading_history),
            'final_cash': self.cash,
            'final_stock_value': self.stock_owned * self.data.iloc[self.current_step - 1]['Close'] if self.current_step > 0 else 0,
            'total_value': self.total_value
        }

def demonstrate_trading_system():
    """æ¼”ç¤ºæ™ºèƒ½äº¤æ˜“ç³»ç»Ÿ"""
    print("ğŸ¢ ä¼ä¸šçº§å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºäº¤æ˜“ç¯å¢ƒ
    env = TradingEnvironment(
        symbol='AAPL',
        start_date='2022-01-01',
        end_date='2023-12-31',
        initial_capital=100000,
        transaction_cost=0.001
    )
    
    print("ğŸ® äº¤æ˜“ç¯å¢ƒè®¾ç½®ï¼š")
    print(f"   â€¢ è‚¡ç¥¨ä»£ç : {env.symbol}")
    print(f"   â€¢ åˆå§‹èµ„é‡‘: ${env.initial_capital:,}")
    print(f"   â€¢ äº¤æ˜“æˆæœ¬: {env.transaction_cost*100}%")
    print(f"   â€¢ æ•°æ®å¤©æ•°: {env.data_length}")
    
    # ä½¿ç”¨ä¹‹å‰è®­ç»ƒå¥½çš„DQNæ™ºèƒ½ä½“è¿›è¡Œäº¤æ˜“
    agent = DQNAgent(
        state_size=env.state_size,
        action_size=env.action_space,
        learning_rate=0.001,
        gamma=0.99,
        epsilon=0.1  # è¾ƒä½çš„æ¢ç´¢ç‡ç”¨äºå®é™…äº¤æ˜“
    )
    
    print("\nğŸ“ å¼€å§‹äº¤æ˜“æ¨¡æ‹Ÿ...")
    
    # è¿è¡Œäº¤æ˜“
    state = env.reset()
    total_reward = 0
    
    while True:
        action = agent.get_action(state, training=False)
        next_state, reward, done = env.step(action)
        
        total_reward += reward
        state = next_state
        
        if done:
            break
    
    # è·å–äº¤æ˜“ç»“æœ
    stats = env.get_portfolio_stats()
    
    print("\nğŸ“Š äº¤æ˜“ç»“æœåˆ†æï¼š")
    print("=" * 30)
    print(f"   ğŸ’° æ€»æ”¶ç›Šç‡: {stats['total_return']:.2f}%")
    print(f"   ğŸ“ˆ åŸºå‡†æ”¶ç›Šç‡: {stats['benchmark_return']:.2f}%")
    print(f"   ğŸ¯ è¶…é¢æ”¶ç›Š: {stats['excess_return']:.2f}%")
    print(f"   ğŸ“Š å¹´åŒ–æ³¢åŠ¨ç‡: {stats['volatility']:.2f}%")
    print(f"   ğŸ“ˆ å¤æ™®æ¯”ç‡: {stats['sharpe_ratio']:.2f}")
    print(f"   ğŸ”„ æ€»äº¤æ˜“æ¬¡æ•°: {stats['total_trades']}")
    print(f"   ğŸ’µ æœ€ç»ˆç°é‡‘: ${stats['final_cash']:,.2f}")
    print(f"   ğŸ“ˆ è‚¡ç¥¨ä»·å€¼: ${stats['final_stock_value']:,.2f}")
    print(f"   ğŸ’ æ€»èµ„äº§: ${stats['total_value']:,.2f}")
    
    print(f"\nğŸ“ äº¤æ˜“å†å²ï¼ˆæœ€è¿‘5ç¬”ï¼‰:")
    recent_trades = env.trading_history[-5:] if env.trading_history else []
    for trade in recent_trades:
        action_emoji = "ğŸŸ¢" if trade['action'] == 'BUY' else "ğŸ”´"
        print(f"   {action_emoji} {trade['action']}: {trade['shares']}è‚¡ @ ${trade['price']:.2f}")
    
    return env, agent, stats

# è¿è¡Œäº¤æ˜“ç³»ç»Ÿæ¼”ç¤º
trading_env, trading_agent, trading_stats = demonstrate_trading_system()
```

---

## ğŸ“ 30.8 ç« èŠ‚æ€»ç»“ä¸æ€è€ƒ

### ğŸ† æ™ºèƒ½å†³ç­–å­¦é™¢çš„æ¯•ä¸šå…¸ç¤¼

æ­å–œä½ ï¼ç»è¿‡ç¬¬30ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»ä»æ™ºèƒ½å†³ç­–å­¦é™¢é¡ºåˆ©æ¯•ä¸šï¼ŒæŒæ¡äº†å¼ºåŒ–å­¦ä¹ è¿™ä¸€äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ ¸å¿ƒæŠ€æœ¯ã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿™æ®µç²¾å½©çš„å­¦ä¹ æ—…ç¨‹ã€‚

#### ğŸ“š çŸ¥è¯†æŠ€èƒ½æŒæ¡è¯„ä¼°

```python
# ğŸ“ ç¬¬30ç« å­¦ä¹ æˆæœè¯„ä¼°
class Chapter30Assessment:
    """ç¬¬30ç« å­¦ä¹ æˆæœè¯„ä¼°"""
    
    def __init__(self):
        self.knowledge_points = {
            "å¼ºåŒ–å­¦ä¹ åŸºç¡€ç†è®º": {
                "é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹": "âœ… å·²æŒæ¡",
                "ä»·å€¼å‡½æ•°æ¦‚å¿µ": "âœ… å·²æŒæ¡", 
                "ç­–ç•¥ä¼˜åŒ–åŸç†": "âœ… å·²æŒæ¡",
                "æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡": "âœ… å·²æŒæ¡"
            },
            "ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•": {
                "Q-Learningç®—æ³•": "âœ… å·²æŒæ¡",
                "ç­–ç•¥æ¢¯åº¦æ–¹æ³•": "âœ… å·²æŒæ¡",
                "æ—¶åºå·®åˆ†å­¦ä¹ ": "âœ… å·²æŒæ¡",
                "è’™ç‰¹å¡ç½—æ–¹æ³•": "âœ… å·²æŒæ¡"
            },
            "æ·±åº¦å¼ºåŒ–å­¦ä¹ ": {
                "DQNç½‘ç»œæ¶æ„": "âœ… å·²æŒæ¡",
                "ç»éªŒå›æ”¾æœºåˆ¶": "âœ… å·²æŒæ¡",
                "ç›®æ ‡ç½‘ç»œæŠ€æœ¯": "âœ… å·²æŒæ¡",
                "Actor-Criticæ–¹æ³•": "âœ… å·²æŒæ¡"
            },
            "ä¼ä¸šçº§åº”ç”¨å¼€å‘": {
                "äº¤æ˜“ç³»ç»Ÿè®¾è®¡": "âœ… å·²æŒæ¡",
                "é£é™©æ§åˆ¶æœºåˆ¶": "âœ… å·²æŒæ¡",
                "æ€§èƒ½è¯„ä¼°ä½“ç³»": "âœ… å·²æŒæ¡",
                "ç³»ç»Ÿæ¶æ„è®¾è®¡": "âœ… å·²æŒæ¡"
            }
        }
        
        self.practical_skills = {
            "ç¯å¢ƒå»ºæ¨¡èƒ½åŠ›": "èƒ½å¤Ÿä¸ºå¤æ‚é—®é¢˜è®¾è®¡åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ",
            "ç®—æ³•å®ç°èƒ½åŠ›": "èƒ½å¤Ÿä»é›¶å®ç°å„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•",
            "å‚æ•°è°ƒä¼˜èƒ½åŠ›": "èƒ½å¤Ÿé’ˆå¯¹å…·ä½“é—®é¢˜ä¼˜åŒ–ç®—æ³•è¶…å‚æ•°",
            "ç³»ç»Ÿé›†æˆèƒ½åŠ›": "èƒ½å¤Ÿå°†RLç®—æ³•é›†æˆåˆ°å®é™…ä¸šåŠ¡ç³»ç»Ÿä¸­"
        }
    
    def evaluate_knowledge(self):
        """è¯„ä¼°çŸ¥è¯†æŒæ¡æƒ…å†µ"""
        print("ğŸ“š ç¬¬30ç« çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ")
        print("=" * 40)
        
        for category, points in self.knowledge_points.items():
            print(f"\nğŸ¯ {category}:")
            for point, status in points.items():
                print(f"   â€¢ {point}: {status}")
    
    def evaluate_skills(self):
        """è¯„ä¼°æŠ€èƒ½æŒæ¡æƒ…å†µ"""
        print(f"\nğŸ› ï¸ å®è·µæŠ€èƒ½æŒæ¡æƒ…å†µ")
        print("=" * 30)
        
        for skill, description in self.practical_skills.items():
            print(f"âœ… {skill}: {description}")
    
    def generate_learning_report(self):
        """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
        total_points = sum(len(points) for points in self.knowledge_points.values())
        mastered_points = total_points  # å‡è®¾å…¨éƒ¨æŒæ¡
        
        print(f"\nğŸ“Š å­¦ä¹ æˆæœç»Ÿè®¡")
        print("=" * 25)
        print(f"   ğŸ“– æ€»çŸ¥è¯†ç‚¹æ•°: {total_points}")
        print(f"   âœ… å·²æŒæ¡æ•°é‡: {mastered_points}")
        print(f"   ğŸ“ˆ æŒæ¡ç¨‹åº¦: {(mastered_points/total_points)*100:.1f}%")
        print(f"   ğŸ¯ æŠ€èƒ½ç­‰çº§: é«˜çº§å¼ºåŒ–å­¦ä¹ å·¥ç¨‹å¸ˆ")
        
        print(f"\nğŸ† æ ¸å¿ƒæˆå°±:")
        achievements = [
            "ğŸ® æŒæ¡å¼ºåŒ–å­¦ä¹ å®Œæ•´ç†è®ºä½“ç³»",
            "ğŸ§  å®ç°å¤šç§ç»å…¸RLç®—æ³•",
            "ğŸ¤– æ„å»ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ", 
            "ğŸ’¼ å¼€å‘ä¼ä¸šçº§äº¤æ˜“å†³ç­–ç³»ç»Ÿ",
            "ğŸ“Š å»ºç«‹å®Œæ•´çš„æ€§èƒ½è¯„ä¼°æ¡†æ¶"
        ]
        
        for achievement in achievements:
            print(f"   {achievement}")

# è¿è¡Œå­¦ä¹ è¯„ä¼°
assessment = Chapter30Assessment()
assessment.evaluate_knowledge()
assessment.evaluate_skills()
assessment.generate_learning_report()
```

#### ğŸš€ æŠ€æœ¯ä»·å€¼ä¸åº”ç”¨å‰æ™¯

```python
# ğŸš€ å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä»·å€¼åˆ†æ
def analyze_rl_value():
    """åˆ†æå¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯ä»·å€¼å’Œåº”ç”¨å‰æ™¯"""
    
    print("\nğŸŒŸ å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä»·å€¼åˆ†æ")
    print("=" * 40)
    
    technical_value = {
        "è‡ªä¸»å†³ç­–èƒ½åŠ›": {
            "ä»·å€¼": "æ— éœ€äººå·¥å¹²é¢„çš„æ™ºèƒ½å†³ç­–",
            "åº”ç”¨": "è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAI",
            "å‰æ™¯": "æœªæ¥æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯"
        },
        "é€‚åº”æ€§å­¦ä¹ ": {
            "ä»·å€¼": "èƒ½å¤Ÿé€‚åº”ç¯å¢ƒå˜åŒ–çš„å­¦ä¹ èƒ½åŠ›", 
            "åº”ç”¨": "æ¨èç³»ç»Ÿã€é‡‘èäº¤æ˜“ã€èµ„æºè°ƒåº¦",
            "å‰æ™¯": "åŠ¨æ€ç¯å¢ƒä¸‹çš„æœ€ä¼˜è§£å†³æ–¹æ¡ˆ"
        },
        "åºè´¯ä¼˜åŒ–": {
            "ä»·å€¼": "è€ƒè™‘é•¿æœŸå½±å“çš„å†³ç­–ä¼˜åŒ–",
            "åº”ç”¨": "ä¾›åº”é“¾ç®¡ç†ã€æŠ•èµ„ç»„åˆã€æ²»ç–—æ–¹æ¡ˆ",
            "å‰æ™¯": "å¤æ‚ç³»ç»Ÿçš„å…¨å±€ä¼˜åŒ–å·¥å…·"
        },
        "æ¢ç´¢åˆ›æ–°": {
            "ä»·å€¼": "åœ¨æœªçŸ¥é¢†åŸŸå‘ç°æ–°ç­–ç•¥",
            "åº”ç”¨": "è¯ç‰©å‘ç°ã€ææ–™è®¾è®¡ã€ç®—æ³•ä¼˜åŒ–", 
            "å‰æ™¯": "ç§‘å­¦ç ”ç©¶å’ŒæŠ€æœ¯åˆ›æ–°çš„åŠ©æ‰‹"
        }
    }
    
    for aspect, details in technical_value.items():
        print(f"\nğŸ’¡ {aspect}:")
        print(f"   ğŸ¯ ä»·å€¼: {details['ä»·å€¼']}")
        print(f"   ğŸ”§ åº”ç”¨: {details['åº”ç”¨']}")
        print(f"   ğŸš€ å‰æ™¯: {details['å‰æ™¯']}")
    
    print(f"\nğŸ’¼ å•†ä¸šåº”ç”¨ä»·å€¼:")
    business_applications = [
        "ğŸ¦ æ™ºèƒ½é‡‘è: ç®—æ³•äº¤æ˜“ã€é£é™©ç®¡ç†ã€ä¿¡è´·å†³ç­–",
        "ğŸš— è‡ªåŠ¨é©¾é©¶: è·¯å¾„è§„åˆ’ã€è¡Œä¸ºå†³ç­–ã€å®‰å…¨æ§åˆ¶", 
        "ğŸ® æ¸¸æˆAI: æ™ºèƒ½NPCã€è‡ªé€‚åº”éš¾åº¦ã€å†…å®¹ç”Ÿæˆ",
        "ğŸ­ å·¥ä¸šæ§åˆ¶: ç”Ÿäº§ä¼˜åŒ–ã€è´¨é‡æ§åˆ¶ã€è®¾å¤‡ç»´æŠ¤",
        "ğŸ“± ä¸ªæ€§åŒ–æ¨è: å†…å®¹æ¨èã€å¹¿å‘ŠæŠ•æ”¾ã€ç”¨æˆ·ä½“éªŒ"
    ]
    
    for application in business_applications:
        print(f"   {application}")

# è¿è¡Œä»·å€¼åˆ†æ
analyze_rl_value()
```

#### ğŸ¯ æ·±åº¦æ€è€ƒé¢˜

1. **ğŸ¤” ç®—æ³•é€‰æ‹©æ€è€ƒ**ï¼š
   - åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©åŸºäºä»·å€¼çš„æ–¹æ³•ï¼ˆå¦‚DQNï¼‰è€Œä¸æ˜¯åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼ˆå¦‚Policy Gradientï¼‰ï¼Ÿ
   - å¦‚ä½•æ ¹æ®å…·ä½“é—®é¢˜çš„ç‰¹ç‚¹æ¥è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Ÿ

2. **âš–ï¸ æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡**ï¼š
   - åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚ä½•å¹³è¡¡æ¢ç´¢æ–°ç­–ç•¥å’Œåˆ©ç”¨å·²çŸ¥æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„å…³ç³»ï¼Ÿ
   - Îµ-è´ªå¿ƒç­–ç•¥çš„è¡°å‡é€Ÿåº¦åº”è¯¥å¦‚ä½•è®¾è®¡ï¼Ÿ

3. **ğŸ¢ ä¼ä¸šçº§éƒ¨ç½²è€ƒè™‘**ï¼š
   - å°†å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ—¶éœ€è¦è€ƒè™‘å“ªäº›é£é™©å’ŒæŒ‘æˆ˜ï¼Ÿ
   - å¦‚ä½•è®¾è®¡A/Bæµ‹è¯•æ¥éªŒè¯RLç³»ç»Ÿçš„æ•ˆæœï¼Ÿ

4. **ğŸ”® æœªæ¥å‘å±•æ–¹å‘**ï¼š
   - å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨å“ªäº›åœºæ™¯ä¸‹æ¯”å•æ™ºèƒ½ä½“æ›´æœ‰ä¼˜åŠ¿ï¼Ÿ
   - å¼ºåŒ–å­¦ä¹ ä¸å…¶ä»–AIæŠ€æœ¯ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹ï¼‰ç»“åˆä¼šäº§ç”Ÿä»€ä¹ˆæ–°çš„å¯èƒ½æ€§ï¼Ÿ

#### ğŸŠ å­¦ä¹ ç›®æ ‡è¾¾æˆåº¦è¯„ä¼°

**çŸ¥è¯†ç›®æ ‡è¾¾æˆåº¦: 95%** âœ…
- âœ… æ·±å…¥ç†è§£å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µå’Œæ•°å­¦åŸç†
- âœ… æŒæ¡ç»å…¸RLç®—æ³•çš„å®ç°å’Œåº”ç”¨
- âœ… ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯åˆ›æ–°
- âœ… è®¤è¯†RLåœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ä»·å€¼

**æŠ€èƒ½ç›®æ ‡è¾¾æˆåº¦: 98%** âœ…  
- âœ… èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
- âœ… èƒ½å¤Ÿå®ç°å„ç§ç»å…¸å’Œç°ä»£RLç®—æ³•
- âœ… èƒ½å¤Ÿå¼€å‘ä¼ä¸šçº§çš„æ™ºèƒ½å†³ç­–ç³»ç»Ÿ
- âœ… èƒ½å¤Ÿè¯„ä¼°å’Œä¼˜åŒ–RLç³»ç»Ÿæ€§èƒ½

**ç´ å…»ç›®æ ‡è¾¾æˆåº¦: 92%** âœ…
- âœ… å½¢æˆäº†æ™ºèƒ½å†³ç­–çš„ç³»ç»Ÿæ€§æ€ç»´
- âœ… å»ºç«‹äº†è¯•é”™å­¦ä¹ çš„ç§‘å­¦æ€åº¦  
- âœ… æŒæ¡äº†å¤æ‚ç³»ç»Ÿä¼˜åŒ–çš„æ–¹æ³•è®º
- âœ… æ ‘ç«‹äº†AIæŠ€æœ¯åº”ç”¨çš„è´£ä»»æ„è¯†

### ğŸ”® ä¸‹ç« é¢„å‘Šï¼šæ™ºèƒ½ä½“åä½œä¸ç¼–æ’

åœ¨ç¬¬31ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»å•ä¸ªæ™ºèƒ½ä½“çš„å†³ç­–å­¦ä¹ ï¼Œå‡çº§åˆ°**å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ**çš„è®¾è®¡ä¸å¼€å‘ã€‚æˆ‘ä»¬å°†æ¢ç´¢ï¼š

- ğŸ¤ **å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶**ï¼šå¦‚ä½•è®¾è®¡æ™ºèƒ½ä½“é—´çš„é€šä¿¡å’Œåè°ƒæœºåˆ¶
- ğŸ­ **è§’è‰²åˆ†å·¥ä¸ä»»åŠ¡ç¼–æ’**ï¼šå¦‚ä½•å®ç°æ™ºèƒ½ä½“çš„ä¸“ä¸šåŒ–åˆ†å·¥å’Œé«˜æ•ˆåä½œ
- ğŸ§  **ç¾¤ä½“æ™ºèƒ½ä¸æ¶Œç°è¡Œä¸º**ï¼šå¦‚ä½•é€šè¿‡ä¸ªä½“åä½œäº§ç”Ÿè¶…è¶Šå•ä½“çš„é›†ä½“æ™ºèƒ½
- ğŸ¢ **ä¼ä¸šçº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ**ï¼šæ„å»ºå¤§è§„æ¨¡æ™ºèƒ½ä½“åä½œçš„ç”Ÿäº§çº§åº”ç”¨

ä»æ™ºèƒ½å†³ç­–å­¦é™¢åˆ°æ™ºèƒ½ä½“åä½œç½‘ç»œï¼Œè®©æˆ‘ä»¬ç»§ç»­æ¢ç´¢AIæŠ€æœ¯çš„æ— é™å¯èƒ½ï¼

---

> ğŸ“ **ç¬¬30ç« æ€»ç»“**: é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†è®©AIç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ å’Œå†³ç­–çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»ç†è®ºåŸºç¡€åˆ°ä¼ä¸šçº§åº”ç”¨ï¼Œä»ç»å…¸ç®—æ³•åˆ°æ·±åº¦å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ ˆã€‚è¿™ä¸ºæˆ‘ä»¬è¿›å…¥å¤šæ™ºèƒ½ä½“åä½œçš„æ›´é«˜å±‚æ¬¡å¥ å®šäº†åšå®åŸºç¡€ï¼
``` 