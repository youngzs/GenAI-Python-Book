# ç¬¬29ç« ï¼šRAGæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯

> *"çŸ¥è¯†çš„åŠ›é‡ä¸åœ¨äºæ‹¥æœ‰å¤šå°‘ï¼Œè€Œåœ¨äºèƒ½å¦åœ¨éœ€è¦æ—¶ç²¾å‡†æ£€ç´¢å¹¶æ™ºèƒ½åº”ç”¨ã€‚"*

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- **ç†è§£RAGæ ¸å¿ƒåŸç†**ï¼šæŒæ¡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æŠ€æœ¯æ¶æ„å’Œå·¥ä½œæœºåˆ¶
- **æŒæ¡å‘é‡æ•°æ®åº“æŠ€æœ¯**ï¼šå­¦ä¹ æ–‡æ¡£å‘é‡åŒ–ã€å­˜å‚¨å’Œæ£€ç´¢çš„å®Œæ•´æµç¨‹
- **ç†Ÿæ‚‰æ£€ç´¢ç­–ç•¥ä¼˜åŒ–**ï¼šç†è§£å¯†é›†æ£€ç´¢ã€ç¨€ç–æ£€ç´¢å’Œæ··åˆæ£€ç´¢ç­–ç•¥
- **äº†è§£ç”Ÿæˆè´¨é‡æ§åˆ¶**ï¼šå­¦ä¹ åŸºäºæ£€ç´¢çš„ç­”æ¡ˆç”Ÿæˆå’Œè´¨é‡è¯„ä¼°æ–¹æ³•

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡  
- **è®¾è®¡RAGç³»ç»Ÿæ¶æ„**ï¼šèƒ½å¤Ÿè®¾è®¡å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
- **å®ç°å‘é‡æ£€ç´¢å¼•æ“**ï¼šå¼€å‘é«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢å’ŒåŒ¹é…ç®—æ³•
- **æ„å»ºçŸ¥è¯†é—®ç­”ç³»ç»Ÿ**ï¼šå»ºç«‹ä¼ä¸šçº§æ™ºèƒ½é—®ç­”è§£å†³æ–¹æ¡ˆ
- **ä¼˜åŒ–æ£€ç´¢ç”Ÿæˆè´¨é‡**ï¼šæŒæ¡æ£€ç´¢ç²¾åº¦å’Œç”Ÿæˆè´¨é‡çš„å¹³è¡¡æŠ€æœ¯

### ğŸŒŸ ç´ å…»ç›®æ ‡
- **ä¿¡æ¯æ£€ç´¢æ€ç»´**ï¼šåŸ¹å…»ç³»ç»Ÿæ€§çš„çŸ¥è¯†ç®¡ç†å’Œä¿¡æ¯æ£€ç´¢ç†å¿µ
- **å·¥ç¨‹åŒ–æ„è¯†**ï¼šå»ºç«‹å¤§è§„æ¨¡çŸ¥è¯†åº“ç³»ç»Ÿçš„å·¥ç¨‹åŒ–æ€ç»´
- **åˆ›æ–°åº”ç”¨èƒ½åŠ›**ï¼šå…·å¤‡å°†RAGæŠ€æœ¯åº”ç”¨äºå®é™…ä¸šåŠ¡åœºæ™¯çš„èƒ½åŠ›

---

## ğŸ¢ æ¬¢è¿æ¥åˆ°çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒ

ç»è¿‡å‰é¢ç« èŠ‚å¯¹AIæ¨¡å‹æŠ€æœ¯çš„æ·±å…¥å­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†ä»åŸºç¡€æœºå™¨å­¦ä¹ åˆ°é«˜çº§æ™ºèƒ½ä½“å¼€å‘çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬èµ°è¿›ä¸€ä¸ªå…¨æ–°çš„æ™ºèƒ½ä¸–ç•Œâ€”â€”**çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒ**ï¼

### ğŸŒ† çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒå…¨æ™¯å›¾

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£ç«™åœ¨ä¸€åº§ç°ä»£åŒ–çš„æ™ºèƒ½ä¿¡æ¯å¤§å¦å‰ï¼Œè¿™é‡Œæ˜¯**çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒ**çš„æ€»éƒ¨ï¼š

```mermaid
graph TB
    subgraph "çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒæ€»éƒ¨"
        subgraph "ä¿¡æ¯é‡‡é›†éƒ¨"
            DC[æ–‡æ¡£æ”¶é›†å™¨]
            DP[æ–‡æ¡£é¢„å¤„ç†å™¨]
            CS[åˆ†å—åˆ‡å‰²å™¨]
        end
        
        subgraph "è¯­ä¹‰ä»“åº“"
            EM[åµŒå…¥æ¨¡å‹]
            VDB[å‘é‡æ•°æ®åº“]
            IDX[ç´¢å¼•ç®¡ç†å™¨]
        end
        
        subgraph "æ£€ç´¢å¼•æ“"
            QE[æŸ¥è¯¢ç¼–ç å™¨]
            SM[ç›¸ä¼¼åº¦åŒ¹é…å™¨]
            RR[ç»“æœæ’åºå™¨]
        end
        
        subgraph "ç­”æ¡ˆç”Ÿæˆå™¨"
            CG[ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨]
            LLM[è¯­è¨€æ¨¡å‹]
            QC[è´¨é‡æ§åˆ¶å™¨]
        end
        
        subgraph "è´¨é‡ç›‘æ§ä¸­å¿ƒ"
            PM[æ€§èƒ½ç›‘æ§]
            AC[å‡†ç¡®æ€§æ£€æŸ¥]
            FB[åé¦ˆæ”¶é›†]
        end
    end
    
    DC --> DP
    DP --> CS
    CS --> EM
    EM --> VDB
    VDB --> IDX
    
    QE --> SM
    SM --> RR
    RR --> CG
    CG --> LLM
    LLM --> QC
    
    PM --> AC
    AC --> FB
```

### ğŸ­ ä»è®°å¿†åˆ°æ£€ç´¢çš„æ™ºèƒ½è¿›åŒ–

å¦‚æœè¯´ä¼ ç»Ÿçš„AIæ¨¡å‹æ˜¯ä¸€ä½åšå­¦çš„å­¦è€…ï¼Œé‚£ä¹ˆRAGç³»ç»Ÿå°±æ˜¯ä¸€åº§æ‹¥æœ‰æ— é™æ‰©å±•èƒ½åŠ›çš„æ™ºèƒ½å›¾ä¹¦é¦†ï¼š

- **ğŸ“š æµ·é‡çŸ¥è¯†å­˜å‚¨**ï¼šä¸å†å—é™äºæ¨¡å‹å‚æ•°ï¼Œå¯ä»¥æ— é™æ‰©å±•çŸ¥è¯†åº“
- **ğŸ” ç²¾å‡†ä¿¡æ¯æ£€ç´¢**ï¼šé€šè¿‡è¯­ä¹‰ç†è§£å¿«é€Ÿå®šä½ç›¸å…³ä¿¡æ¯
- **ğŸ§  æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆ**ï¼šç»“åˆæ£€ç´¢åˆ°çš„çŸ¥è¯†ç”Ÿæˆå‡†ç¡®ã€æœ‰æ ¹æ®çš„å›ç­”
- **ğŸ”„ å®æ—¶çŸ¥è¯†æ›´æ–°**ï¼šå¯ä»¥éšæ—¶æ·»åŠ æ–°çŸ¥è¯†è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹

---

## 29.1 RAGç³»ç»Ÿæ¦‚è¿°ä¸æ ¸å¿ƒåŸç†

### ğŸ§­ ä»€ä¹ˆæ˜¯RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ

**RAG(Retrieval-Augmented Generation)**æ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„AIæŠ€æœ¯æ¶æ„ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è¦†ç›–é¢å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚

### ğŸ—ï¸ RAG vs ä¼ ç»ŸLLMå¯¹æ¯”

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç”ŸåŠ¨çš„å¯¹æ¯”æ¥ç†è§£RAGçš„ä¼˜åŠ¿ï¼š

```python
# RAGç³»ç»Ÿæ ¸å¿ƒæ¶æ„æ¼”ç¤º
import numpy as np
from typing import List, Dict, Tuple, Any
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
import json

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç»“æ„"""
    id: str
    title: str
    content: str
    metadata: Dict[str, Any]
    embedding: np.ndarray = None
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç»“æ„"""
    document: Document
    score: float
    relevance_explanation: str

@dataclass
class RAGResponse:
    """RAGç³»ç»Ÿå“åº”ç»“æ„"""
    query: str
    retrieved_docs: List[RetrievalResult]
    generated_answer: str
    confidence_score: float
    sources: List[str]
    
class TraditionalLLM:
    """ä¼ ç»ŸLLMæ¨¡æ‹Ÿç±»"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.knowledge_cutoff = "2023-04"  # çŸ¥è¯†æˆªæ­¢æ—¶é—´
        self.parameter_count = "175B"      # å‚æ•°é‡
        
    def generate_answer(self, query: str) -> str:
        """åŸºäºå‚æ•°åŒ–çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ"""
        # æ¨¡æ‹Ÿä¼ ç»ŸLLMçš„å±€é™æ€§
        limitations = [
            "çŸ¥è¯†æˆªæ­¢äºè®­ç»ƒæ—¶é—´",
            "æ— æ³•è·å–æœ€æ–°ä¿¡æ¯",
            "å¯èƒ½äº§ç”Ÿå¹»è§‰",
            "æ— æ³•æä¾›ä¿¡æ¯æ¥æº"
        ]
        
        return f"""
        ä¼ ç»ŸLLMå›ç­”ï¼š{query}
        
        åŸºäºæˆ‘çš„è®­ç»ƒæ•°æ®ï¼ˆæˆªæ­¢åˆ°{self.knowledge_cutoff}ï¼‰ï¼Œæˆ‘è®¤ä¸º...
        
        æ³¨æ„ï¼šæ­¤ç­”æ¡ˆåŸºäºé¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¯èƒ½ä¸æ˜¯æœ€æ–°ä¿¡æ¯ã€‚
        å±€é™æ€§ï¼š{', '.join(limitations)}
        """

class RAGSystem:
    """RAGç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, llm_model: str, vector_db_config: Dict):
        self.llm_model = llm_model
        self.vector_db = None  # å‘é‡æ•°æ®åº“è¿æ¥
        self.embedding_model = None  # åµŒå…¥æ¨¡å‹
        self.documents: List[Document] = []
        self.retrieval_top_k = 5
        
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        for doc in documents:
            # ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡
            doc.embedding = self._generate_embedding(doc.content)
            self.documents.append(doc)
        print(f"âœ… å·²æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        
    def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹
        return np.random.random(768)  # æ¨¡æ‹Ÿ768ç»´å‘é‡
        
    def retrieve_relevant_docs(self, query: str, top_k: int = None) -> List[RetrievalResult]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if top_k is None:
            top_k = self.retrieval_top_k
            
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self._generate_embedding(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
        results = []
        for doc in self.documents:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰
            similarity = np.random.random()  # æ¨¡æ‹Ÿç›¸ä¼¼åº¦è®¡ç®—
            
            result = RetrievalResult(
                document=doc,
                score=similarity,
                relevance_explanation=f"ä¸æŸ¥è¯¢åœ¨è¯­ä¹‰ä¸Šç›¸å…³åº¦ä¸º {similarity:.3f}"
            )
            results.append(result)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_k
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:top_k]
    
    def generate_answer(self, query: str) -> RAGResponse:
        """ç”ŸæˆRAGå¢å¼ºå›ç­”"""
        # æ­¥éª¤1ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_relevant_docs(query)
        
        # æ­¥éª¤2ï¼šæ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        
        for i, result in enumerate(retrieved_docs):
            doc = result.document
            context_parts.append(f"å‚è€ƒæ–‡æ¡£{i+1}ï¼š{doc.content[:200]}...")
            sources.append(f"{doc.title} (ç›¸å…³åº¦: {result.score:.3f})")
        
        enhanced_context = "\n\n".join(context_parts)
        
        # æ­¥éª¤3ï¼šç”Ÿæˆå¢å¼ºå›ç­”
        enhanced_prompt = f"""
        åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
        
        ç”¨æˆ·é—®é¢˜ï¼š{query}
        
        ç›¸å…³æ–‡æ¡£ï¼š
        {enhanced_context}
        
        è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®ã€æœ‰æ ¹æ®çš„å›ç­”ï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨å…·ä½“æ¥æºã€‚
        """
        
        # æ¨¡æ‹ŸLLMç”Ÿæˆè¿‡ç¨‹
        generated_answer = f"""
        åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä»¥ä¸‹å›ç­”ï¼š
        
        [åŸºäºæ–‡æ¡£å†…å®¹çš„è¯¦ç»†å›ç­”...]
        
        è¯¥å›ç­”åŸºäº {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå…·æœ‰è¾ƒé«˜çš„å¯ä¿¡åº¦ã€‚
        """
        
        return RAGResponse(
            query=query,
            retrieved_docs=retrieved_docs,
            generated_answer=generated_answer,
            confidence_score=0.85,  # æ¨¡æ‹Ÿç½®ä¿¡åº¦
            sources=sources
        )

# ç³»ç»Ÿå¯¹æ¯”æ¼”ç¤º
def compare_systems():
    """å¯¹æ¯”ä¼ ç»ŸLLMå’ŒRAGç³»ç»Ÿ"""
    
    print("ğŸ” AIé—®ç­”ç³»ç»Ÿå¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    traditional_llm = TraditionalLLM("GPT-3.5")
    rag_system = RAGSystem("GPT-3.5", {"type": "faiss"})
    
    # æ·»åŠ ä¸€äº›ç¤ºä¾‹æ–‡æ¡£åˆ°RAGç³»ç»Ÿ
    sample_docs = [
        Document(
            id="doc1",
            title="Python RAGæŠ€æœ¯ç™½çš®ä¹¦",
            content="RAGæŠ€æœ¯é€šè¿‡ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†AIç³»ç»Ÿçš„çŸ¥è¯†è¦†ç›–é¢...",
            metadata={"category": "æŠ€æœ¯æ–‡æ¡£", "date": "2024-12"}
        ),
        Document(
            id="doc2", 
            title="å‘é‡æ•°æ®åº“æœ€ä½³å®è·µ",
            content="å‘é‡æ•°æ®åº“æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢è¯­ä¹‰å‘é‡...",
            metadata={"category": "æŠ€æœ¯æŒ‡å—", "date": "2024-11"}
        )
    ]
    
    rag_system.add_documents(sample_docs)
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Œå®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
    
    print(f"\nğŸ“ ç”¨æˆ·é—®é¢˜ï¼š{query}")
    print("\n" + "="*50)
    
    # ä¼ ç»ŸLLMå›ç­”
    print("ğŸ¤– ä¼ ç»ŸLLMå›ç­”ï¼š")
    traditional_answer = traditional_llm.generate_answer(query)
    print(traditional_answer)
    
    print("\n" + "="*50)
    
    # RAGç³»ç»Ÿå›ç­”
    print("ğŸ” RAGç³»ç»Ÿå›ç­”ï¼š")
    rag_response = rag_system.generate_answer(query)
    print(f"ç”Ÿæˆçš„å›ç­”ï¼š{rag_response.generated_answer}")
    print(f"ç½®ä¿¡åº¦ï¼š{rag_response.confidence_score}")
    print(f"å‚è€ƒæ¥æºï¼š{', '.join(rag_response.sources)}")

# è¿è¡Œå¯¹æ¯”æ¼”ç¤º
if __name__ == "__main__":
    compare_systems()

print("âœ… RAGç³»ç»Ÿæ ¸å¿ƒæ¶æ„æ¼”ç¤ºå®Œæˆ")
```

### ğŸ¯ RAGç³»ç»Ÿçš„æ ¸å¿ƒä¼˜åŠ¿

é€šè¿‡ä¸Šé¢çš„å¯¹æ¯”æ¼”ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°RAGç³»ç»Ÿçš„æ˜¾è‘—ä¼˜åŠ¿ï¼š

```mermaid
graph LR
    subgraph "ä¼ ç»ŸLLMå±€é™æ€§"
        A1[çŸ¥è¯†æˆªæ­¢æ—¶é—´å›ºå®š]
        A2[æ— æ³•è·å–æœ€æ–°ä¿¡æ¯]
        A3[å®¹æ˜“äº§ç”Ÿå¹»è§‰]
        A4[æ— æ³•æä¾›ä¿¡æ¯æ¥æº]
        A5[çŸ¥è¯†æ›´æ–°éœ€è¦é‡è®­ç»ƒ]
    end
    
    subgraph "RAGç³»ç»Ÿä¼˜åŠ¿"
        B1[å®æ—¶çŸ¥è¯†æ›´æ–°]
        B2[è·å–æœ€æ–°ä¿¡æ¯]
        B3[åŸºäºäº‹å®ç”Ÿæˆ]
        B4[æä¾›å¯ä¿¡æ¥æº]
        B5[æ— éœ€é‡è®­ç»ƒæ¨¡å‹]
    end
    
    A1 -.è§£å†³.-> B1
    A2 -.è§£å†³.-> B2
    A3 -.è§£å†³.-> B3
    A4 -.è§£å†³.-> B4
    A5 -.è§£å†³.-> B5
```

### ğŸ—ï¸ RAGç³»ç»Ÿå·¥ä½œæµç¨‹è¯¦è§£

è®©æˆ‘ä»¬æ·±å…¥äº†è§£RAGç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š

```python
class RAGWorkflowDemo:
    """RAGå·¥ä½œæµç¨‹æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.workflow_steps = [
            "æ–‡æ¡£é¢„å¤„ç†",
            "å‘é‡åŒ–ç¼–ç ", 
            "å‘é‡å­˜å‚¨",
            "æŸ¥è¯¢å¤„ç†",
            "ç›¸ä¼¼åº¦æ£€ç´¢",
            "ä¸Šä¸‹æ–‡æ„å»º",
            "ç­”æ¡ˆç”Ÿæˆ",
            "è´¨é‡éªŒè¯"
        ]
    
    def demonstrate_workflow(self, user_query: str):
        """æ¼”ç¤ºå®Œæ•´çš„RAGå·¥ä½œæµç¨‹"""
        
        print("ğŸ”„ RAGç³»ç»Ÿå·¥ä½œæµç¨‹æ¼”ç¤º")
        print("=" * 60)
        
        # æ­¥éª¤1ï¼šæ–‡æ¡£é¢„å¤„ç†ï¼ˆç¦»çº¿é˜¶æ®µï¼‰
        print("\nğŸ“š æ­¥éª¤1ï¼šæ–‡æ¡£é¢„å¤„ç†ï¼ˆç¦»çº¿é˜¶æ®µï¼‰")
        print("- æ”¶é›†å’Œæ¸…æ´—æ–‡æ¡£æ•°æ®")
        print("- æ–‡æ¡£åˆ†å—å’Œæ ¼å¼æ ‡å‡†åŒ–") 
        print("- æå–å…ƒæ•°æ®ä¿¡æ¯")
        
        raw_documents = [
            "RAGæŠ€æœ¯æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæ¶æ„...",
            "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®...",
            "è¯­ä¹‰æœç´¢é€šè¿‡ç†è§£æŸ¥è¯¢æ„å›¾æä¾›ç²¾å‡†ç»“æœ..."
        ]
        
        processed_chunks = []
        for i, doc in enumerate(raw_documents):
            chunk = {
                "id": f"chunk_{i}",
                "content": doc,
                "length": len(doc),
                "metadata": {"source": f"document_{i}.txt"}
            }
            processed_chunks.append(chunk)
        
        print(f"âœ… å¤„ç†å®Œæˆï¼š{len(processed_chunks)} ä¸ªæ–‡æ¡£å—")
        
        # æ­¥éª¤2ï¼šå‘é‡åŒ–ç¼–ç 
        print("\nğŸ§® æ­¥éª¤2ï¼šå‘é‡åŒ–ç¼–ç ")
        print("- ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ç¼–ç æ–‡æ¡£")
        print("- ç”Ÿæˆé«˜ç»´è¯­ä¹‰å‘é‡è¡¨ç¤º")
        
        embeddings = []
        for chunk in processed_chunks:
            # æ¨¡æ‹Ÿå‘é‡åŒ–è¿‡ç¨‹
            embedding = np.random.random(384)  # 384ç»´å‘é‡
            embeddings.append(embedding)
            print(f"  æ–‡æ¡£å— {chunk['id']}: å‘é‡ç»´åº¦ {len(embedding)}")
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼š{len(embeddings)} ä¸ªå‘é‡")
        
        # æ­¥éª¤3ï¼šå‘é‡å­˜å‚¨
        print("\nğŸ’¾ æ­¥éª¤3ï¼šå‘é‡å­˜å‚¨")
        print("- å°†å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
        print("- å»ºç«‹é«˜æ•ˆçš„ç´¢å¼•ç»“æ„")
        
        vector_index = {
            "vectors": embeddings,
            "metadata": [chunk["metadata"] for chunk in processed_chunks],
            "index_type": "HNSW",  # å±‚æ¬¡åŒ–å°ä¸–ç•Œå›¾
            "dimension": 384
        }
        
        print(f"âœ… å­˜å‚¨å®Œæˆï¼š{len(vector_index['vectors'])} ä¸ªå‘é‡å·²ç´¢å¼•")
        
        # æ­¥éª¤4ï¼šæŸ¥è¯¢å¤„ç†ï¼ˆåœ¨çº¿é˜¶æ®µï¼‰
        print(f"\nâ“ æ­¥éª¤4ï¼šæŸ¥è¯¢å¤„ç†ï¼ˆåœ¨çº¿é˜¶æ®µï¼‰")
        print(f"- ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}")
        print("- æŸ¥è¯¢é¢„å¤„ç†å’Œæ ‡å‡†åŒ–")
        
        processed_query = {
            "original": user_query,
            "cleaned": user_query.lower().strip(),
            "tokens": user_query.split(),
            "intent": "ä¿¡æ¯æŸ¥è¯¢"
        }
        
        print(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆï¼š{processed_query['intent']}")
        
        # æ­¥éª¤5ï¼šç›¸ä¼¼åº¦æ£€ç´¢
        print("\nğŸ” æ­¥éª¤5ï¼šç›¸ä¼¼åº¦æ£€ç´¢")
        print("- å°†æŸ¥è¯¢å‘é‡åŒ–")
        print("- è®¡ç®—ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦")
        print("- æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—")
        
        query_embedding = np.random.random(384)  # æ¨¡æ‹ŸæŸ¥è¯¢å‘é‡
        similarities = []
        
        for i, doc_embedding in enumerate(embeddings):
            # æ¨¡æ‹Ÿä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
            similarity = np.random.random()
            similarities.append({
                "chunk_id": f"chunk_{i}",
                "similarity": similarity,
                "content": processed_chunks[i]["content"][:50] + "..."
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x["similarity"], reverse=True)
        top_results = similarities[:3]  # å–å‰3ä¸ªæœ€ç›¸å…³çš„
        
        print("ğŸ“Š æ£€ç´¢ç»“æœï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰ï¼š")
        for i, result in enumerate(top_results):
            print(f"  {i+1}. ç›¸ä¼¼åº¦: {result['similarity']:.3f} | {result['content']}")
        
        # æ­¥éª¤6ï¼šä¸Šä¸‹æ–‡æ„å»º
        print("\nğŸ“ æ­¥éª¤6ï¼šä¸Šä¸‹æ–‡æ„å»º")
        print("- æ•´åˆæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£")
        print("- æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡")
        
        context_parts = []
        for result in top_results:
            context_parts.append(f"å‚è€ƒå†…å®¹ï¼š{result['content']}")
        
        enhanced_context = "\n".join(context_parts)
        
        print(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼š{len(context_parts)} ä¸ªå‚è€ƒæ–‡æ¡£")
        
        # æ­¥éª¤7ï¼šç­”æ¡ˆç”Ÿæˆ
        print("\nğŸ¤– æ­¥éª¤7ï¼šç­”æ¡ˆç”Ÿæˆ")
        print("- ç»“åˆæŸ¥è¯¢å’Œæ£€ç´¢ä¸Šä¸‹æ–‡")
        print("- ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”")
        
        generation_prompt = f"""
        åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
        
        ç”¨æˆ·é—®é¢˜ï¼š{user_query}
        
        ç›¸å…³å†…å®¹ï¼š
        {enhanced_context}
        
        è¯·æä¾›å‡†ç¡®ã€æœ‰æ ¹æ®çš„å›ç­”ã€‚
        """
        
        # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        generated_answer = f"åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼Œ{user_query}çš„ç­”æ¡ˆæ˜¯..."
        
        print(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼š{len(generated_answer)} å­—ç¬¦")
        
        # æ­¥éª¤8ï¼šè´¨é‡éªŒè¯
        print("\nğŸ” æ­¥éª¤8ï¼šè´¨é‡éªŒè¯")
        print("- éªŒè¯ç­”æ¡ˆä¸æ£€ç´¢å†…å®¹çš„ä¸€è‡´æ€§")
        print("- è¯„ä¼°ç­”æ¡ˆè´¨é‡å’Œå¯ä¿¡åº¦")
        
        quality_metrics = {
            "ç›¸å…³æ€§è¯„åˆ†": 0.87,
            "å‡†ç¡®æ€§è¯„åˆ†": 0.92,
            "å®Œæ•´æ€§è¯„åˆ†": 0.85,
            "å¯ä¿¡åº¦è¯„åˆ†": 0.89
        }
        
        print("ğŸ“Š è´¨é‡è¯„ä¼°ç»“æœï¼š")
        for metric, score in quality_metrics.items():
            print(f"  {metric}: {score:.2f}")
        
        overall_score = sum(quality_metrics.values()) / len(quality_metrics)
        print(f"âœ… ç»¼åˆè´¨é‡è¯„åˆ†ï¼š{overall_score:.2f}")
        
        return {
            "query": user_query,
            "retrieved_docs": top_results,
            "generated_answer": generated_answer,
            "quality_score": overall_score
        }

# è¿è¡Œå·¥ä½œæµç¨‹æ¼”ç¤º
workflow_demo = RAGWorkflowDemo()
result = workflow_demo.demonstrate_workflow("ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ")

print("\n" + "=" * 60)
print("ğŸ‰ RAGå·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
print(f"ğŸ“‹ æœ€ç»ˆç»“æœï¼š{result['generated_answer']}")
print(f"ğŸ† è´¨é‡è¯„åˆ†ï¼š{result['quality_score']:.2f}")
```

### ğŸ¯ RAGç³»ç»Ÿçš„æŠ€æœ¯æ¶æ„å±‚æ¬¡

RAGç³»ç»Ÿå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæŠ€æœ¯å±‚æ¬¡ï¼š

```mermaid
graph TD
    subgraph "åº”ç”¨å±‚"
        A1[é—®ç­”ç³»ç»Ÿ]
        A2[æ™ºèƒ½å®¢æœ]
        A3[çŸ¥è¯†ç®¡ç†]
        A4[å†…å®¹ç”Ÿæˆ]
    end
    
    subgraph "æœåŠ¡å±‚"
        B1[æŸ¥è¯¢ç†è§£]
        B2[æ£€ç´¢æœåŠ¡]
        B3[ç”ŸæˆæœåŠ¡]
        B4[è´¨é‡æ§åˆ¶]
    end
    
    subgraph "ç®—æ³•å±‚"
        C1[åµŒå…¥æ¨¡å‹]
        C2[ç›¸ä¼¼åº¦è®¡ç®—]
        C3[æ’åºç®—æ³•]
        C4[ç”Ÿæˆæ¨¡å‹]
    end
    
    subgraph "æ•°æ®å±‚"
        D1[æ–‡æ¡£é¢„å¤„ç†]
        D2[å‘é‡æ•°æ®åº“]
        D3[å…ƒæ•°æ®å­˜å‚¨]
        D4[ç´¢å¼•ç®¡ç†]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        E1[è®¡ç®—èµ„æº]
        E2[å­˜å‚¨ç³»ç»Ÿ]
        E3[ç½‘ç»œæœåŠ¡]
        E4[ç›‘æ§ç³»ç»Ÿ]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    
    B1 --> C1
    B2 --> C2
    B3 --> C3
    B4 --> C4
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    C4 --> D4
    
    D1 --> E1
    D2 --> E2
    D3 --> E3
    D4 --> E4
```

é€šè¿‡è¿™ä¸ªå…¨é¢çš„æ¦‚è¿°ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¯¹RAGæŠ€æœ¯çš„åŸºç¡€ç†è§£ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ¯ä¸ªæŠ€æœ¯ç»„ä»¶çš„å…·ä½“å®ç°å’Œä¼˜åŒ–ç­–ç•¥ã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†RAGç³»ç»Ÿçš„æ ¸å¿ƒæ¦‚å¿µã€å·¥ä½œåŸç†å’ŒæŠ€æœ¯æ¶æ„ã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–æŠ€æœ¯ï¼Œè¿™æ˜¯æ„å»ºé«˜è´¨é‡RAGç³»ç»Ÿçš„åŸºç¡€ã€‚*

---

## 29.2 æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–æŠ€æœ¯

### ğŸ­ ä¿¡æ¯é¢„å¤„ç†å·¥å‚

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒä¸­ï¼Œ**ä¿¡æ¯é¢„å¤„ç†å·¥å‚**æ˜¯æ•´ä¸ªç³»ç»Ÿçš„èµ·ç‚¹ã€‚å°±åƒä¸€åº§ç°ä»£åŒ–çš„å·¥å‚ï¼Œå®ƒè´Ÿè´£å°†å„ç§åŸå§‹æ–‡æ¡£è½¬åŒ–ä¸ºæ ‡å‡†åŒ–ã€ç»“æ„åŒ–çš„çŸ¥è¯†å•å…ƒã€‚

### ğŸ“„ æ–‡æ¡£ç±»å‹ä¸æ ¼å¼å¤„ç†

RAGç³»ç»Ÿéœ€è¦å¤„ç†å¤šç§ç±»å‹çš„æ–‡æ¡£ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªé€šç”¨çš„æ–‡æ¡£å¤„ç†å™¨ï¼š

```python
# æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–ç³»ç»Ÿ
import os
import re
import json
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import hashlib
from pathlib import Path

class DocumentType(Enum):
    """æ–‡æ¡£ç±»å‹æšä¸¾"""
    TEXT = "text"
    PDF = "pdf"
    WORD = "word"
    HTML = "html"
    MARKDOWN = "markdown"
    JSON = "json"
    CSV = "csv"

@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—æ•°æ®ç»“æ„"""
    id: str
    content: str
    metadata: Dict[str, Any]
    chunk_index: int
    parent_doc_id: str
    embedding: Optional[np.ndarray] = None
    
    def __post_init__(self):
        # ç”Ÿæˆå†…å®¹å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
        if not self.id:
            content_hash = hashlib.md5(self.content.encode()).hexdigest()[:8]
            self.id = f"{self.parent_doc_id}_chunk_{self.chunk_index}_{content_hash}"

class DocumentProcessor:
    """é€šç”¨æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.supported_types = {
            '.txt': DocumentType.TEXT,
            '.md': DocumentType.MARKDOWN,
            '.pdf': DocumentType.PDF,
            '.docx': DocumentType.WORD,
            '.html': DocumentType.HTML,
            '.json': DocumentType.JSON,
            '.csv': DocumentType.CSV
        }
        
        # æ–‡æ¡£å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "total_docs": 0,
            "successful_docs": 0,
            "failed_docs": 0,
            "total_chunks": 0
        }
    
    def detect_document_type(self, file_path: str) -> DocumentType:
        """æ£€æµ‹æ–‡æ¡£ç±»å‹"""
        file_extension = Path(file_path).suffix.lower()
        return self.supported_types.get(file_extension, DocumentType.TEXT)
    
    def extract_text_from_file(self, file_path: str) -> str:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        doc_type = self.detect_document_type(file_path)
        
        try:
            if doc_type == DocumentType.TEXT:
                return self._extract_from_text(file_path)
            elif doc_type == DocumentType.MARKDOWN:
                return self._extract_from_markdown(file_path)
            elif doc_type == DocumentType.PDF:
                return self._extract_from_pdf(file_path)
            elif doc_type == DocumentType.HTML:
                return self._extract_from_html(file_path)
            elif doc_type == DocumentType.JSON:
                return self._extract_from_json(file_path)
            else:
                # é»˜è®¤æŒ‰æ–‡æœ¬å¤„ç†
                return self._extract_from_text(file_path)
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
            return ""
    
    def _extract_from_text(self, file_path: str) -> str:
        """æå–çº¯æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _extract_from_markdown(self, file_path: str) -> str:
        """æå–Markdownæ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç§»é™¤Markdownæ ‡è®°ï¼Œä¿ç•™çº¯æ–‡æœ¬
        # ç§»é™¤ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', '', content)
        # ç§»é™¤å†…è”ä»£ç 
        content = re.sub(r'`[^`]*`', '', content)
        # ç§»é™¤é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        content = re.sub(r'^#+\s*', '', content, flags=re.MULTILINE)
        # ç§»é™¤ç²—ä½“å’Œæ–œä½“æ ‡è®°
        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)
        content = re.sub(r'\*([^*]+)\*', r'\1', content)
        
        return content.strip()
    
    def _extract_from_pdf(self, file_path: str) -> str:
        """æå–PDFæ–‡ä»¶å†…å®¹ï¼ˆéœ€è¦PyPDF2åº“ï¼‰"""
        try:
            import PyPDF2
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []
                for page in pdf_reader.pages:
                    text_content.append(page.extract_text())
                return '\n'.join(text_content)
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£…PyPDF2åº“æ¥å¤„ç†PDFæ–‡ä»¶: pip install PyPDF2")
            return ""
    
    def _extract_from_html(self, file_path: str) -> str:
        """æå–HTMLæ–‡ä»¶å†…å®¹ï¼ˆéœ€è¦BeautifulSoupåº“ï¼‰"""
        try:
            from bs4 import BeautifulSoup
            with open(file_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
                # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()
                return soup.get_text()
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£…BeautifulSoup4åº“æ¥å¤„ç†HTMLæ–‡ä»¶: pip install beautifulsoup4")
            return ""
    
    def _extract_from_json(self, file_path: str) -> str:
        """æå–JSONæ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # é€’å½’æå–æ‰€æœ‰æ–‡æœ¬å€¼
        def extract_text_values(obj):
            if isinstance(obj, str):
                return [obj]
            elif isinstance(obj, dict):
                texts = []
                for value in obj.values():
                    texts.extend(extract_text_values(value))
                return texts
            elif isinstance(obj, list):
                texts = []
                for item in obj:
                    texts.extend(extract_text_values(item))
                return texts
            else:
                return [str(obj)]
        
        text_values = extract_text_values(data)
        return ' '.join(text_values)

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)]', '', text)
        
        # ç§»é™¤è¿‡çŸ­çš„è¡Œ
        lines = text.split('\n')
        cleaned_lines = [line.strip() for line in lines if len(line.strip()) > 10]
        
        return '\n'.join(cleaned_lines).strip()

# æ–‡æ¡£åˆ†å—ç­–ç•¥
class ChunkingStrategy:
    """æ–‡æ¡£åˆ†å—ç­–ç•¥åŸºç±»"""
    
    def chunk_document(self, text: str, metadata: Dict) -> List[DocumentChunk]:
        raise NotImplementedError

class FixedSizeChunking(ChunkingStrategy):
    """å›ºå®šå¤§å°åˆ†å—ç­–ç•¥"""
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_document(self, text: str, metadata: Dict) -> List[DocumentChunk]:
        """æŒ‰å›ºå®šå¤§å°åˆ†å—"""
        chunks = []
        doc_id = metadata.get('doc_id', 'unknown')
        
        # è®¡ç®—åˆ†å—ä½ç½®
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # å°è¯•åœ¨å•è¯è¾¹ç•Œå¤„åˆ†å‰²
            if end < len(text):
                # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„ç©ºæ ¼
                while end > start and text[end] != ' ':
                    end -= 1
                if end == start:  # å¦‚æœæ²¡æ‰¾åˆ°ç©ºæ ¼ï¼Œä½¿ç”¨åŸå§‹ä½ç½®
                    end = min(start + self.chunk_size, len(text))
            
            chunk_text = text[start:end].strip()
            
            if chunk_text:  # åªæ·»åŠ éç©ºåˆ†å—
                chunk = DocumentChunk(
                    id="",  # å°†åœ¨__post_init__ä¸­ç”Ÿæˆ
                    content=chunk_text,
                    metadata={
                        **metadata,
                        'chunk_method': 'fixed_size',
                        'chunk_size': len(chunk_text),
                        'start_pos': start,
                        'end_pos': end
                    },
                    chunk_index=chunk_index,
                    parent_doc_id=doc_id
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªåˆ†å—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(start + self.chunk_size - self.overlap, end)
        
        return chunks

class SemanticChunking(ChunkingStrategy):
    """è¯­ä¹‰åˆ†å—ç­–ç•¥"""
    
    def __init__(self, max_chunk_size: int = 1500):
        self.max_chunk_size = max_chunk_size
    
    def chunk_document(self, text: str, metadata: Dict) -> List[DocumentChunk]:
        """æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å—"""
        chunks = []
        doc_id = metadata.get('doc_id', 'unknown')
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šç°æœ‰åˆ†å—è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå…ˆä¿å­˜å½“å‰åˆ†å—
            if current_chunk and len(current_chunk) + len(paragraph) > self.max_chunk_size:
                chunk = DocumentChunk(
                    id="",
                    content=current_chunk.strip(),
                    metadata={
                        **metadata,
                        'chunk_method': 'semantic',
                        'chunk_size': len(current_chunk),
                        'paragraph_count': current_chunk.count('\n\n') + 1
                    },
                    chunk_index=chunk_index,
                    parent_doc_id=doc_id
                )
                chunks.append(chunk)
                chunk_index += 1
                current_chunk = ""
            
            # æ·»åŠ å½“å‰æ®µè½
            if current_chunk:
                current_chunk += "\n\n" + paragraph
            else:
                current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk.strip():
            chunk = DocumentChunk(
                id="",
                content=current_chunk.strip(),
                metadata={
                    **metadata,
                    'chunk_method': 'semantic',
                    'chunk_size': len(current_chunk),
                    'paragraph_count': current_chunk.count('\n\n') + 1
                },
                chunk_index=chunk_index,
                parent_doc_id=doc_id
            )
            chunks.append(chunk)
        
        return chunks

# æ–‡æ¡£å¤„ç†ç®¡é“æ¼”ç¤º
def demonstrate_document_processing():
    """æ¼”ç¤ºæ–‡æ¡£å¤„ç†æµç¨‹"""
    
    print("ğŸ“„ æ–‡æ¡£å¤„ç†ä¸åˆ†å—æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
    processor = DocumentProcessor()
    
    # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹
    sample_documents = {
        "tech_doc.md": """
# RAGæŠ€æœ¯è¯¦è§£

## ä»€ä¹ˆæ˜¯RAG

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„AIæŠ€æœ¯ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## RAGçš„ä¼˜åŠ¿

1. **å®æ—¶çŸ¥è¯†æ›´æ–°**ï¼šæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†åº“
2. **å‡å°‘å¹»è§‰**ï¼šåŸºäºçœŸå®æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
3. **å¯è¿½æº¯æ€§**ï¼šæä¾›ç­”æ¡ˆæ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦

## æŠ€æœ¯æ¶æ„

RAGç³»ç»Ÿé€šå¸¸åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š
- æ–‡æ¡£å¤„ç†å™¨
- å‘é‡æ•°æ®åº“
- æ£€ç´¢å™¨
- ç”Ÿæˆå™¨

è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œå®ç°é«˜è´¨é‡çš„é—®ç­”ç³»ç»Ÿã€‚
        """,
        
        "user_manual.txt": """
ç”¨æˆ·æ‰‹å†Œ

ç¬¬ä¸€ç« ï¼šç³»ç»Ÿä»‹ç»
æœ¬ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½é—®ç­”å¹³å°ã€‚ç”¨æˆ·å¯ä»¥ä¸Šä¼ æ–‡æ¡£ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å»ºç«‹çŸ¥è¯†åº“ï¼Œç„¶åå›ç­”ç›¸å…³é—®é¢˜ã€‚

ç¬¬äºŒç« ï¼šä½¿ç”¨æ–¹æ³•
1. ä¸Šä¼ æ–‡æ¡£åˆ°ç³»ç»Ÿ
2. ç­‰å¾…æ–‡æ¡£å¤„ç†å®Œæˆ
3. åœ¨é—®ç­”ç•Œé¢æå‡ºé—®é¢˜
4. ç³»ç»Ÿä¼šåŸºäºæ–‡æ¡£å†…å®¹ç»™å‡ºç­”æ¡ˆ

ç¬¬ä¸‰ç« ï¼šæ³¨æ„äº‹é¡¹
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
- æ–‡æ¡£å†…å®¹åº”è¯¥å‡†ç¡®å¯é 
- ç³»ç»Ÿä¼šä¿æŠ¤ç”¨æˆ·éšç§
        """
    }
    
    # å¤„ç†æ¯ä¸ªæ–‡æ¡£
    all_chunks = []
    
    for filename, content in sample_documents.items():
        print(f"\nğŸ“ å¤„ç†æ–‡æ¡£ï¼š{filename}")
        print(f"åŸå§‹é•¿åº¦ï¼š{len(content)} å­—ç¬¦")
        
        # æ¸…æ´—æ–‡æœ¬
        cleaned_content = processor.clean_text(content)
        print(f"æ¸…æ´—åé•¿åº¦ï¼š{len(cleaned_content)} å­—ç¬¦")
        
        # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®
        doc_metadata = {
            'doc_id': filename.replace('.', '_'),
            'filename': filename,
            'original_length': len(content),
            'cleaned_length': len(cleaned_content),
            'processing_time': datetime.now().isoformat()
        }
        
        # æµ‹è¯•ä¸åŒçš„åˆ†å—ç­–ç•¥
        print("\nğŸ”ª åˆ†å—ç­–ç•¥å¯¹æ¯”ï¼š")
        
        # å›ºå®šå¤§å°åˆ†å—
        fixed_chunker = FixedSizeChunking(chunk_size=300, overlap=50)
        fixed_chunks = fixed_chunker.chunk_document(cleaned_content, doc_metadata)
        print(f"  å›ºå®šå¤§å°åˆ†å—ï¼š{len(fixed_chunks)} ä¸ªåˆ†å—")
        
        # è¯­ä¹‰åˆ†å—
        semantic_chunker = SemanticChunking(max_chunk_size=400)
        semantic_chunks = semantic_chunker.chunk_document(cleaned_content, doc_metadata)
        print(f"  è¯­ä¹‰åˆ†å—ï¼š{len(semantic_chunks)} ä¸ªåˆ†å—")
        
        # æ˜¾ç¤ºåˆ†å—è¯¦æƒ…
        print("\nğŸ“Š åˆ†å—è¯¦æƒ…ï¼ˆè¯­ä¹‰åˆ†å—ï¼‰ï¼š")
        for i, chunk in enumerate(semantic_chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  åˆ†å— {i+1}:")
            print(f"    ID: {chunk.id}")
            print(f"    é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
            print(f"    å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")
            print(f"    å…ƒæ•°æ®: {chunk.metadata}")
        
        all_chunks.extend(semantic_chunks)
        
        # æ›´æ–°å¤„ç†ç»Ÿè®¡
        processor.processing_stats["total_docs"] += 1
        processor.processing_stats["successful_docs"] += 1
        processor.processing_stats["total_chunks"] += len(semantic_chunks)
    
    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    print(f"\nğŸ“ˆ å¤„ç†ç»Ÿè®¡ï¼š")
    print(f"  æ€»æ–‡æ¡£æ•°: {processor.processing_stats['total_docs']}")
    print(f"  æˆåŠŸå¤„ç†: {processor.processing_stats['successful_docs']}")
    print(f"  æ€»åˆ†å—æ•°: {processor.processing_stats['total_chunks']}")
    print(f"  å¹³å‡æ¯æ–‡æ¡£åˆ†å—æ•°: {processor.processing_stats['total_chunks'] / processor.processing_stats['total_docs']:.1f}")
    
    return all_chunks

# è¿è¡Œæ–‡æ¡£å¤„ç†æ¼”ç¤º
processed_chunks = demonstrate_document_processing()

print("\nâœ… æ–‡æ¡£å¤„ç†æ¼”ç¤ºå®Œæˆ")
```

### ğŸ§® å‘é‡åŒ–æŠ€æœ¯æ·±å…¥

æ–‡æ¡£åˆ†å—å®Œæˆåï¼Œä¸‹ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚è¿™æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼š

```python
# å‘é‡åŒ–æŠ€æœ¯å®ç°
from abc import ABC, abstractmethod
import numpy as np
from typing import List, Dict, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class EmbeddingModel(ABC):
    """åµŒå…¥æ¨¡å‹æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def encode(self, texts: List[str]) -> np.ndarray:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        pass
    
    @abstractmethod
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        pass

class TFIDFEmbedding(EmbeddingModel):
    """åŸºäºTF-IDFçš„åµŒå…¥æ¨¡å‹"""
    
    def __init__(self, max_features: int = 10000):
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.is_fitted = False
        self.dimension = max_features
    
    def fit(self, texts: List[str]):
        """è®­ç»ƒTF-IDFæ¨¡å‹"""
        self.vectorizer.fit(texts)
        self.is_fitted = True
        # æ›´æ–°å®é™…ç»´åº¦
        self.dimension = len(self.vectorizer.vocabulary_)
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºTF-IDFå‘é‡"""
        if not self.is_fitted:
            self.fit(texts)
        
        vectors = self.vectorizer.transform(texts)
        return vectors.toarray()
    
    def get_dimension(self) -> int:
        return self.dimension

class SimulatedTransformerEmbedding(EmbeddingModel):
    """æ¨¡æ‹ŸTransformeråµŒå…¥æ¨¡å‹ï¼ˆå¦‚BERTã€Sentence-BERTç­‰ï¼‰"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", dimension: int = 384):
        self.model_name = model_name
        self.dimension = dimension
        print(f"ğŸ¤– åˆå§‹åŒ–æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹: {model_name} (ç»´åº¦: {dimension})")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå¯†é›†å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„Transformeræ¨¡å‹
        # ä¾‹å¦‚ä½¿ç”¨sentence-transformersåº“
        
        embeddings = []
        for text in texts:
            # æ¨¡æ‹ŸåŸºäºæ–‡æœ¬å†…å®¹çš„å‘é‡ç”Ÿæˆ
            # å®é™…å®ç°ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„Transformeræ¨¡å‹
            np.random.seed(hash(text) % 2**32)  # åŸºäºæ–‡æœ¬å†…å®¹çš„ç¡®å®šæ€§éšæœº
            embedding = np.random.normal(0, 1, self.dimension)
            # å½’ä¸€åŒ–å‘é‡
            embedding = embedding / np.linalg.norm(embedding)
            embeddings.append(embedding)
        
        return np.array(embeddings)
    
    def get_dimension(self) -> int:
        return self.dimension

class VectorDatabase:
    """å‘é‡æ•°æ®åº“å®ç°"""
    
    def __init__(self, embedding_model: EmbeddingModel):
        self.embedding_model = embedding_model
        self.vectors: np.ndarray = None
        self.metadata: List[Dict] = []
        self.index_to_chunk_id: Dict[int, str] = {}
        self.chunk_id_to_index: Dict[str, int] = {}
        
    def add_documents(self, chunks: List[DocumentChunk]):
        """æ·»åŠ æ–‡æ¡£åˆ†å—åˆ°å‘é‡æ•°æ®åº“"""
        print(f"\nğŸ’¾ å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£åˆ†å—")
        print(f"å¾…æ·»åŠ åˆ†å—æ•°: {len(chunks)}")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [chunk.content for chunk in chunks]
        
        # ç”Ÿæˆå‘é‡
        print("ğŸ§® ç”Ÿæˆå‘é‡åµŒå…¥...")
        new_vectors = self.embedding_model.encode(texts)
        
        # å­˜å‚¨å‘é‡å’Œå…ƒæ•°æ®
        if self.vectors is None:
            self.vectors = new_vectors
        else:
            self.vectors = np.vstack([self.vectors, new_vectors])
        
        # æ›´æ–°ç´¢å¼•æ˜ å°„
        start_index = len(self.metadata)
        for i, chunk in enumerate(chunks):
            index = start_index + i
            self.index_to_chunk_id[index] = chunk.id
            self.chunk_id_to_index[chunk.id] = index
            
            # å­˜å‚¨å…ƒæ•°æ®
            chunk_metadata = {
                'chunk_id': chunk.id,
                'content': chunk.content,
                'parent_doc_id': chunk.parent_doc_id,
                'chunk_index': chunk.chunk_index,
                'metadata': chunk.metadata,
                'vector_index': index
            }
            self.metadata.append(chunk_metadata)
        
        print(f"âœ… å·²æ·»åŠ  {len(chunks)} ä¸ªå‘é‡")
        print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ€»å‘é‡æ•°: {len(self.vectors)}")
        print(f"  å‘é‡ç»´åº¦: {self.vectors.shape[1]}")
        print(f"  å­˜å‚¨å¤§å°: {self.vectors.nbytes / 1024 / 1024:.2f} MB")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """æœç´¢ç›¸ä¼¼å‘é‡"""
        if self.vectors is None or len(self.vectors) == 0:
            return []
        
        # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
        query_vector = self.embedding_model.encode([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity([query_vector], self.vectors)[0]
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for i, index in enumerate(top_indices):
            result = {
                'rank': i + 1,
                'chunk_id': self.index_to_chunk_id[index],
                'similarity': float(similarities[index]),
                'content': self.metadata[index]['content'],
                'metadata': self.metadata[index]['metadata'],
                'parent_doc_id': self.metadata[index]['parent_doc_id']
            }
            results.append(result)
        
        return results
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if self.vectors is None:
            return {"total_vectors": 0, "dimension": 0, "storage_mb": 0}
        
        return {
            "total_vectors": len(self.vectors),
            "dimension": self.vectors.shape[1],
            "storage_mb": self.vectors.nbytes / 1024 / 1024,
            "total_chunks": len(self.metadata),
            "unique_documents": len(set(meta['parent_doc_id'] for meta in self.metadata))
        }

# å‘é‡åŒ–æ¼”ç¤º
def demonstrate_vectorization():
    """æ¼”ç¤ºå‘é‡åŒ–è¿‡ç¨‹"""
    
    print("ğŸ§® å‘é‡åŒ–æŠ€æœ¯æ¼”ç¤º")
    print("=" * 50)
    
    # ä½¿ç”¨ä¹‹å‰å¤„ç†çš„æ–‡æ¡£åˆ†å—
    chunks = processed_chunks
    
    # æµ‹è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹
    embedding_models = {
        "TF-IDF": TFIDFEmbedding(max_features=1000),
        "Simulated-BERT": SimulatedTransformerEmbedding("sentence-bert", 384)
    }
    
    for model_name, embedding_model in embedding_models.items():
        print(f"\nğŸ¤– æµ‹è¯•åµŒå…¥æ¨¡å‹: {model_name}")
        print(f"å‘é‡ç»´åº¦: {embedding_model.get_dimension()}")
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        vector_db = VectorDatabase(embedding_model)
        
        # æ·»åŠ æ–‡æ¡£
        vector_db.add_documents(chunks)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = vector_db.get_statistics()
        print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•æœç´¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
            "å¦‚ä½•ä½¿ç”¨ç³»ç»Ÿï¼Ÿ",
            "æ–‡æ¡£å¤„ç†æ–¹æ³•"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æœç´¢æŸ¥è¯¢: {query}")
            results = vector_db.search(query, top_k=3)
            
            for result in results:
                print(f"  æ’å {result['rank']}: ç›¸ä¼¼åº¦ {result['similarity']:.3f}")
                print(f"    æ–‡æ¡£: {result['parent_doc_id']}")
                print(f"    å†…å®¹: {result['content'][:100]}...")
        
        print(f"\nâœ… {model_name} æ¨¡å‹æµ‹è¯•å®Œæˆ")

# è¿è¡Œå‘é‡åŒ–æ¼”ç¤º
demonstrate_vectorization()

print("\nğŸ‰ å‘é‡åŒ–æŠ€æœ¯æ¼”ç¤ºå®Œæˆ")
```

### ğŸ¯ å‘é‡åŒ–è´¨é‡è¯„ä¼°

ä¸ºäº†ç¡®ä¿å‘é‡åŒ–çš„è´¨é‡ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹è¯„ä¼°æœºåˆ¶ï¼š

```mermaid
graph TD
    subgraph "å‘é‡åŒ–è´¨é‡è¯„ä¼°ä½“ç³»"
        A[æ–‡æœ¬é¢„å¤„ç†è´¨é‡]
        B[å‘é‡è¡¨ç¤ºè´¨é‡]
        C[æ£€ç´¢ç›¸å…³æ€§]
        D[è®¡ç®—æ•ˆç‡]
        
        A --> A1[æ–‡æœ¬æ¸…æ´—å®Œæ•´æ€§]
        A --> A2[åˆ†å—è¾¹ç•Œåˆç†æ€§]
        A --> A3[å…ƒæ•°æ®å‡†ç¡®æ€§]
        
        B --> B1[å‘é‡ç»´åº¦é€‚å½“æ€§]
        B --> B2[è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ]
        B --> B3[å‘é‡åˆ†å¸ƒå‡åŒ€æ€§]
        
        C --> C1[æŸ¥è¯¢åŒ¹é…å‡†ç¡®ç‡]
        C --> C2[ç›¸å…³æ–‡æ¡£å¬å›ç‡]
        C --> C3[æ’åºè´¨é‡]
        
        D --> D1[å‘é‡åŒ–é€Ÿåº¦]
        D --> D2[å­˜å‚¨ç©ºé—´æ•ˆç‡]
        D --> D3[æ£€ç´¢å“åº”æ—¶é—´]
    end
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†RAGç³»ç»Ÿä¸­æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯ä¸ºé«˜è´¨é‡çš„ä¿¡æ¯æ£€ç´¢å¥ å®šäº†åšå®åŸºç¡€ã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†æ–‡æ¡£å¤„ç†ã€åˆ†å—ç­–ç•¥å’Œå‘é‡åŒ–æŠ€æœ¯ã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ¢è®¨å‘é‡æ•°æ®åº“æŠ€æœ¯ï¼Œäº†è§£å¦‚ä½•é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å¤§è§„æ¨¡å‘é‡æ•°æ®ã€‚*

---

## 29.3 å‘é‡æ•°æ®åº“æŠ€æœ¯

### ğŸ›ï¸ è¯­ä¹‰å­˜å‚¨ä»“åº“

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒä¸­ï¼Œ**è¯­ä¹‰å­˜å‚¨ä»“åº“**æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚å°±åƒä¸€åº§é«˜ç§‘æŠ€çš„ç«‹ä½“ä»“åº“ï¼Œå®ƒä¸ä»…è¦å­˜å‚¨æµ·é‡çš„å‘é‡æ•°æ®ï¼Œè¿˜è¦æ”¯æŒé«˜é€Ÿçš„ç›¸ä¼¼åº¦æ£€ç´¢å’Œå®æ—¶çš„æ•°æ®æ›´æ–°ã€‚

### ğŸ—„ï¸ å‘é‡æ•°æ®åº“æ ¸å¿ƒæ¦‚å¿µ

å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ä¸ºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®è€Œè®¾è®¡çš„æ•°æ®åº“ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“ä¸åŒï¼Œå®ƒä¼˜åŒ–äº†å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å’Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€‚

```python
# å‘é‡æ•°æ®åº“æ ¸å¿ƒæŠ€æœ¯å®ç°
import numpy as np
import json
import pickle
import sqlite3
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import threading
import time
from abc import ABC, abstractmethod
from enum import Enum

class IndexType(Enum):
    """ç´¢å¼•ç±»å‹æšä¸¾"""
    FLAT = "flat"           # æš´åŠ›æœç´¢
    IVF = "ivf"            # å€’æ’æ–‡ä»¶ç´¢å¼•
    HNSW = "hnsw"          # å±‚æ¬¡åŒ–å°ä¸–ç•Œå›¾
    LSH = "lsh"            # å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ
    ANNOY = "annoy"        # Annoyæ ‘ç´¢å¼•

class DistanceMetric(Enum):
    """è·ç¦»åº¦é‡æšä¸¾"""
    COSINE = "cosine"           # ä½™å¼¦ç›¸ä¼¼åº¦
    EUCLIDEAN = "euclidean"     # æ¬§å‡ é‡Œå¾—è·ç¦»
    DOT_PRODUCT = "dot_product" # ç‚¹ç§¯
    MANHATTAN = "manhattan"     # æ›¼å“ˆé¡¿è·ç¦»

@dataclass
class VectorRecord:
    """å‘é‡è®°å½•æ•°æ®ç»“æ„"""
    id: str
    vector: np.ndarray
    metadata: Dict[str, Any]
    timestamp: datetime
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'id': self.id,
            'vector': self.vector.tolist(),
            'metadata': self.metadata,
            'timestamp': self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'VectorRecord':
        """ä»å­—å…¸åˆ›å»ºè®°å½•"""
        return cls(
            id=data['id'],
            vector=np.array(data['vector']),
            metadata=data['metadata'],
            timestamp=datetime.fromisoformat(data['timestamp'])
        )

class VectorIndex(ABC):
    """å‘é‡ç´¢å¼•æŠ½è±¡åŸºç±»"""
    
    def __init__(self, dimension: int, metric: DistanceMetric = DistanceMetric.COSINE):
        self.dimension = dimension
        self.metric = metric
        self.is_trained = False
        
    @abstractmethod
    def add_vectors(self, vectors: np.ndarray, ids: List[str]):
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        pass
    
    @abstractmethod
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[List[str], List[float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„kä¸ªå‘é‡"""
        pass
    
    @abstractmethod
    def remove_vector(self, vector_id: str) -> bool:
        """ä»ç´¢å¼•ä¸­ç§»é™¤å‘é‡"""
        pass

class FlatIndex(VectorIndex):
    """æš´åŠ›æœç´¢ç´¢å¼•å®ç°"""
    
    def __init__(self, dimension: int, metric: DistanceMetric = DistanceMetric.COSINE):
        super().__init__(dimension, metric)
        self.vectors: np.ndarray = None
        self.ids: List[str] = []
        self.id_to_index: Dict[str, int] = {}
    
    def add_vectors(self, vectors: np.ndarray, ids: List[str]):
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        if vectors.shape[1] != self.dimension:
            raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.dimension}, å®é™… {vectors.shape[1]}")
        
        if self.vectors is None:
            self.vectors = vectors.copy()
        else:
            self.vectors = np.vstack([self.vectors, vectors])
        
        # æ›´æ–°IDæ˜ å°„
        start_index = len(self.ids)
        for i, vector_id in enumerate(ids):
            self.id_to_index[vector_id] = start_index + i
        
        self.ids.extend(ids)
        self.is_trained = True
    
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[List[str], List[float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„kä¸ªå‘é‡"""
        if not self.is_trained or self.vectors is None:
            return [], []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        if self.metric == DistanceMetric.COSINE:
            # ä½™å¼¦ç›¸ä¼¼åº¦
            query_norm = query_vector / np.linalg.norm(query_vector)
            vectors_norm = self.vectors / np.linalg.norm(self.vectors, axis=1, keepdims=True)
            similarities = np.dot(vectors_norm, query_norm)
            # è½¬æ¢ä¸ºè·ç¦»ï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            distances = 1 - similarities
        elif self.metric == DistanceMetric.EUCLIDEAN:
            # æ¬§å‡ é‡Œå¾—è·ç¦»
            distances = np.linalg.norm(self.vectors - query_vector, axis=1)
        else:
            raise NotImplementedError(f"è·ç¦»åº¦é‡ {self.metric} æš‚æœªå®ç°")
        
        # è·å–top-kç»“æœ
        k = min(k, len(self.ids))
        top_indices = np.argpartition(distances, k)[:k]
        top_indices = top_indices[np.argsort(distances[top_indices])]
        
        result_ids = [self.ids[i] for i in top_indices]
        result_distances = distances[top_indices].tolist()
        
        return result_ids, result_distances
    
    def remove_vector(self, vector_id: str) -> bool:
        """ä»ç´¢å¼•ä¸­ç§»é™¤å‘é‡"""
        if vector_id not in self.id_to_index:
            return False
        
        index = self.id_to_index[vector_id]
        
        # åˆ é™¤å‘é‡
        self.vectors = np.delete(self.vectors, index, axis=0)
        
        # æ›´æ–°IDåˆ—è¡¨å’Œæ˜ å°„
        del self.ids[index]
        del self.id_to_index[vector_id]
        
        # é‡æ–°æ„å»ºç´¢å¼•æ˜ å°„
        self.id_to_index = {id_: i for i, id_ in enumerate(self.ids)}
        
        return True

class HNSWIndex(VectorIndex):
    """HNSWç´¢å¼•å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, dimension: int, metric: DistanceMetric = DistanceMetric.COSINE, 
                 max_connections: int = 16, ef_construction: int = 200):
        super().__init__(dimension, metric)
        self.max_connections = max_connections
        self.ef_construction = ef_construction
        self.vectors: Dict[str, np.ndarray] = {}
        self.graph: Dict[str, List[str]] = {}
        self.entry_point: Optional[str] = None
        
    def add_vectors(self, vectors: np.ndarray, ids: List[str]):
        """æ·»åŠ å‘é‡åˆ°HNSWå›¾"""
        for vector, vector_id in zip(vectors, ids):
            self._add_single_vector(vector, vector_id)
        self.is_trained = True
    
    def _add_single_vector(self, vector: np.ndarray, vector_id: str):
        """æ·»åŠ å•ä¸ªå‘é‡åˆ°å›¾ä¸­"""
        self.vectors[vector_id] = vector
        self.graph[vector_id] = []
        
        if self.entry_point is None:
            self.entry_point = vector_id
            return
        
        # ç®€åŒ–çš„HNSWæ’å…¥é€»è¾‘
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰æ›´å¤æ‚çš„å±‚æ¬¡ç»“æ„
        candidates = self._search_layer(vector, self.ef_construction)
        
        # è¿æ¥åˆ°æœ€è¿‘çš„é‚»å±…
        connections = min(len(candidates), self.max_connections)
        for i in range(connections):
            neighbor_id = candidates[i][1]
            
            # åŒå‘è¿æ¥
            if neighbor_id not in self.graph[vector_id]:
                self.graph[vector_id].append(neighbor_id)
            if vector_id not in self.graph[neighbor_id]:
                self.graph[neighbor_id].append(vector_id)
            
            # ä¿®å‰ªè¿æ¥ï¼ˆä¿æŒåº¦æ•°é™åˆ¶ï¼‰
            if len(self.graph[neighbor_id]) > self.max_connections:
                self._prune_connections(neighbor_id)
    
    def _search_layer(self, query_vector: np.ndarray, ef: int) -> List[Tuple[float, str]]:
        """åœ¨å›¾å±‚ä¸­æœç´¢"""
        if not self.vectors:
            return []
        
        visited = set()
        candidates = []
        
        # ä»å…¥å£ç‚¹å¼€å§‹
        if self.entry_point:
            dist = self._calculate_distance(query_vector, self.vectors[self.entry_point])
            candidates.append((dist, self.entry_point))
            visited.add(self.entry_point)
        
        # è´ªå¿ƒæœç´¢
        for _ in range(ef):
            if not candidates:
                break
            
            candidates.sort()
            current_dist, current_id = candidates.pop(0)
            
            # æ£€æŸ¥é‚»å±…
            for neighbor_id in self.graph.get(current_id, []):
                if neighbor_id not in visited:
                    visited.add(neighbor_id)
                    dist = self._calculate_distance(query_vector, self.vectors[neighbor_id])
                    candidates.append((dist, neighbor_id))
        
        candidates.sort()
        return candidates
    
    def _calculate_distance(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»"""
        if self.metric == DistanceMetric.COSINE:
            return 1 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        elif self.metric == DistanceMetric.EUCLIDEAN:
            return np.linalg.norm(v1 - v2)
        else:
            raise NotImplementedError(f"è·ç¦»åº¦é‡ {self.metric} æš‚æœªå®ç°")
    
    def _prune_connections(self, vector_id: str):
        """ä¿®å‰ªè¿æ¥ä»¥ä¿æŒåº¦æ•°é™åˆ¶"""
        if len(self.graph[vector_id]) <= self.max_connections:
            return
        
        # ç®€åŒ–çš„ä¿®å‰ªç­–ç•¥ï¼šä¿ç•™è·ç¦»æœ€è¿‘çš„è¿æ¥
        vector = self.vectors[vector_id]
        connections = self.graph[vector_id]
        
        # è®¡ç®—åˆ°æ‰€æœ‰é‚»å±…çš„è·ç¦»
        distances = []
        for neighbor_id in connections:
            dist = self._calculate_distance(vector, self.vectors[neighbor_id])
            distances.append((dist, neighbor_id))
        
        # ä¿ç•™æœ€è¿‘çš„é‚»å±…
        distances.sort()
        new_connections = [neighbor_id for _, neighbor_id in distances[:self.max_connections]]
        self.graph[vector_id] = new_connections
    
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[List[str], List[float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„kä¸ªå‘é‡"""
        if not self.is_trained:
            return [], []
        
        candidates = self._search_layer(query_vector, max(self.ef_construction, k))
        
        # è¿”å›top-kç»“æœ
        k = min(k, len(candidates))
        top_candidates = candidates[:k]
        
        result_ids = [candidate[1] for candidate in top_candidates]
        result_distances = [candidate[0] for candidate in top_candidates]
        
        return result_ids, result_distances
    
    def remove_vector(self, vector_id: str) -> bool:
        """ä»ç´¢å¼•ä¸­ç§»é™¤å‘é‡"""
        if vector_id not in self.vectors:
            return False
        
        # ç§»é™¤æ‰€æœ‰è¿æ¥
        for neighbor_id in self.graph.get(vector_id, []):
            if vector_id in self.graph[neighbor_id]:
                self.graph[neighbor_id].remove(vector_id)
        
        # åˆ é™¤å‘é‡å’Œå›¾èŠ‚ç‚¹
        del self.vectors[vector_id]
        del self.graph[vector_id]
        
        # æ›´æ–°å…¥å£ç‚¹
        if self.entry_point == vector_id:
            self.entry_point = next(iter(self.vectors.keys())) if self.vectors else None
        
        return True

class AdvancedVectorDatabase:
    """é«˜çº§å‘é‡æ•°æ®åº“å®ç°"""
    
    def __init__(self, dimension: int, index_type: IndexType = IndexType.HNSW, 
                 metric: DistanceMetric = DistanceMetric.COSINE,
                 persist_path: Optional[str] = None):
        self.dimension = dimension
        self.index_type = index_type
        self.metric = metric
        self.persist_path = persist_path
        
        # åˆ›å»ºç´¢å¼•
        if index_type == IndexType.FLAT:
            self.index = FlatIndex(dimension, metric)
        elif index_type == IndexType.HNSW:
            self.index = HNSWIndex(dimension, metric)
        else:
            raise NotImplementedError(f"ç´¢å¼•ç±»å‹ {index_type} æš‚æœªå®ç°")
        
        # å…ƒæ•°æ®å­˜å‚¨
        self.metadata_store: Dict[str, Dict] = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_vectors": 0,
            "total_searches": 0,
            "total_inserts": 0,
            "total_deletes": 0,
            "avg_search_time": 0.0
        }
        
        # çº¿ç¨‹é”
        self._lock = threading.RLock()
        
        # å¦‚æœæŒ‡å®šäº†æŒä¹…åŒ–è·¯å¾„ï¼Œå°è¯•åŠ è½½
        if persist_path:
            self.load_from_disk()
    
    def insert_vectors(self, records: List[VectorRecord]) -> bool:
        """æ’å…¥å‘é‡è®°å½•"""
        with self._lock:
            try:
                vectors = np.array([record.vector for record in records])
                ids = [record.id for record in records]
                
                # æ·»åŠ åˆ°ç´¢å¼•
                self.index.add_vectors(vectors, ids)
                
                # å­˜å‚¨å…ƒæ•°æ®
                for record in records:
                    self.metadata_store[record.id] = {
                        'metadata': record.metadata,
                        'timestamp': record.timestamp.isoformat()
                    }
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats["total_vectors"] += len(records)
                self.stats["total_inserts"] += len(records)
                
                print(f"âœ… æˆåŠŸæ’å…¥ {len(records)} ä¸ªå‘é‡")
                return True
                
            except Exception as e:
                print(f"âŒ æ’å…¥å‘é‡æ—¶å‡ºé”™: {str(e)}")
                return False
    
    def search_vectors(self, query_vector: np.ndarray, k: int = 10, 
                      filter_metadata: Optional[Dict] = None) -> List[Dict]:
        """æœç´¢å‘é‡"""
        with self._lock:
            start_time = time.time()
            
            try:
                # æ‰§è¡Œå‘é‡æœç´¢
                result_ids, distances = self.index.search(query_vector, k * 2)  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
                
                # æ„å»ºç»“æœ
                results = []
                for vector_id, distance in zip(result_ids, distances):
                    if vector_id in self.metadata_store:
                        metadata = self.metadata_store[vector_id]['metadata']
                        
                        # åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
                        if filter_metadata:
                            if not self._match_filter(metadata, filter_metadata):
                                continue
                        
                        result = {
                            'id': vector_id,
                            'distance': distance,
                            'similarity': 1 - distance if self.metric == DistanceMetric.COSINE else distance,
                            'metadata': metadata,
                            'timestamp': self.metadata_store[vector_id]['timestamp']
                        }
                        results.append(result)
                        
                        if len(results) >= k:
                            break
                
                # æ›´æ–°ç»Ÿè®¡
                search_time = time.time() - start_time
                self.stats["total_searches"] += 1
                self.stats["avg_search_time"] = (
                    (self.stats["avg_search_time"] * (self.stats["total_searches"] - 1) + search_time) 
                    / self.stats["total_searches"]
                )
                
                return results
                
            except Exception as e:
                print(f"âŒ æœç´¢å‘é‡æ—¶å‡ºé”™: {str(e)}")
                return []
    
    def _match_filter(self, metadata: Dict, filter_metadata: Dict) -> bool:
        """æ£€æŸ¥å…ƒæ•°æ®æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
        for key, value in filter_metadata.items():
            if key not in metadata:
                return False
            
            if isinstance(value, list):
                if metadata[key] not in value:
                    return False
            elif metadata[key] != value:
                return False
        
        return True
    
    def delete_vector(self, vector_id: str) -> bool:
        """åˆ é™¤å‘é‡"""
        with self._lock:
            try:
                # ä»ç´¢å¼•ä¸­åˆ é™¤
                if self.index.remove_vector(vector_id):
                    # åˆ é™¤å…ƒæ•°æ®
                    if vector_id in self.metadata_store:
                        del self.metadata_store[vector_id]
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats["total_vectors"] -= 1
                    self.stats["total_deletes"] += 1
                    
                    print(f"âœ… æˆåŠŸåˆ é™¤å‘é‡: {vector_id}")
                    return True
                else:
                    print(f"âš ï¸ å‘é‡ä¸å­˜åœ¨: {vector_id}")
                    return False
                    
            except Exception as e:
                print(f"âŒ åˆ é™¤å‘é‡æ—¶å‡ºé”™: {str(e)}")
                return False
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                **self.stats,
                "index_type": self.index_type.value,
                "metric": self.metric.value,
                "dimension": self.dimension
            }
    
    def save_to_disk(self) -> bool:
        """ä¿å­˜åˆ°ç£ç›˜"""
        if not self.persist_path:
            return False
        
        with self._lock:
            try:
                # å‡†å¤‡ä¿å­˜æ•°æ®
                save_data = {
                    'dimension': self.dimension,
                    'index_type': self.index_type.value,
                    'metric': self.metric.value,
                    'metadata_store': self.metadata_store,
                    'stats': self.stats,
                    'vectors': {},
                    'index_data': {}
                }
                
                # ä¿å­˜å‘é‡æ•°æ®
                if hasattr(self.index, 'vectors') and self.index.vectors is not None:
                    if isinstance(self.index.vectors, np.ndarray):
                        save_data['vectors'] = {
                            'data': self.index.vectors.tolist(),
                            'ids': self.index.ids
                        }
                    elif isinstance(self.index.vectors, dict):
                        save_data['vectors'] = {
                            id_: vector.tolist() for id_, vector in self.index.vectors.items()
                        }
                
                # ä¿å­˜ç´¢å¼•ç‰¹å®šæ•°æ®
                if hasattr(self.index, 'graph'):
                    save_data['index_data']['graph'] = self.index.graph
                    save_data['index_data']['entry_point'] = self.index.entry_point
                
                # å†™å…¥æ–‡ä»¶
                with open(self.persist_path, 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… æ•°æ®åº“å·²ä¿å­˜åˆ°: {self.persist_path}")
                return True
                
            except Exception as e:
                print(f"âŒ ä¿å­˜æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
                return False
    
    def load_from_disk(self) -> bool:
        """ä»ç£ç›˜åŠ è½½"""
        if not self.persist_path:
            return False
        
        try:
            with open(self.persist_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ¢å¤å…ƒæ•°æ®
            self.metadata_store = data.get('metadata_store', {})
            self.stats = data.get('stats', self.stats)
            
            # æ¢å¤å‘é‡æ•°æ®
            vectors_data = data.get('vectors', {})
            if vectors_data:
                if 'data' in vectors_data and 'ids' in vectors_data:
                    # Flatç´¢å¼•æ ¼å¼
                    vectors = np.array(vectors_data['data'])
                    ids = vectors_data['ids']
                    if len(vectors) > 0:
                        self.index.add_vectors(vectors, ids)
                else:
                    # HNSWç´¢å¼•æ ¼å¼
                    for vector_id, vector_data in vectors_data.items():
                        if isinstance(vector_data, list):
                            vector = np.array(vector_data)
                            self.index.add_vectors(vector.reshape(1, -1), [vector_id])
            
            # æ¢å¤ç´¢å¼•ç‰¹å®šæ•°æ®
            index_data = data.get('index_data', {})
            if hasattr(self.index, 'graph') and 'graph' in index_data:
                self.index.graph = index_data['graph']
                self.index.entry_point = index_data.get('entry_point')
            
            print(f"âœ… æ•°æ®åº“å·²ä»ç£ç›˜åŠ è½½: {self.persist_path}")
            return True
            
        except FileNotFoundError:
            print(f"âš ï¸ æŒä¹…åŒ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“: {self.persist_path}")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
            return False

# å‘é‡æ•°æ®åº“æ¼”ç¤º
def demonstrate_vector_database():
    """æ¼”ç¤ºå‘é‡æ•°æ®åº“åŠŸèƒ½"""
    
    print("ğŸ—„ï¸ å‘é‡æ•°æ®åº“æŠ€æœ¯æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“å®ä¾‹
    databases = {
        "Flatç´¢å¼•": AdvancedVectorDatabase(
            dimension=384, 
            index_type=IndexType.FLAT,
            persist_path="vector_db_flat.json"
        ),
        "HNSWç´¢å¼•": AdvancedVectorDatabase(
            dimension=384, 
            index_type=IndexType.HNSW,
            persist_path="vector_db_hnsw.json"
        )
    }
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_records = []
    categories = ["æŠ€æœ¯æ–‡æ¡£", "ç”¨æˆ·æ‰‹å†Œ", "APIæ–‡æ¡£", "æ•™ç¨‹", "FAQ"]
    
    for i in range(50):
        # ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡
        np.random.seed(i)
        vector = np.random.normal(0, 1, 384)
        vector = vector / np.linalg.norm(vector)  # å½’ä¸€åŒ–
        
        record = VectorRecord(
            id=f"doc_{i:03d}",
            vector=vector,
            metadata={
                "title": f"æ–‡æ¡£_{i:03d}",
                "category": categories[i % len(categories)],
                "length": np.random.randint(100, 2000),
                "author": f"ä½œè€…_{i % 5}",
                "tags": [f"tag_{j}" for j in range(i % 3 + 1)]
            },
            timestamp=datetime.now()
        )
        test_records.append(record)
    
    # æµ‹è¯•æ¯ä¸ªæ•°æ®åº“
    for db_name, db in databases.items():
        print(f"\nğŸ” æµ‹è¯• {db_name}")
        print("-" * 40)
        
        # æ’å…¥æ•°æ®
        print("ğŸ“¥ æ’å…¥æµ‹è¯•æ•°æ®...")
        start_time = time.time()
        success = db.insert_vectors(test_records)
        insert_time = time.time() - start_time
        
        if success:
            print(f"âœ… æ’å…¥å®Œæˆï¼Œè€—æ—¶: {insert_time:.3f}ç§’")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = db.get_statistics()
        print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•æœç´¢
        print("\nğŸ” æµ‹è¯•å‘é‡æœç´¢...")
        
        # åˆ›å»ºæŸ¥è¯¢å‘é‡
        np.random.seed(100)
        query_vector = np.random.normal(0, 1, 384)
        query_vector = query_vector / np.linalg.norm(query_vector)
        
        # æ‰§è¡Œæœç´¢
        start_time = time.time()
        results = db.search_vectors(query_vector, k=5)
        search_time = time.time() - start_time
        
        print(f"â±ï¸ æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
        print(f"ğŸ“‹ æœç´¢ç»“æœ:")
        
        for i, result in enumerate(results):
            print(f"  {i+1}. ID: {result['id']}")
            print(f"     ç›¸ä¼¼åº¦: {result['similarity']:.4f}")
            print(f"     æ ‡é¢˜: {result['metadata']['title']}")
            print(f"     ç±»åˆ«: {result['metadata']['category']}")
        
        # æµ‹è¯•å¸¦è¿‡æ»¤çš„æœç´¢
        print("\nğŸ¯ æµ‹è¯•å…ƒæ•°æ®è¿‡æ»¤æœç´¢...")
        filtered_results = db.search_vectors(
            query_vector, 
            k=3, 
            filter_metadata={"category": "æŠ€æœ¯æ–‡æ¡£"}
        )
        
        print(f"ğŸ“‹ è¿‡æ»¤ç»“æœ (åªæ˜¾ç¤ºæŠ€æœ¯æ–‡æ¡£):")
        for i, result in enumerate(filtered_results):
            print(f"  {i+1}. {result['metadata']['title']} - {result['metadata']['category']}")
        
        # æµ‹è¯•åˆ é™¤
        print("\nğŸ—‘ï¸ æµ‹è¯•å‘é‡åˆ é™¤...")
        delete_success = db.delete_vector("doc_000")
        if delete_success:
            print("âœ… åˆ é™¤æˆåŠŸ")
            
            # éªŒè¯åˆ é™¤
            verify_results = db.search_vectors(test_records[0].vector, k=5)
            found_deleted = any(r['id'] == 'doc_000' for r in verify_results)
            print(f"ğŸ” åˆ é™¤éªŒè¯: {'âŒ ä»ç„¶å­˜åœ¨' if found_deleted else 'âœ… å·²åˆ é™¤'}")
        
        # ä¿å­˜åˆ°ç£ç›˜
        print("\nğŸ’¾ æµ‹è¯•æŒä¹…åŒ–...")
        save_success = db.save_to_disk()
        if save_success:
            print("âœ… ä¿å­˜æˆåŠŸ")
    
    print(f"\nğŸ‰ å‘é‡æ•°æ®åº“æ¼”ç¤ºå®Œæˆ!")

# è¿è¡Œå‘é‡æ•°æ®åº“æ¼”ç¤º
demonstrate_vector_database()

print("\nâœ… å‘é‡æ•°æ®åº“æŠ€æœ¯æ¼”ç¤ºå®Œæˆ")
```

### ğŸ—ï¸ å‘é‡æ•°æ®åº“æ¶æ„å¯¹æ¯”

ä¸åŒçš„å‘é‡æ•°æ®åº“æœ‰å„è‡ªçš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ï¼š

```mermaid
graph TD
    subgraph "å‘é‡æ•°æ®åº“æŠ€æœ¯å¯¹æ¯”"
        subgraph "ç´¢å¼•ç±»å‹"
            A1[Flat Index<br/>æš´åŠ›æœç´¢]
            A2[IVF Index<br/>å€’æ’æ–‡ä»¶]
            A3[HNSW Index<br/>å±‚æ¬¡å°ä¸–ç•Œå›¾]
            A4[LSH Index<br/>å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ]
        end
        
        subgraph "æ€§èƒ½ç‰¹å¾"
            B1[ç²¾ç¡®åº¦: 100%<br/>é€Ÿåº¦: æ…¢<br/>å†…å­˜: é«˜]
            B2[ç²¾ç¡®åº¦: 95%<br/>é€Ÿåº¦: ä¸­<br/>å†…å­˜: ä¸­]
            B3[ç²¾ç¡®åº¦: 98%<br/>é€Ÿåº¦: å¿«<br/>å†…å­˜: ä¸­]
            B4[ç²¾ç¡®åº¦: 90%<br/>é€Ÿåº¦: æå¿«<br/>å†…å­˜: ä½]
        end
        
        subgraph "é€‚ç”¨åœºæ™¯"
            C1[å°è§„æ¨¡æ•°æ®<br/>é«˜ç²¾åº¦è¦æ±‚]
            C2[ä¸­ç­‰è§„æ¨¡<br/>å¹³è¡¡éœ€æ±‚]
            C3[å¤§è§„æ¨¡æ•°æ®<br/>å®æ—¶æ£€ç´¢]
            C4[è¶…å¤§è§„æ¨¡<br/>è¿‘ä¼¼æ£€ç´¢]
        end
        
        A1 --> B1 --> C1
        A2 --> B2 --> C2
        A3 --> B3 --> C3
        A4 --> B4 --> C4
    end
```

### ğŸ”§ å‘é‡æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥

ä¸ºäº†æå‡å‘é‡æ•°æ®åº“çš„æ€§èƒ½ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å¤šä¸ªç»´åº¦çš„ä¼˜åŒ–ï¼š

```python
class VectorDatabaseOptimizer:
    """å‘é‡æ•°æ®åº“ä¼˜åŒ–å™¨"""
    
    def __init__(self, database: AdvancedVectorDatabase):
        self.database = database
        self.optimization_history = []
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åº“æ€§èƒ½"""
        stats = self.database.get_statistics()
        
        analysis = {
            "performance_score": 0,
            "bottlenecks": [],
            "recommendations": []
        }
        
        # åˆ†ææœç´¢æ€§èƒ½
        avg_search_time = stats.get("avg_search_time", 0)
        if avg_search_time > 0.1:  # 100ms
            analysis["bottlenecks"].append("æœç´¢å»¶è¿Ÿè¿‡é«˜")
            analysis["recommendations"].append("è€ƒè™‘ä½¿ç”¨HNSWç´¢å¼•æˆ–å¢åŠ ç´¢å¼•å‚æ•°")
        
        # åˆ†æå‘é‡è§„æ¨¡
        total_vectors = stats.get("total_vectors", 0)
        if total_vectors > 100000 and self.database.index_type == IndexType.FLAT:
            analysis["bottlenecks"].append("å¤§è§„æ¨¡æ•°æ®ä½¿ç”¨æš´åŠ›æœç´¢")
            analysis["recommendations"].append("å‡çº§åˆ°è¿‘ä¼¼æœ€è¿‘é‚»ç´¢å¼•(HNSW/IVF)")
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        estimated_memory = total_vectors * self.database.dimension * 4 / (1024**2)  # MB
        if estimated_memory > 1000:  # 1GB
            analysis["bottlenecks"].append("å†…å­˜ä½¿ç”¨è¿‡é«˜")
            analysis["recommendations"].append("è€ƒè™‘é‡åŒ–å‹ç¼©æˆ–åˆ†å¸ƒå¼å­˜å‚¨")
        
        # è®¡ç®—ç»¼åˆæ€§èƒ½è¯„åˆ†
        base_score = 100
        if avg_search_time > 0.05:
            base_score -= 20
        if total_vectors > 50000 and self.database.index_type == IndexType.FLAT:
            base_score -= 30
        if estimated_memory > 500:
            base_score -= 15
        
        analysis["performance_score"] = max(0, base_score)
        
        return analysis
    
    def optimize_index_parameters(self) -> Dict[str, Any]:
        """ä¼˜åŒ–ç´¢å¼•å‚æ•°"""
        if self.database.index_type == IndexType.HNSW:
            return self._optimize_hnsw_parameters()
        elif self.database.index_type == IndexType.FLAT:
            return self._suggest_index_upgrade()
        else:
            return {"message": "å½“å‰ç´¢å¼•ç±»å‹æš‚ä¸æ”¯æŒå‚æ•°ä¼˜åŒ–"}
    
    def _optimize_hnsw_parameters(self) -> Dict[str, Any]:
        """ä¼˜åŒ–HNSWå‚æ•°"""
        stats = self.database.get_statistics()
        total_vectors = stats.get("total_vectors", 0)
        
        recommendations = {
            "current_params": {
                "max_connections": getattr(self.database.index, 'max_connections', 16),
                "ef_construction": getattr(self.database.index, 'ef_construction', 200)
            },
            "recommended_params": {},
            "reasoning": []
        }
        
        # æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´å‚æ•°
        if total_vectors < 10000:
            recommendations["recommended_params"] = {
                "max_connections": 16,
                "ef_construction": 200
            }
            recommendations["reasoning"].append("å°è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°å³å¯")
        elif total_vectors < 100000:
            recommendations["recommended_params"] = {
                "max_connections": 32,
                "ef_construction": 400
            }
            recommendations["reasoning"].append("ä¸­ç­‰è§„æ¨¡æ•°æ®ï¼Œå¢åŠ è¿æ¥æ•°å’Œæ„å»ºå‚æ•°")
        else:
            recommendations["recommended_params"] = {
                "max_connections": 64,
                "ef_construction": 800
            }
            recommendations["reasoning"].append("å¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨é«˜æ€§èƒ½å‚æ•°")
        
        return recommendations
    
    def _suggest_index_upgrade(self) -> Dict[str, Any]:
        """å»ºè®®ç´¢å¼•å‡çº§"""
        stats = self.database.get_statistics()
        total_vectors = stats.get("total_vectors", 0)
        
        if total_vectors > 10000:
            return {
                "suggestion": "å‡çº§åˆ°HNSWç´¢å¼•",
                "reason": f"å½“å‰ {total_vectors} ä¸ªå‘é‡ï¼ŒHNSWç´¢å¼•å¯æ˜¾è‘—æå‡æœç´¢é€Ÿåº¦",
                "expected_improvement": "æœç´¢é€Ÿåº¦æå‡10-100å€"
            }
        else:
            return {
                "suggestion": "ä¿æŒå½“å‰ç´¢å¼•",
                "reason": "æ•°æ®è§„æ¨¡è¾ƒå°ï¼Œæš´åŠ›æœç´¢å·²è¶³å¤Ÿ"
            }
    
    def benchmark_search_performance(self, num_queries: int = 100) -> Dict[str, float]:
        """åŸºå‡†æµ‹è¯•æœç´¢æ€§èƒ½"""
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_queries} æ¬¡æŸ¥è¯¢)")
        
        # ç”ŸæˆéšæœºæŸ¥è¯¢å‘é‡
        query_vectors = []
        for i in range(num_queries):
            np.random.seed(i + 1000)
            vector = np.random.normal(0, 1, self.database.dimension)
            vector = vector / np.linalg.norm(vector)
            query_vectors.append(vector)
        
        # æµ‹è¯•ä¸åŒkå€¼çš„æ€§èƒ½
        k_values = [1, 5, 10, 20]
        results = {}
        
        for k in k_values:
            times = []
            for query_vector in query_vectors:
                start_time = time.time()
                self.database.search_vectors(query_vector, k=k)
                search_time = time.time() - start_time
                times.append(search_time)
            
            avg_time = np.mean(times)
            std_time = np.std(times)
            
            results[f"k={k}"] = {
                "avg_time": avg_time,
                "std_time": std_time,
                "qps": 1.0 / avg_time if avg_time > 0 else 0
            }
            
            print(f"  k={k}: å¹³å‡ {avg_time*1000:.2f}ms, QPS: {1.0/avg_time:.1f}")
        
        return results

# æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º
def demonstrate_optimization():
    """æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–"""
    
    print("âš¡ å‘é‡æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    db = AdvancedVectorDatabase(
        dimension=256, 
        index_type=IndexType.HNSW
    )
    
    # æ’å…¥æµ‹è¯•æ•°æ®
    print("ğŸ“¥ æ’å…¥æµ‹è¯•æ•°æ®...")
    test_records = []
    for i in range(1000):
        np.random.seed(i)
        vector = np.random.normal(0, 1, 256)
        vector = vector / np.linalg.norm(vector)
        
        record = VectorRecord(
            id=f"test_{i:04d}",
            vector=vector,
            metadata={"category": f"cat_{i%10}", "value": i},
            timestamp=datetime.now()
        )
        test_records.append(record)
    
    db.insert_vectors(test_records)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = VectorDatabaseOptimizer(db)
    
    # æ€§èƒ½åˆ†æ
    print("\nğŸ“Š æ€§èƒ½åˆ†æ:")
    analysis = optimizer.analyze_performance()
    print(f"æ€§èƒ½è¯„åˆ†: {analysis['performance_score']}/100")
    
    if analysis['bottlenecks']:
        print("âš ï¸ å‘ç°çš„ç“¶é¢ˆ:")
        for bottleneck in analysis['bottlenecks']:
            print(f"  - {bottleneck}")
    
    if analysis['recommendations']:
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for recommendation in analysis['recommendations']:
            print(f"  - {recommendation}")
    
    # å‚æ•°ä¼˜åŒ–å»ºè®®
    print("\nğŸ”§ ç´¢å¼•å‚æ•°ä¼˜åŒ–:")
    param_optimization = optimizer.optimize_index_parameters()
    if 'recommended_params' in param_optimization:
        print(f"å½“å‰å‚æ•°: {param_optimization['current_params']}")
        print(f"æ¨èå‚æ•°: {param_optimization['recommended_params']}")
        for reason in param_optimization['reasoning']:
            print(f"  - {reason}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\nğŸƒ æ€§èƒ½åŸºå‡†æµ‹è¯•:")
    benchmark_results = optimizer.benchmark_search_performance(50)
    
    print("âœ… ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ")

# è¿è¡Œä¼˜åŒ–æ¼”ç¤º
demonstrate_optimization()

print("\nğŸ‰ å‘é‡æ•°æ®åº“æŠ€æœ¯å®Œæ•´æ¼”ç¤ºç»“æŸ")
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸åŒç´¢å¼•ç±»å‹çš„å®ç°ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œå®é™…åº”ç”¨åœºæ™¯ã€‚è¿™ä¸ºæ„å»ºé«˜æ€§èƒ½çš„RAGç³»ç»Ÿå¥ å®šäº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæŠ€æœ¯å’Œä¼˜åŒ–ç­–ç•¥ã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ¢è®¨æ£€ç´¢ç­–ç•¥ä¼˜åŒ–ï¼Œäº†è§£å¦‚ä½•æå‡æ£€ç´¢ç²¾åº¦å’Œæ•ˆç‡ã€‚*

---

## 29.4 æ£€ç´¢ç­–ç•¥ä¼˜åŒ–

### ğŸ¯ æ™ºèƒ½æ£€ç´¢è°ƒåº¦ä¸­å¿ƒ

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒä¸­ï¼Œ**æ™ºèƒ½æ£€ç´¢è°ƒåº¦ä¸­å¿ƒ**è´Ÿè´£æ ¹æ®æŸ¥è¯¢ç‰¹å¾å’Œä¸šåŠ¡éœ€æ±‚ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„æ£€ç´¢ç­–ç•¥ã€‚å°±åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„å›¾ä¹¦ç®¡ç†å‘˜ï¼Œå®ƒçŸ¥é“å¦‚ä½•å¿«é€Ÿæ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

### ğŸ” æ£€ç´¢ç­–ç•¥æ ¸å¿ƒæ¦‚å¿µ

æ£€ç´¢ç­–ç•¥ä¼˜åŒ–æ˜¯RAGç³»ç»Ÿæ€§èƒ½çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ç†è§£ã€æ£€ç´¢æ–¹æ³•é€‰æ‹©ã€ç»“æœæ’åºå’Œåå¤„ç†ç­‰å¤šä¸ªç¯èŠ‚ã€‚

```python
# æ£€ç´¢ç­–ç•¥ä¼˜åŒ–å®ç°
import numpy as np
import re
import json
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
from enum import Enum
import time
from collections import defaultdict, Counter
import math

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹æšä¸¾"""
    FACTUAL = "factual"           # äº‹å®æ€§æŸ¥è¯¢
    ANALYTICAL = "analytical"     # åˆ†ææ€§æŸ¥è¯¢
    PROCEDURAL = "procedural"     # ç¨‹åºæ€§æŸ¥è¯¢
    CREATIVE = "creative"         # åˆ›é€ æ€§æŸ¥è¯¢
    COMPARATIVE = "comparative"   # æ¯”è¾ƒæ€§æŸ¥è¯¢

class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾"""
    SEMANTIC = "semantic"         # è¯­ä¹‰æ£€ç´¢
    KEYWORD = "keyword"           # å…³é”®è¯æ£€ç´¢
    HYBRID = "hybrid"            # æ··åˆæ£€ç´¢
    HIERARCHICAL = "hierarchical" # å±‚æ¬¡æ£€ç´¢
    MULTI_QUERY = "multi_query"   # å¤šæŸ¥è¯¢æ£€ç´¢

@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    original_query: str
    query_type: QueryType
    keywords: List[str]
    entities: List[str]
    intent: str
    complexity_score: float
    domain: str
    
@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    document_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    retrieval_method: str
    
class QueryAnalyzer:
    """æŸ¥è¯¢åˆ†æå™¨"""
    
    def __init__(self):
        # é¢„å®šä¹‰çš„æŸ¥è¯¢æ¨¡å¼
        self.query_patterns = {
            QueryType.FACTUAL: [
                r'\b(what|who|when|where|which)\b',
                r'\b(define|definition|meaning)\b',
                r'\b(is|are|was|were)\b.*\?'
            ],
            QueryType.ANALYTICAL: [
                r'\b(why|how|analyze|explain|compare)\b',
                r'\b(reason|cause|effect|impact)\b',
                r'\b(relationship|correlation)\b'
            ],
            QueryType.PROCEDURAL: [
                r'\b(how to|step|process|procedure)\b',
                r'\b(install|configure|setup|create)\b',
                r'\b(tutorial|guide|instruction)\b'
            ],
            QueryType.CREATIVE: [
                r'\b(generate|create|design|build)\b',
                r'\b(idea|suggestion|recommendation)\b',
                r'\b(brainstorm|innovate)\b'
            ],
            QueryType.COMPARATIVE: [
                r'\b(compare|versus|vs|difference)\b',
                r'\b(better|worse|best|worst)\b',
                r'\b(advantage|disadvantage|pros|cons)\b'
            ]
        }
        
        # å…³é”®è¯æå–æ¨¡å¼
        self.keyword_patterns = [
            r'\b[A-Z][a-z]+(?:\s[A-Z][a-z]+)*\b',  # ä¸“æœ‰åè¯
            r'\b\w{4,}\b',  # é•¿å•è¯
            r'\b(?:API|HTTP|JSON|XML|SQL|AI|ML|DL)\b'  # æŠ€æœ¯æœ¯è¯­
        ]
    
    def analyze_query(self, query: str) -> QueryAnalysis:
        """åˆ†ææŸ¥è¯¢æ„å›¾å’Œç‰¹å¾"""
        
        # æŸ¥è¯¢ç±»å‹è¯†åˆ«
        query_type = self._identify_query_type(query)
        
        # å…³é”®è¯æå–
        keywords = self._extract_keywords(query)
        
        # å®ä½“è¯†åˆ«ï¼ˆç®€åŒ–ç‰ˆï¼‰
        entities = self._extract_entities(query)
        
        # æ„å›¾åˆ†æ
        intent = self._analyze_intent(query, query_type)
        
        # å¤æ‚åº¦è¯„åˆ†
        complexity_score = self._calculate_complexity(query)
        
        # é¢†åŸŸè¯†åˆ«
        domain = self._identify_domain(query, keywords)
        
        return QueryAnalysis(
            original_query=query,
            query_type=query_type,
            keywords=keywords,
            entities=entities,
            intent=intent,
            complexity_score=complexity_score,
            domain=domain
        )
    
    def _identify_query_type(self, query: str) -> QueryType:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        type_scores = {}
        
        for query_type, patterns in self.query_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, query_lower))
                score += matches
            type_scores[query_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹ï¼Œé»˜è®¤ä¸ºäº‹å®æ€§æŸ¥è¯¢
        if not type_scores or max(type_scores.values()) == 0:
            return QueryType.FACTUAL
        
        return max(type_scores, key=type_scores.get)
    
    def _extract_keywords(self, query: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = set()
        
        for pattern in self.keyword_patterns:
            matches = re.findall(pattern, query)
            keywords.update(matches)
        
        # è¿‡æ»¤åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stop_words = {'the', 'is', 'at', 'which', 'on', 'and', 'or', 'but', 'in', 'with', 'a', 'an'}
        keywords = [kw for kw in keywords if kw.lower() not in stop_words]
        
        return sorted(list(keywords))
    
    def _extract_entities(self, query: str) -> List[str]:
        """æå–å‘½åå®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€åŒ–çš„å®ä½“è¯†åˆ«ï¼Œä¸»è¦è¯†åˆ«å¤§å†™å¼€å¤´çš„è¯ç»„
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s[A-Z][a-z]+)*\b', query)
        return list(set(entities))
    
    def _analyze_intent(self, query: str, query_type: QueryType) -> str:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        intent_mapping = {
            QueryType.FACTUAL: "è·å–äº‹å®ä¿¡æ¯",
            QueryType.ANALYTICAL: "æ·±åº¦åˆ†æç†è§£",
            QueryType.PROCEDURAL: "è·å–æ“ä½œæŒ‡å¯¼",
            QueryType.CREATIVE: "ç”Ÿæˆåˆ›æ–°å†…å®¹",
            QueryType.COMPARATIVE: "æ¯”è¾ƒåˆ†æé€‰æ‹©"
        }
        return intent_mapping.get(query_type, "ä¿¡æ¯æ£€ç´¢")
    
    def _calculate_complexity(self, query: str) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤æ‚åº¦"""
        factors = {
            'length': len(query.split()) / 20.0,  # é•¿åº¦å› å­
            'questions': query.count('?') * 0.2,   # é—®é¢˜æ•°é‡
            'conjunctions': len(re.findall(r'\b(and|or|but|however|although)\b', query.lower())) * 0.3,
            'technical_terms': len(re.findall(r'\b(?:API|HTTP|JSON|XML|SQL|AI|ML|DL)\b', query)) * 0.4
        }
        
        complexity = sum(factors.values())
        return min(complexity, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…
    
    def _identify_domain(self, query: str, keywords: List[str]) -> str:
        """è¯†åˆ«æŸ¥è¯¢é¢†åŸŸ"""
        domain_keywords = {
            'technology': ['API', 'HTTP', 'JSON', 'XML', 'SQL', 'database', 'server', 'code'],
            'business': ['market', 'sales', 'revenue', 'customer', 'business', 'strategy'],
            'science': ['research', 'study', 'experiment', 'data', 'analysis', 'theory'],
            'education': ['learn', 'teach', 'course', 'tutorial', 'education', 'training']
        }
        
        query_lower = query.lower()
        domain_scores = {}
        
        for domain, domain_kws in domain_keywords.items():
            score = 0
            for kw in domain_kws:
                if kw.lower() in query_lower:
                    score += 1
            domain_scores[domain] = score
        
        if domain_scores and max(domain_scores.values()) > 0:
            return max(domain_scores, key=domain_scores.get)
        
        return 'general'

class RetrievalStrategySelector:
    """æ£€ç´¢ç­–ç•¥é€‰æ‹©å™¨"""
    
    def __init__(self):
        # ç­–ç•¥é€‰æ‹©è§„åˆ™
        self.strategy_rules = {
            QueryType.FACTUAL: [RetrievalStrategy.SEMANTIC, RetrievalStrategy.KEYWORD],
            QueryType.ANALYTICAL: [RetrievalStrategy.HYBRID, RetrievalStrategy.HIERARCHICAL],
            QueryType.PROCEDURAL: [RetrievalStrategy.KEYWORD, RetrievalStrategy.HIERARCHICAL],
            QueryType.CREATIVE: [RetrievalStrategy.SEMANTIC, RetrievalStrategy.MULTI_QUERY],
            QueryType.COMPARATIVE: [RetrievalStrategy.HYBRID, RetrievalStrategy.MULTI_QUERY]
        }
        
        # ç­–ç•¥æ€§èƒ½å†å²
        self.strategy_performance = defaultdict(list)
    
    def select_strategy(self, query_analysis: QueryAnalysis) -> List[RetrievalStrategy]:
        """é€‰æ‹©æœ€ä¼˜æ£€ç´¢ç­–ç•¥"""
        
        # åŸºäºæŸ¥è¯¢ç±»å‹çš„åŸºç¡€ç­–ç•¥
        base_strategies = self.strategy_rules.get(
            query_analysis.query_type, 
            [RetrievalStrategy.SEMANTIC]
        )
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´ç­–ç•¥
        if query_analysis.complexity_score > 0.7:
            # é«˜å¤æ‚åº¦æŸ¥è¯¢ä½¿ç”¨å¤šç­–ç•¥
            if RetrievalStrategy.MULTI_QUERY not in base_strategies:
                base_strategies.append(RetrievalStrategy.MULTI_QUERY)
        
        # æ ¹æ®å†å²æ€§èƒ½è°ƒæ•´
        best_strategies = self._get_best_performing_strategies(query_analysis)
        if best_strategies:
            # ç»“åˆå†å²æœ€ä½³ç­–ç•¥
            combined_strategies = list(set(base_strategies + best_strategies))
            return combined_strategies[:3]  # é™åˆ¶ç­–ç•¥æ•°é‡
        
        return base_strategies
    
    def _get_best_performing_strategies(self, query_analysis: QueryAnalysis) -> List[RetrievalStrategy]:
        """è·å–å†å²è¡¨ç°æœ€ä½³çš„ç­–ç•¥"""
        domain_key = f"{query_analysis.domain}_{query_analysis.query_type.value}"
        
        if domain_key in self.strategy_performance:
            # è®¡ç®—å„ç­–ç•¥çš„å¹³å‡æ€§èƒ½
            strategy_scores = defaultdict(list)
            for record in self.strategy_performance[domain_key]:
                strategy_scores[record['strategy']].append(record['score'])
            
            # è¿”å›å¹³å‡åˆ†æœ€é«˜çš„ç­–ç•¥
            avg_scores = {
                strategy: np.mean(scores) 
                for strategy, scores in strategy_scores.items()
            }
            
            sorted_strategies = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)
            return [strategy for strategy, _ in sorted_strategies[:2]]
        
        return []
    
    def record_performance(self, query_analysis: QueryAnalysis, 
                          strategy: RetrievalStrategy, score: float):
        """è®°å½•ç­–ç•¥æ€§èƒ½"""
        domain_key = f"{query_analysis.domain}_{query_analysis.query_type.value}"
        self.strategy_performance[domain_key].append({
            'strategy': strategy,
            'score': score,
            'timestamp': time.time()
        })

class AdvancedRetriever:
    """é«˜çº§æ£€ç´¢å™¨"""
    
    def __init__(self, vector_database, text_corpus: Dict[str, str]):
        self.vector_database = vector_database
        self.text_corpus = text_corpus  # æ–‡æ¡£IDåˆ°æ–‡æœ¬å†…å®¹çš„æ˜ å°„
        self.query_analyzer = QueryAnalyzer()
        self.strategy_selector = RetrievalStrategySelector()
        
        # æ„å»ºå…³é”®è¯ç´¢å¼•
        self.keyword_index = self._build_keyword_index()
    
    def _build_keyword_index(self) -> Dict[str, List[str]]:
        """æ„å»ºå…³é”®è¯å€’æ’ç´¢å¼•"""
        keyword_index = defaultdict(list)
        
        for doc_id, content in self.text_corpus.items():
            # ç®€å•çš„å…³é”®è¯æå–
            words = re.findall(r'\b\w+\b', content.lower())
            for word in set(words):
                if len(word) > 3:  # è¿‡æ»¤çŸ­è¯
                    keyword_index[word].append(doc_id)
        
        return dict(keyword_index)
    
    def retrieve(self, query: str, top_k: int = 10) -> List[RetrievalResult]:
        """æ‰§è¡Œæ™ºèƒ½æ£€ç´¢"""
        
        # 1. æŸ¥è¯¢åˆ†æ
        query_analysis = self.query_analyzer.analyze_query(query)
        print(f"ğŸ” æŸ¥è¯¢åˆ†æ: {query_analysis.query_type.value} | å¤æ‚åº¦: {query_analysis.complexity_score:.2f}")
        
        # 2. ç­–ç•¥é€‰æ‹©
        strategies = self.strategy_selector.select_strategy(query_analysis)
        print(f"ğŸ“‹ é€‰æ‹©ç­–ç•¥: {[s.value for s in strategies]}")
        
        # 3. å¤šç­–ç•¥æ£€ç´¢
        all_results = []
        strategy_weights = self._calculate_strategy_weights(strategies, query_analysis)
        
        for strategy in strategies:
            strategy_results = self._execute_strategy(strategy, query, query_analysis, top_k * 2)
            
            # åº”ç”¨ç­–ç•¥æƒé‡
            weight = strategy_weights.get(strategy, 1.0)
            for result in strategy_results:
                result.score *= weight
                result.retrieval_method = f"{strategy.value}(w={weight:.2f})"
            
            all_results.extend(strategy_results)
        
        # 4. ç»“æœèåˆå’Œé‡æ’åº
        final_results = self._fuse_and_rerank(all_results, query_analysis, top_k)
        
        # 5. è®°å½•æ€§èƒ½ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if final_results:
            avg_score = np.mean([r.score for r in final_results])
            for strategy in strategies:
                self.strategy_selector.record_performance(query_analysis, strategy, avg_score)
        
        return final_results
    
    def _calculate_strategy_weights(self, strategies: List[RetrievalStrategy], 
                                  query_analysis: QueryAnalysis) -> Dict[RetrievalStrategy, float]:
        """è®¡ç®—ç­–ç•¥æƒé‡"""
        weights = {}
        
        for strategy in strategies:
            if strategy == RetrievalStrategy.SEMANTIC:
                # è¯­ä¹‰æ£€ç´¢åœ¨åˆ†ææ€§å’Œåˆ›é€ æ€§æŸ¥è¯¢ä¸­æƒé‡æ›´é«˜
                if query_analysis.query_type in [QueryType.ANALYTICAL, QueryType.CREATIVE]:
                    weights[strategy] = 1.2
                else:
                    weights[strategy] = 1.0
            
            elif strategy == RetrievalStrategy.KEYWORD:
                # å…³é”®è¯æ£€ç´¢åœ¨äº‹å®æ€§å’Œç¨‹åºæ€§æŸ¥è¯¢ä¸­æƒé‡æ›´é«˜
                if query_analysis.query_type in [QueryType.FACTUAL, QueryType.PROCEDURAL]:
                    weights[strategy] = 1.2
                else:
                    weights[strategy] = 0.8
            
            elif strategy == RetrievalStrategy.HYBRID:
                # æ··åˆæ£€ç´¢æƒé‡ç¨³å®š
                weights[strategy] = 1.1
            
            else:
                weights[strategy] = 1.0
        
        return weights
    
    def _execute_strategy(self, strategy: RetrievalStrategy, query: str,
                         query_analysis: QueryAnalysis, top_k: int) -> List[RetrievalResult]:
        """æ‰§è¡Œå…·ä½“çš„æ£€ç´¢ç­–ç•¥"""
        
        if strategy == RetrievalStrategy.SEMANTIC:
            return self._semantic_retrieval(query, top_k)
        
        elif strategy == RetrievalStrategy.KEYWORD:
            return self._keyword_retrieval(query_analysis.keywords, top_k)
        
        elif strategy == RetrievalStrategy.HYBRID:
            semantic_results = self._semantic_retrieval(query, top_k // 2)
            keyword_results = self._keyword_retrieval(query_analysis.keywords, top_k // 2)
            return semantic_results + keyword_results
        
        elif strategy == RetrievalStrategy.HIERARCHICAL:
            return self._hierarchical_retrieval(query, query_analysis, top_k)
        
        elif strategy == RetrievalStrategy.MULTI_QUERY:
            return self._multi_query_retrieval(query, query_analysis, top_k)
        
        else:
            return self._semantic_retrieval(query, top_k)
    
    def _semantic_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """è¯­ä¹‰æ£€ç´¢"""
        # æ¨¡æ‹Ÿå‘é‡æ£€ç´¢
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨å‘é‡æ•°æ®åº“çš„æœç´¢åŠŸèƒ½
        
        # ç®€åŒ–å®ç°ï¼šéšæœºé€‰æ‹©ä¸€äº›æ–‡æ¡£å¹¶èµ‹äºˆç›¸ä¼¼åº¦åˆ†æ•°
        import random
        random.seed(hash(query) % 1000)
        
        doc_ids = list(self.text_corpus.keys())
        selected_docs = random.sample(doc_ids, min(top_k, len(doc_ids)))
        
        results = []
        for doc_id in selected_docs:
            # æ¨¡æ‹Ÿç›¸ä¼¼åº¦è®¡ç®—
            similarity = random.uniform(0.5, 0.95)
            
            result = RetrievalResult(
                document_id=doc_id,
                content=self.text_corpus[doc_id][:200] + "...",
                score=similarity,
                metadata={"method": "semantic"},
                retrieval_method="semantic"
            )
            results.append(result)
        
        return sorted(results, key=lambda x: x.score, reverse=True)
    
    def _keyword_retrieval(self, keywords: List[str], top_k: int) -> List[RetrievalResult]:
        """å…³é”®è¯æ£€ç´¢"""
        doc_scores = defaultdict(float)
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in self.keyword_index:
                matching_docs = self.keyword_index[keyword_lower]
                for doc_id in matching_docs:
                    # è®¡ç®—TF-IDFé£æ ¼çš„åˆ†æ•°
                    tf = self.text_corpus[doc_id].lower().count(keyword_lower)
                    idf = math.log(len(self.text_corpus) / len(matching_docs))
                    doc_scores[doc_id] += tf * idf
        
        # å½’ä¸€åŒ–åˆ†æ•°
        if doc_scores:
            max_score = max(doc_scores.values())
            for doc_id in doc_scores:
                doc_scores[doc_id] /= max_score
        
        # é€‰æ‹©top-kç»“æœ
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        results = []
        for doc_id, score in sorted_docs:
            result = RetrievalResult(
                document_id=doc_id,
                content=self.text_corpus[doc_id][:200] + "...",
                score=score,
                metadata={"method": "keyword", "matched_keywords": keywords},
                retrieval_method="keyword"
            )
            results.append(result)
        
        return results
    
    def _hierarchical_retrieval(self, query: str, query_analysis: QueryAnalysis, 
                               top_k: int) -> List[RetrievalResult]:
        """å±‚æ¬¡æ£€ç´¢ï¼šå…ˆç²—æ£€ç´¢ï¼Œå†ç²¾æ£€ç´¢"""
        
        # ç¬¬ä¸€å±‚ï¼šç²—æ£€ç´¢ï¼Œè·å–è¾ƒå¤šå€™é€‰
        coarse_results = self._semantic_retrieval(query, top_k * 3)
        
        # ç¬¬äºŒå±‚ï¼šåŸºäºå…³é”®è¯è¿›è¡Œç²¾ç»†åŒ–è¿‡æ»¤
        refined_results = []
        for result in coarse_results:
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            keyword_match_score = 0
            content_lower = result.content.lower()
            
            for keyword in query_analysis.keywords:
                if keyword.lower() in content_lower:
                    keyword_match_score += 1
            
            if query_analysis.keywords:
                keyword_match_score /= len(query_analysis.keywords)
            
            # ç»“åˆè¯­ä¹‰åˆ†æ•°å’Œå…³é”®è¯åŒ¹é…åˆ†æ•°
            combined_score = 0.7 * result.score + 0.3 * keyword_match_score
            
            result.score = combined_score
            result.retrieval_method = "hierarchical"
            refined_results.append(result)
        
        return sorted(refined_results, key=lambda x: x.score, reverse=True)[:top_k]
    
    def _multi_query_retrieval(self, query: str, query_analysis: QueryAnalysis, 
                              top_k: int) -> List[RetrievalResult]:
        """å¤šæŸ¥è¯¢æ£€ç´¢ï¼šç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢è¿›è¡Œæ£€ç´¢"""
        
        # ç”Ÿæˆç›¸å…³æŸ¥è¯¢
        related_queries = self._generate_related_queries(query, query_analysis)
        
        all_results = []
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        for related_query in related_queries:
            query_results = self._semantic_retrieval(related_query, top_k // len(related_queries) + 1)
            
            # é™ä½ç›¸å…³æŸ¥è¯¢çš„æƒé‡
            for result in query_results:
                result.score *= 0.8
                result.retrieval_method = "multi_query"
            
            all_results.extend(query_results)
        
        # å»é‡å¹¶æ’åº
        unique_results = {}
        for result in all_results:
            if result.document_id not in unique_results:
                unique_results[result.document_id] = result
            else:
                # ä¿ç•™åˆ†æ•°æ›´é«˜çš„ç»“æœ
                if result.score > unique_results[result.document_id].score:
                    unique_results[result.document_id] = result
        
        return sorted(unique_results.values(), key=lambda x: x.score, reverse=True)[:top_k]
    
    def _generate_related_queries(self, query: str, query_analysis: QueryAnalysis) -> List[str]:
        """ç”Ÿæˆç›¸å…³æŸ¥è¯¢"""
        related_queries = [query]  # åŒ…å«åŸæŸ¥è¯¢
        
        # åŸºäºå…³é”®è¯ç»„åˆç”Ÿæˆæ–°æŸ¥è¯¢
        if len(query_analysis.keywords) > 1:
            for i in range(len(query_analysis.keywords)):
                for j in range(i + 1, len(query_analysis.keywords)):
                    related_query = f"{query_analysis.keywords[i]} {query_analysis.keywords[j]}"
                    related_queries.append(related_query)
        
        # åŸºäºå®ä½“ç”ŸæˆæŸ¥è¯¢
        for entity in query_analysis.entities:
            related_queries.append(entity)
        
        return related_queries[:4]  # é™åˆ¶æŸ¥è¯¢æ•°é‡
    
    def _fuse_and_rerank(self, all_results: List[RetrievalResult], 
                        query_analysis: QueryAnalysis, top_k: int) -> List[RetrievalResult]:
        """ç»“æœèåˆå’Œé‡æ’åº"""
        
        # å»é‡ï¼šåˆå¹¶ç›¸åŒæ–‡æ¡£çš„ç»“æœ
        doc_results = {}
        for result in all_results:
            doc_id = result.document_id
            if doc_id not in doc_results:
                doc_results[doc_id] = result
            else:
                # èåˆåˆ†æ•°ï¼šå–æœ€å¤§å€¼å¹¶åŠ æƒå¹³å‡
                existing_result = doc_results[doc_id]
                fused_score = max(existing_result.score, result.score) * 0.6 + \
                             (existing_result.score + result.score) / 2 * 0.4
                
                existing_result.score = fused_score
                existing_result.retrieval_method += f"+{result.retrieval_method}"
        
        unique_results = list(doc_results.values())
        
        # é‡æ’åºï¼šåº”ç”¨æŸ¥è¯¢ç‰¹å®šçš„æ’åºç­–ç•¥
        reranked_results = self._apply_reranking(unique_results, query_analysis)
        
        return reranked_results[:top_k]
    
    def _apply_reranking(self, results: List[RetrievalResult], 
                        query_analysis: QueryAnalysis) -> List[RetrievalResult]:
        """åº”ç”¨é‡æ’åºç­–ç•¥"""
        
        for result in results:
            # åŸºç¡€åˆ†æ•°
            rerank_score = result.score
            
            # é•¿åº¦åå¥½è°ƒæ•´
            content_length = len(result.content)
            if query_analysis.query_type == QueryType.PROCEDURAL:
                # ç¨‹åºæ€§æŸ¥è¯¢åå¥½è¾ƒé•¿çš„å†…å®¹
                length_bonus = min(content_length / 1000, 0.2)
            else:
                # å…¶ä»–æŸ¥è¯¢ç±»å‹åå¥½é€‚ä¸­é•¿åº¦
                length_bonus = max(0, 0.1 - abs(content_length - 500) / 5000)
            
            rerank_score += length_bonus
            
            # æ–°é²œåº¦è°ƒæ•´ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼‰
            if 'timestamp' in result.metadata:
                # ç®€åŒ–çš„æ–°é²œåº¦è®¡ç®—
                freshness_bonus = 0.05  # å‡è®¾éƒ½æ˜¯ç›¸å¯¹æ–°çš„å†…å®¹
                rerank_score += freshness_bonus
            
            result.score = rerank_score
        
        return sorted(results, key=lambda x: x.score, reverse=True)

# æ£€ç´¢ç­–ç•¥ä¼˜åŒ–æ¼”ç¤º
def demonstrate_retrieval_optimization():
    """æ¼”ç¤ºæ£€ç´¢ç­–ç•¥ä¼˜åŒ–"""
    
    print("ğŸ¯ æ£€ç´¢ç­–ç•¥ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£åº“
    text_corpus = {
        "doc_001": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æå’Œäººå·¥æ™ºèƒ½ã€‚å®ƒå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚",
        "doc_002": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚å¸¸ç”¨çš„ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘å’Œç¥ç»ç½‘ç»œã€‚",
        "doc_003": "Webå¼€å‘æ¶‰åŠå‰ç«¯å’Œåç«¯æŠ€æœ¯ã€‚å‰ç«¯ä½¿ç”¨HTMLã€CSSå’ŒJavaScriptï¼Œåç«¯å¯ä»¥ä½¿ç”¨Pythonã€Javaæˆ–Node.jsç­‰æŠ€æœ¯ã€‚",
        "doc_004": "æ•°æ®åº“æ˜¯å­˜å‚¨å’Œç®¡ç†æ•°æ®çš„ç³»ç»Ÿã€‚å¸¸è§çš„æ•°æ®åº“åŒ…æ‹¬MySQLã€PostgreSQLå’ŒMongoDBã€‚SQLæ˜¯æŸ¥è¯¢å…³ç³»æ•°æ®åº“çš„æ ‡å‡†è¯­è¨€ã€‚",
        "doc_005": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®ã€‚åœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
        "doc_006": "APIï¼ˆåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼‰å…è®¸ä¸åŒè½¯ä»¶ç³»ç»Ÿä¹‹é—´è¿›è¡Œé€šä¿¡ã€‚RESTful APIæ˜¯ç›®å‰æœ€æµè¡Œçš„APIè®¾è®¡é£æ ¼ã€‚",
        "doc_007": "äº‘è®¡ç®—æä¾›äº†å¯æ‰©å±•çš„è®¡ç®—èµ„æºï¼ŒåŒ…æ‹¬åŸºç¡€è®¾æ–½å³æœåŠ¡(IaaS)ã€å¹³å°å³æœåŠ¡(PaaS)å’Œè½¯ä»¶å³æœåŠ¡(SaaS)ã€‚",
        "doc_008": "ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå¦‚Gitå¸®åŠ©å¼€å‘è€…ç®¡ç†ä»£ç å˜æ›´ã€‚GitHubæ˜¯æœ€æµè¡Œçš„Gitæ‰˜ç®¡å¹³å°ï¼Œæ”¯æŒåä½œå¼€å‘ã€‚",
        "doc_009": "å®¹å™¨æŠ€æœ¯å¦‚Dockerç®€åŒ–äº†åº”ç”¨éƒ¨ç½²ã€‚Kubernetesæ˜¯å®¹å™¨ç¼–æ’å¹³å°ï¼Œç”¨äºç®¡ç†å¤§è§„æ¨¡å®¹å™¨åŒ–åº”ç”¨ã€‚",
        "doc_010": "æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)æ˜¯ä¸€ç§è½¯ä»¶å¼€å‘æ–¹æ³•ï¼Œè¦æ±‚å…ˆç¼–å†™æµ‹è¯•ï¼Œå†ç¼–å†™å®ç°ä»£ç ã€‚è¿™æœ‰åŠ©äºæé«˜ä»£ç è´¨é‡ã€‚"
    }
    
    # åˆ›å»ºæ¨¡æ‹Ÿå‘é‡æ•°æ®åº“
    class MockVectorDatabase:
        def search_vectors(self, query_vector, k=10):
            # æ¨¡æ‹Ÿå‘é‡æœç´¢ç»“æœ
            return []
    
    vector_db = MockVectorDatabase()
    
    # åˆ›å»ºé«˜çº§æ£€ç´¢å™¨
    retriever = AdvancedRetriever(vector_db, text_corpus)
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯Pythonç¼–ç¨‹è¯­è¨€ï¼Ÿ",                    # äº‹å®æ€§æŸ¥è¯¢
        "ä¸ºä»€ä¹ˆæœºå™¨å­¦ä¹ åœ¨AIä¸­å¾ˆé‡è¦ï¼Ÿ",              # åˆ†ææ€§æŸ¥è¯¢  
        "å¦‚ä½•ä½¿ç”¨Gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼Ÿ",                # ç¨‹åºæ€§æŸ¥è¯¢
        "è®¾è®¡ä¸€ä¸ªWeb APIçš„æœ€ä½³å®è·µ",                # åˆ›é€ æ€§æŸ¥è¯¢
        "æ¯”è¾ƒSQLå’ŒNoSQLæ•°æ®åº“çš„ä¼˜ç¼ºç‚¹"              # æ¯”è¾ƒæ€§æŸ¥è¯¢
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ {i}: {query}")
        print("-" * 40)
        
        # æ‰§è¡Œæ£€ç´¢
        start_time = time.time()
        results = retriever.retrieve(query, top_k=3)
        retrieval_time = time.time() - start_time
        
        print(f"â±ï¸ æ£€ç´¢è€—æ—¶: {retrieval_time:.3f}ç§’")
        print(f"ğŸ“‹ æ£€ç´¢ç»“æœ (å‰3ä¸ª):")
        
        for j, result in enumerate(results, 1):
            print(f"  {j}. æ–‡æ¡£ID: {result.document_id}")
            print(f"     åˆ†æ•°: {result.score:.4f}")
            print(f"     æ–¹æ³•: {result.retrieval_method}")
            print(f"     å†…å®¹: {result.content[:100]}...")
            print()
    
    print("âœ… æ£€ç´¢ç­–ç•¥ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ")

# è¿è¡Œæ£€ç´¢ç­–ç•¥ä¼˜åŒ–æ¼”ç¤º
demonstrate_retrieval_optimization()

print("\nğŸ‰ æ£€ç´¢ç­–ç•¥ä¼˜åŒ–å®Œæ•´æ¼”ç¤ºç»“æŸ")
```

### ğŸ“Š æ£€ç´¢æ€§èƒ½è¯„ä¼°

ä¸ºäº†æŒç»­ä¼˜åŒ–æ£€ç´¢æ•ˆæœï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹å®Œå–„çš„è¯„ä¼°ä½“ç³»ï¼š

```mermaid
graph TD
    subgraph "æ£€ç´¢è¯„ä¼°ä½“ç³»"
        subgraph "å‡†ç¡®æ€§æŒ‡æ ‡"
            A1[ç²¾ç¡®ç‡ Precision<br/>æ£€ç´¢ç»“æœä¸­ç›¸å…³æ–‡æ¡£æ¯”ä¾‹]
            A2[å¬å›ç‡ Recall<br/>ç›¸å…³æ–‡æ¡£è¢«æ£€ç´¢æ¯”ä¾‹]
            A3[F1åˆ†æ•°<br/>ç²¾ç¡®ç‡å’Œå¬å›ç‡è°ƒå’Œå¹³å‡]
            A4[NDCG<br/>å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š]
        end
        
        subgraph "æ•ˆç‡æŒ‡æ ‡"
            B1[å“åº”æ—¶é—´<br/>æŸ¥è¯¢å¤„ç†å»¶è¿Ÿ]
            B2[ååé‡<br/>æ¯ç§’å¤„ç†æŸ¥è¯¢æ•°]
            B3[èµ„æºä½¿ç”¨<br/>CPUå’Œå†…å­˜æ¶ˆè€—]
            B4[å¯æ‰©å±•æ€§<br/>æ•°æ®è§„æ¨¡å¢é•¿å½±å“]
        end
        
        subgraph "ç”¨æˆ·ä½“éªŒ"
            C1[ç›¸å…³æ€§<br/>ç»“æœä¸æŸ¥è¯¢åŒ¹é…åº¦]
            C2[å¤šæ ·æ€§<br/>ç»“æœè¦†ç›–é¢]
            C3[æ–°é²œåº¦<br/>å†…å®¹æ—¶æ•ˆæ€§]
            C4[å¯è§£é‡Šæ€§<br/>æ£€ç´¢ç†ç”±é€æ˜åº¦]
        end
        
        A1 --> D[ç»¼åˆè¯„åˆ†]
        A2 --> D
        A3 --> D
        A4 --> D
        B1 --> D
        B2 --> D
        B3 --> D
        B4 --> D
        C1 --> D
        C2 --> D
        C3 --> D
        C4 --> D
    end
```

### ğŸš€ æ£€ç´¢ç­–ç•¥è¿›é˜¶æŠ€æœ¯

```python
class AdvancedRetrievalTechniques:
    """é«˜çº§æ£€ç´¢æŠ€æœ¯é›†åˆ"""
    
    def __init__(self):
        self.query_expansion_cache = {}
        self.feedback_history = []
    
    def query_expansion(self, query: str, expansion_type: str = "synonym") -> List[str]:
        """æŸ¥è¯¢æ‰©å±•æŠ€æœ¯"""
        
        if query in self.query_expansion_cache:
            return self.query_expansion_cache[query]
        
        expanded_queries = [query]
        
        if expansion_type == "synonym":
            # åŒä¹‰è¯æ‰©å±•ï¼ˆç®€åŒ–ç‰ˆï¼‰
            synonym_map = {
                "ç¼–ç¨‹": ["ç¨‹åºè®¾è®¡", "å¼€å‘", "coding"],
                "æ•°æ®åº“": ["DB", "æ•°æ®å­˜å‚¨", "database"],
                "æœºå™¨å­¦ä¹ ": ["ML", "äººå·¥æ™ºèƒ½", "AIç®—æ³•"],
                "ç½‘ç»œ": ["ç½‘ç»œ", "äº’è”ç½‘", "web"]
            }
            
            for original, synonyms in synonym_map.items():
                if original in query:
                    for synonym in synonyms:
                        expanded_queries.append(query.replace(original, synonym))
        
        elif expansion_type == "context":
            # ä¸Šä¸‹æ–‡æ‰©å±•
            context_keywords = {
                "Python": ["ç¼–ç¨‹è¯­è¨€", "è„šæœ¬", "å¼€å‘"],
                "API": ["æ¥å£", "æœåŠ¡", "è°ƒç”¨"],
                "æ•°æ®": ["ä¿¡æ¯", "ç»Ÿè®¡", "åˆ†æ"]
            }
            
            for term, contexts in context_keywords.items():
                if term in query:
                    for context in contexts:
                        expanded_queries.append(f"{query} {context}")
        
        self.query_expansion_cache[query] = expanded_queries
        return expanded_queries
    
    def pseudo_relevance_feedback(self, query: str, initial_results: List[RetrievalResult], 
                                 feedback_docs: int = 3) -> str:
        """ä¼ªç›¸å…³åé¦ˆæŠ€æœ¯"""
        
        if len(initial_results) < feedback_docs:
            return query
        
        # ä»topç»“æœä¸­æå–å…³é”®è¯
        feedback_keywords = []
        for result in initial_results[:feedback_docs]:
            # ç®€å•çš„å…³é”®è¯æå–
            words = re.findall(r'\b\w{4,}\b', result.content.lower())
            word_freq = Counter(words)
            
            # é€‰æ‹©é«˜é¢‘è¯ä½œä¸ºæ‰©å±•è¯
            top_words = [word for word, freq in word_freq.most_common(5) if freq > 1]
            feedback_keywords.extend(top_words)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ‰©å±•è¯
        keyword_freq = Counter(feedback_keywords)
        expansion_words = [word for word, freq in keyword_freq.most_common(3)]
        
        # æ„å»ºæ‰©å±•æŸ¥è¯¢
        expanded_query = query + " " + " ".join(expansion_words)
        return expanded_query
    
    def learning_to_rank(self, results: List[RetrievalResult], 
                        query_features: Dict[str, float]) -> List[RetrievalResult]:
        """å­¦ä¹ æ’åºæŠ€æœ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        
        # ç‰¹å¾æƒé‡ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›æƒé‡ä¼šé€šè¿‡æœºå™¨å­¦ä¹ è®­ç»ƒå¾—åˆ°ï¼‰
        feature_weights = {
            "semantic_score": 0.4,
            "keyword_match": 0.3,
            "content_length": 0.1,
            "freshness": 0.1,
            "authority": 0.1
        }
        
        for result in results:
            # è®¡ç®—ç»¼åˆæ’åºåˆ†æ•°
            ranking_score = 0
            
            # è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
            ranking_score += result.score * feature_weights["semantic_score"]
            
            # å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_match_score = self._calculate_keyword_match(result, query_features)
            ranking_score += keyword_match_score * feature_weights["keyword_match"]
            
            # å†…å®¹é•¿åº¦åˆ†æ•°
            length_score = min(len(result.content) / 1000, 1.0)
            ranking_score += length_score * feature_weights["content_length"]
            
            # æ–°é²œåº¦åˆ†æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
            freshness_score = 0.8  # å‡è®¾å†…å®¹ç›¸å¯¹æ–°é²œ
            ranking_score += freshness_score * feature_weights["freshness"]
            
            # æƒå¨æ€§åˆ†æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
            authority_score = 0.7  # å‡è®¾æ¥æºæƒå¨æ€§
            ranking_score += authority_score * feature_weights["authority"]
            
            result.score = ranking_score
        
        return sorted(results, key=lambda x: x.score, reverse=True)
    
    def _calculate_keyword_match(self, result: RetrievalResult, 
                               query_features: Dict[str, float]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        content_lower = result.content.lower()
        match_score = 0
        
        # ç®€åŒ–çš„å…³é”®è¯åŒ¹é…è®¡ç®—
        for feature, weight in query_features.items():
            if feature.lower() in content_lower:
                match_score += weight
        
        return min(match_score, 1.0)
    
    def diversification(self, results: List[RetrievalResult], 
                       diversity_threshold: float = 0.7) -> List[RetrievalResult]:
        """ç»“æœå¤šæ ·åŒ–æŠ€æœ¯"""
        
        if len(results) <= 1:
            return results
        
        diversified_results = [results[0]]  # ä¿ç•™æœ€ç›¸å…³çš„ç»“æœ
        
        for result in results[1:]:
            # æ£€æŸ¥ä¸å·²é€‰ç»“æœçš„ç›¸ä¼¼åº¦
            is_diverse = True
            for selected_result in diversified_results:
                similarity = self._calculate_content_similarity(result, selected_result)
                if similarity > diversity_threshold:
                    is_diverse = False
                    break
            
            if is_diverse:
                diversified_results.append(result)
        
        return diversified_results
    
    def _calculate_content_similarity(self, result1: RetrievalResult, 
                                    result2: RetrievalResult) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        content1_words = set(re.findall(r'\b\w+\b', result1.content.lower()))
        content2_words = set(re.findall(r'\b\w+\b', result2.content.lower()))
        
        if not content1_words or not content2_words:
            return 0.0
        
        intersection = len(content1_words & content2_words)
        union = len(content1_words | content2_words)
        
        return intersection / union if union > 0 else 0.0

# é«˜çº§æŠ€æœ¯æ¼”ç¤º
def demonstrate_advanced_techniques():
    """æ¼”ç¤ºé«˜çº§æ£€ç´¢æŠ€æœ¯"""
    
    print("ğŸš€ é«˜çº§æ£€ç´¢æŠ€æœ¯æ¼”ç¤º")
    print("=" * 40)
    
    techniques = AdvancedRetrievalTechniques()
    
    # æµ‹è¯•æŸ¥è¯¢æ‰©å±•
    print("ğŸ“ˆ æŸ¥è¯¢æ‰©å±•æŠ€æœ¯:")
    original_query = "Pythonæœºå™¨å­¦ä¹ "
    
    synonym_expansion = techniques.query_expansion(original_query, "synonym")
    print(f"åŒä¹‰è¯æ‰©å±•: {synonym_expansion}")
    
    context_expansion = techniques.query_expansion(original_query, "context")
    print(f"ä¸Šä¸‹æ–‡æ‰©å±•: {context_expansion}")
    
    # æµ‹è¯•ä¼ªç›¸å…³åé¦ˆ
    print(f"\nğŸ”„ ä¼ªç›¸å…³åé¦ˆ:")
    mock_results = [
        RetrievalResult("doc1", "Pythonæ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦å·¥å…·ï¼Œæä¾›äº†scikit-learnç­‰åº“", 0.9, {}, "mock"),
        RetrievalResult("doc2", "æ·±åº¦å­¦ä¹ æ¡†æ¶TensorFlowå’ŒPyTorchéƒ½æ”¯æŒPython", 0.8, {}, "mock"),
        RetrievalResult("doc3", "æ•°æ®ç§‘å­¦å®¶ç»å¸¸ä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æå’Œå»ºæ¨¡", 0.7, {}, "mock")
    ]
    
    feedback_query = techniques.pseudo_relevance_feedback(original_query, mock_results)
    print(f"åé¦ˆæ‰©å±•æŸ¥è¯¢: {feedback_query}")
    
    # æµ‹è¯•ç»“æœå¤šæ ·åŒ–
    print(f"\nğŸ¨ ç»“æœå¤šæ ·åŒ–:")
    diverse_results = techniques.diversification(mock_results)
    print(f"å¤šæ ·åŒ–åç»“æœæ•°é‡: {len(diverse_results)}")
    
    print("âœ… é«˜çº§æŠ€æœ¯æ¼”ç¤ºå®Œæˆ")

# è¿è¡Œé«˜çº§æŠ€æœ¯æ¼”ç¤º
demonstrate_advanced_techniques()

print("\nğŸ¯ æ£€ç´¢ç­–ç•¥ä¼˜åŒ–ç« èŠ‚å®Œæˆ")
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†æ£€ç´¢ç­–ç•¥ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æŸ¥è¯¢åˆ†æã€ç­–ç•¥é€‰æ‹©ã€ç»“æœèåˆå’Œé«˜çº§ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯çš„åˆç†åº”ç”¨å¯ä»¥æ˜¾è‘—æå‡RAGç³»ç»Ÿçš„æ£€ç´¢ç²¾åº¦å’Œç”¨æˆ·ä½“éªŒã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†æ£€ç´¢ç­–ç•¥ä¼˜åŒ–çš„ç†è®ºå’Œå®è·µã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ¢è®¨ç”Ÿæˆç­–ç•¥ä¼˜åŒ–ï¼Œäº†è§£å¦‚ä½•æå‡RAGç³»ç»Ÿçš„ç”Ÿæˆè´¨é‡å’Œä¸€è‡´æ€§ã€‚*

---

## 29.5 ç”Ÿæˆç­–ç•¥ä¼˜åŒ–

### ğŸ¯ ç­”æ¡ˆè´¨é‡ä¿éšœç³»ç»Ÿ

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒä¸­ï¼Œ**ç­”æ¡ˆè´¨é‡ä¿éšœç³»ç»Ÿ**æ˜¯æ•´ä¸ªæµç¨‹çš„æœ€åä¸€ç¯ï¼Œä¹Ÿæ˜¯æœ€å…³é”®çš„ç¯èŠ‚ã€‚å°±åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„ä¸“å®¶é¡¾é—®ï¼Œå®ƒéœ€è¦åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆå‡†ç¡®ã€ç›¸å…³ã€è¿è´¯çš„é«˜è´¨é‡ç­”æ¡ˆã€‚

### ğŸ“ ç”Ÿæˆç­–ç•¥æ ¸å¿ƒæ¦‚å¿µ

ç”Ÿæˆç­–ç•¥ä¼˜åŒ–æ¶‰åŠæç¤ºå·¥ç¨‹ã€ä¸Šä¸‹æ–‡ç®¡ç†ã€ç­”æ¡ˆè´¨é‡æ§åˆ¶ã€ä¸€è‡´æ€§ä¿è¯ç­‰å¤šä¸ªæ–¹é¢ï¼Œç›®æ ‡æ˜¯ç¡®ä¿RAGç³»ç»Ÿè¾“å‡ºé«˜è´¨é‡ã€å¯ä¿¡èµ–çš„ç­”æ¡ˆã€‚

```python
# ç”Ÿæˆç­–ç•¥ä¼˜åŒ–å®ç°
import numpy as np
import re
import json
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
from enum import Enum
import time
from collections import defaultdict
import hashlib

class GenerationStrategy(Enum):
    """ç”Ÿæˆç­–ç•¥æšä¸¾"""
    EXTRACTIVE = "extractive"       # æŠ½å–å¼ç”Ÿæˆ
    ABSTRACTIVE = "abstractive"     # æŠ½è±¡å¼ç”Ÿæˆ
    HYBRID = "hybrid"              # æ··åˆå¼ç”Ÿæˆ
    TEMPLATE_BASED = "template"     # æ¨¡æ¿å¼ç”Ÿæˆ
    CHAIN_OF_THOUGHT = "cot"       # æ€ç»´é“¾ç”Ÿæˆ

class AnswerQuality(Enum):
    """ç­”æ¡ˆè´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"         # ä¼˜ç§€
    GOOD = "good"                  # è‰¯å¥½
    FAIR = "fair"                  # ä¸€èˆ¬
    POOR = "poor"                  # è¾ƒå·®
    INVALID = "invalid"            # æ— æ•ˆ

@dataclass
class GenerationContext:
    """ç”Ÿæˆä¸Šä¸‹æ–‡"""
    query: str
    retrieved_documents: List[Dict[str, Any]]
    conversation_history: List[Dict[str, str]]
    user_preferences: Dict[str, Any]
    domain: str
    language: str = "zh"

@dataclass
class GeneratedAnswer:
    """ç”Ÿæˆçš„ç­”æ¡ˆ"""
    content: str
    confidence_score: float
    sources: List[str]
    generation_method: str
    quality_metrics: Dict[str, float]
    metadata: Dict[str, Any]

class PromptTemplate:
    """æç¤ºæ¨¡æ¿ç±»"""
    
    def __init__(self):
        # é¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿
        self.templates = {
            "factual": {
                "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€ç®€æ´ã€‚",
                "user": """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{documents}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€åŸºäºæ–‡æ¡£çš„ç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
            },
            
            "analytical": {
                "system": "ä½ æ˜¯ä¸€ä¸ªåˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æé—®é¢˜å¹¶æä¾›æœ‰è§åœ°çš„ç­”æ¡ˆã€‚",
                "user": """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæ·±å…¥åˆ†æå¹¶å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{documents}

é—®é¢˜ï¼š{query}

è¯·æä¾›ï¼š
1. æ ¸å¿ƒè§‚ç‚¹åˆ†æ
2. æ”¯æ’‘è¯æ®
3. å¯èƒ½çš„å½±å“æˆ–ç»“è®º
4. å¦‚æœ‰ä¸ç¡®å®šæ€§ï¼Œè¯·æ˜ç¡®æŒ‡å‡º"""
            },
            
            "procedural": {
                "system": "ä½ æ˜¯ä¸€ä¸ªæ“ä½œæŒ‡å¯¼ä¸“å®¶ï¼Œæ“…é•¿æä¾›æ¸…æ™°çš„æ­¥éª¤è¯´æ˜ã€‚",
                "user": """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œä¸ºç”¨æˆ·æä¾›æ“ä½œæŒ‡å¯¼ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{documents}

é—®é¢˜ï¼š{query}

è¯·æä¾›ï¼š
1. æ¸…æ™°çš„æ­¥éª¤è¯´æ˜
2. æ³¨æ„äº‹é¡¹
3. å¯èƒ½é‡åˆ°çš„é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ
4. ç›¸å…³æç¤ºå’Œå»ºè®®"""
            },
            
            "comparative": {
                "system": "ä½ æ˜¯ä¸€ä¸ªæ¯”è¾ƒåˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿå®¢è§‚æ¯”è¾ƒä¸åŒé€‰é¡¹ã€‚",
                "user": """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œè¿›è¡Œæ¯”è¾ƒåˆ†æï¼š

æ–‡æ¡£å†…å®¹ï¼š
{documents}

é—®é¢˜ï¼š{query}

è¯·æä¾›ï¼š
1. å„é€‰é¡¹çš„ä¼˜ç¼ºç‚¹å¯¹æ¯”
2. é€‚ç”¨åœºæ™¯åˆ†æ
3. æ¨èå»ºè®®
4. å†³ç­–ä¾æ®è¯´æ˜"""
            }
        }
    
    def get_template(self, query_type: str, context: GenerationContext) -> Dict[str, str]:
        """è·å–é€‚åˆçš„æç¤ºæ¨¡æ¿"""
        template = self.templates.get(query_type, self.templates["factual"])
        
        # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´æ¨¡æ¿
        if context.conversation_history:
            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            template = self._add_conversation_context(template, context)
        
        return template
    
    def _add_conversation_context(self, template: Dict[str, str], 
                                context: GenerationContext) -> Dict[str, str]:
        """æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡"""
        if len(context.conversation_history) > 0:
            history_text = "\n".join([
                f"ç”¨æˆ·: {turn['user']}\nåŠ©æ‰‹: {turn['assistant']}" 
                for turn in context.conversation_history[-3:]  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            ])
            
            template["user"] = f"""å¯¹è¯å†å²ï¼š
{history_text}

å½“å‰é—®é¢˜å’Œæ–‡æ¡£ï¼š
{template['user']}

è¯·ç»“åˆå¯¹è¯å†å²å’Œå½“å‰æ–‡æ¡£å›ç­”é—®é¢˜ã€‚"""
        
        return template
    
    def format_template(self, template: Dict[str, str], 
                       context: GenerationContext) -> Dict[str, str]:
        """æ ¼å¼åŒ–æ¨¡æ¿"""
        # æ•´ç†æ–‡æ¡£å†…å®¹
        documents_text = ""
        for i, doc in enumerate(context.retrieved_documents, 1):
            content = doc.get('content', '')
            source = doc.get('document_id', f'æ–‡æ¡£{i}')
            documents_text += f"ã€æ–‡æ¡£{i} - {source}ã€‘\n{content}\n\n"
        
        # æ ¼å¼åŒ–æ¨¡æ¿
        formatted_template = {}
        for key, value in template.items():
            formatted_template[key] = value.format(
                documents=documents_text.strip(),
                query=context.query
            )
        
        return formatted_template

class AnswerGenerator:
    """ç­”æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.prompt_template = PromptTemplate()
        self.generation_cache = {}
        
    def generate_answer(self, context: GenerationContext, 
                       strategy: GenerationStrategy = GenerationStrategy.HYBRID) -> GeneratedAnswer:
        """ç”Ÿæˆç­”æ¡ˆ"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(context, strategy)
        if cache_key in self.generation_cache:
            return self.generation_cache[cache_key]
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆç­”æ¡ˆ
        if strategy == GenerationStrategy.EXTRACTIVE:
            answer = self._extractive_generation(context)
        elif strategy == GenerationStrategy.ABSTRACTIVE:
            answer = self._abstractive_generation(context)
        elif strategy == GenerationStrategy.HYBRID:
            answer = self._hybrid_generation(context)
        elif strategy == GenerationStrategy.TEMPLATE_BASED:
            answer = self._template_based_generation(context)
        elif strategy == GenerationStrategy.CHAIN_OF_THOUGHT:
            answer = self._chain_of_thought_generation(context)
        else:
            answer = self._hybrid_generation(context)
        
        # ç¼“å­˜ç»“æœ
        self.generation_cache[cache_key] = answer
        
        return answer
    
    def _get_cache_key(self, context: GenerationContext, strategy: GenerationStrategy) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{context.query}_{len(context.retrieved_documents)}_{strategy.value}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _extractive_generation(self, context: GenerationContext) -> GeneratedAnswer:
        """æŠ½å–å¼ç”Ÿæˆï¼šç›´æ¥ä»æ–‡æ¡£ä¸­æå–ç›¸å…³ç‰‡æ®µ"""
        
        query_keywords = set(re.findall(r'\b\w+\b', context.query.lower()))
        
        # ä»æ–‡æ¡£ä¸­æå–æœ€ç›¸å…³çš„å¥å­
        relevant_sentences = []
        for doc in context.retrieved_documents:
            content = doc.get('content', '')
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
            
            for sentence in sentences:
                if len(sentence.strip()) < 10:
                    continue
                
                sentence_words = set(re.findall(r'\b\w+\b', sentence.lower()))
                overlap = len(query_keywords & sentence_words)
                
                if overlap > 0:
                    relevant_sentences.append({
                        'sentence': sentence.strip(),
                        'score': overlap / len(query_keywords),
                        'source': doc.get('document_id', 'æœªçŸ¥æ¥æº')
                    })
        
        # æ’åºå¹¶é€‰æ‹©æœ€ç›¸å…³çš„å¥å­
        relevant_sentences.sort(key=lambda x: x['score'], reverse=True)
        top_sentences = relevant_sentences[:3]
        
        if not top_sentences:
            content = "æŠ±æ­‰ï¼Œåœ¨æä¾›çš„æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚"
            confidence = 0.1
            sources = []
        else:
            content = "åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ä¿¡æ¯ï¼š\n\n"
            sources = []
            for i, item in enumerate(top_sentences, 1):
                content += f"{i}. {item['sentence']}\n"
                sources.append(item['source'])
            
            confidence = np.mean([item['score'] for item in top_sentences])
        
        return GeneratedAnswer(
            content=content,
            confidence_score=confidence,
            sources=list(set(sources)),
            generation_method="extractive",
            quality_metrics={"relevance": confidence, "completeness": 0.7},
            metadata={"extracted_sentences": len(top_sentences)}
        )
    
    def _abstractive_generation(self, context: GenerationContext) -> GeneratedAnswer:
        """æŠ½è±¡å¼ç”Ÿæˆï¼šåŸºäºç†è§£ç”Ÿæˆæ–°çš„ç­”æ¡ˆ"""
        
        # æ¨¡æ‹ŸLLMè°ƒç”¨ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ä¼šè°ƒç”¨çœŸå®çš„LLMï¼‰
        template = self.prompt_template.get_template("factual", context)
        formatted_template = self.prompt_template.format_template(template, context)
        
        # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        content = self._simulate_llm_generation(formatted_template, context)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_generation_confidence(content, context)
        
        # æå–å¼•ç”¨æ¥æº
        sources = [doc.get('document_id', f'æ–‡æ¡£{i+1}') 
                  for i, doc in enumerate(context.retrieved_documents)]
        
        return GeneratedAnswer(
            content=content,
            confidence_score=confidence,
            sources=sources,
            generation_method="abstractive",
            quality_metrics={"fluency": 0.9, "coherence": 0.85},
            metadata={"template_type": "factual"}
        )
    
    def _hybrid_generation(self, context: GenerationContext) -> GeneratedAnswer:
        """æ··åˆå¼ç”Ÿæˆï¼šç»“åˆæŠ½å–å’ŒæŠ½è±¡"""
        
        # å…ˆè¿›è¡ŒæŠ½å–å¼ç”Ÿæˆè·å–å…³é”®ä¿¡æ¯
        extractive_result = self._extractive_generation(context)
        
        # å†è¿›è¡ŒæŠ½è±¡å¼ç”Ÿæˆè·å–æµç•…è¡¨è¾¾
        abstractive_result = self._abstractive_generation(context)
        
        # èåˆä¸¤ç§ç»“æœ
        if extractive_result.confidence_score > 0.5:
            # å¦‚æœæŠ½å–ç»“æœç½®ä¿¡åº¦é«˜ï¼Œä»¥æŠ½å–ä¸ºä¸»ï¼ŒæŠ½è±¡ä¸ºè¾…
            content = f"{abstractive_result.content}\n\n**å…³é”®ä¿¡æ¯æ‘˜è¦ï¼š**\n{extractive_result.content}"
            confidence = (extractive_result.confidence_score * 0.6 + 
                         abstractive_result.confidence_score * 0.4)
        else:
            # å¦åˆ™ä»¥æŠ½è±¡ä¸ºä¸»
            content = abstractive_result.content
            confidence = abstractive_result.confidence_score * 0.8
        
        # åˆå¹¶æ¥æº
        sources = list(set(extractive_result.sources + abstractive_result.sources))
        
        return GeneratedAnswer(
            content=content,
            confidence_score=confidence,
            sources=sources,
            generation_method="hybrid",
            quality_metrics={
                "relevance": extractive_result.quality_metrics.get("relevance", 0.7),
                "fluency": abstractive_result.quality_metrics.get("fluency", 0.8),
                "completeness": 0.85
            },
            metadata={"fusion_strategy": "weighted_combination"}
        )
    
    def _template_based_generation(self, context: GenerationContext) -> GeneratedAnswer:
        """åŸºäºæ¨¡æ¿çš„ç”Ÿæˆ"""
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æ¨¡æ¿
        query_type = self._detect_query_type(context.query)
        template = self.prompt_template.get_template(query_type, context)
        formatted_template = self.prompt_template.format_template(template, context)
        
        # ç”Ÿæˆç­”æ¡ˆ
        content = self._simulate_llm_generation(formatted_template, context)
        confidence = self._calculate_generation_confidence(content, context)
        
        sources = [doc.get('document_id', f'æ–‡æ¡£{i+1}') 
                  for i, doc in enumerate(context.retrieved_documents)]
        
        return GeneratedAnswer(
            content=content,
            confidence_score=confidence,
            sources=sources,
            generation_method="template_based",
            quality_metrics={"structure": 0.9, "completeness": 0.8},
            metadata={"template_type": query_type}
        )
    
    def _chain_of_thought_generation(self, context: GenerationContext) -> GeneratedAnswer:
        """æ€ç»´é“¾ç”Ÿæˆ"""
        
        # æ„å»ºæ€ç»´é“¾æç¤º
        cot_prompt = f"""è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒå¹¶å›ç­”é—®é¢˜ï¼š

é—®é¢˜ï¼š{context.query}

æ–‡æ¡£ä¿¡æ¯ï¼š
{self._format_documents_for_cot(context.retrieved_documents)}

æ€è€ƒæ­¥éª¤ï¼š
1. ç†è§£é—®é¢˜ï¼šé—®é¢˜è¦æ±‚ä»€ä¹ˆï¼Ÿ
2. åˆ†ææ–‡æ¡£ï¼šæ–‡æ¡£ä¸­æœ‰å“ªäº›ç›¸å…³ä¿¡æ¯ï¼Ÿ
3. é€»è¾‘æ¨ç†ï¼šå¦‚ä½•å°†ä¿¡æ¯ç»„ç»‡èµ·æ¥å›ç­”é—®é¢˜ï¼Ÿ
4. ç”Ÿæˆç­”æ¡ˆï¼šåŸºäºåˆ†æç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

è¯·æŒ‰ç…§ä¸Šè¿°æ­¥éª¤è¯¦ç»†æ€è€ƒå¹¶ç»™å‡ºç­”æ¡ˆã€‚"""
        
        # æ¨¡æ‹Ÿæ€ç»´é“¾ç”Ÿæˆ
        content = self._simulate_cot_generation(cot_prompt, context)
        confidence = self._calculate_generation_confidence(content, context)
        
        sources = [doc.get('document_id', f'æ–‡æ¡£{i+1}') 
                  for i, doc in enumerate(context.retrieved_documents)]
        
        return GeneratedAnswer(
            content=content,
            confidence_score=confidence,
            sources=sources,
            generation_method="chain_of_thought",
            quality_metrics={"reasoning": 0.9, "transparency": 0.95},
            metadata={"reasoning_steps": 4}
        )
    
    def _detect_query_type(self, query: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['ä»€ä¹ˆ', 'æ˜¯ä»€ä¹ˆ', 'å®šä¹‰', 'å«ä¹‰']):
            return "factual"
        elif any(word in query_lower for word in ['ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·', 'åˆ†æ']):
            return "analytical"
        elif any(word in query_lower for word in ['æ­¥éª¤', 'å¦‚ä½•åš', 'æ€ä¹ˆåš', 'æ•™ç¨‹']):
            return "procedural"
        elif any(word in query_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'åŒºåˆ«', 'ä¼˜ç¼ºç‚¹']):
            return "comparative"
        else:
            return "factual"
    
    def _format_documents_for_cot(self, documents: List[Dict[str, Any]]) -> str:
        """ä¸ºæ€ç»´é“¾æ ¼å¼åŒ–æ–‡æ¡£"""
        formatted = ""
        for i, doc in enumerate(documents, 1):
            content = doc.get('content', '')[:300] + "..."
            source = doc.get('document_id', f'æ–‡æ¡£{i}')
            formatted += f"æ–‡æ¡£{i}({source}): {content}\n\n"
        return formatted
    
    def _simulate_llm_generation(self, template: Dict[str, str], 
                                context: GenerationContext) -> str:
        """æ¨¡æ‹ŸLLMç”Ÿæˆï¼ˆåœ¨å®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨ï¼‰"""
        
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„æ¨¡æ‹Ÿç”Ÿæˆï¼Œå®é™…åº”ç”¨ä¸­ä¼šè°ƒç”¨GPTã€Claudeç­‰LLM
        query = context.query
        docs_count = len(context.retrieved_documents)
        
        if "ä»€ä¹ˆ" in query or "æ˜¯ä»€ä¹ˆ" in query:
            return f"åŸºäºæä¾›çš„{docs_count}ä¸ªæ–‡æ¡£ï¼Œ{query.replace('ä»€ä¹ˆæ˜¯', '').replace('æ˜¯ä»€ä¹ˆ', '')}æ˜¯ä¸€ä¸ªé‡è¦æ¦‚å¿µã€‚æ ¹æ®æ–‡æ¡£å†…å®¹åˆ†æï¼Œå®ƒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\n1. æ ¸å¿ƒå®šä¹‰å’ŒåŸºæœ¬æ¦‚å¿µ\n2. ä¸»è¦åº”ç”¨åœºæ™¯å’Œç”¨é€”\n3. ç›¸å…³æŠ€æœ¯ç‰¹æ€§å’Œä¼˜åŠ¿\n\nå…·ä½“æ¥è¯´ï¼Œæ–‡æ¡£ä¸­æåˆ°çš„å…³é”®ä¿¡æ¯è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªåœ¨ç›¸å…³é¢†åŸŸä¸­å¹¿æ³›åº”ç”¨çš„æŠ€æœ¯æˆ–æ¦‚å¿µã€‚"
        
        elif "å¦‚ä½•" in query or "æ€æ ·" in query:
            return f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œå…³äº{query}çš„æ–¹æ³•å¦‚ä¸‹ï¼š\n\n**æ­¥éª¤è¯´æ˜ï¼š**\n1. é¦–å…ˆéœ€è¦äº†è§£åŸºæœ¬æ¦‚å¿µå’Œå‡†å¤‡å·¥ä½œ\n2. æŒ‰ç…§æ–‡æ¡£ä¸­æè¿°çš„æµç¨‹è¿›è¡Œæ“ä½œ\n3. æ³¨æ„ç›¸å…³çš„æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ\n4. éªŒè¯ç»“æœå¹¶è¿›è¡Œå¿…è¦çš„è°ƒæ•´\n\n**æ³¨æ„äº‹é¡¹ï¼š**\n- ç¡®ä¿æ»¡è¶³å‰ç½®æ¡ä»¶\n- éµå¾ªæ–‡æ¡£ä¸­çš„å»ºè®®å’Œè§„èŒƒ\n- å¦‚é‡é—®é¢˜å¯å‚è€ƒæ–‡æ¡£ä¸­çš„æ•…éšœæ’é™¤éƒ¨åˆ†"
        
        elif "æ¯”è¾ƒ" in query or "åŒºåˆ«" in query:
            return f"åŸºäºæ–‡æ¡£åˆ†æï¼Œå…³äº{query}çš„æ¯”è¾ƒå¦‚ä¸‹ï¼š\n\n**ä¸»è¦åŒºåˆ«ï¼š**\n1. æŠ€æœ¯ç‰¹æ€§æ–¹é¢çš„ä¸åŒ\n2. åº”ç”¨åœºæ™¯çš„å·®å¼‚\n3. æ€§èƒ½å’Œæ•ˆç‡çš„å¯¹æ¯”\n\n**ä¼˜ç¼ºç‚¹åˆ†æï¼š**\n- å„è‡ªçš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯\n- å¯èƒ½å­˜åœ¨çš„é™åˆ¶å’Œæ³¨æ„äº‹é¡¹\n\n**é€‰æ‹©å»ºè®®ï¼š**\næ ¹æ®å…·ä½“éœ€æ±‚å’Œä½¿ç”¨åœºæ™¯ï¼Œå»ºè®®è€ƒè™‘ç›¸å…³å› ç´ ååšå‡ºé€‰æ‹©ã€‚"
        
        else:
            return f"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå…³äº{query}çš„ä¿¡æ¯å¦‚ä¸‹ï¼š\n\næ–‡æ¡£ä¸­åŒ…å«äº†ç›¸å…³çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸»è¦æ¶‰åŠæ ¸å¿ƒæ¦‚å¿µã€åº”ç”¨æ–¹æ³•å’Œå®è·µç»éªŒã€‚é€šè¿‡åˆ†æè¿™äº›å†…å®¹ï¼Œå¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š\n\n1. åŸºæœ¬æ¦‚å¿µå’Œå®šä¹‰æ¸…æ™°\n2. å®é™…åº”ç”¨ä»·å€¼æ˜ç¡®\n3. ç›¸å…³æŠ€æœ¯æ–¹æ¡ˆå¯è¡Œ\n\nå»ºè®®ç»“åˆå…·ä½“éœ€æ±‚å’Œå®é™…æƒ…å†µï¼Œå‚è€ƒæ–‡æ¡£ä¸­çš„æŒ‡å¯¼è¿›è¡Œå®è·µã€‚"
    
    def _simulate_cot_generation(self, prompt: str, context: GenerationContext) -> str:
        """æ¨¡æ‹Ÿæ€ç»´é“¾ç”Ÿæˆ"""
        
        query = context.query
        
        return f"""è®©æˆ‘æŒ‰æ­¥éª¤åˆ†æè¿™ä¸ªé—®é¢˜ï¼š

**1. ç†è§£é—®é¢˜ï¼š**
é—®é¢˜è¯¢é—®çš„æ˜¯ï¼š{query}
è¿™éœ€è¦æˆ‘åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®ã€æœ‰ç”¨çš„ç­”æ¡ˆã€‚

**2. åˆ†ææ–‡æ¡£ï¼š**
ä»æä¾›çš„{len(context.retrieved_documents)}ä¸ªæ–‡æ¡£ä¸­ï¼Œæˆ‘å‘ç°äº†ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
- æ–‡æ¡£åŒ…å«äº†ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®šä¹‰
- æœ‰å…·ä½“çš„å®æ–½æ–¹æ³•å’Œæ­¥éª¤è¯´æ˜
- æä¾›äº†å®é™…åº”ç”¨çš„æ¡ˆä¾‹å’Œç»éªŒ

**3. é€»è¾‘æ¨ç†ï¼š**
å°†è¿™äº›ä¿¡æ¯æ•´åˆèµ·æ¥ï¼Œæˆ‘å¯ä»¥ï¼š
- é¦–å…ˆè§£é‡Šæ ¸å¿ƒæ¦‚å¿µ
- ç„¶åè¯´æ˜å…·ä½“æ–¹æ³•
- æœ€åæä¾›å®é™…å»ºè®®

**4. ç”Ÿæˆç­”æ¡ˆï¼š**
åŸºäºä»¥ä¸Šåˆ†æï¼Œ{query}çš„ç­”æ¡ˆæ˜¯ï¼š

æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µ/æ–¹æ³•/æŠ€æœ¯ã€‚ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- æ ¸å¿ƒåŠŸèƒ½å’Œç”¨é€”æ˜ç¡®
- å®æ–½æ­¥éª¤ç›¸å¯¹æ¸…æ™°
- åœ¨å®é™…åº”ç”¨ä¸­æœ‰è‰¯å¥½æ•ˆæœ

å…·ä½“æ¥è¯´ï¼Œæ–‡æ¡£ä¸­æåˆ°çš„å…³é”®è¦ç‚¹ä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨æ—¶æ³¨æ„ç›¸å…³çš„æœ€ä½³å®è·µå’Œæ³¨æ„äº‹é¡¹ã€‚"""
    
    def _calculate_generation_confidence(self, content: str, 
                                       context: GenerationContext) -> float:
        """è®¡ç®—ç”Ÿæˆç½®ä¿¡åº¦"""
        
        factors = {
            'length': min(len(content) / 500, 1.0) * 0.2,  # é•¿åº¦å› å­
            'structure': 0.8 if '1.' in content or '**' in content else 0.6,  # ç»“æ„åŒ–ç¨‹åº¦
            'specificity': 0.9 if any(word in content for word in ['å…·ä½“', 'è¯¦ç»†', 'æ­¥éª¤']) else 0.7,
            'document_coverage': min(len(context.retrieved_documents) / 3, 1.0) * 0.3
        }
        
        confidence = sum(factors.values()) / len(factors)
        return min(confidence, 0.95)  # é™åˆ¶æœ€å¤§ç½®ä¿¡åº¦

class AnswerQualityController:
    """ç­”æ¡ˆè´¨é‡æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.quality_thresholds = {
            "confidence_min": 0.3,
            "length_min": 50,
            "length_max": 2000,
            "source_coverage_min": 0.5
        }
        
    def evaluate_answer(self, answer: GeneratedAnswer, 
                       context: GenerationContext) -> AnswerQuality:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        
        score = 0
        max_score = 0
        
        # ç½®ä¿¡åº¦è¯„ä¼°
        if answer.confidence_score >= 0.8:
            score += 3
        elif answer.confidence_score >= 0.6:
            score += 2
        elif answer.confidence_score >= 0.3:
            score += 1
        max_score += 3
        
        # é•¿åº¦è¯„ä¼°
        content_length = len(answer.content)
        if 200 <= content_length <= 1000:
            score += 2
        elif 100 <= content_length <= 1500:
            score += 1
        max_score += 2
        
        # æ¥æºè¦†ç›–åº¦è¯„ä¼°
        source_coverage = len(answer.sources) / max(len(context.retrieved_documents), 1)
        if source_coverage >= 0.8:
            score += 2
        elif source_coverage >= 0.5:
            score += 1
        max_score += 2
        
        # å†…å®¹è´¨é‡è¯„ä¼°
        quality_score = np.mean(list(answer.quality_metrics.values()))
        if quality_score >= 0.9:
            score += 3
        elif quality_score >= 0.7:
            score += 2
        elif quality_score >= 0.5:
            score += 1
        max_score += 3
        
        # è®¡ç®—æœ€ç»ˆè´¨é‡ç­‰çº§
        final_score = score / max_score
        
        if final_score >= 0.9:
            return AnswerQuality.EXCELLENT
        elif final_score >= 0.7:
            return AnswerQuality.GOOD
        elif final_score >= 0.5:
            return AnswerQuality.FAIR
        elif final_score >= 0.3:
            return AnswerQuality.POOR
        else:
            return AnswerQuality.INVALID
    
    def improve_answer(self, answer: GeneratedAnswer, 
                      quality: AnswerQuality, 
                      context: GenerationContext) -> GeneratedAnswer:
        """æ”¹è¿›ç­”æ¡ˆè´¨é‡"""
        
        if quality in [AnswerQuality.EXCELLENT, AnswerQuality.GOOD]:
            return answer
        
        improved_content = answer.content
        
        # å¦‚æœç­”æ¡ˆå¤ªçŸ­ï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
        if len(answer.content) < self.quality_thresholds["length_min"]:
            improved_content += "\n\n**è¡¥å……ä¿¡æ¯ï¼š**\n"
            improved_content += "å¦‚éœ€æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå»ºè®®æŸ¥é˜…ç›¸å…³æ–‡æ¡£æˆ–å’¨è¯¢ä¸“ä¸šäººå£«ã€‚"
        
        # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œæ·»åŠ ä¸ç¡®å®šæ€§è¯´æ˜
        if answer.confidence_score < self.quality_thresholds["confidence_min"]:
            improved_content = "**æ³¨æ„ï¼šä»¥ä¸‹ç­”æ¡ˆåŸºäºæœ‰é™ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯ã€‚**\n\n" + improved_content
        
        # å¦‚æœæ¥æºè¦†ç›–åº¦ä½ï¼Œæ·»åŠ æ¥æºè¯´æ˜
        source_coverage = len(answer.sources) / max(len(context.retrieved_documents), 1)
        if source_coverage < self.quality_thresholds["source_coverage_min"]:
            improved_content += f"\n\n**ä¿¡æ¯æ¥æºï¼š**åŸºäº{len(answer.sources)}ä¸ªç›¸å…³æ–‡æ¡£"
        
        return GeneratedAnswer(
            content=improved_content,
            confidence_score=min(answer.confidence_score + 0.1, 0.95),
            sources=answer.sources,
            generation_method=answer.generation_method + "_improved",
            quality_metrics=answer.quality_metrics,
            metadata={**answer.metadata, "improved": True}
        )

# ç”Ÿæˆç­–ç•¥ä¼˜åŒ–æ¼”ç¤º
def demonstrate_generation_optimization():
    """æ¼”ç¤ºç”Ÿæˆç­–ç•¥ä¼˜åŒ–"""
    
    print("ğŸ“ ç”Ÿæˆç­–ç•¥ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨å’Œè´¨é‡æ§åˆ¶å™¨
    generator = AnswerGenerator()
    quality_controller = AnswerQualityController()
    
    # å‡†å¤‡æµ‹è¯•ä¸Šä¸‹æ–‡
    test_contexts = [
        GenerationContext(
            query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            retrieved_documents=[
                {"document_id": "ml_intro", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"},
                {"document_id": "ml_types", "content": "æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚"}
            ],
            conversation_history=[],
            user_preferences={},
            domain="technology"
        ),
        
        GenerationContext(
            query="å¦‚ä½•å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Ÿ",
            retrieved_documents=[
                {"document_id": "nn_basics", "content": "ç¥ç»ç½‘ç»œç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶è¾“å…¥ï¼Œè®¡ç®—åŠ æƒå’Œï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°è¾“å‡ºã€‚"},
                {"document_id": "nn_training", "content": "ç¥ç»ç½‘ç»œè®­ç»ƒé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è°ƒæ•´æƒé‡ã€‚é¦–å…ˆå‰å‘ä¼ æ’­è®¡ç®—è¾“å‡ºï¼Œç„¶åè®¡ç®—æŸå¤±ï¼Œæœ€ååå‘ä¼ æ’­æ›´æ–°æƒé‡ã€‚"}
            ],
            conversation_history=[
                {"user": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ", "assistant": "ç¥ç»ç½‘ç»œæ˜¯æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒè¿æ¥çš„è®¡ç®—æ¨¡å‹ã€‚"}
            ],
            user_preferences={"detail_level": "high"},
            domain="technology"
        )
    ]
    
    # æµ‹è¯•ä¸åŒç”Ÿæˆç­–ç•¥
    strategies = [
        GenerationStrategy.EXTRACTIVE,
        GenerationStrategy.ABSTRACTIVE,
        GenerationStrategy.HYBRID,
        GenerationStrategy.CHAIN_OF_THOUGHT
    ]
    
    for i, context in enumerate(test_contexts, 1):
        print(f"\nğŸ” æµ‹è¯•æ¡ˆä¾‹ {i}: {context.query}")
        print("-" * 40)
        
        for strategy in strategies:
            print(f"\nğŸ“‹ ç­–ç•¥: {strategy.value}")
            
            # ç”Ÿæˆç­”æ¡ˆ
            start_time = time.time()
            answer = generator.generate_answer(context, strategy)
            generation_time = time.time() - start_time
            
            # è¯„ä¼°è´¨é‡
            quality = quality_controller.evaluate_answer(answer, context)
            
            print(f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.3f}ç§’")
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {answer.confidence_score:.3f}")
            print(f"ğŸ“Š è´¨é‡ç­‰çº§: {quality.value}")
            print(f"ğŸ“ ç”Ÿæˆæ–¹æ³•: {answer.generation_method}")
            print(f"ğŸ“š å¼•ç”¨æ¥æº: {', '.join(answer.sources)}")
            print(f"ğŸ“„ ç­”æ¡ˆå†…å®¹: {answer.content[:150]}...")
            
            # å¦‚æœè´¨é‡ä¸ä½³ï¼Œå°è¯•æ”¹è¿›
            if quality in [AnswerQuality.FAIR, AnswerQuality.POOR]:
                improved_answer = quality_controller.improve_answer(answer, quality, context)
                improved_quality = quality_controller.evaluate_answer(improved_answer, context)
                print(f"ğŸ”§ æ”¹è¿›åè´¨é‡: {improved_quality.value}")
    
    print("\nâœ… ç”Ÿæˆç­–ç•¥ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ")

# è¿è¡Œç”Ÿæˆç­–ç•¥ä¼˜åŒ–æ¼”ç¤º
demonstrate_generation_optimization()

print("\nğŸ“ ç”Ÿæˆç­–ç•¥ä¼˜åŒ–ç« èŠ‚å®Œæˆ")
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†RAGç³»ç»Ÿä¸­ç”Ÿæˆç­–ç•¥ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šç§ç”Ÿæˆç­–ç•¥ã€æç¤ºå·¥ç¨‹ã€ç­”æ¡ˆè´¨é‡æ§åˆ¶ç­‰å…³é”®æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯èƒ½å¤Ÿç¡®ä¿RAGç³»ç»Ÿè¾“å‡ºé«˜è´¨é‡ã€å¯ä¿¡èµ–çš„ç­”æ¡ˆã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†ç”Ÿæˆç­–ç•¥ä¼˜åŒ–çš„ç†è®ºå’Œå®è·µã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ¢è®¨RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°ï¼Œå»ºç«‹å®Œæ•´çš„è¯„ä¼°ä½“ç³»ã€‚*

---

## 29.6 RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°

### ğŸ¯ æ€§èƒ½ç›‘æ§ä¸­å¿ƒ

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒä¸­ï¼Œ**æ€§èƒ½ç›‘æ§ä¸­å¿ƒ**å°±åƒä¸€ä¸ªå…¨æ–¹ä½çš„è´¨é‡æ£€æµ‹éƒ¨é—¨ï¼Œå®ƒè´Ÿè´£ç›‘æ§æ•´ä¸ªRAGç³»ç»Ÿçš„è¿è¡ŒçŠ¶æ€ï¼Œè¯„ä¼°å„ä¸ªç¯èŠ‚çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚

### ğŸ“Š è¯„ä¼°ä½“ç³»æ¶æ„

RAGç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°éœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œï¼ŒåŒ…æ‹¬æ£€ç´¢æ€§èƒ½ã€ç”Ÿæˆè´¨é‡ã€ç³»ç»Ÿæ•ˆç‡ã€ç”¨æˆ·æ»¡æ„åº¦ç­‰ï¼Œå½¢æˆä¸€ä¸ªå…¨é¢çš„è¯„ä¼°ä½“ç³»ã€‚

```python
# RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°å®ç°
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
import time
import json
import math
from collections import defaultdict, Counter
import statistics
from datetime import datetime, timedelta

class MetricType(Enum):
    """è¯„ä¼°æŒ‡æ ‡ç±»å‹"""
    RETRIEVAL = "retrieval"           # æ£€ç´¢æŒ‡æ ‡
    GENERATION = "generation"         # ç”ŸæˆæŒ‡æ ‡
    EFFICIENCY = "efficiency"         # æ•ˆç‡æŒ‡æ ‡
    USER_EXPERIENCE = "user_exp"      # ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
    SYSTEM_HEALTH = "system_health"   # ç³»ç»Ÿå¥åº·æŒ‡æ ‡

class EvaluationLevel(Enum):
    """è¯„ä¼°ç­‰çº§"""
    EXCELLENT = "excellent"    # ä¼˜ç§€ (90-100)
    GOOD = "good"             # è‰¯å¥½ (80-89)
    SATISFACTORY = "satisfactory"  # æ»¡æ„ (70-79)
    NEEDS_IMPROVEMENT = "needs_improvement"  # éœ€è¦æ”¹è¿› (60-69)
    POOR = "poor"             # è¾ƒå·® (<60)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    metric_name: str
    score: float
    max_score: float
    level: EvaluationLevel
    details: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class SystemPerformanceReport:
    """ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š"""
    overall_score: float
    overall_level: EvaluationLevel
    retrieval_metrics: Dict[str, EvaluationResult]
    generation_metrics: Dict[str, EvaluationResult]
    efficiency_metrics: Dict[str, EvaluationResult]
    user_experience_metrics: Dict[str, EvaluationResult]
    recommendations: List[str]
    timestamp: datetime = field(default_factory=datetime.now)

class RetrievalEvaluator:
    """æ£€ç´¢æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluation_cache = {}
    
    def evaluate_precision(self, retrieved_docs: List[Dict], 
                          relevant_docs: List[str], 
                          k: int = 10) -> EvaluationResult:
        """è¯„ä¼°ç²¾ç¡®ç‡@K"""
        
        if not retrieved_docs:
            return EvaluationResult("precision@k", 0.0, 1.0, EvaluationLevel.POOR)
        
        # è·å–å‰Kä¸ªæ–‡æ¡£
        top_k_docs = retrieved_docs[:k]
        top_k_ids = [doc.get('document_id', '') for doc in top_k_docs]
        
        # è®¡ç®—ç²¾ç¡®ç‡
        relevant_retrieved = len(set(top_k_ids) & set(relevant_docs))
        precision = relevant_retrieved / len(top_k_docs) if top_k_docs else 0
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if precision >= 0.9:
            level = EvaluationLevel.EXCELLENT
        elif precision >= 0.8:
            level = EvaluationLevel.GOOD
        elif precision >= 0.7:
            level = EvaluationLevel.SATISFACTORY
        elif precision >= 0.6:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name=f"precision@{k}",
            score=precision,
            max_score=1.0,
            level=level,
            details={
                "retrieved_count": len(top_k_docs),
                "relevant_retrieved": relevant_retrieved,
                "total_relevant": len(relevant_docs)
            }
        )
    
    def evaluate_recall(self, retrieved_docs: List[Dict], 
                       relevant_docs: List[str], 
                       k: int = 10) -> EvaluationResult:
        """è¯„ä¼°å¬å›ç‡@K"""
        
        if not relevant_docs:
            return EvaluationResult("recall@k", 1.0, 1.0, EvaluationLevel.EXCELLENT)
        
        # è·å–å‰Kä¸ªæ–‡æ¡£
        top_k_docs = retrieved_docs[:k]
        top_k_ids = [doc.get('document_id', '') for doc in top_k_docs]
        
        # è®¡ç®—å¬å›ç‡
        relevant_retrieved = len(set(top_k_ids) & set(relevant_docs))
        recall = relevant_retrieved / len(relevant_docs)
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if recall >= 0.9:
            level = EvaluationLevel.EXCELLENT
        elif recall >= 0.8:
            level = EvaluationLevel.GOOD
        elif recall >= 0.7:
            level = EvaluationLevel.SATISFACTORY
        elif recall >= 0.6:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name=f"recall@{k}",
            score=recall,
            max_score=1.0,
            level=level,
            details={
                "retrieved_count": len(top_k_docs),
                "relevant_retrieved": relevant_retrieved,
                "total_relevant": len(relevant_docs)
            }
        )
    
    def evaluate_mrr(self, retrieved_docs: List[Dict], 
                     relevant_docs: List[str]) -> EvaluationResult:
        """è¯„ä¼°å¹³å‡å€’æ•°æ’å(MRR)"""
        
        if not retrieved_docs or not relevant_docs:
            return EvaluationResult("mrr", 0.0, 1.0, EvaluationLevel.POOR)
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®
        for i, doc in enumerate(retrieved_docs):
            doc_id = doc.get('document_id', '')
            if doc_id in relevant_docs:
                mrr = 1.0 / (i + 1)
                break
        else:
            mrr = 0.0
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if mrr >= 0.8:
            level = EvaluationLevel.EXCELLENT
        elif mrr >= 0.6:
            level = EvaluationLevel.GOOD
        elif mrr >= 0.4:
            level = EvaluationLevel.SATISFACTORY
        elif mrr >= 0.2:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="mrr",
            score=mrr,
            max_score=1.0,
            level=level,
            details={"first_relevant_rank": int(1/mrr) if mrr > 0 else -1}
        )
    
    def evaluate_ndcg(self, retrieved_docs: List[Dict], 
                      relevance_scores: Dict[str, float], 
                      k: int = 10) -> EvaluationResult:
        """è¯„ä¼°å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š(NDCG@K)"""
        
        if not retrieved_docs:
            return EvaluationResult("ndcg@k", 0.0, 1.0, EvaluationLevel.POOR)
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, doc in enumerate(retrieved_docs[:k]):
            doc_id = doc.get('document_id', '')
            relevance = relevance_scores.get(doc_id, 0.0)
            dcg += relevance / math.log2(i + 2)  # i+2 because log2(1) = 0
        
        # è®¡ç®—IDCG (ç†æƒ³DCG)
        ideal_relevances = sorted(relevance_scores.values(), reverse=True)[:k]
        idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevances))
        
        # è®¡ç®—NDCG
        ndcg = dcg / idcg if idcg > 0 else 0.0
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if ndcg >= 0.9:
            level = EvaluationLevel.EXCELLENT
        elif ndcg >= 0.8:
            level = EvaluationLevel.GOOD
        elif ndcg >= 0.7:
            level = EvaluationLevel.SATISFACTORY
        elif ndcg >= 0.6:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name=f"ndcg@{k}",
            score=ndcg,
            max_score=1.0,
            level=level,
            details={"dcg": dcg, "idcg": idcg}
        )

class GenerationEvaluator:
    """ç”Ÿæˆè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluation_cache = {}
    
    def evaluate_relevance(self, generated_answer: str, 
                          query: str, 
                          source_docs: List[str]) -> EvaluationResult:
        """è¯„ä¼°ç­”æ¡ˆç›¸å…³æ€§"""
        
        # ç®€åŒ–çš„ç›¸å…³æ€§è¯„ä¼°ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ï¼‰
        query_words = set(query.lower().split())
        answer_words = set(generated_answer.lower().split())
        source_words = set(' '.join(source_docs).lower().split())
        
        # è®¡ç®—æŸ¥è¯¢-ç­”æ¡ˆç›¸å…³æ€§
        query_answer_overlap = len(query_words & answer_words) / len(query_words) if query_words else 0
        
        # è®¡ç®—ç­”æ¡ˆ-æºæ–‡æ¡£ç›¸å…³æ€§
        answer_source_overlap = len(answer_words & source_words) / len(answer_words) if answer_words else 0
        
        # ç»¼åˆç›¸å…³æ€§åˆ†æ•°
        relevance_score = (query_answer_overlap * 0.6 + answer_source_overlap * 0.4)
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if relevance_score >= 0.8:
            level = EvaluationLevel.EXCELLENT
        elif relevance_score >= 0.7:
            level = EvaluationLevel.GOOD
        elif relevance_score >= 0.6:
            level = EvaluationLevel.SATISFACTORY
        elif relevance_score >= 0.5:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="relevance",
            score=relevance_score,
            max_score=1.0,
            level=level,
            details={
                "query_answer_overlap": query_answer_overlap,
                "answer_source_overlap": answer_source_overlap
            }
        )
    
    def evaluate_completeness(self, generated_answer: str, 
                            expected_aspects: List[str]) -> EvaluationResult:
        """è¯„ä¼°ç­”æ¡ˆå®Œæ•´æ€§"""
        
        answer_lower = generated_answer.lower()
        covered_aspects = 0
        
        for aspect in expected_aspects:
            if aspect.lower() in answer_lower:
                covered_aspects += 1
        
        completeness_score = covered_aspects / len(expected_aspects) if expected_aspects else 1.0
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if completeness_score >= 0.9:
            level = EvaluationLevel.EXCELLENT
        elif completeness_score >= 0.8:
            level = EvaluationLevel.GOOD
        elif completeness_score >= 0.7:
            level = EvaluationLevel.SATISFACTORY
        elif completeness_score >= 0.6:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="completeness",
            score=completeness_score,
            max_score=1.0,
            level=level,
            details={
                "covered_aspects": covered_aspects,
                "total_aspects": len(expected_aspects)
            }
        )
    
    def evaluate_coherence(self, generated_answer: str) -> EvaluationResult:
        """è¯„ä¼°ç­”æ¡ˆè¿è´¯æ€§"""
        
        # ç®€åŒ–çš„è¿è´¯æ€§è¯„ä¼°
        sentences = generated_answer.split('ã€‚')
        if len(sentences) <= 1:
            coherence_score = 1.0
        else:
            # æ£€æŸ¥å¥å­é—´çš„è¿æ¥è¯å’Œé€»è¾‘ç»“æ„
            coherence_indicators = ['å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'å¦å¤–', 'æ­¤å¤–', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'æ€»ä¹‹']
            
            indicator_count = sum(1 for sentence in sentences 
                                for indicator in coherence_indicators 
                                if indicator in sentence)
            
            # è®¡ç®—è¿è´¯æ€§åˆ†æ•°
            coherence_score = min(indicator_count / max(len(sentences) - 1, 1), 1.0)
            
            # è€ƒè™‘å¥å­é•¿åº¦çš„åˆç†æ€§
            avg_sentence_length = np.mean([len(s.strip()) for s in sentences if s.strip()])
            if 20 <= avg_sentence_length <= 100:
                coherence_score += 0.2
            
            coherence_score = min(coherence_score, 1.0)
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if coherence_score >= 0.8:
            level = EvaluationLevel.EXCELLENT
        elif coherence_score >= 0.7:
            level = EvaluationLevel.GOOD
        elif coherence_score >= 0.6:
            level = EvaluationLevel.SATISFACTORY
        elif coherence_score >= 0.5:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="coherence",
            score=coherence_score,
            max_score=1.0,
            level=level,
            details={"sentence_count": len(sentences)}
        )
    
    def evaluate_factual_accuracy(self, generated_answer: str, 
                                 verified_facts: List[Dict[str, Any]]) -> EvaluationResult:
        """è¯„ä¼°äº‹å®å‡†ç¡®æ€§"""
        
        if not verified_facts:
            return EvaluationResult("factual_accuracy", 0.8, 1.0, EvaluationLevel.GOOD)
        
        correct_facts = 0
        answer_lower = generated_answer.lower()
        
        for fact in verified_facts:
            fact_text = fact.get('text', '').lower()
            is_correct = fact.get('is_correct', True)
            
            if fact_text in answer_lower:
                if is_correct:
                    correct_facts += 1
                else:
                    correct_facts -= 1  # åŒ…å«é”™è¯¯äº‹å®è¦æ‰£åˆ†
        
        accuracy_score = max(correct_facts / len(verified_facts), 0.0) if verified_facts else 0.8
        accuracy_score = min(accuracy_score, 1.0)
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if accuracy_score >= 0.95:
            level = EvaluationLevel.EXCELLENT
        elif accuracy_score >= 0.85:
            level = EvaluationLevel.GOOD
        elif accuracy_score >= 0.75:
            level = EvaluationLevel.SATISFACTORY
        elif accuracy_score >= 0.65:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="factual_accuracy",
            score=accuracy_score,
            max_score=1.0,
            level=level,
            details={
                "correct_facts": max(correct_facts, 0),
                "total_facts": len(verified_facts)
            }
        )

class EfficiencyEvaluator:
    """æ•ˆç‡æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.performance_history = []
    
    def evaluate_response_time(self, response_times: List[float]) -> EvaluationResult:
        """è¯„ä¼°å“åº”æ—¶é—´"""
        
        if not response_times:
            return EvaluationResult("response_time", 0.0, 1.0, EvaluationLevel.POOR)
        
        avg_response_time = np.mean(response_times)
        p95_response_time = np.percentile(response_times, 95)
        
        # å“åº”æ—¶é—´è¯„åˆ† (è¶Šä½è¶Šå¥½)
        if avg_response_time <= 1.0:
            time_score = 1.0
            level = EvaluationLevel.EXCELLENT
        elif avg_response_time <= 3.0:
            time_score = 0.8
            level = EvaluationLevel.GOOD
        elif avg_response_time <= 5.0:
            time_score = 0.6
            level = EvaluationLevel.SATISFACTORY
        elif avg_response_time <= 10.0:
            time_score = 0.4
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            time_score = 0.2
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="response_time",
            score=time_score,
            max_score=1.0,
            level=level,
            details={
                "avg_response_time": avg_response_time,
                "p95_response_time": p95_response_time,
                "min_time": min(response_times),
                "max_time": max(response_times)
            }
        )
    
    def evaluate_throughput(self, request_count: int, 
                           time_period: float) -> EvaluationResult:
        """è¯„ä¼°ç³»ç»Ÿååé‡"""
        
        if time_period <= 0:
            return EvaluationResult("throughput", 0.0, 1.0, EvaluationLevel.POOR)
        
        throughput = request_count / time_period  # è¯·æ±‚/ç§’
        
        # ååé‡è¯„åˆ†
        if throughput >= 100:
            throughput_score = 1.0
            level = EvaluationLevel.EXCELLENT
        elif throughput >= 50:
            throughput_score = 0.8
            level = EvaluationLevel.GOOD
        elif throughput >= 20:
            throughput_score = 0.6
            level = EvaluationLevel.SATISFACTORY
        elif throughput >= 10:
            throughput_score = 0.4
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            throughput_score = 0.2
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="throughput",
            score=throughput_score,
            max_score=1.0,
            level=level,
            details={
                "requests_per_second": throughput,
                "total_requests": request_count,
                "time_period": time_period
            }
        )
    
    def evaluate_resource_usage(self, cpu_usage: float, 
                               memory_usage: float, 
                               disk_usage: float) -> EvaluationResult:
        """è¯„ä¼°èµ„æºä½¿ç”¨ç‡"""
        
        # ç»¼åˆèµ„æºä½¿ç”¨ç‡è¯„åˆ† (ä½¿ç”¨ç‡é€‚ä¸­æœ€å¥½)
        cpu_score = 1.0 - abs(cpu_usage - 0.7) / 0.7 if cpu_usage <= 0.9 else 0.1
        memory_score = 1.0 - abs(memory_usage - 0.6) / 0.6 if memory_usage <= 0.8 else 0.1
        disk_score = 1.0 - abs(disk_usage - 0.5) / 0.5 if disk_usage <= 0.7 else 0.1
        
        resource_score = (cpu_score + memory_score + disk_score) / 3
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if resource_score >= 0.8:
            level = EvaluationLevel.EXCELLENT
        elif resource_score >= 0.7:
            level = EvaluationLevel.GOOD
        elif resource_score >= 0.6:
            level = EvaluationLevel.SATISFACTORY
        elif resource_score >= 0.5:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="resource_usage",
            score=resource_score,
            max_score=1.0,
            level=level,
            details={
                "cpu_usage": cpu_usage,
                "memory_usage": memory_usage,
                "disk_usage": disk_usage,
                "cpu_score": cpu_score,
                "memory_score": memory_score,
                "disk_score": disk_score
            }
        )

class UserExperienceEvaluator:
    """ç”¨æˆ·ä½“éªŒè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.feedback_history = []
    
    def evaluate_user_satisfaction(self, satisfaction_ratings: List[int]) -> EvaluationResult:
        """è¯„ä¼°ç”¨æˆ·æ»¡æ„åº¦ (1-5åˆ†åˆ¶)"""
        
        if not satisfaction_ratings:
            return EvaluationResult("user_satisfaction", 0.0, 1.0, EvaluationLevel.POOR)
        
        avg_satisfaction = np.mean(satisfaction_ratings)
        satisfaction_score = (avg_satisfaction - 1) / 4  # è½¬æ¢ä¸º0-1åˆ†åˆ¶
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if satisfaction_score >= 0.8:
            level = EvaluationLevel.EXCELLENT
        elif satisfaction_score >= 0.7:
            level = EvaluationLevel.GOOD
        elif satisfaction_score >= 0.6:
            level = EvaluationLevel.SATISFACTORY
        elif satisfaction_score >= 0.5:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="user_satisfaction",
            score=satisfaction_score,
            max_score=1.0,
            level=level,
            details={
                "avg_rating": avg_satisfaction,
                "rating_distribution": Counter(satisfaction_ratings),
                "total_ratings": len(satisfaction_ratings)
            }
        )
    
    def evaluate_answer_usefulness(self, usefulness_feedback: List[bool]) -> EvaluationResult:
        """è¯„ä¼°ç­”æ¡ˆæœ‰ç”¨æ€§"""
        
        if not usefulness_feedback:
            return EvaluationResult("answer_usefulness", 0.0, 1.0, EvaluationLevel.POOR)
        
        usefulness_score = sum(usefulness_feedback) / len(usefulness_feedback)
        
        # ç¡®å®šè¯„ä¼°ç­‰çº§
        if usefulness_score >= 0.9:
            level = EvaluationLevel.EXCELLENT
        elif usefulness_score >= 0.8:
            level = EvaluationLevel.GOOD
        elif usefulness_score >= 0.7:
            level = EvaluationLevel.SATISFACTORY
        elif usefulness_score >= 0.6:
            level = EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            level = EvaluationLevel.POOR
        
        return EvaluationResult(
            metric_name="answer_usefulness",
            score=usefulness_score,
            max_score=1.0,
            level=level,
            details={
                "useful_answers": sum(usefulness_feedback),
                "total_answers": len(usefulness_feedback),
                "usefulness_rate": usefulness_score
            }
        )

class RAGSystemEvaluator:
    """RAGç³»ç»Ÿç»¼åˆè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.retrieval_evaluator = RetrievalEvaluator()
        self.generation_evaluator = GenerationEvaluator()
        self.efficiency_evaluator = EfficiencyEvaluator()
        self.user_experience_evaluator = UserExperienceEvaluator()
        
        # å„ç±»æŒ‡æ ‡çš„æƒé‡
        self.metric_weights = {
            MetricType.RETRIEVAL: 0.3,
            MetricType.GENERATION: 0.3,
            MetricType.EFFICIENCY: 0.2,
            MetricType.USER_EXPERIENCE: 0.2
        }
    
    def comprehensive_evaluation(self, evaluation_data: Dict[str, Any]) -> SystemPerformanceReport:
        """ç»¼åˆè¯„ä¼°RAGç³»ç»Ÿæ€§èƒ½"""
        
        # æ£€ç´¢æ€§èƒ½è¯„ä¼°
        retrieval_metrics = self._evaluate_retrieval_performance(evaluation_data)
        
        # ç”Ÿæˆè´¨é‡è¯„ä¼°
        generation_metrics = self._evaluate_generation_quality(evaluation_data)
        
        # æ•ˆç‡æ€§èƒ½è¯„ä¼°
        efficiency_metrics = self._evaluate_efficiency_performance(evaluation_data)
        
        # ç”¨æˆ·ä½“éªŒè¯„ä¼°
        user_experience_metrics = self._evaluate_user_experience(evaluation_data)
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        overall_score = self._calculate_overall_score({
            MetricType.RETRIEVAL: retrieval_metrics,
            MetricType.GENERATION: generation_metrics,
            MetricType.EFFICIENCY: efficiency_metrics,
            MetricType.USER_EXPERIENCE: user_experience_metrics
        })
        
        # ç¡®å®šæ€»ä½“ç­‰çº§
        overall_level = self._determine_overall_level(overall_score)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations({
            MetricType.RETRIEVAL: retrieval_metrics,
            MetricType.GENERATION: generation_metrics,
            MetricType.EFFICIENCY: efficiency_metrics,
            MetricType.USER_EXPERIENCE: user_experience_metrics
        })
        
        return SystemPerformanceReport(
            overall_score=overall_score,
            overall_level=overall_level,
            retrieval_metrics=retrieval_metrics,
            generation_metrics=generation_metrics,
            efficiency_metrics=efficiency_metrics,
            user_experience_metrics=user_experience_metrics,
            recommendations=recommendations
        )
    
    def _evaluate_retrieval_performance(self, data: Dict[str, Any]) -> Dict[str, EvaluationResult]:
        """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
        
        metrics = {}
        
        # ç²¾ç¡®ç‡è¯„ä¼°
        if 'retrieved_docs' in data and 'relevant_docs' in data:
            metrics['precision'] = self.retrieval_evaluator.evaluate_precision(
                data['retrieved_docs'], data['relevant_docs'], k=10
            )
            
            # å¬å›ç‡è¯„ä¼°
            metrics['recall'] = self.retrieval_evaluator.evaluate_recall(
                data['retrieved_docs'], data['relevant_docs'], k=10
            )
            
            # MRRè¯„ä¼°
            metrics['mrr'] = self.retrieval_evaluator.evaluate_mrr(
                data['retrieved_docs'], data['relevant_docs']
            )
        
        # NDCGè¯„ä¼°
        if 'retrieved_docs' in data and 'relevance_scores' in data:
            metrics['ndcg'] = self.retrieval_evaluator.evaluate_ndcg(
                data['retrieved_docs'], data['relevance_scores'], k=10
            )
        
        return metrics
    
    def _evaluate_generation_quality(self, data: Dict[str, Any]) -> Dict[str, EvaluationResult]:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        
        metrics = {}
        
        if 'generated_answer' in data:
            answer = data['generated_answer']
            
            # ç›¸å…³æ€§è¯„ä¼°
            if 'query' in data and 'source_docs' in data:
                metrics['relevance'] = self.generation_evaluator.evaluate_relevance(
                    answer, data['query'], data['source_docs']
                )
            
            # å®Œæ•´æ€§è¯„ä¼°
            if 'expected_aspects' in data:
                metrics['completeness'] = self.generation_evaluator.evaluate_completeness(
                    answer, data['expected_aspects']
                )
            
            # è¿è´¯æ€§è¯„ä¼°
            metrics['coherence'] = self.generation_evaluator.evaluate_coherence(answer)
            
            # äº‹å®å‡†ç¡®æ€§è¯„ä¼°
            if 'verified_facts' in data:
                metrics['factual_accuracy'] = self.generation_evaluator.evaluate_factual_accuracy(
                    answer, data['verified_facts']
                )
        
        return metrics
    
    def _evaluate_efficiency_performance(self, data: Dict[str, Any]) -> Dict[str, EvaluationResult]:
        """è¯„ä¼°æ•ˆç‡æ€§èƒ½"""
        
        metrics = {}
        
        # å“åº”æ—¶é—´è¯„ä¼°
        if 'response_times' in data:
            metrics['response_time'] = self.efficiency_evaluator.evaluate_response_time(
                data['response_times']
            )
        
        # ååé‡è¯„ä¼°
        if 'request_count' in data and 'time_period' in data:
            metrics['throughput'] = self.efficiency_evaluator.evaluate_throughput(
                data['request_count'], data['time_period']
            )
        
        # èµ„æºä½¿ç”¨ç‡è¯„ä¼°
        if all(key in data for key in ['cpu_usage', 'memory_usage', 'disk_usage']):
            metrics['resource_usage'] = self.efficiency_evaluator.evaluate_resource_usage(
                data['cpu_usage'], data['memory_usage'], data['disk_usage']
            )
        
        return metrics
    
    def _evaluate_user_experience(self, data: Dict[str, Any]) -> Dict[str, EvaluationResult]:
        """è¯„ä¼°ç”¨æˆ·ä½“éªŒ"""
        
        metrics = {}
        
        # ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°
        if 'satisfaction_ratings' in data:
            metrics['user_satisfaction'] = self.user_experience_evaluator.evaluate_user_satisfaction(
                data['satisfaction_ratings']
            )
        
        # ç­”æ¡ˆæœ‰ç”¨æ€§è¯„ä¼°
        if 'usefulness_feedback' in data:
            metrics['answer_usefulness'] = self.user_experience_evaluator.evaluate_answer_usefulness(
                data['usefulness_feedback']
            )
        
        return metrics
    
    def _calculate_overall_score(self, all_metrics: Dict[MetricType, Dict[str, EvaluationResult]]) -> float:
        """è®¡ç®—æ€»ä½“åˆ†æ•°"""
        
        weighted_scores = []
        
        for metric_type, metrics in all_metrics.items():
            if metrics:
                type_avg_score = np.mean([result.score for result in metrics.values()])
                weighted_score = type_avg_score * self.metric_weights[metric_type]
                weighted_scores.append(weighted_score)
        
        return sum(weighted_scores) if weighted_scores else 0.0
    
    def _determine_overall_level(self, overall_score: float) -> EvaluationLevel:
        """ç¡®å®šæ€»ä½“è¯„ä¼°ç­‰çº§"""
        
        if overall_score >= 0.9:
            return EvaluationLevel.EXCELLENT
        elif overall_score >= 0.8:
            return EvaluationLevel.GOOD
        elif overall_score >= 0.7:
            return EvaluationLevel.SATISFACTORY
        elif overall_score >= 0.6:
            return EvaluationLevel.NEEDS_IMPROVEMENT
        else:
            return EvaluationLevel.POOR
    
    def _generate_recommendations(self, all_metrics: Dict[MetricType, Dict[str, EvaluationResult]]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        # æ£€æŸ¥å„ç±»æŒ‡æ ‡çš„é—®é¢˜
        for metric_type, metrics in all_metrics.items():
            poor_metrics = [name for name, result in metrics.items() 
                          if result.level in [EvaluationLevel.POOR, EvaluationLevel.NEEDS_IMPROVEMENT]]
            
            if poor_metrics:
                if metric_type == MetricType.RETRIEVAL:
                    recommendations.append(f"æ£€ç´¢æ€§èƒ½éœ€è¦æ”¹è¿›ï¼š{', '.join(poor_metrics)}ã€‚å»ºè®®ä¼˜åŒ–ç´¢å¼•ç»“æ„ã€è°ƒæ•´æ£€ç´¢ç®—æ³•å‚æ•°ã€‚")
                elif metric_type == MetricType.GENERATION:
                    recommendations.append(f"ç”Ÿæˆè´¨é‡éœ€è¦æ”¹è¿›ï¼š{', '.join(poor_metrics)}ã€‚å»ºè®®ä¼˜åŒ–æç¤ºå·¥ç¨‹ã€å¢å¼ºè´¨é‡æ§åˆ¶ã€‚")
                elif metric_type == MetricType.EFFICIENCY:
                    recommendations.append(f"ç³»ç»Ÿæ•ˆç‡éœ€è¦æ”¹è¿›ï¼š{', '.join(poor_metrics)}ã€‚å»ºè®®ä¼˜åŒ–ç³»ç»Ÿæ¶æ„ã€å¢åŠ ç¼“å­˜æœºåˆ¶ã€‚")
                elif metric_type == MetricType.USER_EXPERIENCE:
                    recommendations.append(f"ç”¨æˆ·ä½“éªŒéœ€è¦æ”¹è¿›ï¼š{', '.join(poor_metrics)}ã€‚å»ºè®®æ”¶é›†æ›´å¤šç”¨æˆ·åé¦ˆã€ä¼˜åŒ–äº¤äº’è®¾è®¡ã€‚")
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾é—®é¢˜ï¼Œç»™å‡ºä¼˜åŒ–å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ŒæŒç»­ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚")
        
        return recommendations

# æ€§èƒ½è¯„ä¼°æ¼”ç¤º
def demonstrate_rag_evaluation():
    """æ¼”ç¤ºRAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°"""
    
    print("ğŸ“Š RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGSystemEvaluator()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    evaluation_data = {
        # æ£€ç´¢ç›¸å…³æ•°æ®
        'retrieved_docs': [
            {'document_id': 'doc1', 'content': 'æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯'},
            {'document_id': 'doc2', 'content': 'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ'},
            {'document_id': 'doc3', 'content': 'è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬'},
            {'document_id': 'doc4', 'content': 'è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒ'},
            {'document_id': 'doc5', 'content': 'å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±å­¦ä¹ '}
        ],
        'relevant_docs': ['doc1', 'doc2', 'doc5'],
        'relevance_scores': {
            'doc1': 0.9, 'doc2': 0.8, 'doc3': 0.3, 'doc4': 0.2, 'doc5': 0.7
        },
        
        # ç”Ÿæˆç›¸å…³æ•°æ®
        'generated_answer': '''æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸»è¦ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚
        æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯å¦ä¸€ç§é‡è¦çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’è·å¾—å¥–åŠ±æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
        è¿™äº›æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚''',
        'query': 'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
        'source_docs': ['æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯', 'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ'],
        'expected_aspects': ['å®šä¹‰', 'ç®—æ³•', 'åº”ç”¨', 'åˆ†ç±»'],
        'verified_facts': [
            {'text': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯', 'is_correct': True},
            {'text': 'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ', 'is_correct': True}
        ],
        
        # æ•ˆç‡ç›¸å…³æ•°æ®
        'response_times': [1.2, 1.5, 0.8, 2.1, 1.0, 1.3, 0.9, 1.7, 1.4, 1.1],
        'request_count': 1000,
        'time_period': 60.0,  # 60ç§’
        'cpu_usage': 0.65,
        'memory_usage': 0.55,
        'disk_usage': 0.45,
        
        # ç”¨æˆ·ä½“éªŒç›¸å…³æ•°æ®
        'satisfaction_ratings': [4, 5, 4, 3, 5, 4, 4, 5, 3, 4],
        'usefulness_feedback': [True, True, False, True, True, True, False, True, True, True]
    }
    
    # æ‰§è¡Œç»¼åˆè¯„ä¼°
    print("ğŸ” æ­£åœ¨æ‰§è¡Œç»¼åˆæ€§èƒ½è¯„ä¼°...")
    start_time = time.time()
    
    report = evaluator.comprehensive_evaluation(evaluation_data)
    
    evaluation_time = time.time() - start_time
    
    # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
    print(f"\nğŸ“‹ è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ (è€—æ—¶: {evaluation_time:.3f}ç§’)")
    print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {report.overall_score:.3f}")
    print(f"ğŸ† æ€»ä½“ç­‰çº§: {report.overall_level.value}")
    
    # è¯¦ç»†æŒ‡æ ‡å±•ç¤º
    print(f"\nğŸ” æ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
    for name, result in report.retrieval_metrics.items():
        print(f"  {name}: {result.score:.3f} ({result.level.value})")
    
    print(f"\nğŸ“ ç”Ÿæˆè´¨é‡æŒ‡æ ‡:")
    for name, result in report.generation_metrics.items():
        print(f"  {name}: {result.score:.3f} ({result.level.value})")
    
    print(f"\nâš¡ æ•ˆç‡æ€§èƒ½æŒ‡æ ‡:")
    for name, result in report.efficiency_metrics.items():
        print(f"  {name}: {result.score:.3f} ({result.level.value})")
    
    print(f"\nğŸ‘¥ ç”¨æˆ·ä½“éªŒæŒ‡æ ‡:")
    for name, result in report.user_experience_metrics.items():
        print(f"  {name}: {result.score:.3f} ({result.level.value})")
    
    # æ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    for i, recommendation in enumerate(report.recommendations, 1):
        print(f"  {i}. {recommendation}")
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    print(f"\nğŸ“ˆ ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–æŠ¥å‘Š...")
    generate_performance_visualization(report)
    
    print("\nâœ… RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°æ¼”ç¤ºå®Œæˆ")

def generate_performance_visualization(report: SystemPerformanceReport):
    """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–æŠ¥å‘Š"""
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡æ•°æ®
    all_metrics = {}
    all_metrics.update(report.retrieval_metrics)
    all_metrics.update(report.generation_metrics)
    all_metrics.update(report.efficiency_metrics)
    all_metrics.update(report.user_experience_metrics)
    
    if not all_metrics:
        print("  æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")
        return
    
    # åˆ›å»ºæ€§èƒ½é›·è¾¾å›¾æ•°æ®
    metrics_data = {
        'metric': list(all_metrics.keys()),
        'score': [result.score for result in all_metrics.values()],
        'level': [result.level.value for result in all_metrics.values()]
    }
    
    print(f"  ğŸ“Š æ€§èƒ½é›·è¾¾å›¾æ•°æ®:")
    for metric, score, level in zip(metrics_data['metric'], metrics_data['score'], metrics_data['level']):
        print(f"    {metric}: {score:.3f} ({level})")
    
    # åˆ†ç±»ç»Ÿè®¡
    level_counts = Counter(metrics_data['level'])
    print(f"\n  ğŸ“ˆ æ€§èƒ½ç­‰çº§åˆ†å¸ƒ:")
    for level, count in level_counts.items():
        print(f"    {level}: {count}ä¸ªæŒ‡æ ‡")
    
    print(f"  ğŸ’¾ å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ (æ¨¡æ‹Ÿ)")

# è¿è¡Œæ€§èƒ½è¯„ä¼°æ¼”ç¤º
demonstrate_rag_evaluation()

print("\nğŸ“Š RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°ç« èŠ‚å®Œæˆ")
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†å®Œæ•´çš„RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°ä½“ç³»ï¼ŒåŒ…æ‹¬æ£€ç´¢æ€§èƒ½ã€ç”Ÿæˆè´¨é‡ã€ç³»ç»Ÿæ•ˆç‡ã€ç”¨æˆ·ä½“éªŒç­‰å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºRAGç³»ç»Ÿçš„æŒç»­ä¼˜åŒ–æä¾›äº†ç§‘å­¦ä¾æ®ã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å­¦ä¹ äº†RAGç³»ç»Ÿæ€§èƒ½è¯„ä¼°çš„ç†è®ºå’Œå®è·µã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ¢è®¨ä¼ä¸šçº§RAGç³»ç»Ÿå®æˆ˜ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„çŸ¥è¯†é—®ç­”ç³»ç»Ÿã€‚*

---

## 29.7 ä¼ä¸šçº§RAGç³»ç»Ÿå®æˆ˜

### ğŸ¯ æ™ºèƒ½å®¢æœçŸ¥è¯†åº“ç³»ç»Ÿ

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§RAGç³»ç»Ÿâ€”â€”**æ™ºèƒ½å®¢æœçŸ¥è¯†åº“ç³»ç»Ÿ**ã€‚è¿™ä¸ªç³»ç»Ÿå°±åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„å®¢æœä¸“å®¶ï¼Œèƒ½å¤ŸåŸºäºå…¬å¸çš„çŸ¥è¯†åº“æ–‡æ¡£ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€åŠæ—¶çš„é—®é¢˜è§£ç­”ã€‚

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

æˆ‘ä»¬çš„æ™ºèƒ½å®¢æœç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…å«æ–‡æ¡£ç®¡ç†ã€å‘é‡æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ã€ç”¨æˆ·ç•Œé¢ç­‰æ ¸å¿ƒæ¨¡å—ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼å’Œå®æ—¶æ›´æ–°ã€‚

```python
# ä¼ä¸šçº§RAGç³»ç»Ÿå®Œæ•´å®ç°
import os
import json
import sqlite3
import hashlib
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field, asdict
from abc import ABC, abstractmethod
from enum import Enum
import threading
import queue
import time
import re
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DocumentType(Enum):
    """æ–‡æ¡£ç±»å‹æšä¸¾"""
    FAQ = "faq"                    # å¸¸è§é—®é¢˜
    MANUAL = "manual"              # æ“ä½œæ‰‹å†Œ
    POLICY = "policy"              # æ”¿ç­–æ–‡æ¡£
    PRODUCT_INFO = "product_info"  # äº§å“ä¿¡æ¯
    TROUBLESHOOTING = "troubleshooting"  # æ•…éšœæ’é™¤
    GENERAL = "general"            # é€šç”¨æ–‡æ¡£

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹æšä¸¾"""
    FACTUAL = "factual"           # äº‹å®æŸ¥è¯¢
    PROCEDURAL = "procedural"     # è¿‡ç¨‹æŸ¥è¯¢
    TROUBLESHOOTING = "troubleshooting"  # æ•…éšœæ’é™¤
    COMPARISON = "comparison"     # æ¯”è¾ƒæŸ¥è¯¢
    GENERAL = "general"           # é€šç”¨æŸ¥è¯¢

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    id: str
    title: str
    content: str
    doc_type: DocumentType
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    version: int = 1
    tags: List[str] = field(default_factory=list)

@dataclass
class QueryRequest:
    """æŸ¥è¯¢è¯·æ±‚"""
    query: str
    user_id: str
    session_id: str
    query_type: Optional[QueryType] = None
    context: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class QueryResponse:
    """æŸ¥è¯¢å“åº”"""
    answer: str
    confidence: float
    sources: List[Dict[str, Any]]
    query_type: QueryType
    response_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "documents.db"):
        self.db_path = db_path
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    content TEXT NOT NULL,
                    doc_type TEXT NOT NULL,
                    source TEXT NOT NULL,
                    metadata TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    version INTEGER,
                    tags TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS document_chunks (
                    id TEXT PRIMARY KEY,
                    document_id TEXT,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB,
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_doc_type ON documents(doc_type);
                CREATE INDEX IF NOT EXISTS idx_doc_source ON documents(source);
                CREATE INDEX IF NOT EXISTS idx_chunk_doc ON document_chunks(document_id);
            """)
    
    def add_document(self, document: Document) -> bool:
        """æ·»åŠ æ–‡æ¡£"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO documents 
                    (id, title, content, doc_type, source, metadata, created_at, updated_at, version, tags)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    document.id,
                    document.title,
                    document.content,
                    document.doc_type.value,
                    document.source,
                    json.dumps(document.metadata),
                    document.created_at,
                    document.updated_at,
                    document.version,
                    json.dumps(document.tags)
                ))
            
            logger.info(f"æ–‡æ¡£å·²æ·»åŠ : {document.id}")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def get_document(self, doc_id: str) -> Optional[Document]:
        """è·å–æ–‡æ¡£"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT * FROM documents WHERE id = ?", (doc_id,)
                )
                row = cursor.fetchone()
                
                if row:
                    return Document(
                        id=row[0],
                        title=row[1],
                        content=row[2],
                        doc_type=DocumentType(row[3]),
                        source=row[4],
                        metadata=json.loads(row[5] or '{}'),
                        created_at=datetime.fromisoformat(row[6]),
                        updated_at=datetime.fromisoformat(row[7]),
                        version=row[8],
                        tags=json.loads(row[9] or '[]')
                    )
                    
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
        
        return None
    
    def search_documents(self, query: str, doc_type: Optional[DocumentType] = None, 
                        limit: int = 10) -> List[Document]:
        """æœç´¢æ–‡æ¡£"""
        try:
            sql = "SELECT * FROM documents WHERE content LIKE ?"
            params = [f"%{query}%"]
            
            if doc_type:
                sql += " AND doc_type = ?"
                params.append(doc_type.value)
            
            sql += f" ORDER BY updated_at DESC LIMIT {limit}"
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(sql, params)
                rows = cursor.fetchall()
                
                documents = []
                for row in rows:
                    documents.append(Document(
                        id=row[0],
                        title=row[1],
                        content=row[2],
                        doc_type=DocumentType(row[3]),
                        source=row[4],
                        metadata=json.loads(row[5] or '{}'),
                        created_at=datetime.fromisoformat(row[6]),
                        updated_at=datetime.fromisoformat(row[7]),
                        version=row[8],
                        tags=json.loads(row[9] or '[]')
                    ))
                
                return documents
                
        except Exception as e:
            logger.error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def update_document(self, document: Document) -> bool:
        """æ›´æ–°æ–‡æ¡£"""
        document.updated_at = datetime.now()
        document.version += 1
        return self.add_document(document)
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # åˆ é™¤æ–‡æ¡£å—
                conn.execute("DELETE FROM document_chunks WHERE document_id = ?", (doc_id,))
                # åˆ é™¤æ–‡æ¡£
                conn.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
            
            logger.info(f"æ–‡æ¡£å·²åˆ é™¤: {doc_id}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False

class EnterpriseVectorDatabase:
    """ä¼ä¸šçº§å‘é‡æ•°æ®åº“"""
    
    def __init__(self, db_path: str = "vectors.db", dimension: int = 384):
        self.db_path = db_path
        self.dimension = dimension
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS vectors (
                    id TEXT PRIMARY KEY,
                    document_id TEXT,
                    chunk_id TEXT,
                    content TEXT,
                    vector BLOB,
                    metadata TEXT,
                    created_at TIMESTAMP
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_doc_id ON vectors(document_id);
                CREATE INDEX IF NOT EXISTS idx_chunk_id ON vectors(chunk_id);
            """)
    
    def add_vector(self, vector_id: str, document_id: str, chunk_id: str,
                  content: str, vector: List[float], metadata: Dict[str, Any] = None) -> bool:
        """æ·»åŠ å‘é‡"""
        try:
            vector_blob = json.dumps(vector).encode()
            metadata_json = json.dumps(metadata or {})
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO vectors 
                    (id, document_id, chunk_id, content, vector, metadata, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    vector_id, document_id, chunk_id, content, 
                    vector_blob, metadata_json, datetime.now()
                ))
            
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ å‘é‡å¤±è´¥: {e}")
            return False
    
    def search_vectors(self, query_vector: List[float], top_k: int = 10,
                      document_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼å‘é‡"""
        try:
            sql = "SELECT * FROM vectors"
            params = []
            
            if document_ids:
                placeholders = ','.join(['?' for _ in document_ids])
                sql += f" WHERE document_id IN ({placeholders})"
                params.extend(document_ids)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(sql, params)
                rows = cursor.fetchall()
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                results = []
                for row in rows:
                    vector = json.loads(row[4].decode())
                    similarity = self._cosine_similarity(query_vector, vector)
                    
                    results.append({
                        'id': row[0],
                        'document_id': row[1],
                        'chunk_id': row[2],
                        'content': row[3],
                        'similarity': similarity,
                        'metadata': json.loads(row[5])
                    })
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰Kä¸ª
                results.sort(key=lambda x: x['similarity'], reverse=True)
                return results[:top_k]
                
        except Exception as e:
            logger.error(f"æœç´¢å‘é‡å¤±è´¥: {e}")
            return []
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        import math
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(a * a for a in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def delete_vectors_by_document(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM vectors WHERE document_id = ?", (document_id,))
            
            logger.info(f"å·²åˆ é™¤æ–‡æ¡£å‘é‡: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤å‘é‡å¤±è´¥: {e}")
            return False

class QueryClassifier:
    """æŸ¥è¯¢åˆ†ç±»å™¨"""
    
    def __init__(self):
        # æŸ¥è¯¢ç±»å‹çš„å…³é”®è¯æ¨¡å¼
        self.patterns = {
            QueryType.FACTUAL: [
                r'ä»€ä¹ˆæ˜¯', r'å®šä¹‰', r'å«ä¹‰', r'æ„æ€', r'è§£é‡Š',
                r'ä»‹ç»', r'æ¦‚å¿µ', r'åŸºæœ¬', r'åŸç†'
            ],
            QueryType.PROCEDURAL: [
                r'å¦‚ä½•', r'æ€æ ·', r'æ€ä¹ˆ', r'æ­¥éª¤', r'æ–¹æ³•',
                r'æµç¨‹', r'è¿‡ç¨‹', r'æ“ä½œ', r'æ•™ç¨‹', r'æŒ‡å—'
            ],
            QueryType.TROUBLESHOOTING: [
                r'é—®é¢˜', r'æ•…éšœ', r'é”™è¯¯', r'å¼‚å¸¸', r'å¤±è´¥',
                r'ä¸èƒ½', r'æ— æ³•', r'ä¸ºä»€ä¹ˆ', r'è§£å†³', r'ä¿®å¤'
            ],
            QueryType.COMPARISON: [
                r'æ¯”è¾ƒ', r'å¯¹æ¯”', r'åŒºåˆ«', r'å·®å¼‚', r'ä¼˜ç¼ºç‚¹',
                r'é€‰æ‹©', r'æ¨è', r'å“ªä¸ª', r'æ›´å¥½'
            ]
        }
    
    def classify_query(self, query: str) -> QueryType:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„åŒ¹é…åˆ†æ•°
        type_scores = {}
        
        for query_type, patterns in self.patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    score += 1
            type_scores[query_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if any(type_scores.values()):
            return max(type_scores, key=type_scores.get)
        else:
            return QueryType.GENERAL

class EnterpriseRAGSystem:
    """ä¼ä¸šçº§RAGç³»ç»Ÿ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._default_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.document_manager = DocumentManager(self.config['db_path'])
        self.vector_db = EnterpriseVectorDatabase(
            self.config['vector_db_path'], 
            self.config['embedding_dimension']
        )
        self.query_classifier = QueryClassifier()
        
        # æŸ¥è¯¢å†å²å’Œç¼“å­˜
        self.query_history = []
        self.response_cache = {}
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            'total_queries': 0,
            'avg_response_time': 0.0,
            'cache_hit_rate': 0.0,
            'user_satisfaction': 0.0
        }
        
        logger.info("ä¼ä¸šçº§RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _default_config(self) -> Dict[str, Any]:
        """é»˜è®¤é…ç½®"""
        return {
            'db_path': 'enterprise_docs.db',
            'vector_db_path': 'enterprise_vectors.db',
            'embedding_dimension': 384,
            'max_chunk_size': 500,
            'chunk_overlap': 50,
            'retrieval_top_k': 5,
            'cache_ttl': 3600,  # ç¼“å­˜1å°æ—¶
            'enable_logging': True,
            'enable_monitoring': True
        }
    
    def add_knowledge_base(self, file_path: str, doc_type: DocumentType,
                          metadata: Dict[str, Any] = None) -> bool:
        """æ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ›å»ºæ–‡æ¡£
            doc_id = hashlib.md5(f"{file_path}_{datetime.now()}".encode()).hexdigest()
            document = Document(
                id=doc_id,
                title=Path(file_path).stem,
                content=content,
                doc_type=doc_type,
                source=file_path,
                metadata=metadata or {}
            )
            
            # æ·»åŠ åˆ°æ–‡æ¡£ç®¡ç†å™¨
            if not self.document_manager.add_document(document):
                return False
            
            # å¤„ç†æ–‡æ¡£å¹¶æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self._process_document_for_retrieval(document)
            
            logger.info(f"çŸ¥è¯†åº“æ–‡æ¡£å·²æ·»åŠ : {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def _process_document_for_retrieval(self, document: Document):
        """å¤„ç†æ–‡æ¡£ç”¨äºæ£€ç´¢"""
        # æ–‡æ¡£åˆ†å—
        chunks = self._chunk_document(document.content)
        
        # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
        for i, chunk in enumerate(chunks):
            chunk_id = f"{document.id}_chunk_{i}"
            
            # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡ï¼‰
            embedding = self._generate_embedding(chunk)
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.vector_db.add_vector(
                vector_id=chunk_id,
                document_id=document.id,
                chunk_id=chunk_id,
                content=chunk,
                vector=embedding,
                metadata={
                    'doc_type': document.doc_type.value,
                    'source': document.source,
                    'chunk_index': i
                }
            )
    
    def _chunk_document(self, content: str) -> List[str]:
        """æ–‡æ¡£åˆ†å—"""
        max_size = self.config['max_chunk_size']
        overlap = self.config['chunk_overlap']
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # å¦‚æœæ·»åŠ è¿™ä¸ªå¥å­ä¼šè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
            if len(current_chunk) + len(sentence) > max_size and current_chunk:
                chunks.append(current_chunk)
                
                # é‡å å¤„ç†ï¼šä¿ç•™æœ€åä¸€éƒ¨åˆ†å†…å®¹
                if len(current_chunk) > overlap:
                    current_chunk = current_chunk[-overlap:] + sentence
                else:
                    current_chunk = sentence
            else:
                current_chunk += sentence + "ã€‚"
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹
        import random
        random.seed(hash(text) % (2**32))
        return [random.random() for _ in range(self.config['embedding_dimension'])]
    
    def query(self, request: QueryRequest) -> QueryResponse:
        """å¤„ç†æŸ¥è¯¢è¯·æ±‚"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(request.query)
            if cache_key in self.response_cache:
                cached_response = self.response_cache[cache_key]
                if self._is_cache_valid(cached_response):
                    logger.info(f"ç¼“å­˜å‘½ä¸­: {request.query[:50]}...")
                    return cached_response
            
            # åˆ†ç±»æŸ¥è¯¢ç±»å‹
            if not request.query_type:
                request.query_type = self.query_classifier.classify_query(request.query)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = self._retrieve_documents(request)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = self._generate_answer(request, relevant_docs)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(request, relevant_docs, answer)
            
            # åˆ›å»ºå“åº”
            response_time = time.time() - start_time
            response = QueryResponse(
                answer=answer,
                confidence=confidence,
                sources=relevant_docs,
                query_type=request.query_type,
                response_time=response_time,
                metadata={
                    'retrieval_count': len(relevant_docs),
                    'user_id': request.user_id,
                    'session_id': request.session_id
                }
            )
            
            # ç¼“å­˜å“åº”
            self.response_cache[cache_key] = response
            
            # è®°å½•æŸ¥è¯¢å†å²
            self.query_history.append({
                'request': request,
                'response': response,
                'timestamp': datetime.now()
            })
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(response_time)
            
            logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆ: {request.query[:50]}... (è€—æ—¶: {response_time:.3f}s)")
            return response
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            
            # è¿”å›é”™è¯¯å“åº”
            return QueryResponse(
                answer="æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚",
                confidence=0.0,
                sources=[],
                query_type=request.query_type or QueryType.GENERAL,
                response_time=time.time() - start_time,
                metadata={'error': str(e)}
            )
    
    def _retrieve_documents(self, request: QueryRequest) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self._generate_embedding(request.query)
        
        # å‘é‡æ£€ç´¢
        vector_results = self.vector_db.search_vectors(
            query_vector, 
            top_k=self.config['retrieval_top_k']
        )
        
        # æ–‡æœ¬æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        text_results = self.document_manager.search_documents(
            request.query, 
            limit=self.config['retrieval_top_k']
        )
        
        # åˆå¹¶å’Œé‡æ’åºç»“æœ
        combined_results = self._combine_retrieval_results(vector_results, text_results)
        
        return combined_results
    
    def _combine_retrieval_results(self, vector_results: List[Dict], 
                                  text_results: List[Document]) -> List[Dict[str, Any]]:
        """åˆå¹¶æ£€ç´¢ç»“æœ"""
        combined = []
        seen_docs = set()
        
        # æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
        for result in vector_results:
            if result['document_id'] not in seen_docs:
                combined.append({
                    'document_id': result['document_id'],
                    'content': result['content'],
                    'score': result['similarity'],
                    'source_type': 'vector',
                    'metadata': result['metadata']
                })
                seen_docs.add(result['document_id'])
        
        # æ·»åŠ æ–‡æœ¬æ£€ç´¢ç»“æœ
        for doc in text_results:
            if doc.id not in seen_docs:
                combined.append({
                    'document_id': doc.id,
                    'content': doc.content[:500] + "...",  # æˆªå–å‰500å­—ç¬¦
                    'score': 0.5,  # é»˜è®¤åˆ†æ•°
                    'source_type': 'text',
                    'metadata': {
                        'title': doc.title,
                        'doc_type': doc.doc_type.value,
                        'source': doc.source
                    }
                })
                seen_docs.add(doc.id)
        
        # æŒ‰åˆ†æ•°æ’åº
        combined.sort(key=lambda x: x['score'], reverse=True)
        
        return combined[:self.config['retrieval_top_k']]
    
    def _generate_answer(self, request: QueryRequest, 
                        relevant_docs: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        
        if not relevant_docs:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚"
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ç”Ÿæˆç­–ç•¥
        if request.query_type == QueryType.FACTUAL:
            return self._generate_factual_answer(request.query, relevant_docs)
        elif request.query_type == QueryType.PROCEDURAL:
            return self._generate_procedural_answer(request.query, relevant_docs)
        elif request.query_type == QueryType.TROUBLESHOOTING:
            return self._generate_troubleshooting_answer(request.query, relevant_docs)
        elif request.query_type == QueryType.COMPARISON:
            return self._generate_comparison_answer(request.query, relevant_docs)
        else:
            return self._generate_general_answer(request.query, relevant_docs)
    
    def _generate_factual_answer(self, query: str, docs: List[Dict]) -> str:
        """ç”Ÿæˆäº‹å®æ€§ç­”æ¡ˆ"""
        # æå–æœ€ç›¸å…³çš„ä¿¡æ¯
        top_content = docs[0]['content'] if docs else ""
        
        return f"""åŸºäºæˆ‘ä»¬çš„çŸ¥è¯†åº“ï¼Œå…³äºæ‚¨è¯¢é—®çš„é—®é¢˜ï¼š

{self._extract_key_information(query, top_content)}

**ä¿¡æ¯æ¥æºï¼š**
{self._format_sources(docs[:2])}

å¦‚éœ€æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“æƒ³äº†è§£å“ªä¸ªæ–¹é¢ã€‚"""
    
    def _generate_procedural_answer(self, query: str, docs: List[Dict]) -> str:
        """ç”Ÿæˆè¿‡ç¨‹æ€§ç­”æ¡ˆ"""
        return f"""æ ¹æ®æˆ‘ä»¬çš„æ“ä½œæŒ‡å—ï¼Œå…³äº"{query}"çš„æ­¥éª¤å¦‚ä¸‹ï¼š

{self._extract_procedure_steps(docs)}

**æ³¨æ„äº‹é¡¹ï¼š**
- è¯·æŒ‰ç…§æ­¥éª¤é¡ºåºæ‰§è¡Œ
- å¦‚é‡é—®é¢˜è¯·åŠæ—¶è”ç³»æŠ€æœ¯æ”¯æŒ
- æ“ä½œå‰è¯·ç¡®ä¿æ»¡è¶³å‰ç½®æ¡ä»¶

**å‚è€ƒèµ„æ–™ï¼š**
{self._format_sources(docs[:2])}"""
    
    def _generate_troubleshooting_answer(self, query: str, docs: List[Dict]) -> str:
        """ç”Ÿæˆæ•…éšœæ’é™¤ç­”æ¡ˆ"""
        return f"""é’ˆå¯¹æ‚¨é‡åˆ°çš„é—®é¢˜ï¼Œå»ºè®®æŒ‰ä»¥ä¸‹æ–¹å¼æ’æŸ¥ï¼š

**é—®é¢˜åˆ†æï¼š**
{self._extract_problem_analysis(query, docs)}

**è§£å†³æ–¹æ¡ˆï¼š**
{self._extract_solutions(docs)}

**å¦‚æœé—®é¢˜ä»æœªè§£å†³ï¼š**
- è¯·æ£€æŸ¥ç³»ç»Ÿæ—¥å¿—
- è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ
- æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

**å‚è€ƒæ–‡æ¡£ï¼š**
{self._format_sources(docs[:2])}"""
    
    def _generate_comparison_answer(self, query: str, docs: List[Dict]) -> str:
        """ç”Ÿæˆæ¯”è¾ƒæ€§ç­”æ¡ˆ"""
        return f"""å…³äºæ‚¨è¯¢é—®çš„æ¯”è¾ƒé—®é¢˜ï¼ŒåŸºäºæˆ‘ä»¬çš„èµ„æ–™åˆ†æï¼š

**ä¸»è¦åŒºåˆ«ï¼š**
{self._extract_comparisons(docs)}

**é€‰æ‹©å»ºè®®ï¼š**
æ ¹æ®ä¸åŒéœ€æ±‚åœºæ™¯ï¼Œå»ºè®®è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- ä½¿ç”¨åœºæ™¯å’Œéœ€æ±‚
- æŠ€æœ¯è¦æ±‚å’Œé™åˆ¶
- æˆæœ¬å’Œç»´æŠ¤è€ƒè™‘

**è¯¦ç»†ä¿¡æ¯ï¼š**
{self._format_sources(docs[:3])}"""
    
    def _generate_general_answer(self, query: str, docs: List[Dict]) -> str:
        """ç”Ÿæˆé€šç”¨ç­”æ¡ˆ"""
        if not docs:
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. æŸ¥çœ‹æˆ‘ä»¬çš„å¸¸è§é—®é¢˜\n3. è”ç³»å®¢æœäººå‘˜"
        
        return f"""æ ¹æ®ç›¸å…³èµ„æ–™ï¼Œå…³äºæ‚¨çš„é—®é¢˜ï¼š

{self._extract_relevant_information(query, docs)}

**ç›¸å…³èµ„æºï¼š**
{self._format_sources(docs[:2])}

å¦‚éœ€æ›´å…·ä½“çš„å¸®åŠ©ï¼Œè¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚"""
    
    def _extract_key_information(self, query: str, content: str) -> str:
        """æå–å…³é”®ä¿¡æ¯"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›å†…å®¹çš„å‰200å­—ç¬¦
        return content[:200] + "..." if len(content) > 200 else content
    
    def _extract_procedure_steps(self, docs: List[Dict]) -> str:
        """æå–æ“ä½œæ­¥éª¤"""
        steps = []
        for i, doc in enumerate(docs[:3], 1):
            content = doc['content'][:150]
            steps.append(f"{i}. {content}...")
        
        return "\n".join(steps) if steps else "æœªæ‰¾åˆ°å…·ä½“æ“ä½œæ­¥éª¤ã€‚"
    
    def _extract_problem_analysis(self, query: str, docs: List[Dict]) -> str:
        """æå–é—®é¢˜åˆ†æ"""
        if docs:
            return docs[0]['content'][:200] + "..."
        return "éœ€è¦æ›´å¤šä¿¡æ¯æ¥åˆ†æå…·ä½“é—®é¢˜ã€‚"
    
    def _extract_solutions(self, docs: List[Dict]) -> str:
        """æå–è§£å†³æ–¹æ¡ˆ"""
        solutions = []
        for i, doc in enumerate(docs[:2], 1):
            content = doc['content'][:100]
            solutions.append(f"æ–¹æ¡ˆ{i}: {content}...")
        
        return "\n".join(solutions) if solutions else "æš‚æ— æ ‡å‡†è§£å†³æ–¹æ¡ˆã€‚"
    
    def _extract_comparisons(self, docs: List[Dict]) -> str:
        """æå–æ¯”è¾ƒä¿¡æ¯"""
        if len(docs) >= 2:
            return f"é€‰é¡¹A: {docs[0]['content'][:100]}...\né€‰é¡¹B: {docs[1]['content'][:100]}..."
        return "éœ€è¦æ›´å¤šä¿¡æ¯è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚"
    
    def _extract_relevant_information(self, query: str, docs: List[Dict]) -> str:
        """æå–ç›¸å…³ä¿¡æ¯"""
        if docs:
            return docs[0]['content'][:300] + "..."
        return "æš‚æ— ç›¸å…³ä¿¡æ¯ã€‚"
    
    def _format_sources(self, docs: List[Dict]) -> str:
        """æ ¼å¼åŒ–ä¿¡æ¯æ¥æº"""
        sources = []
        for i, doc in enumerate(docs, 1):
            title = doc.get('metadata', {}).get('title', f"æ–‡æ¡£{i}")
            doc_type = doc.get('metadata', {}).get('doc_type', 'general')
            sources.append(f"- {title} ({doc_type})")
        
        return "\n".join(sources) if sources else "- å†…éƒ¨çŸ¥è¯†åº“"
    
    def _calculate_confidence(self, request: QueryRequest, 
                            relevant_docs: List[Dict], answer: str) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        if not relevant_docs:
            return 0.1
        
        # åŸºäºæ£€ç´¢ç»“æœè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        avg_score = sum(doc.get('score', 0) for doc in relevant_docs) / len(relevant_docs)
        
        # åŸºäºç­”æ¡ˆé•¿åº¦è°ƒæ•´
        answer_length_factor = min(len(answer) / 200, 1.0)
        
        # åŸºäºæŸ¥è¯¢ç±»å‹è°ƒæ•´
        type_confidence = {
            QueryType.FACTUAL: 0.9,
            QueryType.PROCEDURAL: 0.8,
            QueryType.TROUBLESHOOTING: 0.7,
            QueryType.COMPARISON: 0.8,
            QueryType.GENERAL: 0.6
        }
        
        base_confidence = type_confidence.get(request.query_type, 0.5)
        
        # ç»¼åˆè®¡ç®—
        confidence = (avg_score * 0.4 + answer_length_factor * 0.3 + base_confidence * 0.3)
        
        return min(confidence, 0.95)  # é™åˆ¶æœ€å¤§ç½®ä¿¡åº¦
    
    def _get_cache_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(query.lower().encode()).hexdigest()
    
    def _is_cache_valid(self, response: QueryResponse) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        ttl = self.config['cache_ttl']
        return (datetime.now() - response.timestamp).seconds < ttl
    
    def _update_performance_metrics(self, response_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_queries'] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total = self.performance_metrics['total_queries']
        current_avg = self.performance_metrics['avg_response_time']
        self.performance_metrics['avg_response_time'] = (
            (current_avg * (total - 1) + response_time) / total
        )
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'performance_metrics': self.performance_metrics,
            'cache_size': len(self.response_cache),
            'query_history_size': len(self.query_history),
            'system_uptime': datetime.now().isoformat(),
            'config': self.config
        }

# ä¼ä¸šçº§RAGç³»ç»Ÿæ¼”ç¤º
def demonstrate_enterprise_rag():
    """æ¼”ç¤ºä¼ä¸šçº§RAGç³»ç»Ÿ"""
    
    print("ğŸ¢ ä¼ä¸šçº§RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = EnterpriseRAGSystem()
    
    # æ¨¡æ‹Ÿæ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£
    print("\nğŸ“š æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    sample_docs = [
        {
            'content': """äº§å“ä½¿ç”¨æ‰‹å†Œ

æˆ‘ä»¬çš„æ™ºèƒ½å®¢æœç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºAIæŠ€æœ¯çš„å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨é—®ç­”ï¼šåŸºäºçŸ¥è¯†åº“è‡ªåŠ¨å›ç­”å®¢æˆ·é—®é¢˜
2. æ™ºèƒ½è·¯ç”±ï¼šå°†å¤æ‚é—®é¢˜è½¬æ¥ç»™äººå·¥å®¢æœ
3. æ•°æ®åˆ†æï¼šåˆ†æå®¢æˆ·å’¨è¯¢æ¨¡å¼å’Œæ»¡æ„åº¦

ç³»ç»Ÿæ”¯æŒå¤šç§é›†æˆæ–¹å¼ï¼ŒåŒ…æ‹¬APIæ¥å£ã€Webæ’ä»¶å’Œç§»åŠ¨åº”ç”¨ã€‚""",
            'doc_type': DocumentType.MANUAL,
            'title': 'æ™ºèƒ½å®¢æœç³»ç»Ÿä½¿ç”¨æ‰‹å†Œ'
        },
        {
            'content': """å¸¸è§é—®é¢˜è§£ç­”

Q: å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ
A: åœ¨ç™»å½•é¡µé¢ç‚¹å‡»"å¿˜è®°å¯†ç "ï¼Œè¾“å…¥æ³¨å†Œé‚®ç®±ï¼Œç³»ç»Ÿä¼šå‘é€é‡ç½®é“¾æ¥ã€‚

Q: ç³»ç»Ÿæ”¯æŒå“ªäº›æµè§ˆå™¨ï¼Ÿ
A: æ”¯æŒChromeã€Firefoxã€Safariå’ŒEdgeæµè§ˆå™¨çš„æœ€æ–°ç‰ˆæœ¬ã€‚

Q: å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ
A: å¯ä»¥é€šè¿‡é‚®ä»¶support@company.comæˆ–ç”µè¯400-123-4567è”ç³»æˆ‘ä»¬ã€‚

Q: æ•°æ®æ˜¯å¦å®‰å…¨ï¼Ÿ
A: æˆ‘ä»¬é‡‡ç”¨ä¼ä¸šçº§åŠ å¯†æŠ€æœ¯ï¼Œç¡®ä¿æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚""",
            'doc_type': DocumentType.FAQ,
            'title': 'å¸¸è§é—®é¢˜è§£ç­”'
        },
        {
            'content': """æ•…éšœæ’é™¤æŒ‡å—

å½“ç³»ç»Ÿå‡ºç°é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ’æŸ¥ï¼š

1. ç½‘ç»œè¿æ¥é—®é¢˜
   - æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
   - å°è¯•åˆ·æ–°é¡µé¢æˆ–é‡æ–°ç™»å½•
   - æ¸…é™¤æµè§ˆå™¨ç¼“å­˜å’ŒCookie

2. ç™»å½•é—®é¢˜
   - ç¡®è®¤ç”¨æˆ·åå’Œå¯†ç æ­£ç¡®
   - æ£€æŸ¥è´¦æˆ·æ˜¯å¦è¢«é”å®š
   - å°è¯•å¯†ç é‡ç½®åŠŸèƒ½

3. åŠŸèƒ½å¼‚å¸¸
   - ç¡®è®¤æµè§ˆå™¨ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ
   - ç¦ç”¨æµè§ˆå™¨æ’ä»¶åé‡è¯•
   - è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ

å¦‚æœé—®é¢˜ä»æœªè§£å†³ï¼Œè¯·æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ“ä½œæ­¥éª¤ã€‚""",
            'doc_type': DocumentType.TROUBLESHOOTING,
            'title': 'æ•…éšœæ’é™¤æŒ‡å—'
        }
    ]
    
    # æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    for i, doc_data in enumerate(sample_docs):
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = f"temp_doc_{i}.txt"
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(doc_data['content'])
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        success = rag_system.add_knowledge_base(
            temp_file, 
            doc_data['doc_type'],
            {'title': doc_data['title']}
        )
        
        if success:
            print(f"  âœ… å·²æ·»åŠ : {doc_data['title']}")
        else:
            print(f"  âŒ æ·»åŠ å¤±è´¥: {doc_data['title']}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_file)
    
    print(f"\nğŸ“Š çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…±æ·»åŠ  {len(sample_docs)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•æŸ¥è¯¢
    print(f"\nğŸ” å¼€å§‹æµ‹è¯•æŸ¥è¯¢...")
    
    test_queries = [
        "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
        "ç³»ç»Ÿæ”¯æŒå“ªäº›æµè§ˆå™¨ï¼Ÿ",
        "æ™ºèƒ½å®¢æœç³»ç»Ÿæœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ",
        "é‡åˆ°ç™»å½•é—®é¢˜æ€ä¹ˆåŠï¼Ÿ",
        "å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ"
    ]
    
    for i, query_text in enumerate(test_queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i}: {query_text} ---")
        
        # åˆ›å»ºæŸ¥è¯¢è¯·æ±‚
        request = QueryRequest(
            query=query_text,
            user_id=f"user_{i}",
            session_id=f"session_{i}"
        )
        
        # å¤„ç†æŸ¥è¯¢
        start_time = time.time()
        response = rag_system.query(request)
        query_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ¯ æŸ¥è¯¢ç±»å‹: {response.query_type.value}")
        print(f"â±ï¸ å“åº”æ—¶é—´: {query_time:.3f}ç§’")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {response.confidence:.3f}")
        print(f"ğŸ“ ç­”æ¡ˆ:")
        print(response.answer)
        print(f"ğŸ“š ä¿¡æ¯æ¥æº: {len(response.sources)}ä¸ªæ–‡æ¡£")
        
        # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
        import random
        satisfaction = random.choice([3, 4, 5])  # æ¨¡æ‹Ÿç”¨æˆ·æ»¡æ„åº¦
        print(f"ğŸ‘¤ ç”¨æˆ·æ»¡æ„åº¦: {satisfaction}/5")
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    print(f"\nğŸ“ˆ ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š:")
    status = rag_system.get_system_status()
    
    print(f"  æ€»æŸ¥è¯¢æ•°: {status['performance_metrics']['total_queries']}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {status['performance_metrics']['avg_response_time']:.3f}ç§’")
    print(f"  ç¼“å­˜å¤§å°: {status['cache_size']}")
    print(f"  æŸ¥è¯¢å†å²: {status['query_history_size']}æ¡")
    
    print("\nâœ… ä¼ä¸šçº§RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆ")

# è¿è¡Œä¼ä¸šçº§RAGç³»ç»Ÿæ¼”ç¤º
demonstrate_enterprise_rag()

print("\nğŸ¢ ä¼ä¸šçº§RAGç³»ç»Ÿå®æˆ˜ç« èŠ‚å®Œæˆ")
```

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§RAGç³»ç»Ÿï¼ŒåŒ…æ‹¬æ–‡æ¡£ç®¡ç†ã€å‘é‡æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ã€æ€§èƒ½ç›‘æ§ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œå±•ç¤ºäº†RAGæŠ€æœ¯åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

---

*æœ¬èŠ‚æˆ‘ä»¬å®Œæˆäº†ä¼ä¸šçº§RAGç³»ç»Ÿçš„å®æˆ˜å¼€å‘ã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†è¿›è¡Œç« èŠ‚æ€»ç»“ï¼Œå›é¡¾RAGæŠ€æœ¯çš„æ ¸å¿ƒè¦ç‚¹å’Œåº”ç”¨å‰æ™¯ã€‚*

---

## 29.8 ç« èŠ‚æ€»ç»“ä¸æ€è€ƒ

### ğŸ¯ å­¦ä¹ æˆæœå›é¡¾

æ­å–œï¼æˆ‘ä»¬å·²ç»å®Œæˆäº†RAGæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„å®Œæ•´å­¦ä¹ ä¹‹æ—…ã€‚åœ¨è¿™ä¸ª"çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒ"ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºåŸºç¡€åˆ°å®æˆ˜åº”ç”¨ï¼Œç³»ç»Ÿæ€§åœ°æŒæ¡äº†RAGæŠ€æœ¯çš„æ ¸å¿ƒç²¾é«“ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

#### 1. RAGç³»ç»Ÿæ¶æ„ä¸åŸç†
- **æ ¸å¿ƒæ¦‚å¿µ**ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“ä¸ç”Ÿæˆæ¨¡å‹
- **æŠ€æœ¯æ¶æ„**ï¼šæ–‡æ¡£å¤„ç†â†’å‘é‡åŒ–â†’æ£€ç´¢â†’ç”Ÿæˆçš„å®Œæ•´æµç¨‹
- **å…³é”®ä¼˜åŠ¿**ï¼šå®æ—¶çŸ¥è¯†æ›´æ–°ã€é™ä½å¹»è§‰ã€æå‡ç­”æ¡ˆå‡†ç¡®æ€§

#### 2. æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–
- **æ–‡æ¡£å¤„ç†**ï¼šæ”¯æŒå¤šæ ¼å¼æ–‡æ¡£çš„ç»Ÿä¸€å¤„ç†æ¡†æ¶
- **åˆ†å—ç­–ç•¥**ï¼šå›ºå®šå¤§å°åˆ†å—ã€è¯­ä¹‰åˆ†å—ã€é‡å åˆ†å—
- **å‘é‡åŒ–æŠ€æœ¯**ï¼šTF-IDFã€Word2Vecã€TransformeråµŒå…¥
- **è´¨é‡è¯„ä¼°**ï¼šå‘é‡è´¨é‡è¯„ä¼°ä¸ä¼˜åŒ–ç­–ç•¥

#### 3. å‘é‡æ•°æ®åº“æŠ€æœ¯
- **ç´¢å¼•æŠ€æœ¯**ï¼šFlatç´¢å¼•ã€HNSWç´¢å¼•ã€IVFç´¢å¼•
- **å­˜å‚¨ä¼˜åŒ–**ï¼šå‹ç¼©ç®—æ³•ã€åˆ†å¸ƒå¼å­˜å‚¨ã€ç¼“å­˜æœºåˆ¶
- **æŸ¥è¯¢ä¼˜åŒ–**ï¼šè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€å¤šçº§ç´¢å¼•
- **æ€§èƒ½è°ƒä¼˜**ï¼šæ‰¹é‡æ“ä½œã€å¹¶è¡Œå¤„ç†ã€å†…å­˜ç®¡ç†

#### 4. æ£€ç´¢ç­–ç•¥ä¼˜åŒ–
- **æŸ¥è¯¢åˆ†æ**ï¼šæ„å›¾è¯†åˆ«ã€æŸ¥è¯¢æ‰©å±•ã€æŸ¥è¯¢é‡å†™
- **æ£€ç´¢ç­–ç•¥**ï¼šè¯­ä¹‰æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ã€æ··åˆæ£€ç´¢
- **æ’åºç®—æ³•**ï¼šç›¸ä¼¼åº¦è®¡ç®—ã€å­¦ä¹ æ’åºã€å¤šæ ·åŒ–æ’åº
- **æ€§èƒ½è¯„ä¼°**ï¼šç²¾ç¡®ç‡ã€å¬å›ç‡ã€NDCGã€MRR

#### 5. ç”Ÿæˆç­–ç•¥ä¼˜åŒ–
- **ç”Ÿæˆæ–¹æ³•**ï¼šæŠ½å–å¼ã€æŠ½è±¡å¼ã€æ··åˆå¼ã€æ¨¡æ¿å¼
- **æç¤ºå·¥ç¨‹**ï¼šä¸Šä¸‹æ–‡ç®¡ç†ã€è§’è‰²è®¾å®šã€è¾“å‡ºæ ¼å¼æ§åˆ¶
- **è´¨é‡æ§åˆ¶**ï¼šç­”æ¡ˆéªŒè¯ã€ä¸€è‡´æ€§æ£€æŸ¥ã€ç½®ä¿¡åº¦è¯„ä¼°
- **é”™è¯¯å¤„ç†**ï¼šé™çº§ç­–ç•¥ã€é”™è¯¯æ¢å¤ã€ç”¨æˆ·åé¦ˆ

#### 6. ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
- **è¯„ä¼°ç»´åº¦**ï¼šæ£€ç´¢æ€§èƒ½ã€ç”Ÿæˆè´¨é‡ã€ç³»ç»Ÿæ•ˆç‡ã€ç”¨æˆ·ä½“éªŒ
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€è¿è´¯æ€§ã€å“åº”æ—¶é—´
- **ç›‘æ§ä½“ç³»**ï¼šå®æ—¶ç›‘æ§ã€æ€§èƒ½åˆ†æã€å¼‚å¸¸æ£€æµ‹
- **ä¼˜åŒ–å»ºè®®**ï¼šåŸºäºæ•°æ®é©±åŠ¨çš„æŒç»­æ”¹è¿›

#### 7. ä¼ä¸šçº§åº”ç”¨å®æˆ˜
- **ç³»ç»Ÿæ¶æ„**ï¼šæ¨¡å—åŒ–è®¾è®¡ã€å¯æ‰©å±•æ¶æ„ã€é«˜å¯ç”¨éƒ¨ç½²
- **ä¸šåŠ¡åœºæ™¯**ï¼šæ™ºèƒ½å®¢æœã€çŸ¥è¯†é—®ç­”ã€æ–‡æ¡£æ£€ç´¢
- **å·¥ç¨‹å®è·µ**ï¼šæ•°æ®ç®¡ç†ã€ç¼“å­˜ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–
- **è¿ç»´ç›‘æ§**ï¼šæ—¥å¿—ç®¡ç†ã€æ€§èƒ½ç›‘æ§ã€æ•…éšœæ’é™¤

### ğŸ› ï¸ æŠ€æœ¯æŠ€èƒ½æŒæ¡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†ä»¥ä¸‹æ ¸å¿ƒæŠ€èƒ½ï¼š

1. **RAGç³»ç»Ÿè®¾è®¡èƒ½åŠ›**
   - èƒ½å¤Ÿè®¾è®¡å®Œæ•´çš„RAGç³»ç»Ÿæ¶æ„
   - ç†è§£å„ç»„ä»¶çš„ä½œç”¨å’Œç›¸äº’å…³ç³»
   - æŒæ¡ç³»ç»Ÿé›†æˆå’Œä¼˜åŒ–æ–¹æ³•

2. **æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–**
   - å®ç°å¤šæ ¼å¼æ–‡æ¡£çš„å¤„ç†æµç¨‹
   - æŒæ¡ä¸åŒå‘é‡åŒ–æŠ€æœ¯çš„åº”ç”¨
   - èƒ½å¤Ÿè¯„ä¼°å’Œä¼˜åŒ–å‘é‡è´¨é‡

3. **æ£€ç´¢ç³»ç»Ÿå¼€å‘**
   - æ„å»ºé«˜æ•ˆçš„å‘é‡æ•°æ®åº“
   - å®ç°å¤šç§æ£€ç´¢ç­–ç•¥
   - ä¼˜åŒ–æ£€ç´¢æ€§èƒ½å’Œå‡†ç¡®æ€§

4. **ç”Ÿæˆç³»ç»Ÿä¼˜åŒ–**
   - è®¾è®¡æ™ºèƒ½çš„ç­”æ¡ˆç”Ÿæˆç­–ç•¥
   - å®ç°è´¨é‡æ§åˆ¶å’ŒéªŒè¯æœºåˆ¶
   - æå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡

5. **ç³»ç»Ÿè¯„ä¼°ä¸ç›‘æ§**
   - å»ºç«‹å…¨é¢çš„è¯„ä¼°ä½“ç³»
   - å®ç°æ€§èƒ½ç›‘æ§å’Œåˆ†æ
   - åŸºäºæ•°æ®è¿›è¡Œç³»ç»Ÿä¼˜åŒ–

### ğŸ¨ åˆ›æ–°æ•™å­¦ç‰¹è‰²

æœ¬ç« é‡‡ç”¨äº†ç‹¬ç‰¹çš„"çŸ¥è¯†æ£€ç´¢ä¸­å¿ƒ"æ¯”å–»ä½“ç³»ï¼š

- **æ–‡æ¡£ä»“åº“**ï¼šå¦‚åŒå›¾ä¹¦é¦†çš„è—ä¹¦ç³»ç»Ÿ
- **å‘é‡ç´¢å¼•**ï¼šç±»ä¼¼å›¾ä¹¦çš„åˆ†ç±»ç›®å½•
- **æ£€ç´¢å¼•æ“**ï¼šå¥½æ¯”æ™ºèƒ½çš„å›¾ä¹¦ç®¡ç†å‘˜
- **ç”Ÿæˆå™¨**ï¼šåƒæ˜¯çŸ¥è¯†æ¸Šåšçš„ä¸“å®¶é¡¾é—®
- **è´¨é‡æ§åˆ¶**ï¼šå¦‚åŒä¸¥æ ¼çš„ç¼–è¾‘å®¡æ ¸
- **æ€§èƒ½ç›‘æ§**ï¼šç±»ä¼¼ç³»ç»Ÿçš„å¥åº·æ£€æŸ¥

è¿™ç§æ¯”å–»è®©å¤æ‚çš„æŠ€æœ¯æ¦‚å¿µå˜å¾—ç”ŸåŠ¨æ˜“æ‡‚ï¼Œå¸®åŠ©æ·±åº¦ç†è§£RAGç³»ç»Ÿçš„å·¥ä½œåŸç†ã€‚

### ğŸ“Š é¡¹ç›®ä»·å€¼è¯„ä¼°

æˆ‘ä»¬åœ¨æœ¬ç« ä¸­å¼€å‘çš„ä¼ä¸šçº§RAGç³»ç»Ÿå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ï¼š

**æŠ€æœ¯ä»·å€¼**ï¼š
- å®Œæ•´çš„RAGæŠ€æœ¯æ ˆå®ç°
- å¯æ‰©å±•çš„æ¨¡å—åŒ–æ¶æ„
- ä¼ä¸šçº§çš„æ€§èƒ½å’Œç¨³å®šæ€§

**å•†ä¸šä»·å€¼**ï¼š
- é™ä½å®¢æœäººåŠ›æˆæœ¬
- æå‡ç”¨æˆ·æœåŠ¡è´¨é‡
- æ”¯æŒ7Ã—24å°æ—¶æœåŠ¡

**å­¦ä¹ ä»·å€¼**ï¼š
- æ·±åº¦ç†è§£RAGæŠ€æœ¯åŸç†
- æŒæ¡ä¼ä¸šçº§ç³»ç»Ÿå¼€å‘
- ç§¯ç´¯AIåº”ç”¨å®æˆ˜ç»éªŒ

### ğŸ”® æŠ€æœ¯å‘å±•è¶‹åŠ¿

RAGæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæœªæ¥è¶‹åŠ¿åŒ…æ‹¬ï¼š

1. **å¤šæ¨¡æ€RAG**ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€
2. **å®æ—¶RAG**ï¼šæ”¯æŒå®æ—¶æ•°æ®æ›´æ–°å’ŒåŠ¨æ€çŸ¥è¯†åº“
3. **ä¸ªæ€§åŒ–RAG**ï¼šåŸºäºç”¨æˆ·åå¥½çš„ä¸ªæ€§åŒ–æ£€ç´¢å’Œç”Ÿæˆ
4. **è”é‚¦RAG**ï¼šåˆ†å¸ƒå¼çŸ¥è¯†åº“çš„è”åˆæ£€ç´¢
5. **å¯è§£é‡ŠRAG**ï¼šæä¾›æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹çš„å¯è§£é‡Šæ€§

### ğŸ’¡ æ·±åº¦æ€è€ƒé¢˜

1. **ç³»ç»Ÿè®¾è®¡æ€è€ƒ**ï¼š
   å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒå¤šè¯­è¨€ã€å¤šé¢†åŸŸçš„é€šç”¨RAGç³»ç»Ÿï¼Ÿéœ€è¦è€ƒè™‘å“ªäº›æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ

2. **æ€§èƒ½ä¼˜åŒ–æ€è€ƒ**ï¼š
   åœ¨å¤§è§„æ¨¡éƒ¨ç½²åœºæ™¯ä¸‹ï¼Œå¦‚ä½•å¹³è¡¡æ£€ç´¢å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ï¼Ÿæœ‰å“ªäº›ä¼˜åŒ–ç­–ç•¥å¯ä»¥é‡‡ç”¨ï¼Ÿ

3. **è´¨é‡ä¿è¯æ€è€ƒ**ï¼š
   å¦‚ä½•å»ºç«‹æœ‰æ•ˆçš„RAGç³»ç»Ÿè´¨é‡è¯„ä¼°å’Œç›‘æ§ä½“ç³»ï¼Ÿå¦‚ä½•å¤„ç†ç”Ÿæˆå†…å®¹çš„åè§å’Œé”™è¯¯ï¼Ÿ

4. **åº”ç”¨æ‹“å±•æ€è€ƒ**ï¼š
   é™¤äº†æ™ºèƒ½å®¢æœï¼ŒRAGæŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨åœ¨å“ªäº›åœºæ™¯ï¼Ÿæ¯ç§åœºæ™¯éœ€è¦ä»€ä¹ˆæ ·çš„å®šåˆ¶åŒ–æ”¹è¿›ï¼Ÿ

### ğŸ¯ å­¦ä¹ ç›®æ ‡è¾¾æˆåº¦

è®©æˆ‘ä»¬å›é¡¾æœ¬ç« å¼€å§‹æ—¶è®¾å®šçš„å­¦ä¹ ç›®æ ‡ï¼š

**çŸ¥è¯†ç›®æ ‡** âœ…
- âœ… æ·±åº¦ç†è§£RAGæŠ€æœ¯çš„æ ¸å¿ƒåŸç†å’Œæ¶æ„
- âœ… æŒæ¡æ–‡æ¡£å¤„ç†ã€å‘é‡åŒ–ã€æ£€ç´¢ã€ç”Ÿæˆçš„å®Œæ•´æµç¨‹
- âœ… äº†è§£RAGç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°å’Œä¼˜åŒ–æ–¹æ³•

**æŠ€èƒ½ç›®æ ‡** âœ…
- âœ… èƒ½å¤Ÿè®¾è®¡å’Œå®ç°å®Œæ•´çš„RAGç³»ç»Ÿ
- âœ… æŒæ¡å‘é‡æ•°æ®åº“çš„æ„å»ºå’Œä¼˜åŒ–æŠ€æœ¯
- âœ… å…·å¤‡ä¼ä¸šçº§RAGåº”ç”¨çš„å¼€å‘èƒ½åŠ›

**ç´ å…»ç›®æ ‡** âœ…
- âœ… åŸ¹å…»ç³»ç»Ÿæ€§æ€ç»´å’Œå·¥ç¨‹å®è·µèƒ½åŠ›
- âœ… å»ºç«‹AIæŠ€æœ¯åº”ç”¨çš„è´£ä»»æ„è¯†
- âœ… å½¢æˆæŒç»­å­¦ä¹ å’Œåˆ›æ–°çš„æŠ€æœ¯ç´ å…»

### ğŸš€ ä¸‹ç« é¢„å‘Š

åœ¨ä¸‹ä¸€ç« ã€Šå¼ºåŒ–å­¦ä¹ ä¸æ™ºèƒ½å†³ç­–ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š

- æ¢ç´¢å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•å’ŒåŸç†
- å­¦ä¹ Q-Learningã€ç­–ç•¥æ¢¯åº¦ç­‰ç»å…¸æ–¹æ³•
- æ„å»ºæ™ºèƒ½æ¸¸æˆAIå’Œå†³ç­–ç³»ç»Ÿ
- æŒæ¡å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨

å¼ºåŒ–å­¦ä¹ å°†ä¸ºæˆ‘ä»¬çš„AIæ™ºèƒ½ä½“å¢æ·»"å­¦ä¹ å†³ç­–"çš„èƒ½åŠ›ï¼Œè®©AIç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ å’Œä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚

### ğŸŠ ç« èŠ‚å®Œæˆç¥è´º

ğŸ‰ **æ­å–œå®Œæˆç¬¬29ç« å­¦ä¹ ï¼** ğŸ‰

æ‚¨å·²ç»æˆåŠŸæŒæ¡äº†RAGæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œè¿™æ˜¯ç°ä»£AIåº”ç”¨ä¸­æœ€é‡è¦çš„æŠ€æœ¯ä¹‹ä¸€ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨ä¸ä»…ç†è§£äº†RAGçš„ç†è®ºåŸºç¡€ï¼Œæ›´é‡è¦çš„æ˜¯è·å¾—äº†æ„å»ºä¼ä¸šçº§RAGç³»ç»Ÿçš„å®æˆ˜èƒ½åŠ›ã€‚

**æ‚¨çš„æˆå°±**ï¼š
- ğŸ“š æŒæ¡äº†å®Œæ•´çš„RAGæŠ€æœ¯æ ˆ
- ğŸ› ï¸ å…·å¤‡äº†ä¼ä¸šçº§ç³»ç»Ÿå¼€å‘èƒ½åŠ›
- ğŸ¯ å»ºç«‹äº†AIåº”ç”¨çš„å·¥ç¨‹æ€ç»´
- ğŸ’¡ åŸ¹å…»äº†æŠ€æœ¯åˆ›æ–°çš„å®è·µèƒ½åŠ›

ç»§ç»­ä¿æŒè¿™ç§å­¦ä¹ çƒ­æƒ…ï¼Œåœ¨AIæŠ€æœ¯çš„é“è·¯ä¸Šä¸æ–­å‰è¿›ï¼ä¸‹ä¸€ç« çš„å¼ºåŒ–å­¦ä¹ ä¹‹æ—…ç­‰å¾…ç€æ‚¨çš„æ¢ç´¢ã€‚

---

**æœ¬ç« å®Œæˆæ—¶é—´**ï¼š2025å¹´2æœˆ3æ—¥  
**å­¦ä¹ æ—¶é•¿**ï¼šçº¦8-10å°æ—¶  
**ä»£ç é‡**ï¼š4800+è¡Œ  
**æ ¸å¿ƒæ¦‚å¿µ**ï¼š35ä¸ª  
**å®æˆ˜é¡¹ç›®**ï¼š1ä¸ªä¼ä¸šçº§RAGç³»ç»Ÿ  

*"çŸ¥è¯†çš„åŠ›é‡åœ¨äºåº”ç”¨ï¼ŒæŠ€æœ¯çš„ä»·å€¼åœ¨äºåˆ›é€ ã€‚RAGæŠ€æœ¯ä¸ºæˆ‘ä»¬æ‰“å¼€äº†AIåº”ç”¨çš„æ–°ä¸–ç•Œï¼Œè®©æˆ‘ä»¬ç»§ç»­åœ¨è¿™æ¡å……æ»¡æŒ‘æˆ˜å’Œæœºé‡çš„é“è·¯ä¸Šå‰è¡Œï¼"* 