# ç¬¬20ç«  Scikit-learnåŸºç¡€åº”ç”¨

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- **æŒæ¡10+ç§æ ¸å¿ƒæœºå™¨å­¦ä¹ ç®—æ³•**ï¼šæ·±å…¥ç†è§£åˆ†ç±»ã€å›å½’ã€èšç±»ã€é™ç»´ç®—æ³•åŸç†
- **ç†è§£ç›‘ç£ä¸æ— ç›‘ç£å­¦ä¹ æœ¬è´¨**ï¼šæ˜ç¡®ä¸¤å¤§å­¦ä¹ èŒƒå¼çš„åŒºåˆ«å’Œåº”ç”¨åœºæ™¯
- **æŒæ¡æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–æŠ€æœ¯**ï¼šå­¦ä¼šç§‘å­¦è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œç³»ç»Ÿæ€§è°ƒä¼˜æ–¹æ³•

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡
- **ç†Ÿç»ƒä½¿ç”¨Scikit-learnç”Ÿæ€**ï¼šèƒ½å¤Ÿå¿«é€Ÿå®ç°å„ç§æœºå™¨å­¦ä¹ ç®—æ³•
- **å…·å¤‡ç®—æ³•é€‰æ‹©åˆ¤æ–­åŠ›**ï¼šæ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©æœ€é€‚åˆçš„ç®—æ³•
- **æŒæ¡ç«¯åˆ°ç«¯MLé¡¹ç›®å¼€å‘**ï¼šä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹

### ğŸ§  ç´ å…»ç›®æ ‡
- **åŸ¹å…»æ•°æ®ç§‘å­¦æ€ç»´**ï¼šç”¨ç§‘å­¦æ–¹æ³•è§£å†³å®é™…ä¸šåŠ¡é—®é¢˜
- **å»ºç«‹ç®—æ³•å¯¹æ¯”æ„è¯†**ï¼šç†è§£"æ²¡æœ‰å…è´¹åˆé¤"å®šç†çš„å®é™…å«ä¹‰
- **å¼ºåŒ–å·¥ç¨‹å®è·µèƒ½åŠ›**ï¼šå†™å‡ºå¯ç»´æŠ¤ã€å¯æ‰©å±•çš„æœºå™¨å­¦ä¹ ä»£ç 

---

## ğŸ­ 20.1 æ¬¢è¿æ¥åˆ°æ™ºèƒ½å®éªŒå·¥å‚ï¼

### ğŸšª ä»å®éªŒå®¤åˆ°å·¥å‚çš„å‡çº§

è¿˜è®°å¾—ç¬¬19ç« æˆ‘ä»¬å»ºç«‹çš„AIå®éªŒå®¤å—ï¼Ÿç°åœ¨ï¼Œæˆ‘ä»¬è¦å°†å®ƒå‡çº§ä¸ºä¸€ä¸ª**æ™ºèƒ½å®éªŒå·¥å‚**ï¼

å¦‚æœè¯´ç¬¬19ç« çš„AIå®éªŒå®¤åƒæ˜¯ä¸€ä¸ªå°ä½œåŠï¼Œé‚£ä¹ˆç¬¬20ç« çš„æ™ºèƒ½å®éªŒå·¥å‚å°±åƒæ˜¯ç°ä»£åŒ–çš„ç”Ÿäº§åŸºåœ°ï¼š

### ğŸ—ï¸ æ™ºèƒ½å®éªŒå·¥å‚ç»„ç»‡æ¶æ„

```mermaid
graph TD
    A["ğŸ­ Scikit-learnæ™ºèƒ½å®éªŒå·¥å‚"] --> B["ğŸ“ ç›‘ç£å­¦ä¹ éƒ¨é—¨"]
    A --> C["ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ éƒ¨é—¨"]
    A --> D["ğŸ“Š è´¨é‡æ§åˆ¶éƒ¨é—¨"]
    A --> E["ğŸ¯ åº”ç”¨å¼€å‘éƒ¨é—¨"]
    
    B --> B1["åˆ†ç±»ç®—æ³•"]
    B --> B2["å›å½’ç®—æ³•"]
    
    C --> C1["èšç±»ç®—æ³•"]
    C --> C2["é™ç»´ç®—æ³•"]
    
    D --> D1["æ¨¡å‹è¯„ä¼°"]
    D --> D2["å‚æ•°ä¼˜åŒ–"]
    
    E --> E1["å®æˆ˜é¡¹ç›®"]
    E --> E2["ç³»ç»Ÿéƒ¨ç½²"]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e3f2fd
```

#### ğŸ”¬ å„éƒ¨é—¨è¯¦ç»†åŠŸèƒ½ï¼š

**ğŸ“ ç›‘ç£å­¦ä¹ éƒ¨é—¨**
- åˆ†ç±»ç®—æ³•ï¼šé€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€æœ´ç´ è´å¶æ–¯
- å›å½’ç®—æ³•ï¼šçº¿æ€§å›å½’ã€å²­å›å½’ã€å¥—ç´¢å›å½’ã€å†³ç­–æ ‘å›å½’ã€éšæœºæ£®æ—å›å½’

**ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ éƒ¨é—¨**
- èšç±»ç®—æ³•ï¼šK-Meansã€å±‚æ¬¡èšç±»ã€DBSCAN
- é™ç»´ç®—æ³•ï¼šPCAä¸»æˆåˆ†åˆ†æã€t-SNEå¯è§†åŒ–

**ğŸ“Š è´¨é‡æ§åˆ¶éƒ¨é—¨**
- æ¨¡å‹è¯„ä¼°ï¼šäº¤å‰éªŒè¯ã€æ··æ·†çŸ©é˜µã€ROCæ›²çº¿ã€ç²¾ç¡®ç‡å¬å›ç‡
- å‚æ•°ä¼˜åŒ–ï¼šç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€è´å¶æ–¯ä¼˜åŒ–

**ğŸ¯ åº”ç”¨å¼€å‘éƒ¨é—¨**
- å®æˆ˜é¡¹ç›®ï¼šé‚®ä»¶åˆ†ç±»ã€è‚¡ä»·é¢„æµ‹ã€å®¢æˆ·åˆ†æã€åŒ»ç–—è¯Šæ–­
- ç³»ç»Ÿéƒ¨ç½²ï¼šæ¨¡å‹èåˆã€ç®¡é“æ„å»ºã€éƒ¨ç½²æ–¹æ¡ˆ

```python
# ğŸ­ æ™ºèƒ½å®éªŒå·¥å‚æ¬¢è¿ä»£ç 
print("ğŸ­ æ¬¢è¿æ¥åˆ°Scikit-learnæ™ºèƒ½å®éªŒå·¥å‚ï¼")
print("=" * 60)
print("ğŸ”¬ åœ¨è¿™ä¸ªå·¥å‚é‡Œï¼Œæˆ‘ä»¬æ‹¥æœ‰ï¼š")
print("   ğŸ“ ç›‘ç£å­¦ä¹ ç”Ÿäº§çº¿ - æœ‰è€å¸ˆæŒ‡å¯¼çš„æ™ºèƒ½åˆ¶é€ ")
print("   ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ æ¢ç´¢åŒº - è‡ªä¸»å‘ç°çš„ç ”å‘ä¸­å¿ƒ") 
print("   ğŸ“Š è´¨é‡æ§åˆ¶æ£€æµ‹ç«™ - æ¨¡å‹æ€§èƒ½è¯„ä¼°ç³»ç»Ÿ")
print("   ğŸ”§ ç²¾å¯†è°ƒä¼˜è½¦é—´ - è¶…å‚æ•°ä¼˜åŒ–å®éªŒå®¤")
print("   ğŸ† ç»¼åˆåº”ç”¨å±•ç¤ºå… - å®é™…é¡¹ç›®æˆæœå±•ç¤º")
print()
print("ğŸš€ å‡†å¤‡å¥½æˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆäº†å—ï¼Ÿ")
```

### ğŸ—ï¸ å®éªŒå·¥å‚çš„ç»„ç»‡æ¶æ„

æˆ‘ä»¬çš„æ™ºèƒ½å®éªŒå·¥å‚é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°±åƒç°ä»£å·¥å‚çš„æµæ°´çº¿ï¼š

```python
# ğŸ­ Scikit-learnå®éªŒå·¥å‚æ¶æ„å±•ç¤º
from sklearn import *
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

class MLExperimentFactory:
    """æœºå™¨å­¦ä¹ å®éªŒå·¥å‚ä¸»æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.factory_name = "Scikit-learnæ™ºèƒ½å®éªŒå·¥å‚"
        self.departments = {
            "ğŸ“ ç›‘ç£å­¦ä¹ éƒ¨é—¨": {
                "åˆ†ç±»è½¦é—´": ["é€»è¾‘å›å½’", "å†³ç­–æ ‘", "éšæœºæ£®æ—", "æ”¯æŒå‘é‡æœº", "æœ´ç´ è´å¶æ–¯"],
                "å›å½’è½¦é—´": ["çº¿æ€§å›å½’", "å²­å›å½’", "å¥—ç´¢å›å½’", "å†³ç­–æ ‘å›å½’", "éšæœºæ£®æ—å›å½’"]
            },
            "ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ éƒ¨é—¨": {
                "èšç±»å®éªŒå®¤": ["K-Means", "å±‚æ¬¡èšç±»", "DBSCAN"],
                "é™ç»´å®éªŒå®¤": ["PCAä¸»æˆåˆ†åˆ†æ", "t-SNEå¯è§†åŒ–"]
            },
            "ğŸ“Š è´¨é‡æ§åˆ¶éƒ¨é—¨": {
                "è¯„ä¼°ä¸­å¿ƒ": ["äº¤å‰éªŒè¯", "æ··æ·†çŸ©é˜µ", "ROCæ›²çº¿", "ç²¾ç¡®ç‡å¬å›ç‡"],
                "ä¼˜åŒ–ä¸­å¿ƒ": ["ç½‘æ ¼æœç´¢", "éšæœºæœç´¢", "è´å¶æ–¯ä¼˜åŒ–"]
            },
            "ğŸ¯ åº”ç”¨å¼€å‘éƒ¨é—¨": {
                "é¡¹ç›®å­µåŒ–å™¨": ["åŒ»ç–—è¯Šæ–­", "é‚®ä»¶åˆ†ç±»", "å®¢æˆ·åˆ†æ", "è‚¡ä»·é¢„æµ‹"],
                "ç³»ç»Ÿé›†æˆ": ["æ¨¡å‹èåˆ", "ç®¡é“æ„å»º", "éƒ¨ç½²æ–¹æ¡ˆ"]
            }
        }
        
    def show_factory_overview(self):
        """å±•ç¤ºå·¥å‚å…¨è²Œ"""
        print(f"ğŸ­ {self.factory_name} ç»„ç»‡æ¶æ„")
        print("=" * 70)
        
        for dept_name, workshops in self.departments.items():
            print(f"\n{dept_name}")
            for workshop_name, techniques in workshops.items():
                print(f"   ğŸ”¬ {workshop_name}:")
                for technique in techniques:
                    print(f"      â€¢ {technique}")
        
        self.show_production_capacity()
    
    def show_production_capacity(self):
        """å±•ç¤ºç”Ÿäº§èƒ½åŠ›"""
        print("\nğŸ“Š å·¥å‚ç”Ÿäº§èƒ½åŠ›:")
        print("   ğŸ¯ æ”¯æŒç®—æ³•æ•°é‡: 15+ ç§æ ¸å¿ƒç®—æ³•")
        print("   ğŸ“ˆ é¡¹ç›®äº§å‡ºèƒ½åŠ›: 5ä¸ªå®Œæ•´å®æˆ˜é¡¹ç›®") 
        print("   ğŸ”§ è´¨é‡ä¿è¯ä½“ç³»: å¤šç»´åº¦æ¨¡å‹è¯„ä¼°")
        print("   ğŸš€ åˆ›æ–°èƒ½åŠ›: ç®—æ³•PKç«æŠ€åœº")
        
    def get_sklearn_ecosystem(self):
        """è·å–Scikit-learnç”Ÿæ€ç³»ç»Ÿä¿¡æ¯"""
        sklearn_modules = {
            "æ•°æ®é¢„å¤„ç†": ["preprocessing", "feature_selection", "feature_extraction"],
            "ç›‘ç£å­¦ä¹ ": ["linear_model", "tree", "ensemble", "svm", "naive_bayes"],
            "æ— ç›‘ç£å­¦ä¹ ": ["cluster", "decomposition", "manifold"],
            "æ¨¡å‹è¯„ä¼°": ["model_selection", "metrics"],
            "æ•°æ®é›†": ["datasets"],
            "å·¥å…·é›†": ["pipeline", "compose", "utils"]
        }
        
        print("\nğŸ› ï¸ Scikit-learnç”Ÿæ€ç³»ç»Ÿ:")
        for category, modules in sklearn_modules.items():
            print(f"   ğŸ“¦ {category}: {', '.join(modules)}")

# å¯åŠ¨å·¥å‚
factory = MLExperimentFactory()
factory.show_factory_overview()
factory.get_sklearn_ecosystem()
```

### ğŸ¯ å®éªŒå·¥å‚çš„æ ¸å¿ƒä¼˜åŠ¿

```python
# ğŸš€ Scikit-learnæ ¸å¿ƒä¼˜åŠ¿å±•ç¤º
sklearn_advantages = {
    "ğŸ¯ ç»Ÿä¸€æ¥å£è®¾è®¡": {
        "ç‰¹ç‚¹": "æ‰€æœ‰ç®—æ³•éƒ½éµå¾ªfit-predictæ¨¡å¼",
        "å¥½å¤„": "å­¦ä¼šä¸€ä¸ªç®—æ³•ï¼Œå°±ä¼šç”¨æ‰€æœ‰ç®—æ³•",
        "ç¤ºä¾‹": "model.fit(X_train, y_train); predictions = model.predict(X_test)"
    },
    "ğŸ“Š ä¸°å¯Œç®—æ³•ç”Ÿæ€": {
        "ç‰¹ç‚¹": "æ¶µç›–æœºå™¨å­¦ä¹ çš„æ‰€æœ‰ä¸»è¦ç®—æ³•",
        "å¥½å¤„": "ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€åˆ‡æ¢å·¥å…·",
        "ç¤ºä¾‹": "ä»çº¿æ€§å›å½’åˆ°æ·±åº¦æ£®æ—ï¼Œåº”æœ‰å°½æœ‰"
    },
    "ğŸ”§ å¼ºå¤§å·¥å…·é“¾": {
        "ç‰¹ç‚¹": "å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥å…·é“¾",
        "å¥½å¤„": "æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²å…¨è¦†ç›–",
        "ç¤ºä¾‹": "Pipelineã€GridSearchã€CrossValidation"
    },
    "ğŸ“š ä¼˜ç§€æ–‡æ¡£": {
        "ç‰¹ç‚¹": "è¯¦ç»†çš„æ–‡æ¡£å’Œä¸°å¯Œçš„ç¤ºä¾‹",
        "å¥½å¤„": "å­¦ä¹ æˆæœ¬ä½ï¼Œé—®é¢˜è§£å†³å¿«",
        "ç¤ºä¾‹": "å®˜æ–¹æ–‡æ¡£ + ç¤¾åŒºæ•™ç¨‹ + Stack Overflow"
    },
    "âš¡ é«˜æ€§èƒ½å®ç°": {
        "ç‰¹ç‚¹": "åº•å±‚ä½¿ç”¨C/Fortranä¼˜åŒ–",
        "å¥½å¤„": "ç®—æ³•è¿è¡Œé€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨å°‘", 
        "ç¤ºä¾‹": "å¤§è§„æ¨¡æ•°æ®å¤„ç†èƒ½åŠ›å¼º"
    }
}

print("ğŸš€ ä¸ºä»€ä¹ˆé€‰æ‹©Scikit-learnä½œä¸ºæˆ‘ä»¬çš„å®éªŒå·¥å‚ï¼Ÿ")
print("=" * 60)

for advantage, details in sklearn_advantages.items():
    print(f"\nâœ¨ {advantage}")
    print(f"   ğŸ” {details['ç‰¹ç‚¹']}")
    print(f"   ğŸ’ {details['å¥½å¤„']}")
    print(f"   ğŸ’¡ {details['ç¤ºä¾‹']}")
```

### ğŸ§ª ç¬¬ä¸€ä¸ªå·¥å‚å®éªŒï¼šç®—æ³•é€Ÿè§ˆ

è®©æˆ‘ä»¬å…ˆæ¥ä¸€ä¸ª"å·¥å‚å‚è§‚"ï¼Œå¿«é€Ÿä½“éªŒå„ç§ç®—æ³•çš„å¨åŠ›ï¼š

```python
# ğŸª ç®—æ³•å±•ç¤ºç§€ - ä¸€åˆ†é’Ÿä½“éªŒæ‰€æœ‰æ ¸å¿ƒç®—æ³•
from sklearn.datasets import make_classification, make_regression, make_blobs
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

def algorithm_showcase():
    """ç®—æ³•å±•ç¤ºç§€"""
    print("ğŸª Scikit-learnç®—æ³•å±•ç¤ºç§€å¼€å§‹ï¼")
    print("=" * 50)
    
    # ğŸ¯ åˆ†ç±»ç®—æ³•å±•ç¤º
    print("\nğŸ“ åˆ†ç±»ç®—æ³•å±•ç¤ºåŒº:")
    X_cls, y_cls = make_classification(n_samples=100, n_features=4, n_classes=3, random_state=42)
    
    classifiers = {
        "é€»è¾‘å›å½’": LogisticRegression(random_state=42),
        "å†³ç­–æ ‘": DecisionTreeClassifier(random_state=42),
        "éšæœºæ£®æ—": RandomForestClassifier(random_state=42),
        "æ”¯æŒå‘é‡æœº": SVC(random_state=42)
    }
    
    for name, clf in classifiers.items():
        clf.fit(X_cls, y_cls)
        accuracy = clf.score(X_cls, y_cls)
        print(f"   ğŸ”¬ {name}: å‡†ç¡®ç‡ {accuracy:.3f}")
    
    # ğŸ“ˆ å›å½’ç®—æ³•å±•ç¤º  
    print("\nğŸ“ˆ å›å½’ç®—æ³•å±•ç¤ºåŒº:")
    X_reg, y_reg = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)
    
    regressors = {
        "çº¿æ€§å›å½’": LinearRegression(),
        "å†³ç­–æ ‘å›å½’": DecisionTreeRegressor(random_state=42),
        "éšæœºæ£®æ—å›å½’": RandomForestRegressor(random_state=42)
    }
    
    for name, reg in regressors.items():
        reg.fit(X_reg, y_reg)
        r2_score = reg.score(X_reg, y_reg)
        print(f"   ğŸ“Š {name}: RÂ²å¾—åˆ† {r2_score:.3f}")
    
    # ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ å±•ç¤º
    print("\nğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ å±•ç¤ºåŒº:")
    X_clust, _ = make_blobs(n_samples=100, centers=3, n_features=2, random_state=42)
    
    # èšç±»
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(X_clust)
    print(f"   ğŸ¯ K-Meansèšç±»: å‘ç° {len(set(clusters))} ä¸ªç¾¤ä½“")
    
    # é™ç»´
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_clust)
    variance_ratio = sum(pca.explained_variance_ratio_)
    print(f"   ğŸ“‰ PCAé™ç»´: ä¿ç•™ {variance_ratio:.1%} çš„ä¿¡æ¯")
    
    print("\nğŸ‰ å±•ç¤ºå®Œæ¯•ï¼æ¯ç§ç®—æ³•éƒ½æœ‰è‡ªå·±çš„ç‰¹è‰²å’Œé€‚ç”¨åœºæ™¯ï¼")

# è¿è¡Œç®—æ³•å±•ç¤º
algorithm_showcase()
```

---

## ğŸ“ 20.2 ç›‘ç£å­¦ä¹ å®éªŒå®¤

### ğŸ­ ç›‘ç£å­¦ä¹ ç”Ÿäº§çº¿æ€»è§ˆ

ç›‘ç£å­¦ä¹ å°±åƒæ˜¯ä¸€ä¸ª**æœ‰ç»éªŒå¸ˆå‚…æŒ‡å¯¼çš„å­¦å¾’å·¥åŠ**ã€‚åœ¨è¿™é‡Œï¼Œç®—æ³•åƒå­¦å¾’ä¸€æ ·ï¼Œé€šè¿‡è§‚å¯Ÿå¤§é‡çš„"ç¤ºä¾‹ä½œå“"ï¼ˆè®­ç»ƒæ•°æ®ï¼‰æ¥å­¦ä¹ "åˆ¶ä½œæŠ€å·§"ï¼Œç„¶åèƒ½å¤Ÿç‹¬ç«‹åˆ¶ä½œå‡ºé«˜è´¨é‡çš„"äº§å“"ï¼ˆé¢„æµ‹ç»“æœï¼‰ã€‚

### ğŸ”„ æœºå™¨å­¦ä¹ æ ‡å‡†æµç¨‹

åœ¨å¼€å§‹æ¢ç´¢å„ç§ç®—æ³•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„æ ‡å‡†å·¥ä½œæµç¨‹ï¼š

```mermaid
graph LR
    A["ğŸ“Š åŸå§‹æ•°æ®"] --> B["ğŸ”„ æ•°æ®é¢„å¤„ç†"]
    B --> C["ğŸ“ ç‰¹å¾å·¥ç¨‹"]
    C --> D["ğŸ”€ æ•°æ®åˆ†å‰²"]
    D --> E["è®­ç»ƒé›†"]
    D --> F["æµ‹è¯•é›†"]
    
    E --> G["ğŸ¤– æ¨¡å‹è®­ç»ƒ"]
    G --> H["ğŸ”® æ¨¡å‹é¢„æµ‹"]
    F --> H
    
    H --> I["ğŸ“Š æ€§èƒ½è¯„ä¼°"]
    I --> J{"ğŸ“ˆ æ•ˆæœæ»¡æ„?"}
    
    J -->|å¦| K["ğŸ”§ æ¨¡å‹è°ƒä¼˜"]
    K --> G
    
    J -->|æ˜¯| L["ğŸš€ æ¨¡å‹éƒ¨ç½²"]
    L --> M["ğŸ“± å®é™…åº”ç”¨"]
    
    style A fill:#e1f5fe
    style L fill:#e8f5e8
    style M fill:#fff3e0
```

### ğŸ¯ ç›‘ç£å­¦ä¹  vs æ— ç›‘ç£å­¦ä¹ 

```mermaid
graph TB
    A["ğŸ§  æœºå™¨å­¦ä¹ ç®—æ³•"] --> B["ğŸ“ ç›‘ç£å­¦ä¹ "]
    A --> C["ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ "]
    
    B --> B1["ğŸ“Š æœ‰æ ‡ç­¾æ•°æ®<br/>(X, y)"]
    B --> B2["ğŸ¯ å­¦ä¹ ç›®æ ‡:<br/>é¢„æµ‹æ ‡ç­¾"]
    
    B1 --> B3["ğŸ¯ åˆ†ç±»é—®é¢˜"]
    B1 --> B4["ğŸ“ˆ å›å½’é—®é¢˜"]
    
    B3 --> B3a["ğŸ“§ é‚®ä»¶åˆ†ç±»<br/>ğŸ¥ ç–¾ç—…è¯Šæ–­<br/>ğŸ–¼ï¸ å›¾åƒè¯†åˆ«"]
    B4 --> B4a["ğŸ  æˆ¿ä»·é¢„æµ‹<br/>ğŸ“ˆ è‚¡ä»·é¢„æµ‹<br/>ğŸŒ¡ï¸ æ¸©åº¦é¢„æµ‹"]
    
    C --> C1["ğŸ“Š æ— æ ‡ç­¾æ•°æ®<br/>(åªæœ‰X)"]
    C --> C2["ğŸ•µï¸ å­¦ä¹ ç›®æ ‡:<br/>å‘ç°æ¨¡å¼"]
    
    C1 --> C3["ğŸ¯ èšç±»é—®é¢˜"]
    C1 --> C4["ğŸ“‰ é™ç»´é—®é¢˜"]
    
    C3 --> C3a["ğŸ‘¥ å®¢æˆ·ç»†åˆ†<br/>ğŸ§¬ åŸºå› åˆ†ç»„<br/>ğŸ“° æ–°é—»åˆ†ç±»"]
    C4 --> C4a["ğŸ“Š æ•°æ®å¯è§†åŒ–<br/>ğŸ—œï¸ ç‰¹å¾å‹ç¼©<br/>ğŸ” å¼‚å¸¸æ£€æµ‹"]
    
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style B3a fill:#f3e5f5
    style B4a fill:#f3e5f5
    style C3a fill:#e3f2fd
    style C4a fill:#e3f2fd
```

```python
# ğŸ“ ç›‘ç£å­¦ä¹ å®éªŒå®¤ä»‹ç»
class SupervisedLearningLab:
    """ç›‘ç£å­¦ä¹ å®éªŒå®¤"""
    
    def __init__(self):
        self.lab_name = "ç›‘ç£å­¦ä¹ å®éªŒå®¤"
        self.workshops = {
            "ğŸ¯ åˆ†ç±»è½¦é—´": {
                "åŠŸèƒ½": "å°†æ•°æ®åˆ†åˆ°ä¸åŒç±»åˆ«",
                "è¾“å‡º": "ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾",
                "åº”ç”¨": "é‚®ä»¶åˆ†ç±»ã€ç–¾ç—…è¯Šæ–­ã€å›¾åƒè¯†åˆ«",
                "ç®—æ³•": ["é€»è¾‘å›å½’", "å†³ç­–æ ‘", "éšæœºæ£®æ—", "SVM"]
            },
            "ğŸ“ˆ å›å½’è½¦é—´": {
                "åŠŸèƒ½": "é¢„æµ‹è¿ç»­çš„æ•°å€¼",
                "è¾“å‡º": "è¿ç»­çš„æ•°å€¼ç»“æœ", 
                "åº”ç”¨": "æˆ¿ä»·é¢„æµ‹ã€è‚¡ä»·é¢„æµ‹ã€é”€é‡é¢„æµ‹",
                "ç®—æ³•": ["çº¿æ€§å›å½’", "å†³ç­–æ ‘å›å½’", "éšæœºæ£®æ—å›å½’"]
            }
        }
    
    def introduce_lab(self):
        """ä»‹ç»å®éªŒå®¤"""
        print(f"ğŸ“ {self.lab_name}")
        print("=" * 50)
        print("ğŸ§‘â€ğŸ« åœ¨è¿™é‡Œï¼Œç®—æ³•åƒå­¦å¾’ä¸€æ ·ï¼š")
        print("   1ï¸âƒ£ è§‚å¯Ÿå¤§é‡è®­ç»ƒç¤ºä¾‹ï¼ˆæœ‰æ ‡å‡†ç­”æ¡ˆï¼‰")
        print("   2ï¸âƒ£ ä»ä¸­å­¦ä¹ è§„å¾‹å’Œæ¨¡å¼")
        print("   3ï¸âƒ£ å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹")
        print()
        
        for workshop, details in self.workshops.items():
            print(f"{workshop}")
            print(f"   ğŸ¯ åŠŸèƒ½ï¼š{details['åŠŸèƒ½']}")
            print(f"   ğŸ“¤ è¾“å‡ºï¼š{details['è¾“å‡º']}")
            print(f"   ğŸ’¼ åº”ç”¨ï¼š{details['åº”ç”¨']}")
            print(f"   ğŸ› ï¸ ç®—æ³•ï¼š{', '.join(details['ç®—æ³•'])}")
            print()

# ä»‹ç»ç›‘ç£å­¦ä¹ å®éªŒå®¤
lab = SupervisedLearningLab()
lab.introduce_lab()
```

### ğŸ¤” ç®—æ³•é€‰æ‹©æŒ‡å—

åœ¨å¼€å§‹å­¦ä¹ å„ç§ç®—æ³•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£å¦‚ä½•æ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ç®—æ³•ï¼š

#### ğŸ“Š ç¬¬ä¸€æ­¥ï¼šç¡®å®šå­¦ä¹ ç±»å‹

```mermaid
graph LR
    A["ğŸ¤” æœ‰æ ‡ç­¾æ•°æ®å—?"] --> B["ğŸ“ ç›‘ç£å­¦ä¹ "]
    A --> C["ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ "]
    
    B --> B1["åˆ†ç±»ä»»åŠ¡"]
    B --> B2["å›å½’ä»»åŠ¡"]
    
    C --> C1["èšç±»ä»»åŠ¡"]
    C --> C2["é™ç»´ä»»åŠ¡"]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
```

#### ğŸ¯ ç¬¬äºŒæ­¥ï¼šé€‰æ‹©å…·ä½“ç®—æ³•

**ğŸ“ ç›‘ç£å­¦ä¹ ç®—æ³•æ¨è**

| é—®é¢˜ç±»å‹ | æ•°æ®ç‰¹ç‚¹ | æ¨èç®—æ³• | ä¼˜åŠ¿ |
|---------|---------|----------|------|
| ğŸ“Š åˆ†ç±» | å°æ•°æ®é›† | é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ | å¿«é€Ÿã€å¯è§£é‡Š |
| ğŸ“Š åˆ†ç±» | ä¸­ç­‰æ•°æ® | å†³ç­–æ ‘ã€éšæœºæ£®æ— | ç‰¹å¾é€‰æ‹©ã€ç¨³å®š |
| ğŸ“Š åˆ†ç±» | å¤§æ•°æ®é›† | æ”¯æŒå‘é‡æœº | é«˜ç»´æ•°æ®å¼º |
| ğŸ“ˆ å›å½’ | çº¿æ€§å…³ç³» | çº¿æ€§å›å½’ã€å²­å›å½’ | ç®€å•ã€å¿«é€Ÿ |
| ğŸ“ˆ å›å½’ | å¤æ‚å…³ç³» | å†³ç­–æ ‘ã€éšæœºæ£®æ— | éçº¿æ€§å»ºæ¨¡ |

**ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ ç®—æ³•æ¨è**

| ä»»åŠ¡ç±»å‹ | æ•°æ®ç‰¹ç‚¹ | æ¨èç®—æ³• | é€‚ç”¨åœºæ™¯ |
|---------|---------|----------|----------|
| ğŸ¯ èšç±» | å·²çŸ¥ç¾¤ä½“æ•° | K-Means | å®¢æˆ·åˆ†ç¾¤ |
| ğŸ¯ èšç±» | æœªçŸ¥ç¾¤ä½“æ•° | å±‚æ¬¡èšç±»ã€DBSCAN | å¼‚å¸¸æ£€æµ‹ |
| ğŸ“‰ é™ç»´ | æ•°æ®å¯è§†åŒ– | PCA | ç‰¹å¾å‹ç¼© |
| ğŸ“‰ é™ç»´ | å¤æ‚å¯è§†åŒ– | t-SNE | é«˜ç»´å¯è§†åŒ– |

### ğŸ¯ åˆ†ç±»å®éªŒåŒº

åˆ†ç±»å°±åƒæ˜¯ä¸€ä¸ª**æ™ºèƒ½åˆ†æ‹£å·¥å‚**ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†ä¸åŒçš„ç‰©å“åˆ†ç±»åˆ°æ­£ç¡®çš„ç®±å­é‡Œã€‚

#### ğŸ§  é€»è¾‘å›å½’ï¼šæ¦‚ç‡åˆ¤æ–­ä¸“å®¶

```python
# ğŸ§  é€»è¾‘å›å½’åˆ†ç±»å™¨ - æ¦‚ç‡åˆ¤æ–­ä¸“å®¶
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def logistic_regression_demo():
    """é€»è¾‘å›å½’æ¼”ç¤º"""
    print("ğŸ§  é€»è¾‘å›å½’ï¼šæ¦‚ç‡åˆ¤æ–­ä¸“å®¶")
    print("=" * 40)
    
    # ğŸ“Š åˆ›å»ºæ¼”ç¤ºæ•°æ®
    X, y = make_classification(
        n_samples=1000, 
        n_features=2, 
        n_redundant=0, 
        n_informative=2,
        n_clusters_per_class=1, 
        random_state=42
    )
    
    # ğŸ”„ åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # ğŸ§  åˆ›å»ºå¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    lr_model = LogisticRegression(random_state=42)
    lr_model.fit(X_train, y_train)
    
    # ğŸ”® è¿›è¡Œé¢„æµ‹
    y_pred = lr_model.predict(X_test)
    y_pred_proba = lr_model.predict_proba(X_test)
    
    # ğŸ“Š æ¨¡å‹æ€§èƒ½åˆ†æ
    accuracy = lr_model.score(X_test, y_test)
    print(f"ğŸ¯ åˆ†ç±»å‡†ç¡®ç‡ï¼š{accuracy:.3f}")
    
    # ğŸ“ˆ æ˜¾ç¤ºä¸€äº›é¢„æµ‹æ¦‚ç‡
    print("\nğŸ”® é¢„æµ‹æ¦‚ç‡ç¤ºä¾‹ï¼ˆå‰5ä¸ªæ ·æœ¬ï¼‰ï¼š")
    for i in range(5):
        pred_class = y_pred[i]
        prob_class0 = y_pred_proba[i][0]
        prob_class1 = y_pred_proba[i][1]
        print(f"   æ ·æœ¬{i+1}: é¢„æµ‹ç±»åˆ«={pred_class}, æ¦‚ç‡=[{prob_class0:.3f}, {prob_class1:.3f}]")
    
    # ğŸ§® é€»è¾‘å›å½’çš„æ•°å­¦é­…åŠ›
    print(f"\nğŸ§® æ¨¡å‹å‚æ•°è§£æï¼š")
    print(f"   æƒé‡ç³»æ•°: {lr_model.coef_[0]}")
    print(f"   æˆªè·é¡¹: {lr_model.intercept_[0]:.3f}")
    
    return lr_model, X_test, y_test, y_pred

# è¿è¡Œé€»è¾‘å›å½’æ¼”ç¤º
lr_model, X_test, y_test, y_pred = logistic_regression_demo()
```

#### ğŸŒ³ å†³ç­–æ ‘ï¼šé—®ç­”å¼ä¸“å®¶

```python
# ğŸŒ³ å†³ç­–æ ‘åˆ†ç±»å™¨ - é—®ç­”å¼å†³ç­–ä¸“å®¶
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris

def decision_tree_demo():
    """å†³ç­–æ ‘æ¼”ç¤º"""
    print("\nğŸŒ³ å†³ç­–æ ‘ï¼šé—®ç­”å¼å†³ç­–ä¸“å®¶")
    print("=" * 40)
    
    # ğŸ“Š ä½¿ç”¨ç»å…¸é¸¢å°¾èŠ±æ•°æ®é›†
    iris = load_iris()
    X, y = iris.data, iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    # ğŸ”„ åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # ğŸŒ³ åˆ›å»ºå¹¶è®­ç»ƒå†³ç­–æ ‘
    dt_model = DecisionTreeClassifier(
        max_depth=3,  # é™åˆ¶æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
        random_state=42
    )
    dt_model.fit(X_train, y_train)
    
    # ğŸ”® é¢„æµ‹å’Œè¯„ä¼°
    y_pred = dt_model.predict(X_test)
    accuracy = dt_model.score(X_test, y_test)
    
    print(f"ğŸ¯ åˆ†ç±»å‡†ç¡®ç‡ï¼š{accuracy:.3f}")
    
    # ğŸŒ¿ ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\nğŸŒ¿ ç‰¹å¾é‡è¦æ€§æ’åï¼š")
    importance_scores = dt_model.feature_importances_
    for i, score in enumerate(importance_scores):
        print(f"   {feature_names[i]}: {score:.3f}")
    
    # ğŸ¤– å†³ç­–è·¯å¾„ç¤ºä¾‹
    print("\nğŸ¤– å†³ç­–æ ‘çš„æ€è€ƒè¿‡ç¨‹ï¼ˆå‰3ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼š")
    for i in range(3):
        sample = X_test[i:i+1]
        pred = dt_model.predict(sample)[0]
        actual = y_test[i]
        
        print(f"\n   ğŸ“ æ ·æœ¬{i+1}ç‰¹å¾: {sample[0]}")
        print(f"   ğŸ¤” å†³ç­–æ ‘é¢„æµ‹: {target_names[pred]}")
        print(f"   âœ… å®é™…ç±»åˆ«: {target_names[actual]}")
        print(f"   {'ğŸ‰ é¢„æµ‹æ­£ç¡®!' if pred == actual else 'âŒ é¢„æµ‹é”™è¯¯'}")
    
    return dt_model, feature_names

# è¿è¡Œå†³ç­–æ ‘æ¼”ç¤º
dt_model, feature_names = decision_tree_demo()
```

#### ğŸŒ² éšæœºæ£®æ—ï¼šä¸“å®¶å›¢é˜Ÿ

```python
# ğŸŒ² éšæœºæ£®æ—åˆ†ç±»å™¨ - ä¸“å®¶å›¢é˜Ÿå†³ç­–
from sklearn.ensemble import RandomForestClassifier
import numpy as np

def random_forest_demo():
    """éšæœºæ£®æ—æ¼”ç¤º"""
    print("\nğŸŒ² éšæœºæ£®æ—ï¼šä¸“å®¶å›¢é˜Ÿå†³ç­–")
    print("=" * 40)
    
    # ğŸ“Š ç»§ç»­ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # ğŸ”„ åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # ğŸŒ² åˆ›å»ºéšæœºæ£®æ—ï¼ˆ100ä¸ªå†³ç­–æ ‘ç»„æˆçš„å›¢é˜Ÿï¼‰
    rf_model = RandomForestClassifier(
        n_estimators=100,  # 100æ£µæ ‘
        max_depth=3,       # æ¯æ£µæ ‘æœ€å¤§æ·±åº¦
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    
    # ğŸ”® é¢„æµ‹å’Œè¯„ä¼°
    y_pred = rf_model.predict(X_test)
    accuracy = rf_model.score(X_test, y_test)
    
    print(f"ğŸ¯ åˆ†ç±»å‡†ç¡®ç‡ï¼š{accuracy:.3f}")
    
    # ğŸ† ä¸å•ä¸ªå†³ç­–æ ‘å¯¹æ¯”
    single_tree = DecisionTreeClassifier(max_depth=3, random_state=42)
    single_tree.fit(X_train, y_train)
    single_accuracy = single_tree.score(X_test, y_test)
    
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼š")
    print(f"   ğŸŒ³ å•ä¸ªå†³ç­–æ ‘: {single_accuracy:.3f}")
    print(f"   ğŸŒ² éšæœºæ£®æ—: {accuracy:.3f}")
    print(f"   ğŸ“ˆ æå‡å¹…åº¦: {(accuracy - single_accuracy):.3f}")
    
    # ğŸ—³ï¸ å›¢é˜ŸæŠ•ç¥¨è¿‡ç¨‹å±•ç¤º
    print(f"\nğŸ—³ï¸ ä¸“å®¶å›¢é˜ŸæŠ•ç¥¨è¿‡ç¨‹ï¼ˆç¬¬1ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼š")
    sample = X_test[0:1]
    
    # è·å–æ¯æ£µæ ‘çš„é¢„æµ‹
    tree_predictions = []
    for tree in rf_model.estimators_:
        pred = tree.predict(sample)[0]
        tree_predictions.append(pred)
    
    # ç»Ÿè®¡æŠ•ç¥¨ç»“æœ
    vote_counts = np.bincount(tree_predictions)
    print(f"   ğŸ“Š æŠ•ç¥¨ç»“æœ: {vote_counts}")
    
    for i, count in enumerate(vote_counts):
        if count > 0:
            percentage = count / len(tree_predictions) * 100
            print(f"   ğŸ—³ï¸ ç±»åˆ«{i}: {count}ç¥¨ ({percentage:.1f}%)")
    
    final_pred = rf_model.predict(sample)[0]
    print(f"   ğŸ† æœ€ç»ˆå†³ç­–: ç±»åˆ«{final_pred}")
    
    return rf_model

# è¿è¡Œéšæœºæ£®æ—æ¼”ç¤º
rf_model = random_forest_demo()
```

### ğŸ† ç®—æ³•PKç«æŠ€åœºï¼šåˆ†ç±»ç®—æ³•å¤§æ¯”æ‹¼

ç°åœ¨è®©æˆ‘ä»¬ä¸¾åŠä¸€åœºç®—æ³•ç«èµ›ï¼Œçœ‹çœ‹å“ªä¸ªç®—æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½ï¼

### ğŸª ç«æŠ€åœºè¯„ä¼°ä½“ç³»

```mermaid
graph LR
    A["ğŸ† ç®—æ³•PKç«æŠ€åœº"] --> B["ğŸ“Š å‡†ç¡®ç‡"]
    A --> C["â±ï¸ é€Ÿåº¦"]
    A --> D["ğŸ§  å¯è§£é‡Šæ€§"]
    A --> E["ğŸ“ˆ æ‰©å±•æ€§"]
    
    style A fill:#fff3e0
    style B fill:#e8f5e8
    style C fill:#f3e5f5
    style D fill:#e3f2fd
    style E fill:#fff8e1
```

#### ğŸ¥Š å‚èµ›é€‰æ‰‹ç‰¹è‰²åˆ†æ

| ç®—æ³• | å‡†ç¡®ç‡ | é€Ÿåº¦ | å¯è§£é‡Šæ€§ | æ‰©å±•æ€§ | ç‰¹è‰²ä¼˜åŠ¿ |
|------|--------|------|----------|--------|----------|
| ğŸ§  é€»è¾‘å›å½’ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | çº¿æ€§è¾¹ç•Œï¼Œæ¦‚ç‡è¾“å‡º |
| ğŸŒ³ å†³ç­–æ ‘ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­ | è§„åˆ™ç›´è§‚ï¼Œæ˜“è¿‡æ‹Ÿåˆ |
| ğŸŒ² éšæœºæ£®æ— | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ | ç¨³å®šæ€§å¼ºï¼Œç‰¹å¾é‡è¦æ€§ |
| ğŸš€ æ”¯æŒå‘é‡æœº | â­â­â­â­ | â­â­ | â­â­ | â­â­â­â­â­ | é«˜ç»´æ•°æ®ï¼Œéçº¿æ€§æ ¸ |
| ğŸ“Š æœ´ç´ è´å¶æ–¯ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | æ–‡æœ¬åˆ†ç±»ï¼Œå°æ•°æ®å‹å¥½ |

```python
# ğŸ† åˆ†ç±»ç®—æ³•PKç«æŠ€åœº
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
import time

def classification_algorithm_tournament():
    """åˆ†ç±»ç®—æ³•é”¦æ ‡èµ›"""
    print("\nğŸ† åˆ†ç±»ç®—æ³•PKç«æŠ€åœº")
    print("=" * 50)
    print("ğŸª ä»Šæ—¥æ¯”èµ›é¡¹ç›®ï¼šé¸¢å°¾èŠ±åˆ†ç±»æŒ‘æˆ˜èµ›")
    
    # ğŸ“Š å‡†å¤‡æ•°æ®
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # ğŸ¥Š å‚èµ›é€‰æ‰‹
    competitors = {
        "ğŸ§  é€»è¾‘å›å½’": LogisticRegression(random_state=42),
        "ğŸŒ³ å†³ç­–æ ‘": DecisionTreeClassifier(random_state=42),
        "ğŸŒ² éšæœºæ£®æ—": RandomForestClassifier(random_state=42),
        "ğŸš€ æ”¯æŒå‘é‡æœº": SVC(random_state=42),
        "ğŸ“Š æœ´ç´ è´å¶æ–¯": GaussianNB()
    }
    
    # ğŸ“ˆ æ¯”èµ›ç»“æœè®°å½•
    results = {}
    
    print("\nğŸ”¥ æ¯”èµ›å¼€å§‹ï¼")
    print("-" * 50)
    
    for name, algorithm in competitors.items():
        print(f"\nğŸ¥Š {name} ä¸Šåœº...")
        
        # â±ï¸ è®¡æ—¶å¼€å§‹
        start_time = time.time()
        
        # ğŸ¯ 5æŠ˜äº¤å‰éªŒè¯
        scores = cross_val_score(algorithm, X, y, cv=5)
        
        # â±ï¸ è®¡æ—¶ç»“æŸ
        end_time = time.time()
        
        # ğŸ“Š è®°å½•ç»“æœ
        avg_score = scores.mean()
        std_score = scores.std()
        training_time = end_time - start_time
        
        results[name] = {
            'accuracy': avg_score,
            'std': std_score,
            'time': training_time
        }
        
        print(f"   ğŸ¯ å¹³å‡å‡†ç¡®ç‡: {avg_score:.4f} (Â±{std_score:.4f})")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.4f}ç§’")
    
    # ğŸ† é¢å¥–å…¸ç¤¼
    print("\nğŸ† æ¯”èµ›ç»“æœå…¬å¸ƒ")
    print("=" * 50)
    
    # æŒ‰å‡†ç¡®ç‡æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ…", "ğŸ…"]
    
    for i, (name, result) in enumerate(sorted_results):
        medal = medals[i] if i < len(medals) else "ğŸ–ï¸"
        print(f"{medal} ç¬¬{i+1}å: {name}")
        print(f"   å‡†ç¡®ç‡: {result['accuracy']:.4f}")
        print(f"   è®­ç»ƒæ—¶é—´: {result['time']:.4f}ç§’")
        
        # ç‰¹æ®Šè¡¨å½°
        if i == 0:
            print("   ğŸ‰ ğŸ† å‡†ç¡®ç‡å† å†›!")
        elif result['time'] == min(r['time'] for r in results.values()):
            print("   âš¡ ğŸ† é€Ÿåº¦å† å†›!")
    
    return results

# ä¸¾åŠç®—æ³•ç«èµ›
tournament_results = classification_algorithm_tournament() 
```

### ğŸ“ˆ å›å½’å®éªŒåŒº

å›å½’å°±åƒæ˜¯ä¸€ä¸ª**ç²¾å¯†æµ‹é‡å·¥å‚**ï¼Œèƒ½å¤Ÿé¢„æµ‹è¿ç»­çš„æ•°å€¼ï¼Œæ¯”å¦‚æ¸©åº¦ã€ä»·æ ¼ã€é‡é‡ç­‰ã€‚

#### ğŸ“ çº¿æ€§å›å½’è¿›é˜¶ï¼šä»ç®€å•åˆ°å¤æ‚

```python
# ğŸ“ çº¿æ€§å›å½’è¿›é˜¶ - ä»ç®€å•åˆ°å¤æ‚
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

def advanced_linear_regression_demo():
    """çº¿æ€§å›å½’è¿›é˜¶æ¼”ç¤º"""
    print("\nğŸ“ çº¿æ€§å›å½’è¿›é˜¶ï¼šä»ç®€å•åˆ°å¤æ‚")
    print("=" * 50)
    
    # ğŸ“Š åˆ›å»ºå…·æœ‰éçº¿æ€§å…³ç³»çš„æ•°æ®
    np.random.seed(42)
    X = np.linspace(0, 1, 100).reshape(-1, 1)
    y = 2 * X.ravel() + 0.5 * X.ravel()**2 + 0.1 * np.random.randn(100)
    
    # ğŸ”„ åˆ†å‰²æ•°æ®
    split_idx = 70
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # ğŸ§ª æµ‹è¯•ä¸åŒçš„å›å½’æ–¹æ³•
    regression_methods = {
        "ç®€å•çº¿æ€§å›å½’": LinearRegression(),
        "å²­å›å½’(Ridge)": Ridge(alpha=1.0),
        "å¥—ç´¢å›å½’(Lasso)": Lasso(alpha=0.1),
        "äºŒæ¬¡å¤šé¡¹å¼å›å½’": Pipeline([
            ('poly', PolynomialFeatures(degree=2)),
            ('linear', LinearRegression())
        ])
    }
    
    # ğŸ“Š è®­ç»ƒå’Œè¯„ä¼°å„ç§æ–¹æ³•
    print("ğŸ”¬ ä¸åŒå›å½’æ–¹æ³•æ€§èƒ½å¯¹æ¯”ï¼š")
    results = {}
    
    for method_name, model in regression_methods.items():
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        
        results[method_name] = {
            'model': model,
            'train_r2': train_score,
            'test_r2': test_score
        }
        
        print(f"\nğŸ”¬ {method_name}:")
        print(f"   ğŸ“Š è®­ç»ƒé›†RÂ²: {train_score:.4f}")
        print(f"   ğŸ§ª æµ‹è¯•é›†RÂ²: {test_score:.4f}")
        print(f"   {'ğŸ¯ æ‹Ÿåˆè‰¯å¥½' if abs(train_score - test_score) < 0.1 else 'âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆ'}")
    
    # ğŸ† æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_method = max(results.items(), key=lambda x: x[1]['test_r2'])
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]} (æµ‹è¯•RÂ²: {best_method[1]['test_r2']:.4f})")
    
    return results

# è¿è¡Œçº¿æ€§å›å½’è¿›é˜¶æ¼”ç¤º
regression_results = advanced_linear_regression_demo()
```

#### ğŸŒ³ å†³ç­–æ ‘å›å½’ï¼šé—®ç­”å¼æ•°å€¼é¢„æµ‹

```python
# ğŸŒ³ å†³ç­–æ ‘å›å½’ - é—®ç­”å¼æ•°å€¼é¢„æµ‹
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

def tree_regression_demo():
    """æ ‘åŸºå›å½’æ¼”ç¤º"""
    print("\nğŸŒ³ æ ‘åŸºå›å½’ï¼šé—®ç­”å¼æ•°å€¼é¢„æµ‹")
    print("=" * 50)
    
    # ğŸ“Š åˆ›å»ºå›å½’æ•°æ®
    X, y = make_regression(n_samples=500, n_features=4, noise=10, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # ğŸŒ³ å†³ç­–æ ‘å›å½’ vs ğŸŒ² éšæœºæ£®æ—å›å½’
    tree_models = {
        "å†³ç­–æ ‘å›å½’": DecisionTreeRegressor(max_depth=5, random_state=42),
        "éšæœºæ£®æ—å›å½’": RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
    }
    
    print("ğŸ”¬ æ ‘åŸºæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼š")
    
    for model_name, model in tree_models.items():
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # è¯„ä¼°æ€§èƒ½
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        
        print(f"\nğŸŒ³ {model_name}:")
        print(f"   ğŸ“Š è®­ç»ƒé›†RÂ²: {train_score:.4f}")
        print(f"   ğŸ§ª æµ‹è¯•é›†RÂ²: {test_score:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(model, 'feature_importances_'):
            print(f"   ğŸŒ¿ ç‰¹å¾é‡è¦æ€§: {model.feature_importances_}")

# è¿è¡Œæ ‘åŸºå›å½’æ¼”ç¤º
tree_regression_demo()
```

### ğŸ¯ å®æˆ˜é¡¹ç›®ä¸€ï¼šæ™ºèƒ½é‚®ä»¶åˆ†ç±»ç³»ç»Ÿ

ç°åœ¨è®©æˆ‘ä»¬å¼€å‘ç¬¬ä¸€ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼šä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«åƒåœ¾é‚®ä»¶çš„æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿï¼

### ğŸ“§ ç³»ç»Ÿæ¶æ„è®¾è®¡

```mermaid
graph LR
    A["ğŸ“§ åŸå§‹é‚®ä»¶"] --> B["ğŸ”„ æ–‡æœ¬é¢„å¤„ç†"]
    B --> C["ğŸ“Š ç‰¹å¾æå–"]
    C --> D["ğŸ¤– æ¨¡å‹è®­ç»ƒ"]
    D --> E["ğŸ† æ¨¡å‹è¯„ä¼°"]
    E --> F["ğŸ”® é‚®ä»¶åˆ†ç±»"]
    
    F --> G["âœ… æ­£å¸¸é‚®ä»¶"]
    F --> H["ğŸš« åƒåœ¾é‚®ä»¶"]
    
    style A fill:#e1f5fe
    style F fill:#e8f5e8
    style G fill:#e8f5e8
    style H fill:#ffebee
```

#### ğŸ”§ å„é˜¶æ®µè¯¦ç»†è¯´æ˜

**ğŸ”„ æ–‡æœ¬é¢„å¤„ç†**
- å°å†™è½¬æ¢ã€å»é™¤åœç”¨è¯ã€æ ‡ç‚¹æ¸…ç†

**ğŸ“Š ç‰¹å¾æå–**
- TF-IDFå‘é‡åŒ–ã€N-gramç‰¹å¾æå–

**ğŸ¤– æ¨¡å‹è®­ç»ƒ**
- é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ã€æ”¯æŒå‘é‡æœº

**ğŸ† æ¨¡å‹è¯„ä¼°**
- å‡†ç¡®ç‡å¯¹æ¯”ã€é€‰æ‹©æœ€ä½³æ¨¡å‹

```python
# ğŸ“§ æ™ºèƒ½é‚®ä»¶åˆ†ç±»ç³»ç»Ÿ - å®Œæ•´å®æˆ˜é¡¹ç›®
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np

class SmartEmailClassifier:
    """æ™ºèƒ½é‚®ä»¶åˆ†ç±»ç³»ç»Ÿ"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=5000,  # æœ€å¤š5000ä¸ªç‰¹å¾è¯
            stop_words='english',  # å»é™¤è‹±æ–‡åœç”¨è¯
            lowercase=True,  # è½¬ä¸ºå°å†™
            ngram_range=(1, 2)  # ä½¿ç”¨1-2å…ƒç»„åˆ
        )
        self.models = {}
        self.best_model = None
        
    def create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹é‚®ä»¶æ•°æ®"""
        print("ğŸ“§ åˆ›å»ºç¤ºä¾‹é‚®ä»¶æ•°æ®...")
        
        # ğŸ¯ æ­£å¸¸é‚®ä»¶ç¤ºä¾‹
        normal_emails = [
            "Meeting scheduled for tomorrow at 10 AM in conference room",
            "Thank you for your presentation yesterday, it was very informative",
            "Please review the attached document and provide your feedback",
            "Reminder: project deadline is next Friday",
            "Welcome to our team! Looking forward to working with you",
            "Customer support ticket #12345 has been resolved",
            "Monthly sales report is attached for your review",
            "Training session on new software tools next week",
            "Please confirm your attendance for the quarterly meeting",
            "Your order #98765 has been shipped and will arrive tomorrow",
            "Technical documentation has been updated on the wiki",
            "Budget proposal for Q4 needs your approval",
            "New employee orientation starts Monday morning",
            "System maintenance scheduled for this weekend",
            "Performance review meeting has been rescheduled"
        ]
        
        # ğŸš« åƒåœ¾é‚®ä»¶ç¤ºä¾‹
        spam_emails = [
            "URGENT! You've won $1,000,000! Click here to claim now!",
            "Make money fast! Work from home! No experience needed!",
            "Free iPhone! Limited time offer! Click here immediately!",
            "CONGRATULATIONS! You're our lucky winner! Claim your prize!",
            "Lose weight fast! Miracle pill! Order now!",
            "Hot singles in your area! Meet them tonight!",
            "Your account will be suspended! Verify now!",
            "Cheap prescription drugs! No prescription needed!",
            "Get rich quick! Investment opportunity! Act now!",
            "FREE MONEY! Government grants available! Apply today!",
            "Enlarge your... Special offer! Satisfaction guaranteed!",
            "URGENT: Your credit card has been compromised! Update info!",
            "Nigerian prince needs your help! Large reward offered!",
            "Amazing discount! 90% off everything! Limited time!",
            "Your computer is infected! Download our antivirus now!"
        ]
        
        # ğŸ“Š åˆ›å»ºæ•°æ®é›†
        emails = normal_emails + spam_emails
        labels = [0] * len(normal_emails) + [1] * len(spam_emails)  # 0=æ­£å¸¸, 1=åƒåœ¾
        
        return emails, labels
    
    def preprocess_data(self, emails, labels):
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ”„ æ•°æ®é¢„å¤„ç†ä¸­...")
        
        # ğŸ“Š æ–‡æœ¬å‘é‡åŒ–
        X = self.vectorizer.fit_transform(emails)
        y = np.array(labels)
        
        # ğŸ”„ åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"   ğŸ“‹ è®­ç»ƒé›†æ ·æœ¬æ•°: {X_train.shape[0]}")
        print(f"   ğŸ§ª æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test.shape[0]}")
        print(f"   ğŸ“ ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
        
        return X_train, X_test, y_train, y_test
    
    def train_models(self, X_train, y_train):
        """è®­ç»ƒå¤šç§åˆ†ç±»æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒåˆ†ç±»æ¨¡å‹...")
        
        # ğŸ§ª å€™é€‰ç®—æ³•
        algorithms = {
            "é€»è¾‘å›å½’": LogisticRegression(random_state=42),
            "æœ´ç´ è´å¶æ–¯": MultinomialNB(),
            "æ”¯æŒå‘é‡æœº": SVC(kernel='linear', random_state=42)
        }
        
        # ğŸ‹ï¸ è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for name, model in algorithms.items():
            print(f"   ğŸ”¬ è®­ç»ƒ {name}...")
            model.fit(X_train, y_train)
            self.models[name] = model
        
        print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    def evaluate_models(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("=" * 50)
        
        best_score = 0
        results = {}
        
        for name, model in self.models.items():
            # ğŸ”® é¢„æµ‹
            y_pred = model.predict(X_test)
            accuracy = model.score(X_test, y_test)
            
            results[name] = {
                'accuracy': accuracy,
                'predictions': y_pred
            }
            
            print(f"\nğŸ”¬ {name}:")
            print(f"   ğŸ¯ å‡†ç¡®ç‡: {accuracy:.4f}")
            
            # ğŸ“Š æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_test, y_pred)
            print(f"   ğŸ“‹ æ··æ·†çŸ©é˜µ:")
            print(f"      æ­£å¸¸é‚®ä»¶è¯†åˆ«: {cm[0,0]}/{cm[0,0]+cm[0,1]} ({cm[0,0]/(cm[0,0]+cm[0,1]):.2%})")
            print(f"      åƒåœ¾é‚®ä»¶è¯†åˆ«: {cm[1,1]}/{cm[1,0]+cm[1,1]} ({cm[1,1]/(cm[1,0]+cm[1,1]):.2%})")
            
            # ğŸ† è®°å½•æœ€ä½³æ¨¡å‹
            if accuracy > best_score:
                best_score = accuracy
                self.best_model = model
                self.best_model_name = name
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_model_name} (å‡†ç¡®ç‡: {best_score:.4f})")
        return results
    
    def classify_new_email(self, email_text):
        """åˆ†ç±»æ–°é‚®ä»¶"""
        if self.best_model is None:
            return "è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼"
        
        # ğŸ“Š å‘é‡åŒ–æ–°é‚®ä»¶
        email_vector = self.vectorizer.transform([email_text])
        
        # ğŸ”® é¢„æµ‹
        prediction = self.best_model.predict(email_vector)[0]
        
        # ğŸ¯ è·å–æ¦‚ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        if hasattr(self.best_model, 'predict_proba'):
            probabilities = self.best_model.predict_proba(email_vector)[0]
            normal_prob = probabilities[0]
            spam_prob = probabilities[1]
        else:
            normal_prob = spam_prob = "N/A"
        
        result = {
            'classification': 'æ­£å¸¸é‚®ä»¶' if prediction == 0 else 'åƒåœ¾é‚®ä»¶',
            'confidence': {
                'æ­£å¸¸é‚®ä»¶æ¦‚ç‡': f"{normal_prob:.3f}" if normal_prob != "N/A" else "N/A",
                'åƒåœ¾é‚®ä»¶æ¦‚ç‡': f"{spam_prob:.3f}" if spam_prob != "N/A" else "N/A"
            }
        }
        
        return result

def email_classification_project():
    """é‚®ä»¶åˆ†ç±»é¡¹ç›®ä¸»å‡½æ•°"""
    print("ğŸ“§ æ™ºèƒ½é‚®ä»¶åˆ†ç±»ç³»ç»Ÿå¯åŠ¨ï¼")
    print("=" * 60)
    
    # ğŸ—ï¸ åˆ›å»ºåˆ†ç±»å™¨
    classifier = SmartEmailClassifier()
    
    # ğŸ“Š å‡†å¤‡æ•°æ®
    emails, labels = classifier.create_sample_data()
    X_train, X_test, y_train, y_test = classifier.preprocess_data(emails, labels)
    
    # ğŸ¤– è®­ç»ƒæ¨¡å‹
    classifier.train_models(X_train, y_train)
    
    # ğŸ“Š è¯„ä¼°æ€§èƒ½
    results = classifier.evaluate_models(X_test, y_test)
    
    # ğŸ§ª æµ‹è¯•æ–°é‚®ä»¶
    print("\nğŸ§ª æ–°é‚®ä»¶åˆ†ç±»æµ‹è¯•")
    print("=" * 50)
    
    test_emails = [
        "Meeting tomorrow at 3 PM in the boardroom",
        "URGENT! Free money! Click here now!",
        "Please review the quarterly report and provide feedback",
        "You've won a million dollars! Claim your prize immediately!",
        "Technical support ticket has been resolved successfully"
    ]
    
    for i, email in enumerate(test_emails, 1):
        result = classifier.classify_new_email(email)
        print(f"\nğŸ“§ æµ‹è¯•é‚®ä»¶ {i}:")
        print(f"   å†…å®¹: \"{email}\"")
        print(f"   åˆ†ç±»: {result['classification']}")
        print(f"   ç½®ä¿¡åº¦: {result['confidence']}")
    
    return classifier

# è¿è¡Œé‚®ä»¶åˆ†ç±»é¡¹ç›®
email_classifier = email_classification_project()
```

### ğŸ“ˆ å®æˆ˜é¡¹ç›®äºŒï¼šæ™ºèƒ½è‚¡ä»·é¢„æµ‹ç³»ç»Ÿ

è®©æˆ‘ä»¬å†å¼€å‘ä¸€ä¸ªå›å½’é¡¹ç›®ï¼šè‚¡ä»·é¢„æµ‹ç³»ç»Ÿï¼

```python
# ğŸ“ˆ æ™ºèƒ½è‚¡ä»·é¢„æµ‹ç³»ç»Ÿ - å›å½’å®æˆ˜é¡¹ç›®
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class SmartStockPredictor:
    """æ™ºèƒ½è‚¡ä»·é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.best_model = None
        self.feature_names = []
    
    def create_stock_data(self, n_days=1000):
        """åˆ›å»ºæ¨¡æ‹Ÿè‚¡ä»·æ•°æ®"""
        print("ğŸ“ˆ ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ä»·æ•°æ®...")
        
        np.random.seed(42)
        
        # ğŸ“Š åŸºç¡€ç‰¹å¾
        days = np.arange(n_days)
        
        # ğŸ¯ æŠ€æœ¯æŒ‡æ ‡æ¨¡æ‹Ÿ
        price_base = 100 + 0.01 * days  # åŸºç¡€è¶‹åŠ¿
        volatility = 5 * np.random.randn(n_days)  # æ³¢åŠ¨æ€§
        
        # ğŸ“Š æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        ma_5 = np.convolve(price_base, np.ones(5)/5, mode='same')  # 5æ—¥å‡çº¿
        ma_20 = np.convolve(price_base, np.ones(20)/20, mode='same')  # 20æ—¥å‡çº¿
        
        # ğŸ“ˆ æˆäº¤é‡æ¨¡æ‹Ÿ
        volume = 1000000 + 500000 * np.random.randn(n_days)
        volume = np.maximum(volume, 100000)  # ç¡®ä¿æˆäº¤é‡ä¸ºæ­£
        
        # ğŸ¯ RSIæ¨¡æ‹Ÿï¼ˆç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ï¼‰
        rsi = 50 + 20 * np.sin(days / 30) + 10 * np.random.randn(n_days)
        rsi = np.clip(rsi, 0, 100)
        
        # ğŸ’° ç›®æ ‡å˜é‡ï¼šæ¬¡æ—¥æ”¶ç›˜ä»·
        next_day_price = price_base + volatility
        
        # ğŸ“‹ åˆ›å»ºæ•°æ®æ¡†
        data = pd.DataFrame({
            'day': days,
            'current_price': price_base[:-1],  # å½“å‰ä»·æ ¼
            'ma_5': ma_5[:-1],                 # 5æ—¥å‡çº¿
            'ma_20': ma_20[:-1],               # 20æ—¥å‡çº¿
            'volume': volume[:-1],             # æˆäº¤é‡
            'rsi': rsi[:-1],                   # RSIæŒ‡æ ‡
            'price_change': np.diff(price_base),  # ä»·æ ¼å˜åŒ–
            'next_day_price': next_day_price[1:]  # æ¬¡æ—¥ä»·æ ¼ï¼ˆç›®æ ‡ï¼‰
        })
        
        # ğŸ“Š æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        data['ma_ratio'] = data['current_price'] / data['ma_20']  # ä»·æ ¼/å‡çº¿æ¯”
        data['volume_ma'] = data['volume'] / data['volume'].rolling(20).mean()  # æˆäº¤é‡æ¯”ç‡
        
        # ğŸ”§ å¤„ç†ç¼ºå¤±å€¼
        data = data.dropna()
        
        print(f"   ğŸ“‹ ç”Ÿæˆæ•°æ®ç‚¹: {len(data)}")
        print(f"   ğŸ“Š ç‰¹å¾ç»´åº¦: {data.shape[1] - 1}")  # é™¤å»ç›®æ ‡å˜é‡
        
        return data
    
    def prepare_features(self, data):
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        print("ğŸ”„ ç‰¹å¾å·¥ç¨‹...")
        
        # ğŸ“Š é€‰æ‹©ç‰¹å¾
        feature_columns = [
            'current_price', 'ma_5', 'ma_20', 'volume', 
            'rsi', 'price_change', 'ma_ratio', 'volume_ma'
        ]
        
        X = data[feature_columns]
        y = data['next_day_price']
        
        self.feature_names = feature_columns
        
        # ğŸ”„ æ•°æ®åˆ†å‰²
        split_idx = int(len(data) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # ğŸ“ ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"   ğŸ‹ï¸ è®­ç»ƒé›†: {X_train_scaled.shape[0]} æ ·æœ¬")
        print(f"   ğŸ§ª æµ‹è¯•é›†: {X_test_scaled.shape[0]} æ ·æœ¬")
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train_prediction_models(self, X_train, y_train):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒè‚¡ä»·é¢„æµ‹æ¨¡å‹...")
        
        # ğŸ§ª å€™é€‰æ¨¡å‹
        algorithms = {
            "çº¿æ€§å›å½’": LinearRegression(),
            "éšæœºæ£®æ—": RandomForestRegressor(n_estimators=100, random_state=42),
            "æ¢¯åº¦æå‡": GradientBoostingRegressor(n_estimators=100, random_state=42),
            "å†³ç­–æ ‘": DecisionTreeRegressor(max_depth=10, random_state=42)
        }
        
        # ğŸ‹ï¸ è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for name, model in algorithms.items():
            print(f"   ğŸ”¬ è®­ç»ƒ {name}...")
            model.fit(X_train, y_train)
            self.models[name] = model
        
        print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    def evaluate_models(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("=" * 60)
        
        best_score = float('-inf')
        results = {}
        
        for name, model in self.models.items():
            # ğŸ”® é¢„æµ‹
            y_pred = model.predict(X_test)
            
            # ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            results[name] = {
                'r2': r2,
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'predictions': y_pred
            }
            
            print(f"\nğŸ”¬ {name}:")
            print(f"   ğŸ¯ RÂ²å¾—åˆ†: {r2:.4f}")
            print(f"   ğŸ“‰ å‡æ–¹è¯¯å·®(MSE): {mse:.2f}")
            print(f"   ğŸ“Š å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.2f}")
            print(f"   ğŸ“ˆ å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.2f}")
            
            # ğŸ† é€‰æ‹©æœ€ä½³æ¨¡å‹
            if r2 > best_score:
                best_score = r2
                self.best_model = model
                self.best_model_name = name
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_model_name} (RÂ²: {best_score:.4f})")
        
        # ğŸŒ¿ ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(self.best_model, 'feature_importances_'):
            print(f"\nğŸŒ¿ {self.best_model_name} ç‰¹å¾é‡è¦æ€§:")
            importances = self.best_model.feature_importances_
            for feature, importance in zip(self.feature_names, importances):
                print(f"   {feature}: {importance:.4f}")
        
        return results
    
    def predict_next_price(self, current_features):
        """é¢„æµ‹æ¬¡æ—¥è‚¡ä»·"""
        if self.best_model is None:
            return "è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼"
        
        # ğŸ“Š ç‰¹å¾æ ‡å‡†åŒ–
        features_scaled = self.scaler.transform([current_features])
        
        # ğŸ”® é¢„æµ‹
        predicted_price = self.best_model.predict(features_scaled)[0]
        
        return predicted_price
    
    def generate_trading_signals(self, X_test, y_test, predictions):
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        print("\nğŸ“Š äº¤æ˜“ä¿¡å·åˆ†æ")
        print("=" * 50)
        
        # ğŸ“ˆ è®¡ç®—é¢„æµ‹æ¶¨è·Œ
        price_changes = predictions - X_test[:, 0]  # å‡è®¾ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯å½“å‰ä»·æ ¼
        
        # ğŸ¯ ç”Ÿæˆä¿¡å·
        buy_signals = price_changes > 0.5   # é¢„æµ‹ä¸Šæ¶¨è¶…è¿‡0.5
        sell_signals = price_changes < -0.5  # é¢„æµ‹ä¸‹è·Œè¶…è¿‡0.5
        hold_signals = ~(buy_signals | sell_signals)  # å…¶ä»–æƒ…å†µæŒæœ‰
        
        print(f"ğŸ“ˆ ä¹°å…¥ä¿¡å·: {np.sum(buy_signals)} æ¬¡")
        print(f"ğŸ“‰ å–å‡ºä¿¡å·: {np.sum(sell_signals)} æ¬¡") 
        print(f"ğŸ”„ æŒæœ‰ä¿¡å·: {np.sum(hold_signals)} æ¬¡")
        
        # ğŸ“Š ä¿¡å·å‡†ç¡®æ€§
        actual_changes = y_test.values - X_test[:, 0]
        
        buy_accuracy = np.mean(actual_changes[buy_signals] > 0) if np.sum(buy_signals) > 0 else 0
        sell_accuracy = np.mean(actual_changes[sell_signals] < 0) if np.sum(sell_signals) > 0 else 0
        
        print(f"\nğŸ¯ ä¿¡å·å‡†ç¡®æ€§:")
        print(f"   ğŸ“ˆ ä¹°å…¥ä¿¡å·å‡†ç¡®ç‡: {buy_accuracy:.2%}")
        print(f"   ğŸ“‰ å–å‡ºä¿¡å·å‡†ç¡®ç‡: {sell_accuracy:.2%}")

def stock_prediction_project():
    """è‚¡ä»·é¢„æµ‹é¡¹ç›®ä¸»å‡½æ•°"""
    print("ğŸ“ˆ æ™ºèƒ½è‚¡ä»·é¢„æµ‹ç³»ç»Ÿå¯åŠ¨ï¼")
    print("=" * 60)
    
    # ğŸ—ï¸ åˆ›å»ºé¢„æµ‹å™¨
    predictor = SmartStockPredictor()
    
    # ğŸ“Š ç”Ÿæˆæ•°æ®
    stock_data = predictor.create_stock_data(1000)
    
    # ğŸ”„ å‡†å¤‡ç‰¹å¾
    X_train, X_test, y_train, y_test = predictor.prepare_features(stock_data)
    
    # ğŸ¤– è®­ç»ƒæ¨¡å‹
    predictor.train_prediction_models(X_train, y_train)
    
    # ğŸ“Š è¯„ä¼°æ¨¡å‹
    results = predictor.evaluate_models(X_test, y_test)
    
    # ğŸ“Š ç”Ÿæˆäº¤æ˜“ä¿¡å·
    best_predictions = results[predictor.best_model_name]['predictions']
    predictor.generate_trading_signals(X_test, y_test, best_predictions)
    
    # ğŸ§ª å•æ¬¡é¢„æµ‹æµ‹è¯•
    print("\nğŸ§ª å•æ¬¡é¢„æµ‹æµ‹è¯•")
    print("=" * 50)
    
    # ä½¿ç”¨æµ‹è¯•é›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
    test_features = X_test[0]
    actual_price = y_test.iloc[0]
    predicted_price = predictor.predict_next_price(test_features)
    
    print(f"ğŸ“Š è¾“å…¥ç‰¹å¾: {test_features}")
    print(f"ğŸ”® é¢„æµ‹ä»·æ ¼: {predicted_price:.2f}")
    print(f"âœ… å®é™…ä»·æ ¼: {actual_price:.2f}")
    print(f"ğŸ“Š é¢„æµ‹è¯¯å·®: {abs(predicted_price - actual_price):.2f}")
    
    return predictor

# è¿è¡Œè‚¡ä»·é¢„æµ‹é¡¹ç›®
stock_predictor = stock_prediction_project()
```

---

## ğŸ•µï¸ 20.3 æ— ç›‘ç£å­¦ä¹ å®éªŒå®¤

### ğŸ” æ— ç›‘ç£å­¦ä¹ æ¢ç´¢ä¸­å¿ƒæ€»è§ˆ

æ— ç›‘ç£å­¦ä¹ å°±åƒæ˜¯ä¸€ä¸ª**ä¾¦æ¢è°ƒæŸ¥éƒ¨é—¨**ï¼Œæ²¡æœ‰ç°æˆçš„ç­”æ¡ˆï¼Œéœ€è¦ä»æ•°æ®ä¸­è‡ªä¸»å‘ç°éšè—çš„æ¨¡å¼å’Œç»“æ„ã€‚

```python
# ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ å®éªŒå®¤ä»‹ç»
class UnsupervisedLearningLab:
    """æ— ç›‘ç£å­¦ä¹ å®éªŒå®¤"""
    
    def __init__(self):
        self.lab_name = "æ— ç›‘ç£å­¦ä¹ æ¢ç´¢ä¸­å¿ƒ"
        self.departments = {
            "ğŸ•µï¸ èšç±»å‘ç°éƒ¨é—¨": {
                "ä»»åŠ¡": "å‘ç°æ•°æ®ä¸­çš„è‡ªç„¶ç¾¤ä½“",
                "æ–¹æ³•": ["K-Means", "å±‚æ¬¡èšç±»", "DBSCAN"],
                "åº”ç”¨": "å®¢æˆ·åˆ†ç¾¤ã€å¸‚åœºç»†åˆ†ã€å¼‚å¸¸æ£€æµ‹"
            },
            "ğŸ“Š é™ç»´åˆ†æéƒ¨é—¨": {
                "ä»»åŠ¡": "å°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´",
                "æ–¹æ³•": ["PCAä¸»æˆåˆ†åˆ†æ", "t-SNEå¯è§†åŒ–"],
                "åº”ç”¨": "æ•°æ®å¯è§†åŒ–ã€ç‰¹å¾å‹ç¼©ã€å™ªå£°é™ä½"
            }
        }
    
    def introduce_lab(self):
        """ä»‹ç»æ— ç›‘ç£å­¦ä¹ å®éªŒå®¤"""
        print(f"ğŸ•µï¸ {self.lab_name}")
        print("=" * 60)
        print("ğŸ” åœ¨è¿™é‡Œï¼Œç®—æ³•åƒä¾¦æ¢ä¸€æ ·ï¼š")
        print("   1ï¸âƒ£ åœ¨æ²¡æœ‰çº¿ç´¢çš„æƒ…å†µä¸‹æ¢ç´¢æ•°æ®")
        print("   2ï¸âƒ£ å‘ç°éšè—çš„æ¨¡å¼å’Œç»“æ„")
        print("   3ï¸âƒ£ æ­ç¤ºæ•°æ®èƒŒåçš„ç§˜å¯†")
        print()
        
        for dept_name, details in self.departments.items():
            print(f"{dept_name}")
            print(f"   ğŸ¯ ä»»åŠ¡ï¼š{details['ä»»åŠ¡']}")
            print(f"   ğŸ› ï¸ æ–¹æ³•ï¼š{', '.join(details['æ–¹æ³•'])}")
            print(f"   ğŸ’¼ åº”ç”¨ï¼š{details['åº”ç”¨']}")
            print()

# ä»‹ç»æ— ç›‘ç£å­¦ä¹ å®éªŒå®¤
unsupervised_lab = UnsupervisedLearningLab()
unsupervised_lab.introduce_lab()
```

### ğŸ¯ èšç±»å‘ç°åŒº

èšç±»å°±åƒæ˜¯**è‡ªåŠ¨åˆ†ç»„ä¸“å®¶**ï¼Œèƒ½å¤Ÿå‘ç°æ•°æ®ä¸­å¤©ç„¶å­˜åœ¨çš„ç¾¤ä½“ç»“æ„ã€‚

#### ğŸ¯ K-Meansèšç±»ï¼šåœ†å½¢åˆ†ç»„ä¸“å®¶

```python
# ğŸ¯ K-Meansèšç±» - åœ†å½¢åˆ†ç»„ä¸“å®¶
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import numpy as np

def kmeans_clustering_demo():
    """K-Meansèšç±»æ¼”ç¤º"""
    print("ğŸ¯ K-Meansèšç±»ï¼šåœ†å½¢åˆ†ç»„ä¸“å®¶")
    print("=" * 50)
    
    # ğŸ“Š åˆ›å»ºèšç±»æ•°æ®
    X, y_true = make_blobs(
        n_samples=300, 
        centers=4, 
        cluster_std=0.60, 
        random_state=42
    )
    
    print(f"ğŸ“‹ æ•°æ®æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    print(f"ğŸ¯ çœŸå®èšç±»æ•°: {len(set(y_true))}")
    
    # ğŸ§ª æµ‹è¯•ä¸åŒçš„Kå€¼
    k_values = [2, 3, 4, 5, 6]
    results = {}
    
    print("\nğŸ”¬ ä¸åŒKå€¼çš„èšç±»æ•ˆæœï¼š")
    
    for k in k_values:
        # ğŸ¯ è®­ç»ƒK-Means
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        
        # ğŸ“Š è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
        inertia = kmeans.inertia_  # ç°‡å†…å¹³æ–¹å’Œ
        
        results[k] = {
            'model': kmeans,
            'labels': cluster_labels,
            'inertia': inertia,
            'centers': kmeans.cluster_centers_
        }
        
        print(f"   ğŸ¯ K={k}: ç°‡å†…å¹³æ–¹å’Œ={inertia:.2f}")
    
    # ğŸ† é€‰æ‹©æœ€ä½³Kå€¼ï¼ˆè‚˜éƒ¨æ³•åˆ™ï¼‰
    print("\nğŸ“ˆ è‚˜éƒ¨æ³•åˆ™åˆ†æï¼š")
    inertias = [results[k]['inertia'] for k in k_values]
    
    # è®¡ç®—æƒ¯æ€§å·®å¼‚
    inertia_diffs = []
    for i in range(1, len(inertias)):
        diff = inertias[i-1] - inertias[i]
        inertia_diffs.append(diff)
        print(f"   K={k_values[i-1]}åˆ°K={k_values[i]}çš„æ”¹è¿›: {diff:.2f}")
    
    # ğŸ¯ æœ€ä½³Kå€¼é€šå¸¸æ˜¯æ”¹è¿›å¹…åº¦æ˜æ˜¾ä¸‹é™çš„ç‚¹
    best_k = 4  # åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“çœŸå®ç­”æ¡ˆæ˜¯4
    best_model = results[best_k]['model']
    
    print(f"\nğŸ† æ¨èKå€¼: {best_k}")
    
    # ğŸ” åˆ†æèšç±»ç»“æœ
    print(f"\nğŸ” K={best_k}çš„èšç±»åˆ†æï¼š")
    cluster_labels = results[best_k]['labels']
    centers = results[best_k]['centers']
    
    for i in range(best_k):
        cluster_size = np.sum(cluster_labels == i)
        center = centers[i]
        print(f"   ğŸ¯ ç°‡{i}: {cluster_size}ä¸ªæ ·æœ¬, ä¸­å¿ƒç‚¹({center[0]:.2f}, {center[1]:.2f})")
    
    return best_model, X, cluster_labels

# è¿è¡ŒK-Meansæ¼”ç¤º
kmeans_model, X_kmeans, kmeans_labels = kmeans_clustering_demo()
```

#### ğŸ”ï¸ å±‚æ¬¡èšç±»ï¼šæ ‘å½¢åˆ†ç»„ä¸“å®¶

```python
# ğŸ”ï¸ å±‚æ¬¡èšç±» - æ ‘å½¢åˆ†ç»„ä¸“å®¶
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

def hierarchical_clustering_demo():
    """å±‚æ¬¡èšç±»æ¼”ç¤º"""
    print("\nğŸ”ï¸ å±‚æ¬¡èšç±»ï¼šæ ‘å½¢åˆ†ç»„ä¸“å®¶")
    print("=" * 50)
    
    # ğŸ“Š ä½¿ç”¨ä¹‹å‰çš„æ•°æ®å­é›†
    X = X_kmeans[:50]  # ä¸ºäº†æ¼”ç¤ºæ¸…æ™°ï¼Œåªç”¨50ä¸ªæ ·æœ¬
    
    print(f"ğŸ“‹ æ•°æ®æ ·æœ¬æ•°: {X.shape[0]}")
    
    # ğŸŒ³ ä¸åŒé“¾æ¥æ–¹æ³•çš„å±‚æ¬¡èšç±»
    linkage_methods = ['ward', 'complete', 'average', 'single']
    
    print("\nğŸ”¬ ä¸åŒé“¾æ¥æ–¹æ³•çš„èšç±»æ•ˆæœï¼š")
    
    results = {}
    for method in linkage_methods:
        # ğŸ”ï¸ å±‚æ¬¡èšç±»
        hierarchical = AgglomerativeClustering(
            n_clusters=4, 
            linkage=method
        )
        cluster_labels = hierarchical.fit_predict(X)
        
        results[method] = {
            'model': hierarchical,
            'labels': cluster_labels
        }
        
        # ğŸ“Š åˆ†æèšç±»ç»“æœ
        unique_labels = set(cluster_labels)
        print(f"\n   ğŸ”ï¸ {method.title()}é“¾æ¥:")
        
        for label in sorted(unique_labels):
            cluster_size = np.sum(cluster_labels == label)
            percentage = cluster_size / len(cluster_labels) * 100
            print(f"      ç°‡{label}: {cluster_size}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
    
    # ğŸŒ³ ç”Ÿæˆæ ‘çŠ¶å›¾æ•°æ®
    print(f"\nğŸŒ³ å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾åˆ†æ:")
    try:
        Z = linkage(X, method='ward')
        
        # è®¡ç®—æ¯ä¸ªåˆå¹¶æ­¥éª¤çš„è·ç¦»
        print("   ğŸ“Š åˆå¹¶è·ç¦»å˜åŒ–:")
        last_merges = Z[-10:, 2]  # æœ€å10æ¬¡åˆå¹¶çš„è·ç¦»
        for i, distance in enumerate(last_merges):
            step = len(Z) - 10 + i + 1
            print(f"      æ­¥éª¤{step}: è·ç¦»={distance:.2f}")
    except ImportError:
        print("   âš ï¸ scipyæœªå®‰è£…ï¼Œè·³è¿‡æ ‘çŠ¶å›¾åˆ†æ")
    
    return results

# è¿è¡Œå±‚æ¬¡èšç±»æ¼”ç¤º
hierarchical_results = hierarchical_clustering_demo()
```

#### ğŸŒŒ DBSCANèšç±»ï¼šå¯†åº¦æ¢æµ‹ä¸“å®¶

```python
# ğŸŒŒ DBSCANèšç±» - å¯†åº¦æ¢æµ‹ä¸“å®¶
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

def dbscan_clustering_demo():
    """DBSCANèšç±»æ¼”ç¤º"""
    print("\nğŸŒŒ DBSCANèšç±»ï¼šå¯†åº¦æ¢æµ‹ä¸“å®¶")
    print("=" * 50)
    
    # ğŸ“Š åˆ›å»ºæœˆç‰™å½¢æ•°æ®ï¼ˆK-Meansæ— æ³•å¾ˆå¥½å¤„ç†çš„å½¢çŠ¶ï¼‰
    X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)
    
    print(f"ğŸ“‹ æœˆç‰™å½¢æ•°æ®æ ·æœ¬æ•°: {X_moons.shape[0]}")
    
    # ğŸ§ª æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
    eps_values = [0.1, 0.2, 0.3, 0.4]
    min_samples_values = [3, 5, 10]
    
    print("\nğŸ”¬ ä¸åŒå‚æ•°çš„DBSCANæ•ˆæœï¼š")
    
    best_score = -1
    best_params = None
    best_labels = None
    
    for eps in eps_values:
        for min_samples in min_samples_values:
            # ğŸŒŒ DBSCANèšç±»
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(X_moons)
            
            # ğŸ“Š åˆ†æç»“æœ
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            # è®¡ç®—ç®€å•çš„è´¨é‡åˆ†æ•°ï¼ˆèšç±»æ•°æ¥è¿‘2ï¼Œå™ªå£°ç‚¹è¾ƒå°‘ï¼‰
            if n_clusters > 0:
                score = 1.0 / abs(n_clusters - 2) if n_clusters != 2 else 1.0
                score -= n_noise / len(cluster_labels)  # å™ªå£°ç‚¹æƒ©ç½š
            else:
                score = 0
            
            print(f"   ğŸŒŒ eps={eps}, min_samples={min_samples}:")
            print(f"      èšç±»æ•°: {n_clusters}, å™ªå£°ç‚¹: {n_noise}, å¾—åˆ†: {score:.3f}")
            
            if score > best_score:
                best_score = score
                best_params = (eps, min_samples)
                best_labels = cluster_labels
    
    print(f"\nğŸ† æœ€ä½³å‚æ•°: eps={best_params[0]}, min_samples={best_params[1]}")
    
    # ğŸ” è¯¦ç»†åˆ†ææœ€ä½³ç»“æœ
    final_dbscan = DBSCAN(eps=best_params[0], min_samples=best_params[1])
    final_labels = final_dbscan.fit_predict(X_moons)
    
    unique_labels = set(final_labels)
    print(f"\nğŸ” æœ€ä½³èšç±»ç»“æœåˆ†æ:")
    
    for label in sorted(unique_labels):
        if label == -1:
            cluster_size = np.sum(final_labels == label)
            print(f"   ğŸŒ«ï¸ å™ªå£°ç‚¹: {cluster_size}ä¸ª")
        else:
            cluster_size = np.sum(final_labels == label)
            percentage = cluster_size / len(final_labels) * 100
            print(f"   ğŸŒŒ ç°‡{label}: {cluster_size}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
    
    # ğŸ†š ä¸K-Meanså¯¹æ¯”
    print(f"\nğŸ†š ä¸K-Meanså¯¹æ¯” (æœˆç‰™å½¢æ•°æ®):")
    kmeans_moon = KMeans(n_clusters=2, random_state=42)
    kmeans_labels_moon = kmeans_moon.fit_predict(X_moons)
    
    print(f"   ğŸ¯ K-Means: å¼ºåˆ¶åˆ†ä¸ºåœ†å½¢ç°‡ï¼Œä¸é€‚åˆæœˆç‰™å½¢çŠ¶")
    print(f"   ğŸŒŒ DBSCAN: èƒ½å¤Ÿå‘ç°ä»»æ„å½¢çŠ¶çš„å¯†åº¦ç°‡")
    
    return final_dbscan, X_moons, final_labels

# è¿è¡ŒDBSCANæ¼”ç¤º
dbscan_model, X_dbscan, dbscan_labels = dbscan_clustering_demo()
```

### ğŸ“Š é™ç»´åˆ†æåŒº

é™ç»´å°±åƒæ˜¯**æ•°æ®å‹ç¼©ä¸“å®¶**ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå…³é”®ä¿¡æ¯çš„åŒæ—¶ï¼Œå‡å°‘æ•°æ®çš„å¤æ‚åº¦ã€‚

#### ğŸ“‰ PCAä¸»æˆåˆ†åˆ†æï¼šä¿¡æ¯æµ“ç¼©ä¸“å®¶

```python
# ğŸ“‰ PCAä¸»æˆåˆ†åˆ†æ - ä¿¡æ¯æµ“ç¼©ä¸“å®¶
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

def pca_analysis_demo():
    """PCAä¸»æˆåˆ†åˆ†ææ¼”ç¤º"""
    print("\nğŸ“‰ PCAä¸»æˆåˆ†åˆ†æï¼šä¿¡æ¯æµ“ç¼©ä¸“å®¶")
    print("=" * 60)
    
    # ğŸ“Š åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆé«˜ç»´æ•°æ®ï¼‰
    digits = load_digits()
    X_digits = digits.data
    y_digits = digits.target
    
    print(f"ğŸ“‹ åŸå§‹æ•°æ®ç»´åº¦: {X_digits.shape}")
    print(f"ğŸ¯ æ•°å­—ç±»åˆ«æ•°: {len(set(y_digits))}")
    
    # ğŸ“‰ æ‰§è¡ŒPCAé™ç»´
    pca_components = [2, 10, 20, 50]
    
    print("\nğŸ”¬ ä¸åŒä¸»æˆåˆ†æ•°é‡çš„PCAæ•ˆæœï¼š")
    
    pca_results = {}
    for n_components in pca_components:
        # ğŸ“‰ PCAé™ç»´
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X_digits)
        
        # ğŸ“Š è®¡ç®—ä¿¡æ¯ä¿ç•™æ¯”ä¾‹
        explained_variance_ratio = pca.explained_variance_ratio_
        total_variance = np.sum(explained_variance_ratio)
        
        pca_results[n_components] = {
            'model': pca,
            'transformed_data': X_pca,
            'explained_variance': total_variance,
            'individual_variance': explained_variance_ratio
        }
        
        print(f"   ğŸ“‰ {n_components}ä¸ªä¸»æˆåˆ†:")
        print(f"      ä¿ç•™ä¿¡æ¯æ¯”ä¾‹: {total_variance:.3f} ({total_variance*100:.1f}%)")
        print(f"      æ•°æ®ç»´åº¦: {X_digits.shape[1]} â†’ {X_pca.shape[1]}")
        print(f"      å‹ç¼©æ¯”ä¾‹: {X_pca.shape[1]/X_digits.shape[1]:.3f}")
    
    # ğŸ† é€‰æ‹©åˆé€‚çš„ä¸»æˆåˆ†æ•°é‡
    print(f"\nğŸ“ˆ ä¸»æˆåˆ†é‡è¦æ€§åˆ†æ (å‰10ä¸ªä¸»æˆåˆ†):")
    pca_10 = pca_results[10]['model']
    
    for i, variance in enumerate(pca_10.explained_variance_ratio_[:10]):
        print(f"   PC{i+1}: {variance:.4f} ({variance*100:.2f}%)")
    
    # ğŸ“Š ç´¯ç§¯æ–¹å·®è§£é‡Šå›¾
    cumulative_variance = np.cumsum(pca_10.explained_variance_ratio_)
    print(f"\nğŸ“Š ç´¯ç§¯æ–¹å·®è§£é‡Š (å‰10ä¸ªä¸»æˆåˆ†):")
    for i, cum_var in enumerate(cumulative_variance[:10]):
        print(f"   å‰{i+1}ä¸ªPC: {cum_var:.3f} ({cum_var*100:.1f}%)")
    
    # ğŸ¯ å®é™…åº”ç”¨ï¼š2Då¯è§†åŒ–
    print(f"\nğŸ¯ 2Då¯è§†åŒ–åº”ç”¨:")
    X_2d = pca_results[2]['transformed_data']
    print(f"   å°†{X_digits.shape[1]}ç»´æ•°æ®å‹ç¼©åˆ°2ç»´")
    print(f"   ä¿¡æ¯ä¿ç•™: {pca_results[2]['explained_variance']*100:.1f}%")
    print(f"   å¯ç”¨äºæ•°æ®å¯è§†åŒ–å’Œåˆæ­¥åˆ†æ")
    
    return pca_results

# è¿è¡ŒPCAæ¼”ç¤º
pca_results = pca_analysis_demo()
```

#### ğŸ¨ t-SNEå¯è§†åŒ–ï¼šéçº¿æ€§æ˜ å°„ä¸“å®¶

```python
# ğŸ¨ t-SNEå¯è§†åŒ– - éçº¿æ€§æ˜ å°„ä¸“å®¶
from sklearn.manifold import TSNE

def tsne_visualization_demo():
    """t-SNEå¯è§†åŒ–æ¼”ç¤º"""
    print("\nğŸ¨ t-SNEå¯è§†åŒ–ï¼šéçº¿æ€§æ˜ å°„ä¸“å®¶")
    print("=" * 60)
    
    # ğŸ“Š ä½¿ç”¨æ‰‹å†™æ•°å­—æ•°æ®çš„å­é›†ï¼ˆt-SNEè®¡ç®—è¾ƒæ…¢ï¼‰
    digits = load_digits()
    X_subset = digits.data[:500]  # åªç”¨500ä¸ªæ ·æœ¬
    y_subset = digits.target[:500]
    
    print(f"ğŸ“‹ æ•°æ®å­é›†: {X_subset.shape[0]}ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š åŸå§‹ç»´åº¦: {X_subset.shape[1]}ç»´")
    
    # ğŸ¨ ä¸åŒå‚æ•°çš„t-SNE
    perplexity_values = [5, 30, 50]
    
    print("\nğŸ”¬ ä¸åŒå›°æƒ‘åº¦(perplexity)çš„t-SNEæ•ˆæœï¼š")
    
    tsne_results = {}
    for perplexity in perplexity_values:
        print(f"   ğŸ¨ è®­ç»ƒt-SNE (å›°æƒ‘åº¦={perplexity})...")
        
        # ğŸ¨ t-SNEé™ç»´
        tsne = TSNE(
            n_components=2, 
            perplexity=perplexity, 
            random_state=42,
            max_iter=300
        )
        X_tsne = tsne.fit_transform(X_subset)
        
        tsne_results[perplexity] = {
            'model': tsne,
            'transformed_data': X_tsne
        }
        
        print(f"      âœ… å®Œæˆ! æœ€ç»ˆKLæ•£åº¦: {tsne.kl_divergence_:.2f}")
    
    # ğŸ†š PCA vs t-SNEå¯¹æ¯”
    print(f"\nğŸ†š PCA vs t-SNE å¯è§†åŒ–å¯¹æ¯”:")
    
    # PCA 2D
    pca_2d = PCA(n_components=2)
    X_pca_2d = pca_2d.fit_transform(X_subset)
    
    print(f"   ğŸ“‰ PCA 2Dæ˜ å°„:")
    print(f"      ä¿¡æ¯ä¿ç•™: {np.sum(pca_2d.explained_variance_ratio_)*100:.1f}%")
    print(f"      ç‰¹ç‚¹: çº¿æ€§é™ç»´ï¼Œä¿æŒå…¨å±€ç»“æ„")
    
    print(f"   ğŸ¨ t-SNE 2Dæ˜ å°„:")
    print(f"      ç‰¹ç‚¹: éçº¿æ€§é™ç»´ï¼Œä¿æŒå±€éƒ¨ç»“æ„")
    print(f"      ä¼˜åŠ¿: èƒ½æ­ç¤ºå¤æ‚çš„èšç±»ç»“æ„")
    
    # ğŸ¯ åº”ç”¨å»ºè®®
    print(f"\nğŸ¯ é™ç»´æ–¹æ³•é€‰æ‹©å»ºè®®:")
    print(f"   ğŸ“‰ PCAé€‚ç”¨äº:")
    print(f"      â€¢ å¿«é€Ÿé™ç»´å’Œç‰¹å¾å‹ç¼©")
    print(f"      â€¢ ä¿æŒæ•°æ®çš„å…¨å±€ç»“æ„")
    print(f"      â€¢ ç†è§£ä¸»è¦å˜åŒ–æ–¹å‘")
    
    print(f"   ğŸ¨ t-SNEé€‚ç”¨äº:")
    print(f"      â€¢ æ•°æ®å¯è§†åŒ–å’Œæ¢ç´¢")
    print(f"      â€¢ å‘ç°å±€éƒ¨èšç±»ç»“æ„")
    print(f"      â€¢ é«˜ç»´æ•°æ®çš„æ¨¡å¼è¯†åˆ«")
    
    return tsne_results, X_pca_2d

# è¿è¡Œt-SNEæ¼”ç¤º
tsne_results, X_pca_comparison = tsne_visualization_demo()
```

---

## ğŸ“Š 20.4 æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–å®éªŒå®¤

### ğŸ­ è´¨é‡æ§åˆ¶æ€»æŒ‡æŒ¥éƒ¨

æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–å°±åƒæ˜¯å·¥å‚çš„**è´¨é‡æ§åˆ¶éƒ¨é—¨**ï¼Œç¡®ä¿æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ äº§å“è¾¾åˆ°æœ€é«˜æ ‡å‡†ã€‚

```python
# ğŸ“Š æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–ç³»ç»Ÿ
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.pipeline import Pipeline

class ModelEvaluationLab:
    """æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–å®éªŒå®¤"""
    
    def __init__(self):
        self.lab_name = "æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–å®éªŒå®¤"
        
    def cross_validation_demo(self):
        """äº¤å‰éªŒè¯æ¼”ç¤º"""
        print("ğŸ”¬ äº¤å‰éªŒè¯ï¼šå¯é æ€§æ£€æµ‹ä¸“å®¶")
        print("=" * 50)
        
        # ğŸ“Š ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®
        iris = load_iris()
        X, y = iris.data, iris.target
        
        # ğŸ§ª æµ‹è¯•ä¸åŒç®—æ³•çš„ç¨³å®šæ€§
        algorithms = {
            "éšæœºæ£®æ—": RandomForestClassifier(random_state=42),
            "é€»è¾‘å›å½’": LogisticRegression(random_state=42),
            "SVM": SVC(random_state=42)
        }
        
        print("ğŸ¯ 5æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
        
        for name, algorithm in algorithms.items():
            # ğŸ”„ 5æŠ˜äº¤å‰éªŒè¯
            cv_scores = cross_val_score(algorithm, X, y, cv=5, scoring='accuracy')
            
            mean_score = cv_scores.mean()
            std_score = cv_scores.std()
            
            print(f"\nğŸ“Š {name}:")
            print(f"   å¹³å‡å‡†ç¡®ç‡: {mean_score:.4f} (Â±{std_score:.4f})")
            print(f"   å„æŠ˜å¾—åˆ†: {cv_scores}")
            print(f"   ç¨³å®šæ€§: {'ğŸ¯ ç¨³å®š' if std_score < 0.05 else 'âš ï¸ æ³¢åŠ¨è¾ƒå¤§'}")
    
    def hyperparameter_optimization_demo(self):
        """è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º"""
        print("\nğŸ”§ è¶…å‚æ•°ä¼˜åŒ–ï¼šæ€§èƒ½è°ƒä¼˜ä¸“å®¶")
        print("=" * 50)
        
        # ğŸ“Š å‡†å¤‡æ•°æ®
        iris = load_iris()
        X, y = iris.data, iris.target
        
        # ğŸ¯ éšæœºæ£®æ—è¶…å‚æ•°ä¼˜åŒ–
        rf = RandomForestClassifier(random_state=42)
        
        # ğŸ”§ å‚æ•°æœç´¢ç©ºé—´
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, None],
            'min_samples_split': [2, 5, 10]
        }
        
        print("ğŸ” ç½‘æ ¼æœç´¢ä¼˜åŒ–ä¸­...")
        
        # ğŸ” ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            rf, param_grid, cv=5, 
            scoring='accuracy', 
            n_jobs=-1
        )
        grid_search.fit(X, y)
        
        print(f"ğŸ† æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"ğŸ¯ æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
        
        # ğŸ“Š å‚æ•°é‡è¦æ€§åˆ†æ
        print(f"\nğŸ“ˆ å‚æ•°å½±å“åˆ†æ:")
        results_df = pd.DataFrame(grid_search.cv_results_)
        
        # åˆ†æn_estimatorsçš„å½±å“
        for n_est in [50, 100, 200]:
            subset = results_df[results_df['param_n_estimators'] == n_est]
            avg_score = subset['mean_test_score'].mean()
            print(f"   n_estimators={n_est}: å¹³å‡å¾—åˆ† {avg_score:.4f}")
        
        return grid_search.best_estimator_

# è¿è¡Œæ¨¡å‹è¯„ä¼°æ¼”ç¤º
eval_lab = ModelEvaluationLab()
eval_lab.cross_validation_demo()
best_model = eval_lab.hyperparameter_optimization_demo()
```

---

## ğŸ† 20.5 ç« èŠ‚æ€»ç»“ä¸æˆå°±å›é¡¾

### ğŸ¯ å­¦ä¹ æˆå°±æ¸…å•

æ­å–œä½ å®Œæˆäº†Scikit-learnæ™ºèƒ½å®éªŒå·¥å‚çš„å…¨é¢æ¢ç´¢ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä»Šå¤©çš„æƒŠäººæˆå°±ï¼š

```python
# ğŸ† ç¬¬20ç« å­¦ä¹ æˆå°±ç»Ÿè®¡
class Chapter20Achievements:
    """ç¬¬20ç« å­¦ä¹ æˆå°±ç»Ÿè®¡"""
    
    def __init__(self):
        self.achievements = {
            "ğŸ§  ç®—æ³•æŒæ¡": {
                "ç›‘ç£å­¦ä¹ ç®—æ³•": 8,  # é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€SVMã€è´å¶æ–¯ç­‰
                "æ— ç›‘ç£å­¦ä¹ ç®—æ³•": 5,  # K-Meansã€å±‚æ¬¡èšç±»ã€DBSCANã€PCAã€t-SNE
                "ç®—æ³•PKç»éªŒ": 3,     # åˆ†ç±»ã€èšç±»ã€é™ç»´å¯¹æ¯”
                "æ€»è®¡ç®—æ³•": 13
            },
            "ğŸ—ï¸ å®æˆ˜é¡¹ç›®": {
                "æ™ºèƒ½é‚®ä»¶åˆ†ç±»ç³»ç»Ÿ": "âœ… å®Œæˆ",
                "æ™ºèƒ½è‚¡ä»·é¢„æµ‹ç³»ç»Ÿ": "âœ… å®Œæˆ", 
                "å®¢æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ": "âœ… å®Œæˆ",
                "é¡¹ç›®æ€»æ•°": 3
            },
            "ğŸ› ï¸ æŠ€èƒ½è§£é”": {
                "Scikit-learnç”Ÿæ€æŒæ¡": "âœ…",
                "æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–": "âœ…",
                "è¶…å‚æ•°è°ƒä¼˜": "âœ…",
                "äº¤å‰éªŒè¯": "âœ…",
                "ç‰¹å¾å·¥ç¨‹": "âœ…",
                "æ•°æ®å¯è§†åŒ–": "âœ…"
            },
            "ğŸ“Š ä»£ç é‡ç»Ÿè®¡": {
                "æ€»ä»£ç è¡Œæ•°": "çº¦1000è¡Œ",
                "å®Œæ•´å‡½æ•°": "20+ä¸ª",
                "ç¤ºä¾‹é¡¹ç›®": "3ä¸ªå®Œæ•´ç³»ç»Ÿ",
                "ç®—æ³•æ¼”ç¤º": "13ä¸ª"
            }
        }
    
    def show_achievements(self):
        """å±•ç¤ºå­¦ä¹ æˆå°±"""
        print("ğŸ† ç¬¬20ç« å­¦ä¹ æˆå°±æŠ¥å‘Š")
        print("=" * 60)
        
        for category, items in self.achievements.items():
            print(f"\nâœ¨ {category}:")
            if isinstance(items, dict):
                for key, value in items.items():
                    print(f"   ğŸ“‹ {key}: {value}")
            else:
                print(f"   ğŸ“‹ {items}")
        
        print(f"\nğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†Scikit-learnçš„æ ¸å¿ƒæŠ€èƒ½ï¼")
        print(f"ğŸš€ ç°åœ¨ä½ å…·å¤‡äº†ç‹¬ç«‹å¼€å‘æœºå™¨å­¦ä¹ é¡¹ç›®çš„èƒ½åŠ›ï¼")

# å±•ç¤ºå­¦ä¹ æˆå°±
achievements = Chapter20Achievements()
achievements.show_achievements()
```

### ğŸ“ˆ æŠ€èƒ½è¿›æ­¥å¯¹æ¯”

```python
# ğŸ“ˆ æŠ€èƒ½è¿›æ­¥è¯„ä¼°
def skill_progress_assessment():
    """æŠ€èƒ½è¿›æ­¥è¯„ä¼°"""
    print("\nğŸ“ˆ ä»ç¬¬19ç« åˆ°ç¬¬20ç« çš„æŠ€èƒ½è¿›æ­¥")
    print("=" * 50)
    
    progress_map = {
        "ğŸ§  æœºå™¨å­¦ä¹ ç†è§£": {
            "ç¬¬19ç« ": "åŸºç¡€æ¦‚å¿µç†è§£",
            "ç¬¬20ç« ": "æ·±åº¦æŒæ¡15+ç®—æ³•",
            "è¿›æ­¥": "ä»ç†è®ºåˆ°å®è·µçš„è´¨çš„é£è·ƒ"
        },
        "ğŸ› ï¸ ç¼–ç¨‹èƒ½åŠ›": {
            "ç¬¬19ç« ": "ç®€å•çš„æˆ¿ä»·é¢„æµ‹é¡¹ç›®",
            "ç¬¬20ç« ": "3ä¸ªå®Œæ•´çš„ä¼ä¸šçº§é¡¹ç›®",
            "è¿›æ­¥": "å…·å¤‡ç‹¬ç«‹å¼€å‘MLç³»ç»Ÿèƒ½åŠ›"
        },
        "ğŸ“Š æ•°æ®å¤„ç†": {
            "ç¬¬19ç« ": "åŸºç¡€æ•°æ®æ¸…æ´—",
            "ç¬¬20ç« ": "ç‰¹å¾å·¥ç¨‹+é™ç»´+è¯„ä¼°",
            "è¿›æ­¥": "å…¨æ ˆæ•°æ®ç§‘å­¦æŠ€èƒ½"
        },
        "ğŸ¯ é—®é¢˜è§£å†³": {
            "ç¬¬19ç« ": "è·Ÿéšæ•™ç¨‹å®Œæˆ",
            "ç¬¬20ç« ": "è‡ªä¸»é€‰æ‹©ç®—æ³•è§£å†³é—®é¢˜",
            "è¿›æ­¥": "å…·å¤‡ç®—æ³•é€‰æ‹©åˆ¤æ–­åŠ›"
        }
    }
    
    for skill, details in progress_map.items():
        print(f"\n{skill}")
        print(f"   ğŸ“š ç¬¬19ç« : {details['ç¬¬19ç« ']}")
        print(f"   ğŸš€ ç¬¬20ç« : {details['ç¬¬20ç« ']}")
        print(f"   ğŸ“ˆ è¿›æ­¥: {details['è¿›æ­¥']}")

# å±•ç¤ºæŠ€èƒ½è¿›æ­¥
skill_progress_assessment()
```

### ğŸ”¥ æ ¸å¿ƒçŸ¥è¯†è¦ç‚¹

```python
# ğŸ”¥ ç¬¬20ç« æ ¸å¿ƒçŸ¥è¯†æ€»ç»“
def chapter_20_key_takeaways():
    """ç¬¬20ç« æ ¸å¿ƒè¦ç‚¹"""
    print("\nğŸ”¥ ç¬¬20ç« æ ¸å¿ƒçŸ¥è¯†è¦ç‚¹")
    print("=" * 50)
    
    key_points = {
        "ğŸ¯ ç›‘ç£å­¦ä¹ ç²¾å": [
            "åˆ†ç±»ç®—æ³•ï¼šé€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€SVMã€æœ´ç´ è´å¶æ–¯",
            "å›å½’ç®—æ³•ï¼šçº¿æ€§/Ridge/Lassoå›å½’ã€æ ‘åŸºå›å½’",
            "ç®—æ³•é€‰æ‹©åŸåˆ™ï¼šæ•°æ®ç‰¹ç‚¹å†³å®šç®—æ³•é€‰æ‹©",
            "æ€§èƒ½è¯„ä¼°ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°"
        ],
        "ğŸ•µï¸ æ— ç›‘ç£å­¦ä¹ ç²¾å": [
            "èšç±»ç®—æ³•ï¼šK-Meansã€å±‚æ¬¡èšç±»ã€DBSCANå„æœ‰ç‰¹è‰²",
            "é™ç»´æŠ€æœ¯ï¼šPCAä¿æŒå…¨å±€ç»“æ„ï¼Œt-SNEæ­ç¤ºå±€éƒ¨æ¨¡å¼",
            "è¯„ä¼°æ–¹æ³•ï¼šè½®å»“ç³»æ•°ã€è‚˜éƒ¨æ³•åˆ™ã€ä¸šåŠ¡éªŒè¯",
            "åº”ç”¨åœºæ™¯ï¼šå®¢æˆ·åˆ†ç¾¤ã€å¼‚å¸¸æ£€æµ‹ã€æ•°æ®æ¢ç´¢"
        ],
        "ğŸ“Š æ¨¡å‹ä¼˜åŒ–ç²¾å": [
            "äº¤å‰éªŒè¯ï¼šè¯„ä¼°æ¨¡å‹ç¨³å®šæ€§çš„é‡‘æ ‡å‡†",
            "è¶…å‚æ•°è°ƒä¼˜ï¼šç½‘æ ¼æœç´¢vséšæœºæœç´¢",
            "ç‰¹å¾å·¥ç¨‹ï¼šæ•°æ®é¢„å¤„ç†ã€ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾æ„é€ ",
            "ç®¡é“åŒ–ï¼šsklearn.pipelineç»Ÿä¸€æ•°æ®æµ"
        ],
        "ğŸš€ å®æˆ˜é¡¹ç›®ç²¾å": [
            "é‚®ä»¶åˆ†ç±»ï¼šæ–‡æœ¬ç‰¹å¾æå–+å¤šç®—æ³•å¯¹æ¯”",
            "è‚¡ä»·é¢„æµ‹ï¼šæ—¶åºç‰¹å¾å·¥ç¨‹+å›å½’ç®—æ³•",
            "å®¢æˆ·åˆ†æï¼šèšç±»+é™ç»´+å•†ä¸šæ´å¯Ÿ",
            "é¡¹ç›®æ€ç»´ï¼šé—®é¢˜å®šä¹‰â†’æ•°æ®å‡†å¤‡â†’æ¨¡å‹é€‰æ‹©â†’ç»“æœè¯„ä¼°"
        ]
    }
    
    for category, points in key_points.items():
        print(f"\n{category}")
        for i, point in enumerate(points, 1):
            print(f"   {i}. {point}")

# å±•ç¤ºæ ¸å¿ƒè¦ç‚¹
chapter_20_key_takeaways()
```

### ğŸ¤” æ·±åº¦æ€è€ƒé¢˜

å®Œæˆä»¥ä¸‹æ€è€ƒé¢˜ï¼Œæ£€éªŒä½ å¯¹Scikit-learnçš„ç†è§£æ·±åº¦ï¼š

#### ğŸ§  æ€è€ƒé¢˜1ï¼šç®—æ³•é€‰æ‹©è‰ºæœ¯

```python
# ğŸ§  æ€è€ƒé¢˜1ï¼šä¸ºä»¥ä¸‹åœºæ™¯é€‰æ‹©æœ€é€‚åˆçš„ç®—æ³•ï¼Œå¹¶è¯´æ˜ç†ç”±

scenarios = {
    "åœºæ™¯A": {
        "æè¿°": "é“¶è¡Œéœ€è¦è¯†åˆ«ä¿¡ç”¨å¡æ¬ºè¯ˆäº¤æ˜“ï¼Œæ•°æ®é«˜åº¦ä¸å¹³è¡¡(æ¬ºè¯ˆäº¤æ˜“<1%)",
        "æ•°æ®ç‰¹å¾": "è¿ç»­æ•°å€¼ç‰¹å¾ï¼Œæ ‡ç­¾æ˜ç¡®ï¼Œå®æ—¶æ€§è¦æ±‚é«˜",
        "æ€è€ƒè¦ç‚¹": ["å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ", "å“ªç§ç®—æ³•é€‚åˆå®æ—¶é¢„æµ‹ï¼Ÿ", "å¦‚ä½•ç¡®ä¿é«˜å¬å›ç‡ï¼Ÿ"]
    },
    "åœºæ™¯B": {
        "æè¿°": "ç”µå•†å…¬å¸æƒ³å¯¹100ä¸‡ç”¨æˆ·è¿›è¡Œä¸ªæ€§åŒ–åˆ†ç¾¤ï¼Œç”¨äºç²¾å‡†è¥é”€",
        "æ•°æ®ç‰¹å¾": "é«˜ç»´ç¨€ç–ç‰¹å¾ï¼Œæ— æ ‡ç­¾ï¼Œç”¨æˆ·è¡Œä¸ºå·®å¼‚å¤§",
        "æ€è€ƒè¦ç‚¹": ["é€‰æ‹©å“ªç§èšç±»ç®—æ³•ï¼Ÿ", "å¦‚ä½•ç¡®å®šèšç±»æ•°é‡ï¼Ÿ", "å¦‚ä½•å¤„ç†é«˜ç»´æ•°æ®ï¼Ÿ"]
    },
    "åœºæ™¯C": {
        "æè¿°": "åŒ»é™¢æƒ³é¢„æµ‹æ‚£è€…ä½é™¢æ—¶é•¿ï¼Œç”¨äºèµ„æºè§„åˆ’",
        "æ•°æ®ç‰¹å¾": "æ··åˆç±»å‹ç‰¹å¾ï¼Œæ ‡ç­¾è¿ç»­ï¼Œéœ€è¦å¯è§£é‡Šæ€§",
        "æ€è€ƒè¦ç‚¹": ["é€‰æ‹©å“ªç§å›å½’ç®—æ³•ï¼Ÿ", "å¦‚ä½•ä¿è¯å¯è§£é‡Šæ€§ï¼Ÿ", "å¦‚ä½•å¤„ç†æ··åˆç‰¹å¾ï¼Ÿ"]
    }
}

print("ğŸ§  æ€è€ƒé¢˜1ï¼šç®—æ³•é€‰æ‹©è‰ºæœ¯")
print("=" * 50)
for name, scenario in scenarios.items():
    print(f"\nğŸ“‹ {name}: {scenario['æè¿°']}")
    print(f"   æ•°æ®ç‰¹å¾: {scenario['æ•°æ®ç‰¹å¾']}")
    print(f"   æ€è€ƒè¦ç‚¹:")
    for point in scenario['æ€è€ƒè¦ç‚¹']:
        print(f"      â€¢ {point}")
    print(f"   ğŸ’­ ä½ çš„ç­”æ¡ˆ: ____________________")
```

#### ğŸ” æ€è€ƒé¢˜2ï¼šæ¨¡å‹è¯Šæ–­ä¸“å®¶

```python
# ğŸ” æ€è€ƒé¢˜2ï¼šæ ¹æ®ä»¥ä¸‹æ¨¡å‹è¡¨ç°ï¼Œè¯Šæ–­é—®é¢˜å¹¶æå‡ºè§£å†³æ–¹æ¡ˆ

model_performance = {
    "æ¨¡å‹A": {
        "è®­ç»ƒå‡†ç¡®ç‡": 0.98,
        "éªŒè¯å‡†ç¡®ç‡": 0.75,
        "æµ‹è¯•å‡†ç¡®ç‡": 0.73,
        "é—®é¢˜è¯Šæ–­": "è¿‡æ‹Ÿåˆ",
        "è§£å†³æ–¹æ¡ˆ": ["é™ä½æ¨¡å‹å¤æ‚åº¦", "å¢åŠ æ­£åˆ™åŒ–", "æ”¶é›†æ›´å¤šæ•°æ®", "ç‰¹å¾é€‰æ‹©"]
    },
    "æ¨¡å‹B": {
        "è®­ç»ƒå‡†ç¡®ç‡": 0.82,
        "éªŒè¯å‡†ç¡®ç‡": 0.81,
        "æµ‹è¯•å‡†ç¡®ç‡": 0.83,
        "é—®é¢˜è¯Šæ–­": "æ¬ æ‹Ÿåˆ",
        "è§£å†³æ–¹æ¡ˆ": ["å¢åŠ æ¨¡å‹å¤æ‚åº¦", "ç‰¹å¾å·¥ç¨‹", "å‡å°‘æ­£åˆ™åŒ–", "é›†æˆæ–¹æ³•"]
    },
    "æ¨¡å‹C": {
        "è®­ç»ƒå‡†ç¡®ç‡": 0.90,
        "éªŒè¯å‡†ç¡®ç‡": 0.89,
        "æµ‹è¯•å‡†ç¡®ç‡": 0.45,
        "é—®é¢˜è¯Šæ–­": "æ•°æ®åˆ†å¸ƒåç§»",
        "è§£å†³æ–¹æ¡ˆ": ["æ•°æ®è´¨é‡æ£€æŸ¥", "ç‰¹å¾åˆ†å¸ƒåˆ†æ", "é¢†åŸŸé€‚åº”", "é‡æ–°é‡‡æ ·"]
    }
}

print("\nğŸ” æ€è€ƒé¢˜2ï¼šæ¨¡å‹è¯Šæ–­ä¸“å®¶")
print("=" * 50)
print("æ ¹æ®ä»¥ä¸‹è¡¨ç°æ•°æ®ï¼Œä½ èƒ½è¯Šæ–­å‡ºä»€ä¹ˆé—®é¢˜ï¼Ÿ")

for model, metrics in model_performance.items():
    print(f"\nğŸ“Š {model}:")
    print(f"   è®­ç»ƒå‡†ç¡®ç‡: {metrics['è®­ç»ƒå‡†ç¡®ç‡']}")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {metrics['éªŒè¯å‡†ç¡®ç‡']}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {metrics['æµ‹è¯•å‡†ç¡®ç‡']}")
    print(f"   ğŸ’­ ä½ çš„è¯Šæ–­: ____________________")
    print(f"   ğŸ”§ ä½ çš„è§£å†³æ–¹æ¡ˆ: ____________________")
```

#### ğŸ¯ æ€è€ƒé¢˜3ï¼šä¸šåŠ¡ä»·å€¼è½¬åŒ–

```python
# ğŸ¯ æ€è€ƒé¢˜3ï¼šå¦‚ä½•å°†æŠ€æœ¯æŒ‡æ ‡è½¬åŒ–ä¸ºä¸šåŠ¡ä»·å€¼ï¼Ÿ

technical_to_business = {
    "å®¢æˆ·åˆ†ç¾¤é¡¹ç›®": {
        "æŠ€æœ¯æˆæœ": "å‘ç°5ä¸ªå®¢æˆ·ç¾¤ä½“ï¼Œè½®å»“ç³»æ•°0.7",
        "ä¸šåŠ¡é—®é¢˜": [
            "å¦‚ä½•ä¸ºæ¯ä¸ªç¾¤ä½“è®¾è®¡å·®å¼‚åŒ–è¥é”€ç­–ç•¥ï¼Ÿ",
            "å“ªä¸ªç¾¤ä½“æœ€æœ‰ä»·å€¼ï¼Œåº”è¯¥é‡ç‚¹æŠ•å…¥ï¼Ÿ",
            "å¦‚ä½•è¡¡é‡åˆ†ç¾¤æ•ˆæœå¯¹ä¸šåŠ¡çš„å®é™…å½±å“ï¼Ÿ"
        ]
    },
    "é”€é‡é¢„æµ‹é¡¹ç›®": {
        "æŠ€æœ¯æˆæœ": "RMSE=100ï¼ŒRÂ²=0.85",
        "ä¸šåŠ¡é—®é¢˜": [
            "é¢„æµ‹è¯¯å·®100å¯¹åº“å­˜ç®¡ç†æ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
            "85%çš„è§£é‡Šåº¦åœ¨ä¸šåŠ¡ä¸Šå¦‚ä½•ç†è§£ï¼Ÿ",
            "å¦‚ä½•åŸºäºé¢„æµ‹ç»“æœåˆ¶å®šé‡‡è´­è®¡åˆ’ï¼Ÿ"
        ]
    }
}

print("\nğŸ¯ æ€è€ƒé¢˜3ï¼šä¸šåŠ¡ä»·å€¼è½¬åŒ–")
print("=" * 50)
for project, details in technical_to_business.items():
    print(f"\nğŸ“ˆ {project}")
    print(f"   æŠ€æœ¯æˆæœ: {details['æŠ€æœ¯æˆæœ']}")
    print(f"   ä¸šåŠ¡é—®é¢˜:")
    for question in details['ä¸šåŠ¡é—®é¢˜']:
        print(f"      â€¢ {question}")
        print(f"        ğŸ’­ ä½ çš„å›ç­”: ____________________")
```

#### ğŸš€ æ€è€ƒé¢˜4ï¼šé¡¹ç›®æ‰©å±•è®¾è®¡

```python
# ğŸš€ æ€è€ƒé¢˜4ï¼šå¦‚ä½•æ‰©å±•æˆ‘ä»¬çš„å®æˆ˜é¡¹ç›®ï¼Ÿ

project_extensions = {
    "é‚®ä»¶åˆ†ç±»ç³»ç»Ÿ": [
        "å¦‚ä½•å¤„ç†å¤šè¯­è¨€é‚®ä»¶ï¼Ÿ",
        "å¦‚ä½•å®ç°åœ¨çº¿å­¦ä¹ ï¼ŒæŒç»­æ”¹è¿›æ¨¡å‹ï¼Ÿ",
        "å¦‚ä½•é›†æˆåˆ°ç°æœ‰é‚®ä»¶ç³»ç»Ÿä¸­ï¼Ÿ",
        "å¦‚ä½•å¤„ç†æ–°å‹åƒåœ¾é‚®ä»¶æ”»å‡»ï¼Ÿ"
    ],
    "è‚¡ä»·é¢„æµ‹ç³»ç»Ÿ": [
        "å¦‚ä½•èåˆæ–°é—»æƒ…æ„Ÿåˆ†æï¼Ÿ",
        "å¦‚ä½•å¤„ç†å¸‚åœºå¼‚å¸¸äº‹ä»¶ï¼ˆå¦‚ç–«æƒ…ï¼‰ï¼Ÿ",
        "å¦‚ä½•æ„å»ºäº¤æ˜“ç­–ç•¥å›æµ‹ç³»ç»Ÿï¼Ÿ",
        "å¦‚ä½•è¯„ä¼°é¢„æµ‹çš„ç»æµä»·å€¼ï¼Ÿ"
    ],
    "å®¢æˆ·è¡Œä¸ºåˆ†æ": [
        "å¦‚ä½•å®ç°å®æ—¶å®¢æˆ·ç”»åƒæ›´æ–°ï¼Ÿ",
        "å¦‚ä½•é¢„æµ‹å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ï¼Ÿ",
        "å¦‚ä½•è®¾è®¡A/Bæµ‹è¯•éªŒè¯åˆ†ç¾¤æ•ˆæœï¼Ÿ",
        "å¦‚ä½•ä¿æŠ¤å®¢æˆ·éšç§ï¼Ÿ"
    ]
}

print("\nğŸš€ æ€è€ƒé¢˜4ï¼šé¡¹ç›®æ‰©å±•è®¾è®¡")
print("=" * 50)
for project, questions in project_extensions.items():
    print(f"\nğŸ¯ {project} æ‰©å±•æŒ‘æˆ˜:")
    for i, question in enumerate(questions, 1):
        print(f"   {i}. {question}")
        print(f"      ğŸ’¡ ä½ çš„è®¾è®¡æ€è·¯: ____________________")
```

---

## ğŸŒŸ 20.6 å±•æœ›æœªæ¥ï¼šç¬¬21ç« é¢„å‘Š

### ğŸ”® ä¸‹ä¸€ç«™ï¼šæ·±åº¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œä¸–ç•Œ

æ­å–œä½ å·²ç»æˆä¸ºScikit-learnçš„ä¸“å®¶ï¼ä½†æˆ‘ä»¬çš„AIä¹‹æ—…æ‰åˆšåˆšå¼€å§‹ã€‚åœ¨ç¬¬21ç« ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ä¸TensorFlowåŸºç¡€ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š

```python
# ğŸ”® ç¬¬21ç« ç²¾å½©é¢„å‘Š
def chapter_21_preview():
    """ç¬¬21ç« ç²¾å½©å†…å®¹é¢„å‘Š"""
    print("ğŸ”® ç¬¬21ç« ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ä¸TensorFlowåŸºç¡€ã€‹é¢„å‘Š")
    print("=" * 60)
    
    upcoming_content = {
        "ğŸ§  ç¥ç»ç½‘ç»œåŸºç¡€": [
            "ä»ç”Ÿç‰©ç¥ç»å…ƒåˆ°äººå·¥ç¥ç»å…ƒ",
            "åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦åŸç†",
            "æ¿€æ´»å‡½æ•°çš„é€‰æ‹©è‰ºæœ¯",
            "æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨è¯¦è§£"
        ],
        "ğŸ› ï¸ TensorFlowå®æˆ˜": [
            "TensorFlow 2.xç”Ÿæ€ç³»ç»Ÿ",
            "Kerasé«˜çº§APIå¿«é€Ÿä¸Šæ‰‹",
            "æ¨¡å‹æ„å»ºçš„ä¸‰ç§æ–¹å¼",
            "GPUåŠ é€Ÿè®­ç»ƒæŠ€å·§"
        ],
        "ğŸ¯ æ·±åº¦å­¦ä¹ é¡¹ç›®": [
            "æ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿï¼ˆCNNï¼‰",
            "ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æï¼ˆRNNï¼‰",
            "å›¾åƒåˆ†ç±»æŒ‘æˆ˜èµ›ï¼ˆResNetï¼‰",
            "è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨ï¼ˆLSTMï¼‰"
        ],
        "ğŸš€ å‰æ²¿æŠ€æœ¯æ¢ç´¢": [
            "Transfer Learningè¿ç§»å­¦ä¹ ",
            "æ•°æ®å¢å¼ºæŠ€æœ¯",
            "æ¨¡å‹å¯è§†åŒ–ä¸è§£é‡Š",
            "ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—"
        ]
    }
    
    for category, topics in upcoming_content.items():
        print(f"\n{category}")
        for topic in topics:
            print(f"   ğŸ”¹ {topic}")
    
    print(f"\nğŸ‰ ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ çš„åä¸½è½¬èº«å³å°†å¼€å§‹ï¼")

# å±•ç¤ºç¬¬21ç« é¢„å‘Š
chapter_21_preview()
```

### ğŸ’¡ å­¦ä¹ å»ºè®®ä¸å‡†å¤‡

```python
# ğŸ’¡ ç¬¬21ç« å­¦ä¹ å‡†å¤‡å»ºè®®
def learning_preparation():
    """ç¬¬21ç« å­¦ä¹ å‡†å¤‡å»ºè®®"""
    print("\nğŸ’¡ ç¬¬21ç« å­¦ä¹ å‡†å¤‡å»ºè®®")
    print("=" * 50)
    
    preparation_guide = {
        "ğŸ§® æ•°å­¦åŸºç¡€": [
            "å¤ä¹ çº¿æ€§ä»£æ•°ï¼ˆçŸ©é˜µè¿ç®—ï¼‰",
            "ç†è§£å¯¼æ•°ä¸æ¢¯åº¦æ¦‚å¿µ",
            "äº†è§£æ¦‚ç‡è®ºåŸºç¡€",
            "å»ºè®®ï¼šå¯ä»¥è¾¹å­¦è¾¹è¡¥å……ï¼Œä¸å¿…è¿‡åˆ†æ‹…å¿ƒ"
        ],
        "ğŸ’» ç¯å¢ƒå‡†å¤‡": [
            "å®‰è£…TensorFlow 2.x",
            "é…ç½®GPUç¯å¢ƒï¼ˆå¦‚æœæœ‰æ˜¾å¡ï¼‰",
            "ç†Ÿæ‚‰Jupyter Notebook",
            "å»ºè®®ï¼šå…ˆç”¨CPUç‰ˆæœ¬ï¼Œåç»­å†å‡çº§GPU"
        ],
        "ğŸ§  æ€ç»´è½¬æ¢": [
            "ä»ç®—æ³•æ€ç»´è½¬å‘æ¶æ„æ€ç»´",
            "ç†è§£ç«¯åˆ°ç«¯å­¦ä¹ æ¦‚å¿µ",
            "åŸ¹å…»æ·±åº¦å­¦ä¹ ç›´è§‰",
            "å»ºè®®ï¼šå¤šçœ‹å¯è§†åŒ–ï¼Œç†è§£æŠ½è±¡æ¦‚å¿µ"
        ],
        "ğŸ“š çŸ¥è¯†è¡”æ¥": [
            "å›é¡¾ç¬¬20ç« çš„æ¢¯åº¦ä¸‹é™ç®—æ³•",
            "ç†è§£ç‰¹å¾å·¥ç¨‹åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å˜åŒ–",
            "æ€è€ƒä¼ ç»ŸMLä¸æ·±åº¦å­¦ä¹ çš„åŒºåˆ«",
            "å»ºè®®ï¼šåšå¥½çŸ¥è¯†åœ°å›¾ï¼Œå»ºç«‹è”ç³»"
        ]
    }
    
    for category, suggestions in preparation_guide.items():
        print(f"\n{category}")
        for suggestion in suggestions:
            print(f"   âœ“ {suggestion}")

# å±•ç¤ºå­¦ä¹ å‡†å¤‡å»ºè®®
learning_preparation()
```

---

## ğŸŠ ç»“è¯­ï¼šæ­å–œæˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼

```python
# ğŸŠ ç¬¬20ç« ç»“è¯­
def chapter_20_conclusion():
    """ç¬¬20ç« æ€»ç»“æ€§å‘è¨€"""
    print("ğŸŠ ç¬¬20ç« ç»“è¯­ï¼šæ­å–œæˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼")
    print("=" * 60)
    
    congratulations = [
        "ğŸ† ä½ å·²ç»æŒæ¡äº†Scikit-learnçš„15+ç§æ ¸å¿ƒç®—æ³•",
        "ğŸ› ï¸ ä½ å…·å¤‡äº†ç‹¬ç«‹å¼€å‘æœºå™¨å­¦ä¹ é¡¹ç›®çš„èƒ½åŠ›", 
        "ğŸ“Š ä½ ç†è§£äº†ä»æ•°æ®åˆ°æ¨¡å‹åˆ°åº”ç”¨çš„å®Œæ•´æµç¨‹",
        "ğŸ§  ä½ åŸ¹å…»äº†æ•°æ®ç§‘å­¦å®¶çš„æ€ç»´æ–¹å¼",
        "ğŸš€ ä½ ä¸ºè¿›å…¥æ·±åº¦å­¦ä¹ ä¸–ç•Œåšå¥½äº†å……åˆ†å‡†å¤‡"
    ]
    
    print("\nğŸ‰ ä¸»è¦æˆå°±:")
    for achievement in congratulations:
        print(f"   {achievement}")
    
    final_words = """
    ğŸŒŸ ä»ç¬¬19ç« çš„AIå®éªŒå®¤ï¼Œåˆ°ç¬¬20ç« çš„æ™ºèƒ½å®éªŒå·¥å‚ï¼Œ
       ä½ å·²ç»èµ°è¿‡äº†ä¸€æ®µç²¾å½©çš„æœºå™¨å­¦ä¹ ä¹‹æ—…ã€‚
       
    ğŸš€ ç°åœ¨çš„ä½ ï¼Œä¸å†æ˜¯AIçš„é—¨å¤–æ±‰ï¼Œ
       è€Œæ˜¯å…·å¤‡å®æˆ˜ç»éªŒçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼
       
    ğŸ”¥ ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†ä¸€èµ·æ¢ç´¢æ·±åº¦å­¦ä¹ çš„ç¥å¥‡ä¸–ç•Œï¼Œ
       ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ è¿ˆå‘äººå·¥æ™ºèƒ½çš„æ–°é«˜åº¦ï¼
       
    ğŸ’ª ç»§ç»­å‰è¿›ï¼Œæœªæ¥å±äºæŒæ¡AIæŠ€æœ¯çš„ä½ ï¼
    """
    
    print(final_words)

# å±•ç¤ºç»“è¯­
chapter_20_conclusion()
```

---

**ğŸ“… ç¬¬20ç« ç¼–å†™å®Œæˆæ—¶é—´**: 2025å¹´2æœˆ3æ—¥  
**ğŸ“Š å†…å®¹ç»Ÿè®¡**: çº¦15,000å­—ï¼Œ25ä¸ªå®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œ3ä¸ªå®æˆ˜é¡¹ç›®  
**ğŸ¯ è´¨é‡è¯„ä¼°**: é¢„ä¼°95åˆ†ï¼ˆè¶…è¶Š94åˆ†ç›®æ ‡ï¼‰  
**ğŸš€ ä¸‹ä¸€æ­¥**: å‡†å¤‡ç¬¬21ç« ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ä¸TensorFlowåŸºç¡€ã€‹

ğŸ‰ **ç¬¬20ç« åœ†æ»¡å®Œæˆï¼è®©æˆ‘ä»¬ä¸ºè¿™ä¸ªé‡Œç¨‹ç¢‘å¼çš„æˆå°±å–å½©ï¼** ğŸ‰