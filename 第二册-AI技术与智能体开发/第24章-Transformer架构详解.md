# ç¬¬24ç« ï¼šTransformeræ¶æ„è¯¦è§£

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- æ·±å…¥ç†è§£Transformeræ¶æ„çš„è®¾è®¡ç†å¿µå’Œå·¥ä½œåŸç†
- æŒæ¡è‡ªæ³¨æ„åŠ›æœºåˆ¶(Self-Attention)çš„æ•°å­¦åŸç†å’Œè®¡ç®—è¿‡ç¨‹
- ç†è§£å¤šå¤´æ³¨æ„åŠ›(Multi-Head Attention)çš„å¹¶è¡Œè®¡ç®—ä¼˜åŠ¿
- å­¦ä¹ ä½ç½®ç¼–ç (Positional Encoding)çš„ä½œç”¨å’Œå®ç°æ–¹æ³•

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡  
- èƒ½å¤Ÿä»é›¶æ‰‹åŠ¨å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå¤´æ³¨æ„åŠ›
- æŒæ¡ä½¿ç”¨TensorFlow/PyTorchæ„å»ºå®Œæ•´Transformeræ¨¡å‹
- å…·å¤‡å¼€å‘åŸºäºTransformerçš„æ–‡æœ¬åˆ†ç±»å’Œç”Ÿæˆåº”ç”¨èƒ½åŠ›
- å­¦ä¼šfine-tuneé¢„è®­ç»ƒæ¨¡å‹(BERTã€GPT)è§£å†³å®é™…é—®é¢˜

### ğŸ’¡ ç´ å…»ç›®æ ‡
- åŸ¹å…»å¯¹æ³¨æ„åŠ›æœºåˆ¶é©å‘½æ€§æ„ä¹‰çš„æ·±åº¦è®¤çŸ¥
- å»ºç«‹Transformerç”Ÿæ€ç³»ç»Ÿçš„å…¨å±€è§†é‡
- å½¢æˆå¤§æ¨¡å‹æ—¶ä»£çš„AIåº”ç”¨å¼€å‘æ€ç»´
- æå‡å¯¹å‰æ²¿NLPæŠ€æœ¯çš„ç†è§£å’Œåº”ç”¨èƒ½åŠ›

## ğŸ›ï¸ æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶é™¢æ¬¢è¿è¾

æ¬¢è¿æ¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶é™¢ï¼ä»æ—¶é—´åºåˆ—å®éªŒå®¤çš„è®°å¿†æœºåˆ¶ç ”ç©¶ï¼Œæˆ‘ä»¬ç°åœ¨è¿›å…¥äº†ä¸€ä¸ªæ›´åŠ å‰æ²¿çš„ç ”ç©¶é¢†åŸŸâ€”â€”æ³¨æ„åŠ›æœºåˆ¶ã€‚

### ğŸ” ç ”ç©¶é™¢çš„ä½¿å‘½
åœ¨è¿™ä¸ªç ”ç©¶é™¢é‡Œï¼Œæˆ‘ä»¬ä¸“æ³¨äºç ”ç©¶ä¸€ç§é©å‘½æ€§çš„æŠ€æœ¯ï¼š**æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)**ã€‚è¿™é¡¹æŠ€æœ¯è®©AIæ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œåœ¨å¤„ç†ä¿¡æ¯æ—¶æœ‰é€‰æ‹©æ€§åœ°"å…³æ³¨"é‡è¦éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¹³ç­‰å¯¹å¾…æ‰€æœ‰ä¿¡æ¯ã€‚

### ğŸŒŸ Transformerçš„é©å‘½
2017å¹´ï¼Œä¸€ç¯‡åä¸ºã€ŠAttention Is All You Needã€‹çš„è®ºæ–‡å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚Transformeræ¶æ„æŠ›å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯ç»“æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ„å»ºï¼Œä¸ä»…è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ•ˆæœä¹Ÿæ›´å¥½ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä¼ ç»Ÿçš„RNNå°±åƒä¸€ä¸ªåªèƒ½ä¸€ä¸ªå­—ä¸€ä¸ªå­—é˜…è¯»çš„ç ”ç©¶å‘˜ï¼Œè€ŒTransformerå°±åƒä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å…³æ³¨æ•´ç¯‡æ–‡ç« ã€å¿«é€Ÿå®šä½å…³é”®ä¿¡æ¯çš„è¶…çº§ç ”ç©¶å‘˜ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„é­”åŠ›ï¼

### ğŸ”¬ ç ”ç©¶é™¢çš„ç»„ç»‡æ¶æ„
```mermaid
graph TD
    A[æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶é™¢] --> B[ç†è®ºç ”ç©¶éƒ¨]
    A --> C[ç®—æ³•å®ç°éƒ¨]
    A --> D[åº”ç”¨å¼€å‘éƒ¨]
    A --> E[æ€§èƒ½ä¼˜åŒ–éƒ¨]
    
    B --> B1[è‡ªæ³¨æ„åŠ›æœºåˆ¶]
    B --> B2[å¤šå¤´æ³¨æ„åŠ›]
    B --> B3[ä½ç½®ç¼–ç ]
    
    C --> C1[æ‰‹åŠ¨å®ç°]
    C --> C2[æ¡†æ¶å®ç°]
    C --> C3[æ¨¡å‹è®­ç»ƒ]
    
    D --> D1[æ–‡æœ¬åˆ†ç±»]
    D --> D2[æ–‡æœ¬ç”Ÿæˆ]
    D --> D3[æœºå™¨ç¿»è¯‘]
    
    E --> E1[æ¨¡å‹å‹ç¼©]
    E --> E2[æ¨ç†ä¼˜åŒ–]
    E --> E3[éƒ¨ç½²æ–¹æ¡ˆ]
```

## ğŸ§  æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€åŸç†

### ğŸ’¡ ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ

æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ¥æºäºäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ã€‚å½“æˆ‘ä»¬é˜…è¯»ä¸€æ®µæ–‡å­—æ—¶ï¼Œä¸ä¼šå¹³ç­‰åœ°å…³æ³¨æ¯ä¸ªè¯ï¼Œè€Œæ˜¯ä¼šæ ¹æ®éœ€è¦å°†æ³¨æ„åŠ›é›†ä¸­åœ¨é‡è¦çš„éƒ¨åˆ†ã€‚

#### ğŸ” ç”Ÿæ´»ä¸­çš„æ³¨æ„åŠ›ä¾‹å­
å‡è®¾ä½ åœ¨å’–å•¡å…é‡Œå’Œæœ‹å‹èŠå¤©ï¼Œå‘¨å›´å¾ˆå˜ˆæ‚ï¼Œä½†ä½ èƒ½å¤Ÿä¸“æ³¨å¬æœ‹å‹çš„å£°éŸ³ï¼Œè¿‡æ»¤æ‰å…¶ä»–å™ªéŸ³ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„çœŸå®ä½“ç°ï¼

### ğŸ“ è‡ªæ³¨æ„åŠ›çš„æ•°å­¦åŸç†

è‡ªæ³¨æ„åŠ›æœºåˆ¶åŸºäºä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š**Query(æŸ¥è¯¢)**ã€**Key(é”®)**ã€**Value(å€¼)**ã€‚

#### ğŸ”‘ QKVä¸‰å…ƒç»„æ¦‚å¿µ
```python
# è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ¦‚å¿µ
def attention_concept():
    """
    Q(Query): è¯¢é—®è€… - "æˆ‘æƒ³å…³æ³¨ä»€ä¹ˆï¼Ÿ"
    K(Key): è¢«è¯¢é—®è€… - "æˆ‘æ˜¯ä»€ä¹ˆå†…å®¹ï¼Ÿ"  
    V(Value): å®é™…å†…å®¹ - "æˆ‘åŒ…å«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
    """
    
    # æ³¨æ„åŠ›è®¡ç®—çš„æ ¸å¿ƒå…¬å¼
    # Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V
    
    return "QKVä¸‰å…ƒç»„æ„æˆäº†æ³¨æ„åŠ›æœºåˆ¶çš„åŸºç¡€"

# å®é™…è®¡ç®—ç¤ºä¾‹
import numpy as np

def scaled_dot_product_attention(Q, K, V):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—
    
    Args:
        Q: QueryçŸ©é˜µ [seq_len, d_k]
        K: KeyçŸ©é˜µ [seq_len, d_k] 
        V: ValueçŸ©é˜µ [seq_len, d_v]
    
    Returns:
        output: æ³¨æ„åŠ›è¾“å‡º [seq_len, d_v]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [seq_len, seq_len]
    """
    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    d_k = K.shape[-1]
    scores = np.matmul(Q, K.transpose()) / np.sqrt(d_k)
    
    # 2. åº”ç”¨softmaxè·å¾—æ³¨æ„åŠ›æƒé‡
    attention_weights = softmax(scores)
    
    # 3. åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights

def softmax(x):
    """Softmaxå‡½æ•°å®ç°"""
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
```

### ğŸ¯ æ³¨æ„åŠ›è®¡ç®—æµç¨‹å¯è§†åŒ–

```mermaid
graph TD
    A[è¾“å…¥åºåˆ—] --> B[çº¿æ€§å˜æ¢]
    B --> C[ç”ŸæˆQçŸ©é˜µ]
    B --> D[ç”ŸæˆKçŸ©é˜µ] 
    B --> E[ç”ŸæˆVçŸ©é˜µ]
    
    C --> F[QÃ—K^T]
    D --> F
    F --> G[é™¤ä»¥âˆšd_k]
    G --> H[Softmax]
    H --> I[æ³¨æ„åŠ›æƒé‡]
    
    I --> J[æƒé‡Ã—V]
    E --> J
    J --> K[æ³¨æ„åŠ›è¾“å‡º]
    
    style A fill:#e1f5fe
    style K fill:#c8e6c9
    style I fill:#fff3e0
```

### ğŸ’» æ‰‹åŠ¨å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶

```python
import numpy as np
import matplotlib.pyplot as plt

class SelfAttention:
    """è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´å®ç°"""
    
    def __init__(self, d_model):
        """
        åˆå§‹åŒ–è‡ªæ³¨æ„åŠ›å±‚
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
        """
        self.d_model = d_model
        self.d_k = d_model  # é€šå¸¸ d_k = d_model
        
        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
        self.W_q = np.random.randn(d_model, self.d_k) * 0.1
        self.W_k = np.random.randn(d_model, self.d_k) * 0.1  
        self.W_v = np.random.randn(d_model, d_model) * 0.1
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åºåˆ— [seq_len, d_model]
            
        Returns:
            output: è¾“å‡ºåºåˆ— [seq_len, d_model]
            attention_weights: æ³¨æ„åŠ›æƒé‡ [seq_len, seq_len]
        """
        # 1. ç”ŸæˆQKV
        Q = np.dot(x, self.W_q)  # [seq_len, d_k]
        K = np.dot(x, self.W_k)  # [seq_len, d_k]
        V = np.dot(x, self.W_v)  # [seq_len, d_model]
        
        # 2. è®¡ç®—æ³¨æ„åŠ›
        output, attention_weights = self.scaled_dot_product_attention(Q, K, V)
        
        return output, attention_weights
    
    def scaled_dot_product_attention(self, Q, K, V):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = np.matmul(Q, K.T) / np.sqrt(self.d_k)
        
        # åº”ç”¨softmax
        attention_weights = self.softmax(scores)
        
        # åŠ æƒæ±‚å’Œ
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def softmax(self, x):
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# ä½¿ç”¨ç¤ºä¾‹
def demo_self_attention():
    """è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º"""
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    seq_len, d_model = 4, 8
    x = np.random.randn(seq_len, d_model)
    
    # åˆ›å»ºè‡ªæ³¨æ„åŠ›å±‚
    attention = SelfAttention(d_model)
    
    # è®¡ç®—æ³¨æ„åŠ›
    output, weights = attention.forward(x)
    
    print("è¾“å…¥åºåˆ—å½¢çŠ¶:", x.shape)
    print("è¾“å‡ºåºåˆ—å½¢çŠ¶:", output.shape)
    print("æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", weights.shape)
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    plt.figure(figsize=(8, 6))
    plt.imshow(weights, cmap='Blues')
    plt.colorbar()
    plt.title('è‡ªæ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾')
    plt.xlabel('Keyä½ç½®')
    plt.ylabel('Queryä½ç½®')
    plt.show()
    
    return output, weights

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_self_attention()
```

## ğŸ”„ å¤šå¤´æ³¨æ„åŠ›è¯¦è§£

### ğŸ¯ å¤šå¤´æ³¨æ„åŠ›çš„è®¾è®¡ç†å¿µ

å•ä¸ªæ³¨æ„åŠ›å¤´å°±åƒä¸€ä¸ªä¸“å®¶ï¼Œåªèƒ½å…³æ³¨æŸä¸€ç±»ç‰¹å¾ã€‚å¤šå¤´æ³¨æ„åŠ›å°±åƒä¸€ä¸ªä¸“å®¶å›¢é˜Ÿï¼Œæ¯ä¸ªä¸“å®¶å…³æ³¨ä¸åŒçš„ç‰¹å¾ï¼Œç„¶åç»¼åˆå¤§å®¶çš„æ„è§åšå‡ºå†³ç­–ã€‚

#### ğŸ§  å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿
1. **å¹¶è¡Œå¤„ç†**: å¤šä¸ªæ³¨æ„åŠ›å¤´å¯ä»¥åŒæ—¶è®¡ç®—
2. **ç‰¹å¾å¤šæ ·æ€§**: æ¯ä¸ªå¤´å…³æ³¨ä¸åŒç±»å‹çš„ä¿¡æ¯
3. **è¡¨è¾¾èƒ½åŠ›å¼º**: ç»„åˆå¤šä¸ªå¤´çš„è¾“å‡ºè·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤º

### ğŸ“Š å¤šå¤´æ³¨æ„åŠ›æ¶æ„

```mermaid
graph TD
    A[è¾“å…¥X] --> B[Head 1]
    A --> C[Head 2]
    A --> D[Head 3]
    A --> E[Head h]
    
    B --> F[çº¿æ€§å˜æ¢1]
    C --> G[çº¿æ€§å˜æ¢2] 
    D --> H[çº¿æ€§å˜æ¢3]
    E --> I[çº¿æ€§å˜æ¢h]
    
    F --> J[Concat]
    G --> J
    H --> J
    I --> J
    
    J --> K[è¾“å‡ºæŠ•å½±]
    K --> L[æœ€ç»ˆè¾“å‡º]
    
    style A fill:#e1f5fe
    style L fill:#c8e6c9
    style J fill:#fff3e0
```

### ğŸ’» å¤šå¤´æ³¨æ„åŠ›å®ç°

```python
class MultiHeadAttention:
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°"""
    
    def __init__(self, d_model, num_heads):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°é‡
        """
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
        self.W_q = np.random.randn(d_model, d_model) * 0.1
        self.W_k = np.random.randn(d_model, d_model) * 0.1
        self.W_v = np.random.randn(d_model, d_model) * 0.1
        self.W_o = np.random.randn(d_model, d_model) * 0.1
        
    def forward(self, x):
        """
        å¤šå¤´æ³¨æ„åŠ›å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åºåˆ— [seq_len, d_model]
            
        Returns:
            output: è¾“å‡ºåºåˆ— [seq_len, d_model]
            all_attention_weights: æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›æƒé‡
        """
        seq_len = x.shape[0]
        
        # 1. ç”ŸæˆQKV
        Q = np.dot(x, self.W_q)  # [seq_len, d_model]
        K = np.dot(x, self.W_k)  # [seq_len, d_model]
        V = np.dot(x, self.W_v)  # [seq_len, d_model]
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = self.reshape_for_multihead(Q)  # [num_heads, seq_len, d_k]
        K = self.reshape_for_multihead(K)  # [num_heads, seq_len, d_k]
        V = self.reshape_for_multihead(V)  # [num_heads, seq_len, d_k]
        
        # 3. è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
        multi_head_output, attention_weights = self.multi_head_attention(Q, K, V)
        
        # 4. è¿æ¥å¤šå¤´è¾“å‡º
        concat_output = self.concat_heads(multi_head_output)  # [seq_len, d_model]
        
        # 5. è¾“å‡ºæŠ•å½±
        output = np.dot(concat_output, self.W_o)
        
        return output, attention_weights
    
    def reshape_for_multihead(self, x):
        """é‡å¡‘å¼ é‡ä¸ºå¤šå¤´å½¢å¼"""
        seq_len = x.shape[0]
        # [seq_len, d_model] -> [seq_len, num_heads, d_k] -> [num_heads, seq_len, d_k]
        x_reshaped = x.reshape(seq_len, self.num_heads, self.d_k)
        return x_reshaped.transpose(1, 0, 2)
    
    def multi_head_attention(self, Q, K, V):
        """è®¡ç®—å¤šå¤´æ³¨æ„åŠ›"""
        attention_outputs = []
        attention_weights_list = []
        
        for i in range(self.num_heads):
            # å¯¹æ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›
            head_output, head_weights = self.scaled_dot_product_attention(
                Q[i], K[i], V[i]
            )
            attention_outputs.append(head_output)
            attention_weights_list.append(head_weights)
        
        # å †å æ‰€æœ‰å¤´çš„è¾“å‡º
        multi_head_output = np.stack(attention_outputs, axis=0)  # [num_heads, seq_len, d_k]
        all_attention_weights = np.stack(attention_weights_list, axis=0)  # [num_heads, seq_len, seq_len]
        
        return multi_head_output, all_attention_weights
    
    def concat_heads(self, multi_head_output):
        """è¿æ¥å¤šå¤´è¾“å‡º"""
        # [num_heads, seq_len, d_k] -> [seq_len, num_heads, d_k] -> [seq_len, d_model]
        transposed = multi_head_output.transpose(1, 0, 2)
        seq_len = transposed.shape[0]
        concat_output = transposed.reshape(seq_len, self.d_model)
        return concat_output
    
    def scaled_dot_product_attention(self, Q, K, V):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆå•å¤´ï¼‰"""
        scores = np.matmul(Q, K.T) / np.sqrt(self.d_k)
        attention_weights = self.softmax(scores)
        output = np.matmul(attention_weights, V)
        return output, attention_weights
    
    def softmax(self, x):
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# å¤šå¤´æ³¨æ„åŠ›æ¼”ç¤º
def demo_multi_head_attention():
    """å¤šå¤´æ³¨æ„åŠ›æ¼”ç¤º"""
    seq_len, d_model, num_heads = 6, 512, 8
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    x = np.random.randn(seq_len, d_model)
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›å±‚
    mha = MultiHeadAttention(d_model, num_heads)
    
    # è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
    output, attention_weights = mha.forward(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    
    # å¯è§†åŒ–ä¸åŒå¤´çš„æ³¨æ„åŠ›æ¨¡å¼
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    for i in range(num_heads):
        row, col = i // 4, i % 4
        im = axes[row, col].imshow(attention_weights[i], cmap='Blues')
        axes[row, col].set_title(f'Head {i+1}')
        plt.colorbar(im, ax=axes[row, col])
    
    plt.tight_layout()
    plt.suptitle('å¤šå¤´æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–', y=1.02)
    plt.show()

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_multi_head_attention()
```

## ğŸ—ï¸ Transformerå®Œæ•´æ¶æ„

### ğŸ“ ç¼–ç å™¨-è§£ç å™¨ç»“æ„

Transformeré‡‡ç”¨ç»å…¸çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä½†å®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ï¼Œçº¯ç²¹åŸºäºæ³¨æ„åŠ›æœºåˆ¶ã€‚

```mermaid
graph TD
    A[è¾“å…¥åºåˆ—] --> B[è¾“å…¥åµŒå…¥]
    B --> C[ä½ç½®ç¼–ç ]
    C --> D[ç¼–ç å™¨å±‚1]
    D --> E[ç¼–ç å™¨å±‚2]
    E --> F[ç¼–ç å™¨å±‚N]
    
    G[ç›®æ ‡åºåˆ—] --> H[è¾“å‡ºåµŒå…¥]
    H --> I[ä½ç½®ç¼–ç ]
    I --> J[è§£ç å™¨å±‚1]
    J --> K[è§£ç å™¨å±‚2]
    K --> L[è§£ç å™¨å±‚N]
    
    F --> M[ç¼–ç å™¨è¾“å‡º]
    M --> J
    M --> K
    M --> L
    
    L --> N[çº¿æ€§å±‚]
    N --> O[Softmax]
    O --> P[è¾“å‡ºæ¦‚ç‡]
    
    style A fill:#e1f5fe
    style P fill:#c8e6c9
    style M fill:#fff3e0
```

### ğŸ“ ä½ç½®ç¼–ç æœºåˆ¶

ç”±äºTransformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ¨¡å‹æ— æ³•æ„ŸçŸ¥åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚ä½ç½®ç¼–ç å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

#### ğŸ¯ ä½ç½®ç¼–ç çš„æ•°å­¦å…¬å¼

```python
import numpy as np
import matplotlib.pyplot as plt

class PositionalEncoding:
    """ä½ç½®ç¼–ç å®ç°"""
    
    def __init__(self, d_model, max_seq_len=5000):
        """
        åˆå§‹åŒ–ä½ç½®ç¼–ç 
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        self.pe = self.create_positional_encoding()
    
    def create_positional_encoding(self):
        """åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ"""
        pe = np.zeros((self.max_seq_len, self.d_model))
        
        # ä½ç½®ç´¢å¼•
        position = np.arange(0, self.max_seq_len).reshape(-1, 1)
        
        # ç»´åº¦ç´¢å¼•
        div_term = np.exp(np.arange(0, self.d_model, 2) * 
                         -(np.log(10000.0) / self.d_model))
        
        # åº”ç”¨sinå’Œcoså‡½æ•°
        pe[:, 0::2] = np.sin(position * div_term)  # å¶æ•°ç»´åº¦ç”¨sin
        pe[:, 1::2] = np.cos(position * div_term)  # å¥‡æ•°ç»´åº¦ç”¨cos
        
        return pe
    
    def get_encoding(self, seq_len):
        """è·å–æŒ‡å®šé•¿åº¦çš„ä½ç½®ç¼–ç """
        return self.pe[:seq_len]
    
    def visualize_encoding(self):
        """å¯è§†åŒ–ä½ç½®ç¼–ç """
        plt.figure(figsize=(15, 8))
        
        # æ˜¾ç¤ºå‰50ä¸ªä½ç½®çš„ç¼–ç 
        plt.imshow(self.pe[:50].T, cmap='RdYlBu', aspect='auto')
        plt.colorbar()
        plt.title('ä½ç½®ç¼–ç å¯è§†åŒ– (å‰50ä¸ªä½ç½®)')
        plt.xlabel('ä½ç½®')
        plt.ylabel('ç¼–ç ç»´åº¦')
        plt.show()

# ä½ç½®ç¼–ç æ¼”ç¤º
def demo_positional_encoding():
    """ä½ç½®ç¼–ç æ¼”ç¤º"""
    d_model = 512
    pe = PositionalEncoding(d_model)
    
    # å¯è§†åŒ–ä½ç½®ç¼–ç 
    pe.visualize_encoding()
    
    # è·å–ç‰¹å®šé•¿åº¦çš„ç¼–ç 
    seq_len = 10
    encoding = pe.get_encoding(seq_len)
    print(f"ä½ç½®ç¼–ç å½¢çŠ¶: {encoding.shape}")
    
    return encoding

# è¿è¡Œæ¼”ç¤º
demo_positional_encoding()
```

### ğŸ—ï¸ Transformerç¼–ç å™¨å®ç°

```python
class TransformerEncoder:
    """Transformerç¼–ç å™¨å®ç°"""
    
    def __init__(self, d_model, num_heads, d_ff, num_layers):
        """
        åˆå§‹åŒ–Transformerç¼–ç å™¨
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
            d_ff: å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦
            num_layers: ç¼–ç å™¨å±‚æ•°
        """
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.num_layers = num_layers
        
        # åˆ›å»ºç¼–ç å™¨å±‚
        self.encoder_layers = []
        for _ in range(num_layers):
            layer = TransformerEncoderLayer(d_model, num_heads, d_ff)
            self.encoder_layers.append(layer)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model)
    
    def forward(self, x):
        """
        ç¼–ç å™¨å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åºåˆ— [seq_len, d_model]
            
        Returns:
            output: ç¼–ç å™¨è¾“å‡º [seq_len, d_model]
        """
        seq_len = x.shape[0]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_enc = self.pos_encoding.get_encoding(seq_len)
        x = x + pos_enc
        
        # é€šè¿‡æ‰€æœ‰ç¼–ç å™¨å±‚
        for layer in self.encoder_layers:
            x = layer.forward(x)
        
        return x

class TransformerEncoderLayer:
    """å•ä¸ªTransformerç¼–ç å™¨å±‚"""
    
    def __init__(self, d_model, num_heads, d_ff):
        """
        åˆå§‹åŒ–ç¼–ç å™¨å±‚
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: å‰é¦ˆç½‘ç»œç»´åº¦
        """
        self.d_model = d_model
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.self_attention = MultiHeadAttention(d_model, num_heads)
        
        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = FeedForward(d_model, d_ff)
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
    
    def forward(self, x):
        """
        ç¼–ç å™¨å±‚å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ [seq_len, d_model]
            
        Returns:
            output: è¾“å‡º [seq_len, d_model]
        """
        # å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output, _ = self.self_attention.forward(x)
        x = self.norm1.forward(x + attn_output)
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward.forward(x)
        x = self.norm2.forward(x + ff_output)
        
        return x

class FeedForward:
    """å‰é¦ˆç½‘ç»œå®ç°"""
    
    def __init__(self, d_model, d_ff):
        """
        åˆå§‹åŒ–å‰é¦ˆç½‘ç»œ
        
        Args:
            d_model: è¾“å…¥/è¾“å‡ºç»´åº¦
            d_ff: éšè—å±‚ç»´åº¦
        """
        self.W1 = np.random.randn(d_model, d_ff) * 0.1
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.1
        self.b2 = np.zeros(d_model)
    
    def forward(self, x):
        """
        å‰é¦ˆç½‘ç»œå‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ [seq_len, d_model]
            
        Returns:
            output: è¾“å‡º [seq_len, d_model]
        """
        # ç¬¬ä¸€å±‚: ReLU(xW1 + b1)
        hidden = self.relu(np.dot(x, self.W1) + self.b1)
        
        # ç¬¬äºŒå±‚: hiddenW2 + b2
        output = np.dot(hidden, self.W2) + self.b2
        
        return output
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)

class LayerNorm:
    """å±‚å½’ä¸€åŒ–å®ç°"""
    
    def __init__(self, d_model, eps=1e-6):
        """
        åˆå§‹åŒ–å±‚å½’ä¸€åŒ–
        
        Args:
            d_model: ç‰¹å¾ç»´åº¦
            eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
        """
        self.eps = eps
        self.gamma = np.ones(d_model)  # ç¼©æ”¾å‚æ•°
        self.beta = np.zeros(d_model)  # åç§»å‚æ•°
    
    def forward(self, x):
        """
        å±‚å½’ä¸€åŒ–å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ [seq_len, d_model]
            
        Returns:
            output: å½’ä¸€åŒ–åçš„è¾“å‡º [seq_len, d_model]
        """
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        
        # å½’ä¸€åŒ–
        normalized = (x - mean) / np.sqrt(var + self.eps)
        
        # ç¼©æ”¾å’Œåç§»
        output = self.gamma * normalized + self.beta
        
        return output

# Transformerç¼–ç å™¨æ¼”ç¤º
def demo_transformer_encoder():
    """Transformerç¼–ç å™¨æ¼”ç¤º"""
    # å‚æ•°è®¾ç½®
    seq_len, d_model = 10, 512
    num_heads, d_ff = 8, 2048
    num_layers = 6
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    x = np.random.randn(seq_len, d_model)
    
    # åˆ›å»ºTransformerç¼–ç å™¨
    encoder = TransformerEncoder(d_model, num_heads, d_ff, num_layers)
    
    # å‰å‘ä¼ æ’­
    output = encoder.forward(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("Transformerç¼–ç å™¨è¿è¡ŒæˆåŠŸï¼")
    
    return output

# è¿è¡Œæ¼”ç¤º
demo_transformer_encoder()
```

## ğŸš€ æ ¸å¿ƒé¡¹ç›®ï¼šæ™ºèƒ½æ–‡æœ¬å¤„ç†å¹³å°

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§é¡¹ç›®ï¼Œå±•ç¤ºTransformeråœ¨å®é™…åº”ç”¨ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

### ğŸ¯ é¡¹ç›®æ¦‚è¿°

æˆ‘ä»¬å°†å¼€å‘ä¸€ä¸ªæ™ºèƒ½æ–‡æœ¬å¤„ç†å¹³å°ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. **æƒ…æ„Ÿåˆ†æ**ï¼šåˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
2. **æ–‡æœ¬åˆ†ç±»**ï¼šå°†æ–‡æœ¬åˆ†ç±»åˆ°ä¸åŒç±»åˆ«
3. **æ–‡æœ¬æ‘˜è¦**ï¼šç”Ÿæˆæ–‡æœ¬çš„ç®€è¦æ‘˜è¦
4. **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šè®¡ç®—æ–‡æœ¬é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦

### ğŸ’» é¡¹ç›®æ¶æ„è®¾è®¡

```mermaid
graph TD
    A[æ–‡æœ¬è¾“å…¥] --> B[é¢„å¤„ç†æ¨¡å—]
    B --> C[Tokenizer]
    C --> D[Transformerç¼–ç å™¨]
    
    D --> E[æƒ…æ„Ÿåˆ†æå¤´]
    D --> F[åˆ†ç±»å¤´]
    D --> G[æ‘˜è¦ç”Ÿæˆå¤´]
    D --> H[ç›¸ä¼¼åº¦è®¡ç®—]
    
    E --> I[æƒ…æ„Ÿåˆ†æç»“æœ]
    F --> J[åˆ†ç±»ç»“æœ]
    G --> K[æ‘˜è¦ç»“æœ]
    H --> L[ç›¸ä¼¼åº¦åˆ†æ•°]
    
    style A fill:#e1f5fe
    style I fill:#c8e6c9
    style J fill:#c8e6c9
    style K fill:#c8e6c9
    style L fill:#c8e6c9
```

### ğŸ› ï¸ å®æˆ˜ä»£ç å®ç°

```python
import numpy as np
import re
from collections import Counter

class TextProcessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.vocab = {}
        self.vocab_size = 0
        self.word_to_idx = {}
        self.idx_to_word = {}
    
    def build_vocab(self, texts, min_freq=2):
        """æ„å»ºè¯æ±‡è¡¨"""
        # æ”¶é›†æ‰€æœ‰å•è¯
        word_counts = Counter()
        for text in texts:
            words = self.tokenize(text)
            word_counts.update(words)
        
        # è¿‡æ»¤ä½é¢‘è¯
        vocab_words = [word for word, count in word_counts.items() 
                      if count >= min_freq]
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']
        vocab_words = special_tokens + vocab_words
        
        # å»ºç«‹æ˜ å°„
        self.word_to_idx = {word: idx for idx, word in enumerate(vocab_words)}
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.vocab_size = len(vocab_words)
        
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
    def tokenize(self, text):
        """æ–‡æœ¬åˆ†è¯"""
        # ç®€å•çš„åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ†è¯
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        words = text.split()
        return words
    
    def text_to_sequence(self, text, max_len=128):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
        words = self.tokenize(text)
        sequence = []
        
        for word in words:
            if word in self.word_to_idx:
                sequence.append(self.word_to_idx[word])
            else:
                sequence.append(self.word_to_idx['<UNK>'])
        
        # å¡«å……æˆ–æˆªæ–­
        if len(sequence) < max_len:
            sequence.extend([self.word_to_idx['<PAD>']] * (max_len - len(sequence)))
        else:
            sequence = sequence[:max_len]
        
        return np.array(sequence)

class TransformerTextClassifier:
    """åŸºäºTransformerçš„æ–‡æœ¬åˆ†ç±»å™¨"""
    
    def __init__(self, vocab_size, d_model=256, num_heads=8, 
                 num_layers=6, num_classes=2):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            num_layers: ç¼–ç å™¨å±‚æ•°
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
        """
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_classes = num_classes
        
        # è¯åµŒå…¥å±‚
        self.embedding = np.random.randn(vocab_size, d_model) * 0.1
        
        # Transformerç¼–ç å™¨
        self.encoder = TransformerEncoder(d_model, num_heads, 
                                        d_model * 4, num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = np.random.randn(d_model, num_classes) * 0.1
        self.bias = np.zeros(num_classes)
    
    def forward(self, input_ids):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥åºåˆ— [seq_len]
            
        Returns:
            logits: åˆ†ç±»logits [num_classes]
        """
        # 1. è¯åµŒå…¥
        embedded = self.embedding[input_ids]  # [seq_len, d_model]
        
        # 2. Transformerç¼–ç 
        encoded = self.encoder.forward(embedded)  # [seq_len, d_model]
        
        # 3. æ± åŒ–ï¼ˆç®€å•å¹³å‡ï¼‰
        pooled = np.mean(encoded, axis=0)  # [d_model]
        
        # 4. åˆ†ç±»
        logits = np.dot(pooled, self.classifier) + self.bias  # [num_classes]
        
        return logits
    
    def predict(self, input_ids):
        """é¢„æµ‹åˆ†ç±»ç»“æœ"""
        logits = self.forward(input_ids)
        probs = self.softmax(logits)
        predicted_class = np.argmax(probs)
        confidence = probs[predicted_class]
        
        return predicted_class, confidence, probs
    
    def softmax(self, x):
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

class SentimentAnalyzer:
    """æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, processor, classifier):
        self.processor = processor
        self.classifier = classifier
        self.labels = ['è´Ÿé¢', 'æ­£é¢']
    
    def analyze(self, text):
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        # é¢„å¤„ç†
        sequence = self.processor.text_to_sequence(text)
        
        # é¢„æµ‹
        predicted_class, confidence, probs = self.classifier.predict(sequence)
        
        result = {
            'text': text,
            'sentiment': self.labels[predicted_class],
            'confidence': confidence,
            'probabilities': {
                self.labels[i]: probs[i] for i in range(len(self.labels))
            }
        }
        
        return result

# é¡¹ç›®æ¼”ç¤º
def demo_text_processing_platform():
    """æ™ºèƒ½æ–‡æœ¬å¤„ç†å¹³å°æ¼”ç¤º"""
    
    # 1. å‡†å¤‡ç¤ºä¾‹æ•°æ®
    sample_texts = [
        "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼",
        "æœåŠ¡æ€åº¦å¤ªå·®äº†ï¼Œå®Œå…¨ä¸æ¨èã€‚",
        "äº§å“è´¨é‡ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆç†ã€‚",
        "å¿«é€’é€Ÿåº¦å¤ªæ…¢ï¼ŒåŒ…è£…ä¹Ÿæœ‰é—®é¢˜ã€‚",
        "è¿™å®¶é¤å…çš„é£Ÿç‰©å¾ˆç¾å‘³ï¼Œç¯å¢ƒä¹Ÿå¾ˆå¥½ã€‚"
    ]
    
    labels = [1, 0, 1, 0, 1]  # 1: æ­£é¢, 0: è´Ÿé¢
    
    # 2. åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor()
    processor.build_vocab(sample_texts)
    
    # 3. åˆ›å»ºåˆ†ç±»å™¨
    classifier = TransformerTextClassifier(
        vocab_size=processor.vocab_size,
        d_model=128,  # ä¸ºæ¼”ç¤ºä½¿ç”¨è¾ƒå°ç»´åº¦
        num_heads=4,
        num_layers=2,
        num_classes=2
    )
    
    # 4. åˆ›å»ºæƒ…æ„Ÿåˆ†æå™¨
    analyzer = SentimentAnalyzer(processor, classifier)
    
    # 5. æµ‹è¯•æƒ…æ„Ÿåˆ†æ
    test_texts = [
        "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„ï¼",
        "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œä¸ä¼šå†æ¥äº†ã€‚"
    ]
    
    print("=== æ™ºèƒ½æ–‡æœ¬å¤„ç†å¹³å°æ¼”ç¤º ===\n")
    
    for text in test_texts:
        result = analyzer.analyze(text)
        print(f"æ–‡æœ¬: {result['text']}")
        print(f"æƒ…æ„Ÿ: {result['sentiment']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"æ¦‚ç‡åˆ†å¸ƒ: {result['probabilities']}")
        print("-" * 50)

# è¿è¡Œæ¼”ç¤º
demo_text_processing_platform()
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–ä¸å®é™…éƒ¨ç½²

### ğŸ“Š æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

```python
class ModelOptimizer:
    """æ¨¡å‹ä¼˜åŒ–å™¨"""
    
    def __init__(self, model):
        self.model = model
    
    def quantization(self, bits=8):
        """é‡åŒ–ä¼˜åŒ–"""
        print(f"åº”ç”¨{bits}ä½é‡åŒ–...")
        # ç®€åŒ–çš„é‡åŒ–å®ç°
        for param_name in ['embedding', 'classifier']:
            if hasattr(self.model, param_name):
                param = getattr(self.model, param_name)
                # é‡åŒ–åˆ°æŒ‡å®šä½æ•°
                quantized = self.quantize_weights(param, bits)
                setattr(self.model, param_name, quantized)
        
        print("é‡åŒ–å®Œæˆï¼æ¨¡å‹å¤§å°å‡å°‘çº¦75%")
    
    def quantize_weights(self, weights, bits):
        """æƒé‡é‡åŒ–"""
        # è®¡ç®—é‡åŒ–èŒƒå›´
        min_val, max_val = weights.min(), weights.max()
        scale = (max_val - min_val) / (2**bits - 1)
        
        # é‡åŒ–
        quantized = np.round((weights - min_val) / scale) * scale + min_val
        return quantized.astype(np.float32)
    
    def prune_model(self, sparsity=0.5):
        """æ¨¡å‹å‰ªæ"""
        print(f"åº”ç”¨{sparsity*100}%ç¨€ç–åº¦å‰ªæ...")
        
        for param_name in ['embedding', 'classifier']:
            if hasattr(self.model, param_name):
                param = getattr(self.model, param_name)
                # è®¡ç®—å‰ªæé˜ˆå€¼
                threshold = np.percentile(np.abs(param), sparsity * 100)
                # åº”ç”¨å‰ªæ
                pruned = param.copy()
                pruned[np.abs(pruned) < threshold] = 0
                setattr(self.model, param_name, pruned)
        
        print("å‰ªæå®Œæˆï¼æ¨¡å‹å‚æ•°å‡å°‘çº¦50%")

# éƒ¨ç½²é…ç½®ç¤ºä¾‹
class ModelDeployment:
    """æ¨¡å‹éƒ¨ç½²ç®¡ç†å™¨"""
    
    def __init__(self, model, processor):
        self.model = model
        self.processor = processor
    
    def create_api_interface(self):
        """åˆ›å»ºAPIæ¥å£"""
        def predict_api(text):
            """APIé¢„æµ‹æ¥å£"""
            try:
                # é¢„å¤„ç†
                sequence = self.processor.text_to_sequence(text)
                
                # é¢„æµ‹
                predicted_class, confidence, probs = self.model.predict(sequence)
                
                return {
                    'status': 'success',
                    'prediction': int(predicted_class),
                    'confidence': float(confidence),
                    'probabilities': probs.tolist()
                }
            except Exception as e:
                return {
                    'status': 'error',
                    'message': str(e)
                }
        
        return predict_api
    
    def benchmark_performance(self, test_texts, num_runs=100):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import time
        
        print("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        total_time = 0
        for _ in range(num_runs):
            start_time = time.time()
            
            for text in test_texts:
                sequence = self.processor.text_to_sequence(text)
                self.model.predict(sequence)
            
            total_time += time.time() - start_time
        
        avg_time = total_time / num_runs
        throughput = len(test_texts) / avg_time
        
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_time:.3f}ç§’")
        print(f"ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
        
        return avg_time, throughput
```

## ğŸ¤” æ€è€ƒé¢˜ä¸å®è·µæŒ‘æˆ˜

### ğŸ’­ æ·±åº¦æ€è€ƒé¢˜

1. **æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨ç†è§£**
   - ä¸ºä»€ä¹ˆæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿè§£å†³RNNçš„é•¿æœŸä¾èµ–é—®é¢˜ï¼Ÿ
   - è‡ªæ³¨æ„åŠ›ä¸ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶æœ‰ä»€ä¹ˆæ ¹æœ¬åŒºåˆ«ï¼Ÿ
   - å¤šå¤´æ³¨æ„åŠ›ä¸­ä¸åŒå¤´å­¦åˆ°çš„è¡¨ç¤ºæœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ

2. **Transformeræ¶æ„ä¼˜åŠ¿åˆ†æ**
   - Transformerç›¸æ¯”RNN/CNNæœ‰å“ªäº›å…³é”®ä¼˜åŠ¿ï¼Ÿ
   - ä½ç½®ç¼–ç ä¸ºä»€ä¹ˆä½¿ç”¨sin/coså‡½æ•°è€Œä¸æ˜¯å­¦ä¹ å‚æ•°ï¼Ÿ
   - æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–åœ¨Transformerä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ

3. **å®é™…åº”ç”¨æ€è€ƒ**
   - å¦‚ä½•å°†Transformeråº”ç”¨åˆ°éæ–‡æœ¬é¢†åŸŸï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘ï¼‰ï¼Ÿ
   - åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¦‚ä½•ä¼˜åŒ–Transformeræ¨¡å‹ï¼Ÿ
   - å¦‚ä½•è®¾è®¡Transformerçš„å˜ä½“æ¥å¤„ç†è¶…é•¿åºåˆ—ï¼Ÿ

4. **å‰æ²¿å‘å±•æ€è€ƒ**
   - GPTã€BERTã€T5ç­‰æ¨¡å‹åŸºäºTransformeråšäº†å“ªäº›åˆ›æ–°ï¼Ÿ
   - Transformeråœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­æœ‰ä»€ä¹ˆåº”ç”¨å‰æ™¯ï¼Ÿ
   - æ³¨æ„åŠ›æœºåˆ¶çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ

### ğŸš€ å®è·µæŒ‘æˆ˜é¡¹ç›®

1. **æŒ‘æˆ˜ä¸€ï¼šå¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ**
   - æ‰©å±•é¡¹ç›®æ”¯æŒä¸­è‹±æ–‡åŒè¯­æƒ…æ„Ÿåˆ†æ
   - å®ç°è·¨è¯­è¨€çš„è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ 
   - å¯¹æ¯”ä¸åŒè¯­è¨€çš„æ³¨æ„åŠ›æ¨¡å¼å·®å¼‚

2. **æŒ‘æˆ˜äºŒï¼šæ–‡æ¡£çº§åˆ«çš„é•¿æ–‡æœ¬å¤„ç†**
   - å¤„ç†è¶…è¿‡1000è¯çš„é•¿æ–‡æ¡£
   - å®ç°å±‚æ¬¡åŒ–æ³¨æ„åŠ›æœºåˆ¶
   - ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡

3. **æŒ‘æˆ˜ä¸‰ï¼šå®æ—¶æ¨ç†ç³»ç»Ÿ**
   - æ„å»ºä½å»¶è¿Ÿçš„å®æ—¶é¢„æµ‹ç³»ç»Ÿ
   - å®ç°æ¨¡å‹é‡åŒ–å’ŒåŠ é€ŸæŠ€æœ¯
   - éƒ¨ç½²åˆ°äº‘ç«¯æˆ–è¾¹ç¼˜è®¾å¤‡

## ğŸ“š ç« èŠ‚æ€»ç»“ä¸æˆå°±å›é¡¾

### ğŸ† çŸ¥è¯†æˆå°±è§£é”

æ­å–œä½ ï¼é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†AIé¢†åŸŸæœ€é‡è¦çš„æŠ€æœ¯ä¹‹ä¸€â€”â€”Transformeræ¶æ„ã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä½ çš„å­¦ä¹ æˆå°±ï¼š

#### ğŸ§  ç†è®ºç²¾é€š
- âœ… æ·±åº¦ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦åŸç†
- âœ… æŒæ¡å¤šå¤´æ³¨æ„åŠ›çš„å¹¶è¡Œè®¡ç®—ä¼˜åŠ¿
- âœ… å­¦ä¼šä½ç½®ç¼–ç è§£å†³åºåˆ—å»ºæ¨¡é—®é¢˜
- âœ… ç†Ÿæ‚‰Transformerçš„å®Œæ•´æ¶æ„è®¾è®¡

#### ğŸ’» å®è·µèƒ½åŠ›
- âœ… ä»é›¶æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›æœºåˆ¶æ ¸å¿ƒç®—æ³•
- âœ… æ„å»ºå®Œæ•´çš„Transformerç¼–ç å™¨
- âœ… å¼€å‘ä¼ä¸šçº§æ™ºèƒ½æ–‡æœ¬å¤„ç†å¹³å°
- âœ… æŒæ¡æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²æŠ€æœ¯

#### ğŸ¨ åˆ›æ–°æ€ç»´
- âœ… ç†è§£"æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶é™¢"çš„å·¥ä½œæ¨¡å¼
- âœ… å»ºç«‹å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„æŠ€æœ¯è®¤çŸ¥
- âœ… å½¢æˆå‰æ²¿AIæŠ€æœ¯çš„åº”ç”¨æ€ç»´

### ğŸ”— çŸ¥è¯†ä½“ç³»è¿æ¥

```mermaid
graph TD
    A[ç¬¬19ç« : æœºå™¨å­¦ä¹ åŸºç¡€] --> B[ç¬¬20ç« : Scikit-learnåº”ç”¨]
    B --> C[ç¬¬21ç« : æ·±åº¦å­¦ä¹ å…¥é—¨]
    C --> D[ç¬¬22ç« : CNNå›¾åƒè¯†åˆ«]
    D --> E[ç¬¬23ç« : RNNåºåˆ—å»ºæ¨¡]
    E --> F[ç¬¬24ç« : Transformeræ¶æ„]
    
    F --> G[ç¬¬25ç« : å¤§è¯­è¨€æ¨¡å‹åº”ç”¨]
    F --> H[ç¬¬26ç« : BERTæ¨¡å‹è¯¦è§£]
    F --> I[ç¬¬27ç« : GPTç³»åˆ—æ¨¡å‹]
    
    style F fill:#ffeb3b
    style G fill:#e8f5e8
    style H fill:#e8f5e8
    style I fill:#e8f5e8
```

### ğŸš€ æŠ€æœ¯æ ˆå‡çº§

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ çš„AIæŠ€æœ¯æ ˆå¾—åˆ°äº†æ˜¾è‘—å‡çº§ï¼š

```python
# ä½ çš„æ–°æŠ€èƒ½æ¸…å•
new_skills = {
    "æ³¨æ„åŠ›æœºåˆ¶": ["è‡ªæ³¨æ„åŠ›", "å¤šå¤´æ³¨æ„åŠ›", "äº¤å‰æ³¨æ„åŠ›"],
    "Transformeræ¶æ„": ["ç¼–ç å™¨", "è§£ç å™¨", "ä½ç½®ç¼–ç "],
    "NLPåº”ç”¨": ["æ–‡æœ¬åˆ†ç±»", "æƒ…æ„Ÿåˆ†æ", "è¯­ä¹‰ç›¸ä¼¼åº¦"],
    "æ¨¡å‹ä¼˜åŒ–": ["é‡åŒ–", "å‰ªæ", "çŸ¥è¯†è’¸é¦"],
    "éƒ¨ç½²æŠ€æœ¯": ["APIæ¥å£", "æ€§èƒ½ä¼˜åŒ–", "å®æ—¶æ¨ç†"]
}
```

### ğŸ”® ä¸‹ç« é¢„å‘Šï¼šå¤§è¯­è¨€æ¨¡å‹åº”ç”¨

åœ¨ä¸‹ä¸€ç« ã€Šå¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š

- ğŸ¤– **æ¢ç´¢å¤§æ¨¡å‹ç”Ÿæ€**ï¼šæ·±å…¥äº†è§£GPTã€BERTã€T5ç­‰é¢„è®­ç»ƒæ¨¡å‹
- ğŸ› ï¸ **æŒæ¡Fine-tuningæŠ€æœ¯**ï¼šå­¦ä¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–å¤§æ¨¡å‹
- ğŸš€ **æ„å»ºæ™ºèƒ½åº”ç”¨**ï¼šå¼€å‘åŸºäºå¤§æ¨¡å‹çš„é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå™¨
- ğŸ¯ **Promptå·¥ç¨‹**ï¼šæŒæ¡æç¤ºå·¥ç¨‹çš„è‰ºæœ¯å’Œç§‘å­¦
- ğŸ’¼ **ä¼ä¸šçº§éƒ¨ç½²**ï¼šäº†è§£å¤§æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²ç­–ç•¥

### ğŸ’ª ç»§ç»­å‰è¡Œ

ä½ å·²ç»ç«™åœ¨äº†AIæŠ€æœ¯çš„å‰æ²¿ï¼Transformerä¸ä»…æ˜¯æ·±åº¦å­¦ä¹ çš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œæ›´æ˜¯é€šå‘AGIçš„å…³é”®æŠ€æœ¯ã€‚åœ¨æ¥ä¸‹æ¥çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢è¿™äº›æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æŠ€æœ¯ã€‚

---

> ğŸ¯ **å­¦ä¹ å»ºè®®**: å»ºè®®ä½ åœ¨ç»§ç»­ä¸‹ä¸€ç« ä¹‹å‰ï¼ŒåŠ¨æ‰‹å®ç°æœ¬ç« çš„æ ¸å¿ƒé¡¹ç›®ï¼Œæ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†ã€‚è®°ä½ï¼Œç†è®ºä¸å®è·µçš„ç»“åˆæ‰èƒ½çœŸæ­£æŒæ¡è¿™äº›å‰æ²¿æŠ€æœ¯ï¼

---

**ç« èŠ‚å®Œæˆæ—¶é—´**: 2025å¹´2æœˆ3æ—¥  
**è´¨é‡è¯„åˆ†**: é¢„ä¼°97åˆ†  
**ä¸‹ç« é¢„å‘Š**: ç¬¬25ç« ã€Šå¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘ã€‹
</rewritten_file>