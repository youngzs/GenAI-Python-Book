# ç¬¬28ç« ï¼šæ¨¡å‹å¾®è°ƒä¸å®šåˆ¶åŒ–å¼€å‘

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- æŒæ¡æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒåŸç†å’ŒæŠ€æœ¯è·¯çº¿
- ç†è§£LoRAã€Adapterç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- å­¦ä¹ æ•°æ®å·¥ç¨‹å’Œè®­ç»ƒä¼˜åŒ–ç­–ç•¥
- äº†è§£æ¨¡å‹è¯„ä¼°å’Œéƒ¨ç½²ä¼˜åŒ–æŠ€æœ¯

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡
- èƒ½å¤Ÿè®¾è®¡å’Œå®æ–½å®Œæ•´çš„æ¨¡å‹å¾®è°ƒæµç¨‹
- æŒæ¡å¤šç§å¾®è°ƒæŠ€æœ¯çš„é€‰æ‹©å’Œåº”ç”¨
- å…·å¤‡æ•°æ®å¤„ç†å’Œè®­ç»ƒä¼˜åŒ–èƒ½åŠ›
- èƒ½å¤Ÿæ„å»ºè‡ªåŠ¨åŒ–å¾®è°ƒå¹³å°

### ğŸ¨ ç´ å…»ç›®æ ‡
- åŸ¹å…»AIæ¨¡å‹å®šåˆ¶åŒ–çš„ç³»ç»Ÿæ€§æ€ç»´
- å»ºç«‹ä¼ä¸šçº§AIåº”ç”¨çš„å·¥ç¨‹åŒ–æ„è¯†
- å½¢æˆæŒç»­å­¦ä¹ å’ŒæŠ€æœ¯åˆ›æ–°çš„èƒ½åŠ›

## ğŸ­ æ¬¢è¿æ¥åˆ°æ¨¡å‹å®šåˆ¶å·¥å‚

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„**æ¨¡å‹å®šåˆ¶å·¥å‚**ï¼åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†å¤šæ™ºèƒ½ä½“åä½œä¸é€šä¿¡çš„æŠ€æœ¯ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥ä¸€ä¸ªæ›´åŠ ç²¾ç»†åŒ–çš„é¢†åŸŸâ€”â€”æ¨¡å‹å¾®è°ƒä¸å®šåˆ¶åŒ–å¼€å‘ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœè¯´é¢„è®­ç»ƒæ¨¡å‹æ˜¯å·¥å‚ç”Ÿäº§çš„"é€šç”¨äº§å“"ï¼Œé‚£ä¹ˆæ¨¡å‹å¾®è°ƒå°±æ˜¯æ ¹æ®å®¢æˆ·éœ€æ±‚è¿›è¡Œçš„"ä¸ªæ€§åŒ–å®šåˆ¶"ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼š

- **åŸæ–™æŠ•å…¥** â†’ é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚BERTã€GPTç­‰ï¼‰
- **ç”Ÿäº§çº¿é…ç½®** â†’ å¾®è°ƒç­–ç•¥é€‰æ‹©ï¼ˆLoRAã€Adapterç­‰ï¼‰
- **è´¨é‡æ§åˆ¶** â†’ è®­ç»ƒç›‘æ§ä¸è¯„ä¼°
- **äº§å“è¾“å‡º** â†’ å®šåˆ¶åŒ–æ¨¡å‹

## ğŸ“‹ æœ¬ç« å†…å®¹å¯¼è§ˆ

### 28.1 æ¨¡å‹å¾®è°ƒåŸºç¡€ç†è®º
æ·±å…¥ç†è§£æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒåŸç†ï¼Œå­¦ä¹ ä¸åŒå¾®è°ƒç­–ç•¥çš„é€‰æ‹©å’Œåº”ç”¨ã€‚

### 28.2 å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
æŒæ¡LoRAã€Adapterç­‰å‰æ²¿çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ã€‚

### 28.3 æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†
æ„å»ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå»ºç«‹å®Œå–„çš„æ•°æ®å¤„ç†æµç¨‹ã€‚

### 28.4 è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§
ç²¾ç»†åŒ–ç®¡ç†è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°é«˜æ•ˆçš„æ¨¡å‹ä¼˜åŒ–ã€‚

### 28.5 æ¨¡å‹è¯„ä¼°ä¸åˆ†æ
å»ºç«‹å¤šç»´åº¦çš„æ•ˆæœè¯„ä¼°ä½“ç³»ï¼Œç§‘å­¦éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚

### 28.6 å®šåˆ¶åŒ–AIåŠ©æ‰‹å®æˆ˜
å¼€å‘ä¼ä¸šçº§ä¸ªæ€§åŒ–AIåŠ©æ‰‹ï¼Œå®ç°å®Œæ•´çš„å®šåˆ¶åŒ–è§£å†³æ–¹æ¡ˆã€‚

### 28.7 æœ¬ç« æ€»ç»“ä¸å±•æœ›
å›é¡¾å­¦ä¹ æˆæœï¼Œå±•æœ›æ¨¡å‹å®šåˆ¶æŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚

---

## 28.1 æ¨¡å‹å¾®è°ƒåŸºç¡€ç†è®º

### ğŸ”¬ å¾®è°ƒçš„æœ¬è´¨ï¼šçŸ¥è¯†çš„ç²¾å‡†è¿ç§»

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œå¾®è°ƒå°±åƒæ˜¯å¯¹é€šç”¨äº§å“è¿›è¡Œç²¾å‡†æ”¹é€ çš„è¿‡ç¨‹ã€‚è®©æˆ‘ä»¬æ·±å…¥ç†è§£è¿™ä¸ªè¿‡ç¨‹çš„æœ¬è´¨ã€‚

#### ğŸ“Š å¾®è°ƒ vs é¢„è®­ç»ƒï¼šä¸¤ç§ä¸åŒçš„å­¦ä¹ æ¨¡å¼

```mermaid
graph LR
    subgraph "é¢„è®­ç»ƒé˜¶æ®µ"
        A[å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®] --> B[é€šç”¨è¯­è¨€ç†è§£]
        B --> C[åŸºç¡€æ¨¡å‹]
    end
    
    subgraph "å¾®è°ƒé˜¶æ®µ"
        C --> D[ç‰¹å®šä»»åŠ¡æ•°æ®]
        D --> E[ä»»åŠ¡é€‚é…]
        E --> F[å®šåˆ¶åŒ–æ¨¡å‹]
    end
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style C fill:#e8f5e8
    style F fill:#fff3e0
```

**é¢„è®­ç»ƒ**å°±åƒæ˜¯åŸ¹å…»ä¸€ä¸ªåšå­¦çš„é€šæ‰ï¼Œè®©æ¨¡å‹åœ¨æµ·é‡æ•°æ®ä¸Šå­¦ä¹ é€šç”¨çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚è€Œ**å¾®è°ƒ**åˆ™æ˜¯åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¸“ä¸šåŒ–è®­ç»ƒï¼Œå°±åƒè®©é€šæ‰æˆä¸ºæŸä¸ªé¢†åŸŸçš„ä¸“å®¶ã€‚

è®©æˆ‘ä»¬ç”¨ä»£ç æ¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼š

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import numpy as np
import matplotlib.pyplot as plt

class FineTuningDemo:
    """æ¨¡å‹å¾®è°ƒæ¼”ç¤ºç±»"""
    
    def __init__(self, model_name="bert-base-uncased"):
        """
        åˆå§‹åŒ–å¾®è°ƒæ¼”ç¤º
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.base_model = AutoModel.from_pretrained(model_name)
        
        # å†»ç»“é¢„è®­ç»ƒå±‚çš„å‚æ•°ï¼ˆå¯é€‰ï¼‰
        self.freeze_base_model()
        
        print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {self.count_parameters():,}")
    
    def freeze_base_model(self):
        """å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = False
        print("ğŸ”’ å·²å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°")
    
    def unfreeze_base_model(self):
        """è§£å†»é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = True
        print("ğŸ”“ å·²è§£å†»é¢„è®­ç»ƒæ¨¡å‹å‚æ•°")
    
    def count_parameters(self):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.base_model.parameters())
    
    def create_classification_head(self, num_classes=2):
        """
        åˆ›å»ºåˆ†ç±»å¤´
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
        """
        hidden_size = self.base_model.config.hidden_size
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
        print(f"ğŸ¯ å·²åˆ›å»ºåˆ†ç±»å¤´ï¼Œè¾“å‡ºç»´åº¦: {num_classes}")
        return self.classifier
    
    def demonstrate_feature_extraction(self, texts):
        """
        æ¼”ç¤ºç‰¹å¾æå–è¿‡ç¨‹
        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
        """
        print("\nğŸ” ç‰¹å¾æå–æ¼”ç¤º:")
        
        for i, text in enumerate(texts):
            # ç¼–ç æ–‡æœ¬
            inputs = self.tokenizer(text, return_tensors="pt", 
                                  padding=True, truncation=True)
            
            # æå–ç‰¹å¾
            with torch.no_grad():
                outputs = self.base_model(**inputs)
                features = outputs.last_hidden_state.mean(dim=1)  # å¹³å‡æ± åŒ–
            
            print(f"æ–‡æœ¬ {i+1}: {text}")
            print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
            print(f"ç‰¹å¾èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")
            print("-" * 50)

# æ¼”ç¤ºå¾®è°ƒåŸºç¡€æ¦‚å¿µ
demo = FineTuningDemo()

# åˆ›å»ºåˆ†ç±»å¤´
classifier = demo.create_classification_head(num_classes=3)

# æ¼”ç¤ºç‰¹å¾æå–
sample_texts = [
    "This movie is absolutely fantastic!",
    "The service was terrible and disappointing.",
    "It's an okay product, nothing special."
]

demo.demonstrate_feature_extraction(sample_texts)
```

#### ğŸ¯ å¾®è°ƒç­–ç•¥åˆ†ç±»ï¼šä¸åŒçš„å®šåˆ¶æ–¹æ¡ˆ

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œæœ‰å¤šç§ä¸åŒçš„å®šåˆ¶æ–¹æ¡ˆå¯ä¾›é€‰æ‹©ï¼š

```mermaid
graph TD
    A[æ¨¡å‹å¾®è°ƒç­–ç•¥] --> B[å…¨å‚æ•°å¾®è°ƒ]
    A --> C[å‚æ•°é«˜æ•ˆå¾®è°ƒ]
    
    B --> D[å®Œå…¨å¾®è°ƒ]
    B --> E[æ¸è¿›å¼å¾®è°ƒ]
    
    C --> F[LoRA]
    C --> G[Adapter]
    C --> H[Prefix Tuning]
    C --> I[Prompt Tuning]
    
    style A fill:#ff9800
    style B fill:#2196f3
    style C fill:#4caf50
```

è®©æˆ‘ä»¬è¯¦ç»†äº†è§£æ¯ç§ç­–ç•¥ï¼š

```python
class FineTuningStrategy:
    """å¾®è°ƒç­–ç•¥åˆ†æç±»"""
    
    def __init__(self):
        self.strategies = {
            "full_finetuning": {
                "name": "å…¨å‚æ•°å¾®è°ƒ",
                "description": "æ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°",
                "advantages": ["æ•ˆæœæœ€å¥½", "é€‚åº”æ€§å¼º"],
                "disadvantages": ["è®¡ç®—æˆæœ¬é«˜", "å®¹æ˜“è¿‡æ‹Ÿåˆ", "å­˜å‚¨éœ€æ±‚å¤§"],
                "suitable_for": ["æ•°æ®å……è¶³", "è®¡ç®—èµ„æºä¸°å¯Œ", "è¿½æ±‚æœ€ä½³æ•ˆæœ"]
            },
            "lora": {
                "name": "LoRAå¾®è°ƒ",
                "description": "ä½ç§©é€‚åº”ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°",
                "advantages": ["å‚æ•°å°‘", "è®­ç»ƒå¿«", "é˜²æ­¢è¿‡æ‹Ÿåˆ"],
                "disadvantages": ["æ•ˆæœå¯èƒ½ç•¥é€Š", "éœ€è¦è°ƒæ•´ç§©å‚æ•°"],
                "suitable_for": ["æ•°æ®æœ‰é™", "è®¡ç®—èµ„æºå—é™", "å¿«é€Ÿéƒ¨ç½²"]
            },
            "adapter": {
                "name": "Adapterå¾®è°ƒ",
                "description": "åœ¨æ¨¡å‹ä¸­æ’å…¥å°å‹é€‚é…å™¨å±‚",
                "advantages": ["æ¨¡å—åŒ–", "å¯æ’æ‹”", "å‚æ•°æ•ˆç‡é«˜"],
                "disadvantages": ["å¢åŠ æ¨ç†å»¶è¿Ÿ", "æ¶æ„å¤æ‚"],
                "suitable_for": ["å¤šä»»åŠ¡åœºæ™¯", "æ¨¡å‹å…±äº«", "å¢é‡å­¦ä¹ "]
            },
            "prefix_tuning": {
                "name": "å‰ç¼€å¾®è°ƒ",
                "description": "åªè®­ç»ƒè¾“å…¥å‰ç¼€çš„åµŒå…¥",
                "advantages": ["å‚æ•°æå°‘", "ä¸æ”¹å˜æ¨¡å‹ç»“æ„"],
                "disadvantages": ["æ•ˆæœæœ‰é™", "é€‚ç”¨åœºæ™¯çª„"],
                "suitable_for": ["ç”Ÿæˆä»»åŠ¡", "å¿«é€Ÿé€‚é…", "è½»é‡åŒ–éƒ¨ç½²"]
            }
        }
    
    def compare_strategies(self):
        """æ¯”è¾ƒä¸åŒå¾®è°ƒç­–ç•¥"""
        print("ğŸ“Š å¾®è°ƒç­–ç•¥å¯¹æ¯”åˆ†æ:")
        print("=" * 80)
        
        for key, strategy in self.strategies.items():
            print(f"\nğŸ¯ {strategy['name']}")
            print(f"æè¿°: {strategy['description']}")
            print(f"âœ… ä¼˜åŠ¿: {', '.join(strategy['advantages'])}")
            print(f"âŒ åŠ£åŠ¿: {', '.join(strategy['disadvantages'])}")
            print(f"ğŸ¯ é€‚ç”¨åœºæ™¯: {', '.join(strategy['suitable_for'])}")
            print("-" * 60)
    
    def estimate_resources(self, model_size_mb, strategy="full_finetuning"):
        """
        ä¼°ç®—èµ„æºéœ€æ±‚
        Args:
            model_size_mb: æ¨¡å‹å¤§å°(MB)
            strategy: å¾®è°ƒç­–ç•¥
        """
        multipliers = {
            "full_finetuning": {"memory": 4.0, "time": 1.0, "storage": 2.0},
            "lora": {"memory": 1.2, "time": 0.3, "storage": 1.1},
            "adapter": {"memory": 1.5, "time": 0.4, "storage": 1.2},
            "prefix_tuning": {"memory": 1.1, "time": 0.2, "storage": 1.05}
        }
        
        if strategy not in multipliers:
            print(f"âŒ ä¸æ”¯æŒçš„ç­–ç•¥: {strategy}")
            return
        
        mult = multipliers[strategy]
        
        print(f"\nğŸ“Š {self.strategies[strategy]['name']} èµ„æºéœ€æ±‚ä¼°ç®—:")
        print(f"åŸºç¡€æ¨¡å‹å¤§å°: {model_size_mb} MB")
        print(f"è®­ç»ƒå†…å­˜éœ€æ±‚: {model_size_mb * mult['memory']:.1f} MB")
        print(f"ç›¸å¯¹è®­ç»ƒæ—¶é—´: {mult['time']:.1f}x")
        print(f"å­˜å‚¨éœ€æ±‚: {model_size_mb * mult['storage']:.1f} MB")

# æ¼”ç¤ºç­–ç•¥åˆ†æ
strategy_analyzer = FineTuningStrategy()
strategy_analyzer.compare_strategies()

# èµ„æºéœ€æ±‚ä¼°ç®—
strategy_analyzer.estimate_resources(440, "full_finetuning")  # BERT-base
strategy_analyzer.estimate_resources(440, "lora")
```

#### ğŸ”„ è¿ç§»å­¦ä¹ ç†è®ºï¼šçŸ¥è¯†ä¼ æ‰¿çš„è‰ºæœ¯

å¾®è°ƒçš„æ ¸å¿ƒæ˜¯è¿ç§»å­¦ä¹ ï¼Œå°±åƒæ˜¯å°†ä¸€ä¸ªé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†è½¬ç§»åˆ°å¦ä¸€ä¸ªé¢†åŸŸã€‚è®©æˆ‘ä»¬æ·±å…¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import seaborn as sns

class TransferLearningAnalyzer:
    """è¿ç§»å­¦ä¹ åˆ†æå™¨"""
    
    def __init__(self):
        self.layer_types = [
            "è¯åµŒå…¥å±‚", "æµ…å±‚ç¼–ç å™¨", "ä¸­å±‚ç¼–ç å™¨", 
            "æ·±å±‚ç¼–ç å™¨", "ä»»åŠ¡ç‰¹å®šå±‚"
        ]
    
    def visualize_knowledge_transfer(self):
        """å¯è§†åŒ–çŸ¥è¯†è¿ç§»è¿‡ç¨‹"""
        # æ¨¡æ‹Ÿä¸åŒå±‚çš„çŸ¥è¯†é€šç”¨æ€§
        universality = [0.95, 0.8, 0.6, 0.4, 0.1]
        task_specificity = [0.05, 0.2, 0.4, 0.6, 0.9]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # çŸ¥è¯†é€šç”¨æ€§å›¾
        ax1.barh(self.layer_types, universality, color='skyblue', alpha=0.7)
        ax1.set_xlabel('é€šç”¨æ€§ç¨‹åº¦')
        ax1.set_title('ä¸åŒå±‚çš„çŸ¥è¯†é€šç”¨æ€§')
        ax1.set_xlim(0, 1)
        
        # ä»»åŠ¡ç‰¹å¼‚æ€§å›¾
        ax2.barh(self.layer_types, task_specificity, color='lightcoral', alpha=0.7)
        ax2.set_xlabel('ä»»åŠ¡ç‰¹å¼‚æ€§ç¨‹åº¦')
        ax2.set_title('ä¸åŒå±‚çš„ä»»åŠ¡ç‰¹å¼‚æ€§')
        ax2.set_xlim(0, 1)
        
        plt.tight_layout()
        plt.show()
        
        print("ğŸ“Š çŸ¥è¯†è¿ç§»è§„å¾‹:")
        for i, layer in enumerate(self.layer_types):
            print(f"{layer}: é€šç”¨æ€§{universality[i]:.1%}, ç‰¹å¼‚æ€§{task_specificity[i]:.1%}")
    
    def demonstrate_feature_evolution(self):
        """æ¼”ç¤ºç‰¹å¾æ¼”åŒ–è¿‡ç¨‹"""
        # æ¨¡æ‹Ÿé¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­ç‰¹å¾çš„å˜åŒ–
        np.random.seed(42)
        
        # é¢„è®­ç»ƒç‰¹å¾ (é€šç”¨)
        pretrain_features = np.random.normal(0, 1, (100, 2))
        
        # å¾®è°ƒåç‰¹å¾ (ä»»åŠ¡ç‰¹å®š)
        finetune_features = pretrain_features + np.random.normal(0, 0.3, (100, 2))
        finetune_features[:50] += [1.5, 1.5]  # ç±»åˆ«1
        finetune_features[50:] += [-1.5, -1.5]  # ç±»åˆ«2
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒ
        ax1.scatter(pretrain_features[:, 0], pretrain_features[:, 1], 
                   alpha=0.6, s=50, c='gray')
        ax1.set_title('é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒ\n(é€šç”¨è¡¨ç¤º)')
        ax1.set_xlabel('ç‰¹å¾ç»´åº¦1')
        ax1.set_ylabel('ç‰¹å¾ç»´åº¦2')
        ax1.grid(True, alpha=0.3)
        
        # å¾®è°ƒåç‰¹å¾åˆ†å¸ƒ
        colors = ['red'] * 50 + ['blue'] * 50
        ax2.scatter(finetune_features[:, 0], finetune_features[:, 1], 
                   alpha=0.6, s=50, c=colors)
        ax2.set_title('å¾®è°ƒåç‰¹å¾åˆ†å¸ƒ\n(ä»»åŠ¡ç‰¹å®š)')
        ax2.set_xlabel('ç‰¹å¾ç»´åº¦1')
        ax2.set_ylabel('ç‰¹å¾ç»´åº¦2')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print("ğŸ¯ ç‰¹å¾æ¼”åŒ–åˆ†æ:")
        print("â€¢ é¢„è®­ç»ƒé˜¶æ®µ: å­¦ä¹ é€šç”¨çš„è¯­è¨€è¡¨ç¤º")
        print("â€¢ å¾®è°ƒé˜¶æ®µ: é€‚åº”ç‰¹å®šä»»åŠ¡éœ€æ±‚")
        print("â€¢ ç»“æœ: ä¿æŒé€šç”¨æ€§çš„åŒæ—¶è·å¾—ä»»åŠ¡ç‰¹å¼‚æ€§")

# æ¼”ç¤ºè¿ç§»å­¦ä¹ åˆ†æ
transfer_analyzer = TransferLearningAnalyzer()
transfer_analyzer.visualize_knowledge_transfer()
transfer_analyzer.demonstrate_feature_evolution()
```

#### ğŸ› ï¸ å¾®è°ƒæµç¨‹è®¾è®¡ï¼šä»æ•°æ®åˆ°éƒ¨ç½²çš„å®Œæ•´pipeline

ç°åœ¨è®©æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„å¾®è°ƒæµç¨‹ï¼Œå°±åƒåœ¨å·¥å‚ä¸­å»ºç«‹æ ‡å‡†åŒ–çš„ç”Ÿäº§çº¿ï¼š

```python
class FineTuningPipeline:
    """å®Œæ•´çš„å¾®è°ƒæµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, model_name, task_type="classification"):
        """
        åˆå§‹åŒ–å¾®è°ƒæµç¨‹
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            task_type: ä»»åŠ¡ç±»å‹ (classification, regression, generation)
        """
        self.model_name = model_name
        self.task_type = task_type
        self.pipeline_stages = [
            "æ•°æ®å‡†å¤‡", "æ¨¡å‹åŠ è½½", "é…ç½®ä¼˜åŒ–å™¨", 
            "è®­ç»ƒç›‘æ§", "æ¨¡å‹è¯„ä¼°", "æ¨¡å‹ä¿å­˜", "éƒ¨ç½²å‡†å¤‡"
        ]
        
        print(f"ğŸ­ åˆå§‹åŒ–å¾®è°ƒæµæ°´çº¿: {model_name}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
    
    def visualize_pipeline(self):
        """å¯è§†åŒ–å¾®è°ƒæµç¨‹"""
        print("\nğŸ”„ å¾®è°ƒæµç¨‹å›¾:")
        print("=" * 60)
        
        # åˆ›å»ºæµç¨‹å›¾
        flow_chart = """
        ```mermaid
        graph TD
            A[åŸå§‹æ•°æ®] --> B[æ•°æ®æ¸…æ´—]
            B --> C[æ•°æ®æ ‡æ³¨]
            C --> D[æ•°æ®åˆ’åˆ†]
            D --> E[åŠ è½½é¢„è®­ç»ƒæ¨¡å‹]
            E --> F[æ·»åŠ ä»»åŠ¡å¤´]
            F --> G[é…ç½®è®­ç»ƒå‚æ•°]
            G --> H[å¼€å§‹è®­ç»ƒ]
            H --> I[å®æ—¶ç›‘æ§]
            I --> J{æ˜¯å¦æ”¶æ•›?}
            J -->|å¦| H
            J -->|æ˜¯| K[æ¨¡å‹è¯„ä¼°]
            K --> L[æ€§èƒ½ä¼˜åŒ–]
            L --> M[æ¨¡å‹ä¿å­˜]
            M --> N[éƒ¨ç½²å‡†å¤‡]
            
            style A fill:#e1f5fe
            style N fill:#e8f5e8
            style J fill:#fff3e0
        ```
        """
        print(flow_chart)
    
    def estimate_pipeline_time(self, data_size, model_size="base"):
        """
        ä¼°ç®—æµç¨‹æ—¶é—´
        Args:
            data_size: æ•°æ®é›†å¤§å°
            model_size: æ¨¡å‹è§„æ¨¡ (base, large, xl)
        """
        size_multipliers = {"base": 1.0, "large": 2.5, "xl": 6.0}
        
        base_times = {
            "æ•°æ®å‡†å¤‡": max(0.5, data_size / 10000),  # å°æ—¶
            "æ¨¡å‹åŠ è½½": 0.1 * size_multipliers[model_size],
            "è®­ç»ƒè¿‡ç¨‹": max(1.0, data_size / 1000) * size_multipliers[model_size],
            "æ¨¡å‹è¯„ä¼°": 0.2 * size_multipliers[model_size],
            "éƒ¨ç½²å‡†å¤‡": 0.3
        }
        
        print(f"\nâ±ï¸ æµç¨‹æ—¶é—´ä¼°ç®— (æ•°æ®é‡: {data_size}, æ¨¡å‹: {model_size}):")
        print("-" * 50)
        
        total_time = 0
        for stage, time_hours in base_times.items():
            print(f"{stage}: {time_hours:.1f} å°æ—¶")
            total_time += time_hours
        
        print(f"æ€»è®¡æ—¶é—´: {total_time:.1f} å°æ—¶")
        
        return total_time
    
    def create_checklist(self):
        """åˆ›å»ºå¾®è°ƒæ£€æŸ¥æ¸…å•"""
        checklist = {
            "æ•°æ®å‡†å¤‡": [
                "æ•°æ®è´¨é‡æ£€æŸ¥",
                "æ ‡æ³¨ä¸€è‡´æ€§éªŒè¯", 
                "æ•°æ®å¹³è¡¡æ€§åˆ†æ",
                "è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†"
            ],
            "æ¨¡å‹é…ç½®": [
                "é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹",
                "è®¾è®¡ä»»åŠ¡ç‰¹å®šå±‚",
                "é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡",
                "è®¾ç½®æ­£åˆ™åŒ–å‚æ•°"
            ],
            "è®­ç»ƒç›‘æ§": [
                "æŸå¤±å‡½æ•°ç›‘æ§",
                "éªŒè¯é›†æ€§èƒ½è·Ÿè¸ª",
                "è¿‡æ‹Ÿåˆæ£€æµ‹",
                "æ—©åœæœºåˆ¶è®¾ç½®"
            ],
            "æ¨¡å‹è¯„ä¼°": [
                "å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°",
                "é”™è¯¯æ¡ˆä¾‹åˆ†æ",
                "æ¨¡å‹é²æ£’æ€§æµ‹è¯•",
                "æ•ˆç‡æ€§èƒ½æµ‹è¯•"
            ],
            "éƒ¨ç½²å‡†å¤‡": [
                "æ¨¡å‹å‹ç¼©ä¼˜åŒ–",
                "æ¨ç†é€Ÿåº¦æµ‹è¯•",
                "å†…å­˜ä½¿ç”¨è¯„ä¼°",
                "å…¼å®¹æ€§æ£€æŸ¥"
            ]
        }
        
        print("\nğŸ“‹ å¾®è°ƒæ£€æŸ¥æ¸…å•:")
        print("=" * 50)
        
        for category, items in checklist.items():
            print(f"\nğŸ¯ {category}:")
            for item in items:
                print(f"  â˜ {item}")
        
        return checklist

# æ¼”ç¤ºå¾®è°ƒæµç¨‹
pipeline = FineTuningPipeline("bert-base-chinese", "classification")
pipeline.visualize_pipeline()
pipeline.estimate_pipeline_time(10000, "base")
pipeline.create_checklist()
```

### ğŸ¯ å¾®è°ƒç­–ç•¥é€‰æ‹©æŒ‡å—

é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥å°±åƒé€‰æ‹©åˆé€‚çš„ç”Ÿäº§çº¿é…ç½®ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ ï¼š

```python
class StrategySelector:
    """å¾®è°ƒç­–ç•¥é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.decision_tree = {
            "data_size": {
                "small": "å‚æ•°é«˜æ•ˆå¾®è°ƒ",
                "medium": "éƒ¨åˆ†å¾®è°ƒ",
                "large": "å…¨å‚æ•°å¾®è°ƒ"
            },
            "compute_budget": {
                "low": "LoRAæˆ–Adapter",
                "medium": "éƒ¨åˆ†å±‚å¾®è°ƒ",
                "high": "å…¨å‚æ•°å¾®è°ƒ"
            },
            "task_similarity": {
                "high": "è½»é‡å¾®è°ƒ",
                "medium": "æ ‡å‡†å¾®è°ƒ", 
                "low": "æ·±åº¦å¾®è°ƒ"
            }
        }
    
    def recommend_strategy(self, data_size, compute_budget, task_similarity):
        """
        æ¨èå¾®è°ƒç­–ç•¥
        Args:
            data_size: æ•°æ®è§„æ¨¡ (small/medium/large)
            compute_budget: è®¡ç®—é¢„ç®— (low/medium/high)
            task_similarity: ä¸é¢„è®­ç»ƒä»»åŠ¡ç›¸ä¼¼åº¦ (low/medium/high)
        """
        print("ğŸ¤– æ™ºèƒ½ç­–ç•¥æ¨èç³»ç»Ÿ")
        print("=" * 40)
        print(f"æ•°æ®è§„æ¨¡: {data_size}")
        print(f"è®¡ç®—é¢„ç®—: {compute_budget}")
        print(f"ä»»åŠ¡ç›¸ä¼¼åº¦: {task_similarity}")
        print("-" * 40)
        
        # åŸºäºè§„åˆ™çš„æ¨è
        if data_size == "small" or compute_budget == "low":
            if task_similarity == "high":
                recommendation = "Prompt Tuning"
                confidence = 0.9
            else:
                recommendation = "LoRA"
                confidence = 0.85
        elif data_size == "large" and compute_budget == "high":
            recommendation = "Full Fine-tuning"
            confidence = 0.95
        else:
            recommendation = "Adapter"
            confidence = 0.8
        
        print(f"ğŸ¯ æ¨èç­–ç•¥: {recommendation}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.1%}")
        
        # æä¾›æ›¿ä»£æ–¹æ¡ˆ
        alternatives = self._get_alternatives(recommendation)
        print(f"ğŸ”„ å¤‡é€‰æ–¹æ¡ˆ: {', '.join(alternatives)}")
        
        return recommendation, confidence
    
    def _get_alternatives(self, primary):
        """è·å–æ›¿ä»£æ–¹æ¡ˆ"""
        alternatives_map = {
            "Full Fine-tuning": ["LoRA", "Adapter"],
            "LoRA": ["QLoRA", "Adapter"],
            "Adapter": ["LoRA", "Prefix Tuning"],
            "Prompt Tuning": ["P-tuning v2", "LoRA"]
        }
        return alternatives_map.get(primary, ["LoRA", "Adapter"])

# æ¼”ç¤ºç­–ç•¥é€‰æ‹©
selector = StrategySelector()

# ä¸åŒåœºæ™¯çš„ç­–ç•¥æ¨è
scenarios = [
    ("small", "low", "high"),      # å°æ•°æ®ï¼Œä½é¢„ç®—ï¼Œé«˜ç›¸ä¼¼åº¦
    ("large", "high", "low"),      # å¤§æ•°æ®ï¼Œé«˜é¢„ç®—ï¼Œä½ç›¸ä¼¼åº¦
    ("medium", "medium", "medium") # ä¸­ç­‰åœºæ™¯
]

for i, (data, budget, similarity) in enumerate(scenarios, 1):
    print(f"\nğŸ“‹ åœºæ™¯ {i}:")
    selector.recommend_strategy(data, budget, similarity)
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥ç†è§£äº†æ¨¡å‹å¾®è°ƒçš„åŸºç¡€ç†è®ºã€‚å¾®è°ƒä¸ä»…ä»…æ˜¯ç®€å•çš„å‚æ•°æ›´æ–°ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¶‰åŠçŸ¥è¯†è¿ç§»ã€ç­–ç•¥é€‰æ‹©å’Œæµç¨‹ç®¡ç†çš„å¤æ‚è¿‡ç¨‹ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯LoRAã€Adapterç­‰å‰æ²¿æ–¹æ³•ï¼Œè®©æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚èƒ½å¤Ÿæä¾›æ›´åŠ é«˜æ•ˆå’Œç»æµçš„å®šåˆ¶æ–¹æ¡ˆã€‚

---

## 28.2 å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯

### ğŸš€ è¿›å…¥é«˜æ•ˆå¾®è°ƒè½¦é—´

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å°±åƒæ˜¯å¼•å…¥äº†é©å‘½æ€§çš„é«˜æ•ˆç”Ÿäº§çº¿ã€‚ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒå°±åƒæ˜¯é‡æ–°åˆ¶é€ æ•´ä¸ªäº§å“ï¼Œè€Œå‚æ•°é«˜æ•ˆå¾®è°ƒåˆ™åƒæ˜¯åªæ›´æ¢å…³é”®éƒ¨ä»¶ï¼Œæ—¢ä¿æŒäº†äº§å“çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œåˆå¤§å¹…é™ä½äº†æˆæœ¬å’Œæ—¶é—´ã€‚

è®©æˆ‘ä»¬æ·±å…¥è¿™ä¸ªé«˜æ•ˆè½¦é—´ï¼Œå­¦ä¹ æœ€å‰æ²¿çš„å¾®è°ƒæŠ€æœ¯ï¼

#### ğŸ”§ LoRAï¼šä½ç§©é€‚åº”çš„è‰ºæœ¯

**LoRA (Low-Rank Adaptation)** æ˜¯ç›®å‰æœ€å—æ¬¢è¿çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¤§å¤šæ•°æ¨¡å‹å‚æ•°çš„æ›´æ–°éƒ½å¯ä»¥ç”¨ä½ç§©çŸ©é˜µæ¥è¿‘ä¼¼ã€‚

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿå¾®è°ƒ"
        A1[åŸå§‹æƒé‡W] --> B1[æ›´æ–°æ‰€æœ‰å‚æ•°]
        B1 --> C1[æ–°æƒé‡W']
    end
    
    subgraph "LoRAå¾®è°ƒ"
        A2[åŸå§‹æƒé‡W] --> B2[å†»ç»“ä¸å˜]
        D2[ä½ç§©çŸ©é˜µA] --> E2[Î”W = AB]
        F2[ä½ç§©çŸ©é˜µB] --> E2
        E2 --> G2[W' = W + Î”W]
        B2 --> G2
    end
    
    style A1 fill:#ffcdd2
    style C1 fill:#ffcdd2
    style A2 fill:#c8e6c9
    style G2 fill:#c8e6c9
    style E2 fill:#fff3e0
```

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå®Œæ•´çš„LoRAç³»ç»Ÿï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional

class LoRALayer(nn.Module):
    """LoRAå±‚å®ç°"""
    
    def __init__(
        self, 
        in_features: int, 
        out_features: int, 
        rank: int = 4,
        alpha: float = 1.0,
        dropout: float = 0.0
    ):
        """
        åˆå§‹åŒ–LoRAå±‚
        Args:
            in_features: è¾“å…¥ç‰¹å¾ç»´åº¦
            out_features: è¾“å‡ºç‰¹å¾ç»´åº¦  
            rank: ä½ç§©åˆ†è§£çš„ç§©
            alpha: ç¼©æ”¾å› å­
            dropout: dropoutæ¦‚ç‡
        """
        super().__init__()
        
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # LoRAçš„ä¸¤ä¸ªä½ç§©çŸ©é˜µ
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        
        print(f"âœ… åˆ›å»ºLoRAå±‚: {in_features}â†’{out_features}, rank={rank}, Î±={alpha}")
        print(f"ğŸ“Š å‚æ•°é‡: {self.count_parameters():,} (åŸå±‚å‚æ•°: {in_features * out_features:,})")
        print(f"ğŸ“‰ å‚æ•°å‡å°‘: {(1 - self.count_parameters() / (in_features * out_features)):.1%}")
    
    def count_parameters(self):
        """è®¡ç®—LoRAå±‚çš„å‚æ•°é‡"""
        return self.lora_A.numel() + self.lora_B.numel()
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # LoRAçš„å‰å‘è®¡ç®—: x @ (A^T @ B^T) * scaling
        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T * self.scaling
        return lora_output

class LoRALinear(nn.Module):
    """å¸¦LoRAçš„çº¿æ€§å±‚"""
    
    def __init__(
        self,
        original_layer: nn.Linear,
        rank: int = 4,
        alpha: float = 1.0,
        dropout: float = 0.0
    ):
        """
        ä¸ºç°æœ‰çº¿æ€§å±‚æ·»åŠ LoRA
        Args:
            original_layer: åŸå§‹çº¿æ€§å±‚
            rank: LoRAçš„ç§©
            alpha: ç¼©æ”¾å› å­
            dropout: dropoutæ¦‚ç‡
        """
        super().__init__()
        
        # å†»ç»“åŸå§‹å±‚
        self.original_layer = original_layer
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # æ·»åŠ LoRAå±‚
        self.lora = LoRALayer(
            original_layer.in_features,
            original_layer.out_features,
            rank=rank,
            alpha=alpha,
            dropout=dropout
        )
        
        print(f"ğŸ”— ä¸ºçº¿æ€§å±‚æ·»åŠ LoRAé€‚é…å™¨")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­: åŸå§‹è¾“å‡º + LoRAè¾“å‡º"""
        original_output = self.original_layer(x)
        lora_output = self.lora(x)
        return original_output + lora_output

class LoRATransformer(nn.Module):
    """å¸¦LoRAçš„Transformeræ¨¡å‹"""
    
    def __init__(self, base_model, target_modules=None, rank=4, alpha=1.0):
        """
        ä¸ºTransformeræ¨¡å‹æ·»åŠ LoRA
        Args:
            base_model: åŸºç¡€æ¨¡å‹
            target_modules: ç›®æ ‡æ¨¡å—åç§°åˆ—è¡¨
            rank: LoRAç§©
            alpha: ç¼©æ”¾å› å­
        """
        super().__init__()
        
        self.base_model = base_model
        self.rank = rank
        self.alpha = alpha
        
        # é»˜è®¤ç›®æ ‡æ¨¡å—
        if target_modules is None:
            target_modules = ["query", "key", "value", "dense"]
        
        # æ·»åŠ LoRAå±‚
        self.add_lora_layers(target_modules)
        
        print(f"ğŸ¯ ä¸ºæ¨¡å‹æ·»åŠ LoRAé€‚é…å™¨")
        print(f"ğŸ“Š æ€»å‚æ•°é‡: {self.count_total_parameters():,}")
        print(f"ğŸ”§ å¯è®­ç»ƒå‚æ•°: {self.count_trainable_parameters():,}")
        print(f"ğŸ“‰ å¯è®­ç»ƒæ¯”ä¾‹: {self.count_trainable_parameters() / self.count_total_parameters():.2%}")
    
    def add_lora_layers(self, target_modules):
        """ä¸ºæŒ‡å®šæ¨¡å—æ·»åŠ LoRAå±‚"""
        lora_count = 0
        
        for name, module in self.base_model.named_modules():
            if isinstance(module, nn.Linear):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
                should_add_lora = any(target in name for target in target_modules)
                
                if should_add_lora:
                    # è·å–çˆ¶æ¨¡å—å’Œå±æ€§å
                    parent_name = '.'.join(name.split('.')[:-1])
                    attr_name = name.split('.')[-1]
                    
                    if parent_name:
                        parent_module = self.base_model
                        for part in parent_name.split('.'):
                            parent_module = getattr(parent_module, part)
                    else:
                        parent_module = self.base_model
                    
                    # æ›¿æ¢ä¸ºLoRAå±‚
                    lora_layer = LoRALinear(module, self.rank, self.alpha)
                    setattr(parent_module, attr_name, lora_layer)
                    lora_count += 1
        
        print(f"âœ… å·²æ·»åŠ  {lora_count} ä¸ªLoRAé€‚é…å™¨")
    
    def count_total_parameters(self):
        """ç»Ÿè®¡æ€»å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters())
    
    def count_trainable_parameters(self):
        """ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­"""
        return self.base_model(*args, **kwargs)

# æ¼”ç¤ºLoRAçš„ä½¿ç”¨
print("ğŸ”§ LoRAæŠ€æœ¯æ¼”ç¤º")
print("=" * 50)

# åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
original_layer = nn.Linear(768, 768)
print(f"åŸå§‹å±‚å‚æ•°é‡: {sum(p.numel() for p in original_layer.parameters()):,}")

# æ·»åŠ LoRA
lora_layer = LoRALinear(original_layer, rank=8, alpha=16.0)

# æµ‹è¯•è¾“å…¥
test_input = torch.randn(32, 768)  # batch_size=32, hidden_size=768

# å‰å‘ä¼ æ’­
with torch.no_grad():
    original_output = original_layer(test_input)
    lora_output = lora_layer(test_input)

print(f"\nğŸ“Š è¾“å‡ºå¯¹æ¯”:")
print(f"åŸå§‹è¾“å‡ºèŒƒå›´: [{original_output.min():.3f}, {original_output.max():.3f}]")
print(f"LoRAè¾“å‡ºèŒƒå›´: [{lora_output.min():.3f}, {lora_output.max():.3f}]")
print(f"è¾“å‡ºå·®å¼‚: {(lora_output - original_output).abs().mean():.6f}")
```

#### ğŸ”Œ Adapterï¼šå¯æ’æ‹”çš„å¾®è°ƒæ¨¡å—

**Adapter** æŠ€æœ¯é€šè¿‡åœ¨æ¨¡å‹ä¸­æ’å…¥å°å‹çš„é€‚é…å™¨æ¨¡å—æ¥å®ç°å¾®è°ƒï¼Œå°±åƒåœ¨ç”Ÿäº§çº¿ä¸Šå®‰è£…å¯æ’æ‹”çš„åŠŸèƒ½æ¨¡å—ã€‚

```python
class AdapterLayer(nn.Module):
    """Adapterå±‚å®ç°"""
    
    def __init__(
        self, 
        hidden_size: int, 
        adapter_size: int = 64,
        activation: str = "relu",
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–Adapterå±‚
        Args:
            hidden_size: éšè—å±‚å¤§å°
            adapter_size: adapterçš„ä¸­é—´å±‚å¤§å°
            activation: æ¿€æ´»å‡½æ•°
            dropout: dropoutæ¦‚ç‡
        """
        super().__init__()
        
        self.hidden_size = hidden_size
        self.adapter_size = adapter_size
        
        # ä¸‹æŠ•å½±å±‚ (é™ç»´)
        self.down_project = nn.Linear(hidden_size, adapter_size)
        
        # æ¿€æ´»å‡½æ•°
        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "gelu":
            self.activation = nn.GELU()
        else:
            self.activation = nn.Identity()
        
        # ä¸ŠæŠ•å½±å±‚ (å‡ç»´)
        self.up_project = nn.Linear(adapter_size, hidden_size)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        print(f"ğŸ”Œ åˆ›å»ºAdapter: {hidden_size}â†’{adapter_size}â†’{hidden_size}")
        print(f"ğŸ“Š å‚æ•°é‡: {self.count_parameters():,}")
        print(f"ğŸ“‰ å‹ç¼©æ¯”: {adapter_size / hidden_size:.1%}")
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # ä¸ŠæŠ•å½±å±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘é›¶ï¼Œç¡®ä¿åˆå§‹æ—¶adapterå½±å“å¾ˆå°
        nn.init.normal_(self.down_project.weight, std=0.02)
        nn.init.zeros_(self.down_project.bias)
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)
    
    def count_parameters(self):
        """è®¡ç®—å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters())
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, hidden_size]
        Returns:
            adapterè¾“å‡º [batch_size, seq_len, hidden_size]
        """
        # é™ç»´ â†’ æ¿€æ´» â†’ å‡ç»´
        adapter_output = self.down_project(x)
        adapter_output = self.activation(adapter_output)
        adapter_output = self.dropout(adapter_output)
        adapter_output = self.up_project(adapter_output)
        
        return adapter_output

class AdapterTransformerLayer(nn.Module):
    """å¸¦Adapterçš„Transformerå±‚"""
    
    def __init__(self, original_layer, adapter_size=64):
        """
        ä¸ºTransformerå±‚æ·»åŠ Adapter
        Args:
            original_layer: åŸå§‹Transformerå±‚
            adapter_size: adapterå¤§å°
        """
        super().__init__()
        
        # å†»ç»“åŸå§‹å±‚
        self.original_layer = original_layer
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # è·å–éšè—å±‚å¤§å°
        hidden_size = self._get_hidden_size(original_layer)
        
        # æ·»åŠ ä¸¤ä¸ªadapterï¼šä¸€ä¸ªåœ¨attentionåï¼Œä¸€ä¸ªåœ¨FFNå
        self.adapter_after_attn = AdapterLayer(hidden_size, adapter_size)
        self.adapter_after_ffn = AdapterLayer(hidden_size, adapter_size)
        
        print(f"ğŸ”— ä¸ºTransformerå±‚æ·»åŠ Adapteré€‚é…å™¨")
    
    def _get_hidden_size(self, layer):
        """ä»å±‚ä¸­æ¨æ–­éšè—å±‚å¤§å°"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„æ¥ç¡®å®š
        for module in layer.modules():
            if isinstance(module, nn.Linear):
                return module.in_features
        return 768  # é»˜è®¤å€¼
    
    def forward(self, hidden_states, attention_mask=None, **kwargs):
        """å‰å‘ä¼ æ’­"""
        # åŸå§‹å±‚çš„å‰å‘ä¼ æ’­ (éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´)
        outputs = self.original_layer(hidden_states, attention_mask=attention_mask, **kwargs)
        
        if isinstance(outputs, tuple):
            hidden_states = outputs[0]
            other_outputs = outputs[1:]
        else:
            hidden_states = outputs
            other_outputs = ()
        
        # æ·»åŠ adapterè¾“å‡º (æ®‹å·®è¿æ¥)
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“çš„Transformerç»“æ„æ¥è°ƒæ•´
        adapter_output = self.adapter_after_ffn(hidden_states)
        hidden_states = hidden_states + adapter_output
        
        return (hidden_states,) + other_outputs if other_outputs else hidden_states

# æ¼”ç¤ºAdapterçš„ä½¿ç”¨
print("\nğŸ”Œ AdapteræŠ€æœ¯æ¼”ç¤º")
print("=" * 50)

# åˆ›å»ºAdapterå±‚
adapter = AdapterLayer(hidden_size=768, adapter_size=64)

# æµ‹è¯•è¾“å…¥
test_input = torch.randn(8, 128, 768)  # [batch, seq_len, hidden]

# å‰å‘ä¼ æ’­
with torch.no_grad():
    adapter_output = adapter(test_input)

print(f"\nğŸ“Š Adapterè¾“å‡ºåˆ†æ:")
print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {adapter_output.shape}")
print(f"è¾“å‡ºèŒƒå›´: [{adapter_output.min():.6f}, {adapter_output.max():.6f}]")
print(f"è¾“å‡ºå‡å€¼: {adapter_output.mean():.6f}")
print(f"è¾“å‡ºæ ‡å‡†å·®: {adapter_output.std():.6f}")
```

#### ğŸ¯ Prefix Tuningï¼šæ™ºèƒ½å‰ç¼€å­¦ä¹ 

**Prefix Tuning** æ˜¯ä¸€ç§åªè®­ç»ƒè¾“å…¥å‰ç¼€çš„å¾®è°ƒæ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚

```python
class PrefixTuning(nn.Module):
    """Prefix Tuningå®ç°"""
    
    def __init__(
        self,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        prefix_length: int = 10,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–Prefix Tuning
        Args:
            num_layers: Transformerå±‚æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            head_dim: æ¯ä¸ªå¤´çš„ç»´åº¦
            prefix_length: å‰ç¼€é•¿åº¦
            dropout: dropoutæ¦‚ç‡
        """
        super().__init__()
        
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.prefix_length = prefix_length
        self.hidden_size = num_heads * head_dim
        
        # å¯å­¦ä¹ çš„å‰ç¼€å‚æ•°
        # å½¢çŠ¶: [num_layers, 2, num_heads, prefix_length, head_dim]
        # 2è¡¨ç¤ºkeyå’Œvalue
        self.prefix_embeddings = nn.Parameter(
            torch.randn(num_layers, 2, num_heads, prefix_length, head_dim) * 0.02
        )
        
        # å¯é€‰çš„MLPæ¥ç”Ÿæˆå‰ç¼€
        self.prefix_mlp = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, num_layers * 2 * num_heads * prefix_length * head_dim),
            nn.Dropout(dropout)
        )
        
        print(f"ğŸ¯ åˆ›å»ºPrefix Tuning:")
        print(f"ğŸ“Š å±‚æ•°: {num_layers}, å¤´æ•°: {num_heads}, å‰ç¼€é•¿åº¦: {prefix_length}")
        print(f"ğŸ”¢ å‰ç¼€å‚æ•°é‡: {self.prefix_embeddings.numel():,}")
        
    def get_prefix_key_values(self, batch_size: int, device: torch.device):
        """
        è·å–å‰ç¼€çš„keyå’Œvalue
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
        Returns:
            prefix_key_values: å‰ç¼€çš„keyå’Œvalueå¯¹
        """
        # æ‰©å±•åˆ°æ‰¹æ¬¡ç»´åº¦
        prefix_key_values = self.prefix_embeddings.unsqueeze(0).expand(
            batch_size, -1, -1, -1, -1, -1
        )
        
        # é‡æ–°æ•´å½¢ä¸ºé€‚åˆattentionçš„æ ¼å¼
        # [batch, num_layers, 2, num_heads, prefix_length, head_dim]
        return prefix_key_values.to(device)
    
    def forward(self, batch_size: int, device: torch.device):
        """å‰å‘ä¼ æ’­"""
        return self.get_prefix_key_values(batch_size, device)

class PrefixAttention(nn.Module):
    """å¸¦å‰ç¼€çš„æ³¨æ„åŠ›å±‚"""
    
    def __init__(self, original_attention, prefix_tuning: PrefixTuning, layer_idx: int):
        """
        ä¸ºæ³¨æ„åŠ›å±‚æ·»åŠ å‰ç¼€
        Args:
            original_attention: åŸå§‹æ³¨æ„åŠ›å±‚
            prefix_tuning: å‰ç¼€è°ƒä¼˜æ¨¡å—
            layer_idx: å±‚ç´¢å¼•
        """
        super().__init__()
        
        self.original_attention = original_attention
        self.prefix_tuning = prefix_tuning
        self.layer_idx = layer_idx
        
        # å†»ç»“åŸå§‹æ³¨æ„åŠ›å±‚
        for param in self.original_attention.parameters():
            param.requires_grad = False
    
    def forward(self, hidden_states, attention_mask=None, **kwargs):
        """å‰å‘ä¼ æ’­"""
        batch_size = hidden_states.size(0)
        device = hidden_states.device
        
        # è·å–å‰ç¼€key-value
        prefix_kv = self.prefix_tuning(batch_size, device)
        prefix_key = prefix_kv[:, self.layer_idx, 0]  # [batch, num_heads, prefix_len, head_dim]
        prefix_value = prefix_kv[:, self.layer_idx, 1]
        
        # åŸå§‹æ³¨æ„åŠ›è®¡ç®— (è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´)
        outputs = self.original_attention(hidden_states, attention_mask=attention_mask, **kwargs)
        
        # å°†å‰ç¼€key-valueä¸åŸå§‹key-valueè¿æ¥
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ³¨æ„åŠ›å®ç°æ¥è°ƒæ•´
        
        return outputs

# æ¼”ç¤ºPrefix Tuning
print("\nğŸ¯ Prefix Tuningæ¼”ç¤º")
print("=" * 50)

# åˆ›å»ºPrefix Tuning
prefix_tuning = PrefixTuning(
    num_layers=12,
    num_heads=12, 
    head_dim=64,
    prefix_length=10
)

# æµ‹è¯•
batch_size = 4
device = torch.device('cpu')

with torch.no_grad():
    prefix_kv = prefix_tuning(batch_size, device)

print(f"\nğŸ“Š å‰ç¼€Key-Valueåˆ†æ:")
print(f"å‰ç¼€KVå½¢çŠ¶: {prefix_kv.shape}")
print(f"å‰ç¼€å‚æ•°é‡: {prefix_tuning.prefix_embeddings.numel():,}")
print(f"KeyèŒƒå›´: [{prefix_kv[:, :, 0].min():.3f}, {prefix_kv[:, :, 0].max():.3f}]")
print(f"ValueèŒƒå›´: [{prefix_kv[:, :, 1].min():.3f}, {prefix_kv[:, :, 1].max():.3f}]")
```

#### ğŸ“Š å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¯¹æ¯”

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå…¨é¢çš„å¯¹æ¯”åˆ†æç³»ç»Ÿï¼š

```python
class ParameterEfficientComparison:
    """å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¯¹æ¯”åˆ†æ"""
    
    def __init__(self):
        self.methods = {
            "LoRA": {
                "å‚æ•°é‡": "æå°‘ (0.1-1%)",
                "è®­ç»ƒé€Ÿåº¦": "å¿«",
                "æ¨ç†é€Ÿåº¦": "å‡ ä¹æ— å½±å“",
                "å†…å­˜éœ€æ±‚": "ä½",
                "æ•ˆæœ": "ä¼˜ç§€",
                "é€‚ç”¨åœºæ™¯": "é€šç”¨å¾®è°ƒ",
                "ä¼˜åŠ¿": ["å‚æ•°æå°‘", "è®­ç»ƒå¿«é€Ÿ", "æ•ˆæœå¥½"],
                "åŠ£åŠ¿": ["éœ€è¦è°ƒæ•´ç§©", "ç†è®ºåŸºç¡€æœ‰é™"]
            },
            "Adapter": {
                "å‚æ•°é‡": "å°‘ (1-5%)",
                "è®­ç»ƒé€Ÿåº¦": "ä¸­ç­‰",
                "æ¨ç†é€Ÿåº¦": "è½»å¾®å½±å“",
                "å†…å­˜éœ€æ±‚": "ä¸­ç­‰",
                "æ•ˆæœ": "è‰¯å¥½",
                "é€‚ç”¨åœºæ™¯": "å¤šä»»åŠ¡å­¦ä¹ ",
                "ä¼˜åŠ¿": ["æ¨¡å—åŒ–", "å¯æ’æ‹”", "ç¨³å®š"],
                "åŠ£åŠ¿": ["æ¨ç†å»¶è¿Ÿ", "å‚æ•°ç¨å¤š"]
            },
            "Prefix Tuning": {
                "å‚æ•°é‡": "æå°‘ (0.01-0.1%)",
                "è®­ç»ƒé€Ÿåº¦": "å¾ˆå¿«",
                "æ¨ç†é€Ÿåº¦": "æ— å½±å“",
                "å†…å­˜éœ€æ±‚": "æä½",
                "æ•ˆæœ": "ä¸­ç­‰",
                "é€‚ç”¨åœºæ™¯": "ç”Ÿæˆä»»åŠ¡",
                "ä¼˜åŠ¿": ["å‚æ•°æœ€å°‘", "ä¸æ”¹ç»“æ„"],
                "åŠ£åŠ¿": ["æ•ˆæœæœ‰é™", "é€‚ç”¨é¢çª„"]
            },
            "Full Fine-tuning": {
                "å‚æ•°é‡": "å…¨éƒ¨ (100%)",
                "è®­ç»ƒé€Ÿåº¦": "æ…¢",
                "æ¨ç†é€Ÿåº¦": "æ— å½±å“",
                "å†…å­˜éœ€æ±‚": "é«˜",
                "æ•ˆæœ": "æœ€ä½³",
                "é€‚ç”¨åœºæ™¯": "èµ„æºå……è¶³",
                "ä¼˜åŠ¿": ["æ•ˆæœæœ€å¥½", "é€‚åº”æ€§å¼º"],
                "åŠ£åŠ¿": ["æˆæœ¬é«˜", "æ˜“è¿‡æ‹Ÿåˆ"]
            }
        }
    
    def create_comparison_table(self):
        """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼"""
        print("ğŸ“Š å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¯¹æ¯”")
        print("=" * 100)
        
        # è¡¨å¤´
        headers = ["æ–¹æ³•", "å‚æ•°é‡", "è®­ç»ƒé€Ÿåº¦", "æ¨ç†é€Ÿåº¦", "å†…å­˜éœ€æ±‚", "æ•ˆæœ", "é€‚ç”¨åœºæ™¯"]
        print(f"{'æ–¹æ³•':<15} {'å‚æ•°é‡':<12} {'è®­ç»ƒé€Ÿåº¦':<10} {'æ¨ç†é€Ÿåº¦':<12} {'å†…å­˜éœ€æ±‚':<10} {'æ•ˆæœ':<8} {'é€‚ç”¨åœºæ™¯':<15}")
        print("-" * 100)
        
        # æ•°æ®è¡Œ
        for method, props in self.methods.items():
            print(f"{method:<15} {props['å‚æ•°é‡']:<12} {props['è®­ç»ƒé€Ÿåº¦']:<10} {props['æ¨ç†é€Ÿåº¦']:<12} {props['å†…å­˜éœ€æ±‚']:<10} {props['æ•ˆæœ']:<8} {props['é€‚ç”¨åœºæ™¯']:<15}")
    
    def visualize_efficiency_comparison(self):
        """å¯è§†åŒ–æ•ˆç‡å¯¹æ¯”"""
        import matplotlib.pyplot as plt
        import numpy as np
        
        methods = list(self.methods.keys())
        
        # æ¨¡æ‹Ÿæ•°æ® (å½’ä¸€åŒ–åˆ†æ•°)
        efficiency_scores = {
            "å‚æ•°æ•ˆç‡": [0.95, 0.85, 0.98, 0.1],    # LoRA, Adapter, Prefix, Full
            "è®­ç»ƒé€Ÿåº¦": [0.9, 0.7, 0.95, 0.3],
            "æ¨ç†é€Ÿåº¦": [0.95, 0.8, 1.0, 1.0],
            "æ•ˆæœè´¨é‡": [0.9, 0.8, 0.6, 1.0]
        }
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(efficiency_scores), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # é—­åˆ
        
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        for i, method in enumerate(methods):
            values = [efficiency_scores[metric][i] for metric in efficiency_scores.keys()]
            values += [values[0]]  # é—­åˆ
            
            ax.plot(angles, values, 'o-', linewidth=2, label=method, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(efficiency_scores.keys())
        ax.set_ylim(0, 1)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        ax.set_title('å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ç»¼åˆå¯¹æ¯”', size=16, pad=20)
        
        plt.tight_layout()
        plt.show()
    
    def recommend_method(self, scenario):
        """æ ¹æ®åœºæ™¯æ¨èæ–¹æ³•"""
        recommendations = {
            "èµ„æºå—é™": "LoRA",
            "å¤šä»»åŠ¡å­¦ä¹ ": "Adapter", 
            "ç”Ÿæˆä»»åŠ¡": "Prefix Tuning",
            "è¿½æ±‚æœ€ä½³æ•ˆæœ": "Full Fine-tuning",
            "å¿«é€ŸåŸå‹": "LoRA",
            "ç”Ÿäº§éƒ¨ç½²": "LoRA",
            "ç ”ç©¶å®éªŒ": "Full Fine-tuning"
        }
        
        method = recommendations.get(scenario, "LoRA")
        details = self.methods[method]
        
        print(f"\nğŸ¯ åœºæ™¯: {scenario}")
        print(f"ğŸ’¡ æ¨èæ–¹æ³•: {method}")
        print(f"âœ… ä¼˜åŠ¿: {', '.join(details['ä¼˜åŠ¿'])}")
        print(f"âš ï¸ æ³¨æ„: {', '.join(details['åŠ£åŠ¿'])}")
        
        return method

# æ¼”ç¤ºå¯¹æ¯”åˆ†æ
comparison = ParameterEfficientComparison()
comparison.create_comparison_table()
comparison.visualize_efficiency_comparison()

# åœºæ™¯æ¨è
scenarios = ["èµ„æºå—é™", "å¤šä»»åŠ¡å­¦ä¹ ", "ç”Ÿæˆä»»åŠ¡", "è¿½æ±‚æœ€ä½³æ•ˆæœ"]
for scenario in scenarios:
    comparison.recommend_method(scenario)
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥æŒæ¡äº†å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„æ ¸å¿ƒåŸç†å’Œå®é™…åº”ç”¨ã€‚è¿™äº›æŠ€æœ¯è®©æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚èƒ½å¤Ÿä»¥æ›´ä½çš„æˆæœ¬å’Œæ›´é«˜çš„æ•ˆç‡ç”Ÿäº§å‡ºé«˜è´¨é‡çš„å®šåˆ¶åŒ–æ¨¡å‹ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†æŠ€æœ¯ï¼Œè¿™æ˜¯ç¡®ä¿å¾®è°ƒæ•ˆæœçš„é‡è¦åŸºç¡€ã€‚

---

## 28.3 æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†

### ğŸ—ï¸ è¿›å…¥æ•°æ®å·¥ç¨‹è½¦é—´

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œæ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†å°±åƒæ˜¯åŸææ–™çš„ç²¾ç»†åŠ å·¥è½¦é—´ã€‚é«˜è´¨é‡çš„æ•°æ®æ˜¯æˆåŠŸå¾®è°ƒçš„åŸºç¡€ï¼Œå°±åƒä¼˜è´¨çš„åŸææ–™æ˜¯ç”Ÿäº§ç²¾å“çš„å‰æã€‚

è®©æˆ‘ä»¬æ·±å…¥è¿™ä¸ªæ•°æ®å·¥ç¨‹è½¦é—´ï¼Œå­¦ä¹ å¦‚ä½•å°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºè®­ç»ƒå°±ç»ªçš„é«˜è´¨é‡æ•°æ®é›†ï¼

#### ğŸ“Š æ•°æ®å·¥ç¨‹æµæ°´çº¿æ¶æ„

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡Mermaidå›¾äº†è§£å®Œæ•´çš„æ•°æ®å·¥ç¨‹æµæ°´çº¿ï¼š

```mermaid
graph TD
    A[åŸå§‹æ•°æ®æº] --> B{æ•°æ®ç±»å‹æ£€æµ‹}
    B -->|æ–‡æœ¬æ•°æ®| C[æ–‡æœ¬é¢„å¤„ç†]
    B -->|ç»“æ„åŒ–æ•°æ®| D[æ•°æ®æ¸…æ´—]
    B -->|å¤šæ¨¡æ€æ•°æ®| E[å¤šæ¨¡æ€å¤„ç†]
    
    C --> F[æ•°æ®è´¨é‡è¯„ä¼°]
    D --> F
    E --> F
    
    F --> G{è´¨é‡è¾¾æ ‡?}
    G -->|å¦| H[æ•°æ®ä¿®å¤]
    H --> F
    G -->|æ˜¯| I[æ•°æ®å¢å¼º]
    
    I --> J[æ•°æ®åˆ’åˆ†]
    J --> K[è®­ç»ƒé›†]
    J --> L[éªŒè¯é›†]
    J --> M[æµ‹è¯•é›†]
    
    K --> N[æ•°æ®åŠ è½½å™¨]
    L --> N
    M --> N
    
    N --> O[æ‰¹å¤„ç†ä¼˜åŒ–]
    O --> P[å¾®è°ƒå°±ç»ªæ•°æ®]
    
    style A fill:#e1f5fe
    style P fill:#e8f5e8
    style G fill:#fff3e0
    style F fill:#f3e5f5
```

è®©æˆ‘ä»¬å®ç°è¿™ä¸ªå®Œæ•´çš„æ•°æ®å·¥ç¨‹ç³»ç»Ÿï¼š

```python
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import AutoTokenizer
import re
import json
from typing import List, Dict, Tuple, Optional, Union
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class DataQualityAnalyzer:
    """æ•°æ®è´¨é‡åˆ†æå™¨"""
    
    def __init__(self):
        self.quality_metrics = {}
        self.recommendations = []
    
    def analyze_text_quality(self, texts: List[str]) -> Dict:
        """
        åˆ†ææ–‡æœ¬æ•°æ®è´¨é‡
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        Returns:
            è´¨é‡åˆ†ææŠ¥å‘Š
        """
        print("ğŸ” å¼€å§‹æ–‡æœ¬è´¨é‡åˆ†æ...")
        
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(texts)
        empty_texts = sum(1 for text in texts if not text or text.strip() == "")
        avg_length = np.mean([len(text) for text in texts if text])
        std_length = np.std([len(text) for text in texts if text])
        
        # å­—ç¬¦åˆ†æ
        char_counts = Counter()
        for text in texts:
            if text:
                char_counts.update(text)
        
        # è¯­è¨€æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        chinese_chars = sum(1 for char in char_counts if '\u4e00' <= char <= '\u9fff')
        english_chars = sum(1 for char in char_counts if char.isalpha() and ord(char) < 128)
        
        # ç‰¹æ®Šå­—ç¬¦æ£€æµ‹
        special_chars = sum(1 for char in char_counts if not char.isalnum() and not char.isspace())
        
        quality_report = {
            "æ ·æœ¬æ€»æ•°": total_samples,
            "ç©ºæ–‡æœ¬æ•°": empty_texts,
            "ç©ºæ–‡æœ¬æ¯”ä¾‹": empty_texts / total_samples if total_samples > 0 else 0,
            "å¹³å‡é•¿åº¦": round(avg_length, 2),
            "é•¿åº¦æ ‡å‡†å·®": round(std_length, 2),
            "ä¸­æ–‡å­—ç¬¦æ•°": chinese_chars,
            "è‹±æ–‡å­—ç¬¦æ•°": english_chars,
            "ç‰¹æ®Šå­—ç¬¦æ•°": special_chars,
            "å­—ç¬¦å¤šæ ·æ€§": len(char_counts)
        }
        
        # è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(quality_report)
        quality_report["è´¨é‡è¯„åˆ†"] = quality_score
        
        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations(quality_report)
        
        print(f"âœ… æ–‡æœ¬è´¨é‡åˆ†æå®Œæˆï¼Œè´¨é‡è¯„åˆ†: {quality_score:.1f}/100")
        return quality_report
    
    def _calculate_quality_score(self, report: Dict) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        score = 100.0
        
        # ç©ºæ–‡æœ¬æƒ©ç½š
        if report["ç©ºæ–‡æœ¬æ¯”ä¾‹"] > 0.1:
            score -= 20 * report["ç©ºæ–‡æœ¬æ¯”ä¾‹"]
        
        # é•¿åº¦ä¸€è‡´æ€§è¯„åˆ†
        if report["é•¿åº¦æ ‡å‡†å·®"] > report["å¹³å‡é•¿åº¦"]:
            score -= 15
        
        # å­—ç¬¦å¤šæ ·æ€§è¯„åˆ†
        if report["å­—ç¬¦å¤šæ ·æ€§"] < 100:
            score -= 10
        
        return max(0, score)
    
    def _generate_recommendations(self, report: Dict):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        self.recommendations = []
        
        if report["ç©ºæ–‡æœ¬æ¯”ä¾‹"] > 0.05:
            self.recommendations.append("å»ºè®®æ¸…ç†ç©ºæ–‡æœ¬æ•°æ®")
        
        if report["é•¿åº¦æ ‡å‡†å·®"] > report["å¹³å‡é•¿åº¦"]:
            self.recommendations.append("å»ºè®®è¿›è¡Œé•¿åº¦æ ‡å‡†åŒ–å¤„ç†")
        
        if report["ç‰¹æ®Šå­—ç¬¦æ•°"] > report["ä¸­æ–‡å­—ç¬¦æ•°"] + report["è‹±æ–‡å­—ç¬¦æ•°"]:
            self.recommendations.append("å»ºè®®æ¸…ç†è¿‡å¤šçš„ç‰¹æ®Šå­—ç¬¦")
    
    def visualize_quality_report(self, report: Dict):
        """å¯è§†åŒ–è´¨é‡æŠ¥å‘Š"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = ["æ ·æœ¬æ€»æ•°", "ç©ºæ–‡æœ¬æ•°", "å­—ç¬¦å¤šæ ·æ€§"]
        values = [report[stat] for stat in basic_stats]
        ax1.bar(basic_stats, values, color=['skyblue', 'lightcoral', 'lightgreen'])
        ax1.set_title('åŸºç¡€ç»Ÿè®¡ä¿¡æ¯')
        ax1.tick_params(axis='x', rotation=45)
        
        # å­—ç¬¦åˆ†å¸ƒ
        char_types = ["ä¸­æ–‡å­—ç¬¦", "è‹±æ–‡å­—ç¬¦", "ç‰¹æ®Šå­—ç¬¦"]
        char_counts = [report["ä¸­æ–‡å­—ç¬¦æ•°"], report["è‹±æ–‡å­—ç¬¦æ•°"], report["ç‰¹æ®Šå­—ç¬¦æ•°"]]
        ax2.pie(char_counts, labels=char_types, autopct='%1.1f%%', startangle=90)
        ax2.set_title('å­—ç¬¦ç±»å‹åˆ†å¸ƒ')
        
        # è´¨é‡è¯„åˆ†
        ax3.bar(['è´¨é‡è¯„åˆ†'], [report["è´¨é‡è¯„åˆ†"]], color='gold')
        ax3.set_ylim(0, 100)
        ax3.set_title('æ•°æ®è´¨é‡è¯„åˆ†')
        ax3.axhline(y=80, color='green', linestyle='--', label='ä¼˜ç§€çº¿')
        ax3.axhline(y=60, color='orange', linestyle='--', label='åŠæ ¼çº¿')
        ax3.legend()
        
        # é•¿åº¦åˆ†æ
        length_stats = ["å¹³å‡é•¿åº¦", "é•¿åº¦æ ‡å‡†å·®"]
        length_values = [report["å¹³å‡é•¿åº¦"], report["é•¿åº¦æ ‡å‡†å·®"]]
        ax4.bar(length_stats, length_values, color=['lightblue', 'pink'])
        ax4.set_title('æ–‡æœ¬é•¿åº¦åˆ†æ')
        
        plt.tight_layout()
        plt.show()

class AdvancedDataProcessor:
    """é«˜çº§æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, tokenizer_name: str = "bert-base-chinese"):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        Args:
            tokenizer_name: åˆ†è¯å™¨åç§°
        """
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.label_encoder = LabelEncoder()
        self.processing_stats = {}
        
        print(f"ğŸ”§ åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨: {tokenizer_name}")
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—å•ä¸ªæ–‡æœ¬
        Args:
            text: åŸå§‹æ–‡æœ¬
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return ""
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text.strip())
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\u4e00-\u9fff\w\s.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)
        
        # å»é™¤è¿‡çŸ­çš„æ–‡æœ¬
        if len(text) < 3:
            return ""
        
        return text
    
    def batch_clean_texts(self, texts: List[str]) -> List[str]:
        """
        æ‰¹é‡æ¸…æ´—æ–‡æœ¬
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        print("ğŸ§¹ å¼€å§‹æ‰¹é‡æ–‡æœ¬æ¸…æ´—...")
        
        cleaned_texts = []
        removed_count = 0
        
        for text in texts:
            cleaned = self.clean_text(text)
            if cleaned:
                cleaned_texts.append(cleaned)
            else:
                removed_count += 1
        
        self.processing_stats['removed_texts'] = removed_count
        self.processing_stats['clean_ratio'] = len(cleaned_texts) / len(texts)
        
        print(f"âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆï¼Œä¿ç•™: {len(cleaned_texts)}, ç§»é™¤: {removed_count}")
        return cleaned_texts
    
    def augment_data(self, texts: List[str], labels: List[int], 
                    augment_ratio: float = 0.2) -> Tuple[List[str], List[int]]:
        """
        æ•°æ®å¢å¼º
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            augment_ratio: å¢å¼ºæ¯”ä¾‹
        Returns:
            å¢å¼ºåçš„æ–‡æœ¬å’Œæ ‡ç­¾
        """
        print(f"ğŸ”„ å¼€å§‹æ•°æ®å¢å¼ºï¼Œå¢å¼ºæ¯”ä¾‹: {augment_ratio:.1%}")
        
        augmented_texts = texts.copy()
        augmented_labels = labels.copy()
        
        # è®¡ç®—éœ€è¦å¢å¼ºçš„æ ·æœ¬æ•°
        augment_count = int(len(texts) * augment_ratio)
        
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œå¢å¼º
        indices = np.random.choice(len(texts), augment_count, replace=True)
        
        for idx in indices:
            original_text = texts[idx]
            original_label = labels[idx]
            
            # ç®€å•çš„å¢å¼ºç­–ç•¥ï¼šåŒä¹‰è¯æ›¿æ¢ã€å¥å¼å˜æ¢ç­‰
            augmented_text = self._simple_augment(original_text)
            
            if augmented_text and augmented_text != original_text:
                augmented_texts.append(augmented_text)
                augmented_labels.append(original_label)
        
        print(f"âœ… æ•°æ®å¢å¼ºå®Œæˆï¼Œä» {len(texts)} å¢åŠ åˆ° {len(augmented_texts)} æ ·æœ¬")
        return augmented_texts, augmented_labels
    
    def _simple_augment(self, text: str) -> str:
        """ç®€å•çš„æ–‡æœ¬å¢å¼º"""
        # è¿™é‡Œå®ç°ç®€å•çš„å¢å¼ºç­–ç•¥
        # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å¢å¼ºæ–¹æ³•
        
        augment_strategies = [
            lambda x: x,  # ä¿æŒåŸæ ·
            lambda x: x + "ã€‚",  # æ·»åŠ å¥å·
            lambda x: x.replace("å¾ˆ", "éå¸¸"),  # åŒä¹‰è¯æ›¿æ¢
            lambda x: x.replace("å¥½", "ä¸é”™"),  # åŒä¹‰è¯æ›¿æ¢
        ]
        
        strategy = np.random.choice(augment_strategies)
        return strategy(text)

class SmartDataLoader:
    """æ™ºèƒ½æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, tokenizer, max_length: int = 128):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        Args:
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def create_dataset(self, texts: List[str], labels: List[int]) -> 'FineTuningDataset':
        """
        åˆ›å»ºæ•°æ®é›†
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
        Returns:
            æ•°æ®é›†å¯¹è±¡
        """
        return FineTuningDataset(texts, labels, self.tokenizer, self.max_length)
    
    def create_dataloaders(self, dataset, train_ratio: float = 0.8, 
                          val_ratio: float = 0.1, batch_size: int = 32,
                          shuffle: bool = True) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨
        Args:
            dataset: æ•°æ®é›†
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±
        Returns:
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨
        """
        # è®¡ç®—å„éƒ¨åˆ†å¤§å°
        total_size = len(dataset)
        train_size = int(total_size * train_ratio)
        val_size = int(total_size * val_ratio)
        test_size = total_size - train_size - val_size
        
        # åˆ’åˆ†æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = random_split(
            dataset, [train_size, val_size, test_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=shuffle
        )
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False
        )
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False
        )
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {train_size} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {val_size} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {test_size} æ ·æœ¬")
        
        return train_loader, val_loader, test_loader

class FineTuningDataset(Dataset):
    """å¾®è°ƒä¸“ç”¨æ•°æ®é›†"""
    
    def __init__(self, texts: List[str], labels: List[int], 
                 tokenizer, max_length: int = 128):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        assert len(texts) == len(labels), "æ–‡æœ¬å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…"
        
        print(f"ğŸ“¦ åˆ›å»ºæ•°æ®é›†: {len(texts)} æ ·æœ¬, æœ€å¤§é•¿åº¦: {max_length}")
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # åˆ†è¯å’Œç¼–ç 
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long),
            'text': text  # ä¿ç•™åŸæ–‡æœ¬ç”¨äºè°ƒè¯•
        }

# æ¼”ç¤ºå®Œæ•´çš„æ•°æ®å·¥ç¨‹æµç¨‹
print("ğŸ—ï¸ æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†æ¼”ç¤º")
print("=" * 60)

# 1. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
sample_data = {
    'text': [
        "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼æ¼”å‘˜è¡¨æ¼”å‡ºè‰²ï¼Œå‰§æƒ…å¼•äººå…¥èƒœã€‚",
        "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œç­‰äº†å¾ˆä¹…éƒ½æ²¡äººç†ã€‚",
        "   ",  # ç©ºæ–‡æœ¬
        "è¿˜å¯ä»¥ï¼Œä¸€èˆ¬èˆ¬çš„ä½“éªŒï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚",
        "éå¸¸æ»¡æ„è¿™æ¬¡è´­ç‰©ä½“éªŒï¼ï¼ï¼",
        "",  # ç©ºæ–‡æœ¬
        "è´¨é‡ä¸å¤ªå¥½ï¼Œç”¨äº†å‡ å¤©å°±åäº†ã€‚",
        "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—æ¨èç»™æœ‹å‹ã€‚",
        "###ç‰¹æ®Šå­—ç¬¦###æµ‹è¯•æ–‡æœ¬@@@",  # åŒ…å«ç‰¹æ®Šå­—ç¬¦
        "è¶…çº§æ£’çš„äº§å“ï¼Œäº”æ˜Ÿå¥½è¯„ï¼"
    ] * 50,  # æ‰©å±•æ•°æ®
    'label': [2, 0, 1, 1, 2, 1, 0, 2, 1, 2] * 50  # 0:è´Ÿé¢, 1:ä¸­æ€§, 2:æ­£é¢
}

df = pd.DataFrame(sample_data)
print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")

# 2. æ•°æ®è´¨é‡åˆ†æ
quality_analyzer = DataQualityAnalyzer()
quality_report = quality_analyzer.analyze_text_quality(df['text'].tolist())

print("\nğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š:")
for key, value in quality_report.items():
    print(f"  {key}: {value}")

print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
for rec in quality_analyzer.recommendations:
    print(f"  â€¢ {rec}")

# å¯è§†åŒ–è´¨é‡æŠ¥å‘Š
quality_analyzer.visualize_quality_report(quality_report)

# 3. æ•°æ®å¤„ç†
processor = AdvancedDataProcessor()

# æ¸…æ´—æ•°æ®
cleaned_texts = processor.batch_clean_texts(df['text'].tolist())
cleaned_labels = [df['label'].tolist()[i] for i, text in enumerate(df['text'].tolist()) 
                 if processor.clean_text(text)]

print(f"\nğŸ§¹ æ•°æ®æ¸…æ´—ç»“æœ:")
print(f"  æ¸…æ´—å‰: {len(df)} æ ·æœ¬")
print(f"  æ¸…æ´—å: {len(cleaned_texts)} æ ·æœ¬")
print(f"  ä¿ç•™ç‡: {len(cleaned_texts)/len(df):.1%}")

# 4. æ•°æ®å¢å¼º
augmented_texts, augmented_labels = processor.augment_data(
    cleaned_texts, cleaned_labels, augment_ratio=0.3
)

print(f"\nğŸ”„ æ•°æ®å¢å¼ºç»“æœ:")
print(f"  å¢å¼ºå‰: {len(cleaned_texts)} æ ·æœ¬")
print(f"  å¢å¼ºå: {len(augmented_texts)} æ ·æœ¬")
print(f"  å¢é•¿ç‡: {(len(augmented_texts)-len(cleaned_texts))/len(cleaned_texts):.1%}")

# 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
data_loader = SmartDataLoader(tokenizer, max_length=128)

# åˆ›å»ºæ•°æ®é›†
dataset = data_loader.create_dataset(augmented_texts, augmented_labels)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader, test_loader = data_loader.create_dataloaders(
    dataset, train_ratio=0.7, val_ratio=0.15, batch_size=16
)

print(f"\nğŸ“¦ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
print(f"  æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")

# 6. æ•°æ®æ‰¹æ¬¡æ£€æŸ¥
print(f"\nğŸ” æ•°æ®æ‰¹æ¬¡æ£€æŸ¥:")
for batch in train_loader:
    print(f"  input_idså½¢çŠ¶: {batch['input_ids'].shape}")
    print(f"  attention_maskå½¢çŠ¶: {batch['attention_mask'].shape}")
    print(f"  labelså½¢çŠ¶: {batch['label'].shape}")
    print(f"  ç¤ºä¾‹æ–‡æœ¬: {batch['text'][0]}")
    break
```

#### ğŸ›ï¸ æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå®æ—¶çš„æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿï¼š

```mermaid
graph TD
    A[æ•°æ®è¾“å…¥] --> B[å®æ—¶è´¨é‡æ£€æµ‹]
    B --> C{è´¨é‡æ£€æŸ¥}
    C -->|é€šè¿‡| D[æ•°æ®å…¥åº“]
    C -->|ä¸é€šè¿‡| E[è´¨é‡ä¿®å¤]
    E --> F{ä¿®å¤æˆåŠŸ?}
    F -->|æ˜¯| D
    F -->|å¦| G[äººå·¥å®¡æ ¸]
    
    D --> H[è´¨é‡æŒ‡æ ‡æ›´æ–°]
    H --> I[ç›‘æ§é¢æ¿]
    I --> J[å‘Šè­¦ç³»ç»Ÿ]
    
    G --> K[æ ‡è®°é—®é¢˜æ•°æ®]
    K --> L[è´¨é‡æŠ¥å‘Š]
    
    style A fill:#e1f5fe
    style D fill:#e8f5e8
    style C fill:#fff3e0
    style J fill:#ffebee
```

```python
import time
from datetime import datetime
import threading
from queue import Queue
import json

class DataQualityMonitor:
    """æ•°æ®è´¨é‡å®æ—¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, quality_threshold: float = 80.0):
        """
        åˆå§‹åŒ–è´¨é‡ç›‘æ§å™¨
        Args:
            quality_threshold: è´¨é‡é˜ˆå€¼
        """
        self.quality_threshold = quality_threshold
        self.monitoring_active = False
        self.quality_history = []
        self.alert_queue = Queue()
        self.stats = {
            'total_samples': 0,
            'passed_samples': 0,
            'failed_samples': 0,
            'avg_quality': 0.0
        }
        
        print(f"ğŸ” æ•°æ®è´¨é‡ç›‘æ§å™¨åˆå§‹åŒ–ï¼Œè´¨é‡é˜ˆå€¼: {quality_threshold}")
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        self.monitoring_active = True
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=self._monitoring_loop)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # å¯åŠ¨å‘Šè­¦å¤„ç†çº¿ç¨‹
        alert_thread = threading.Thread(target=self._alert_handler)
        alert_thread.daemon = True
        alert_thread.start()
        
        print("ğŸš€ è´¨é‡ç›‘æ§ç³»ç»Ÿå¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        print("â¹ï¸ è´¨é‡ç›‘æ§ç³»ç»Ÿåœæ­¢")
    
    def check_data_quality(self, text: str) -> Dict:
        """
        æ£€æŸ¥å•æ¡æ•°æ®è´¨é‡
        Args:
            text: æ–‡æœ¬æ•°æ®
        Returns:
            è´¨é‡æ£€æŸ¥ç»“æœ
        """
        quality_score = 100.0
        issues = []
        
        # é•¿åº¦æ£€æŸ¥
        if len(text) < 5:
            quality_score -= 30
            issues.append("æ–‡æœ¬è¿‡çŸ­")
        elif len(text) > 1000:
            quality_score -= 20
            issues.append("æ–‡æœ¬è¿‡é•¿")
        
        # å­—ç¬¦æ£€æŸ¥
        if not any(c.isalnum() for c in text):
            quality_score -= 40
            issues.append("ç¼ºå°‘å­—æ¯æ•°å­—å­—ç¬¦")
        
        # ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹æ£€æŸ¥
        special_char_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / len(text)
        if special_char_ratio > 0.3:
            quality_score -= 25
            issues.append("ç‰¹æ®Šå­—ç¬¦è¿‡å¤š")
        
        # é‡å¤å­—ç¬¦æ£€æŸ¥
        if any(text.count(c) > len(text) * 0.5 for c in set(text)):
            quality_score -= 35
            issues.append("é‡å¤å­—ç¬¦è¿‡å¤š")
        
        quality_score = max(0, quality_score)
        
        result = {
            'quality_score': quality_score,
            'passed': quality_score >= self.quality_threshold,
            'issues': issues,
            'timestamp': datetime.now().isoformat()
        }
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(result)
        
        # å¦‚æœè´¨é‡ä¸è¾¾æ ‡ï¼Œæ·»åŠ åˆ°å‘Šè­¦é˜Ÿåˆ—
        if not result['passed']:
            self.alert_queue.put({
                'type': 'quality_alert',
                'text': text[:100] + '...' if len(text) > 100 else text,
                'score': quality_score,
                'issues': issues,
                'timestamp': result['timestamp']
            })
        
        return result
    
    def _update_stats(self, result: Dict):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_samples'] += 1
        
        if result['passed']:
            self.stats['passed_samples'] += 1
        else:
            self.stats['failed_samples'] += 1
        
        # æ›´æ–°å¹³å‡è´¨é‡
        self.quality_history.append(result['quality_score'])
        if len(self.quality_history) > 1000:  # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
            self.quality_history.pop(0)
        
        self.stats['avg_quality'] = np.mean(self.quality_history)
    
    def _monitoring_loop(self):
        """ç›‘æ§ä¸»å¾ªç¯"""
        while self.monitoring_active:
            # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
            if self.stats['total_samples'] > 0:
                pass_rate = self.stats['passed_samples'] / self.stats['total_samples']
                if pass_rate < 0.8:  # é€šè¿‡ç‡ä½äº80%æ—¶å‘Šè­¦
                    self.alert_queue.put({
                        'type': 'pass_rate_alert',
                        'pass_rate': pass_rate,
                        'timestamp': datetime.now().isoformat()
                    })
            
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
    
    def _alert_handler(self):
        """å‘Šè­¦å¤„ç†å™¨"""
        while self.monitoring_active:
            try:
                alert = self.alert_queue.get(timeout=1)
                self._process_alert(alert)
            except:
                continue
    
    def _process_alert(self, alert: Dict):
        """å¤„ç†å‘Šè­¦"""
        if alert['type'] == 'quality_alert':
            print(f"âš ï¸ æ•°æ®è´¨é‡å‘Šè­¦: è´¨é‡è¯„åˆ† {alert['score']:.1f}")
            print(f"   é—®é¢˜: {', '.join(alert['issues'])}")
            print(f"   æ—¶é—´: {alert['timestamp']}")
        elif alert['type'] == 'pass_rate_alert':
            print(f"ğŸš¨ é€šè¿‡ç‡å‘Šè­¦: {alert['pass_rate']:.1%}")
            print(f"   æ—¶é—´: {alert['timestamp']}")
    
    def get_quality_dashboard(self) -> Dict:
        """è·å–è´¨é‡ä»ªè¡¨æ¿æ•°æ®"""
        return {
            'total_samples': self.stats['total_samples'],
            'passed_samples': self.stats['passed_samples'],
            'failed_samples': self.stats['failed_samples'],
            'pass_rate': self.stats['passed_samples'] / max(1, self.stats['total_samples']),
            'avg_quality': self.stats['avg_quality'],
            'recent_quality_trend': self.quality_history[-10:] if self.quality_history else []
        }
    
    def visualize_dashboard(self):
        """å¯è§†åŒ–ç›‘æ§é¢æ¿"""
        dashboard = self.get_quality_dashboard()
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # é€šè¿‡ç‡é¥¼å›¾
        pass_fail = [dashboard['passed_samples'], dashboard['failed_samples']]
        labels = ['é€šè¿‡', 'å¤±è´¥']
        colors = ['lightgreen', 'lightcoral']
        ax1.pie(pass_fail, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        ax1.set_title(f'æ•°æ®è´¨é‡é€šè¿‡ç‡: {dashboard["pass_rate"]:.1%}')
        
        # æ ·æœ¬æ€»æ•°
        ax2.bar(['æ€»æ ·æœ¬æ•°'], [dashboard['total_samples']], color='skyblue')
        ax2.set_title('å¤„ç†æ ·æœ¬æ€»æ•°')
        ax2.set_ylabel('æ ·æœ¬æ•°')
        
        # å¹³å‡è´¨é‡è¯„åˆ†
        ax3.bar(['å¹³å‡è´¨é‡'], [dashboard['avg_quality']], color='gold')
        ax3.set_ylim(0, 100)
        ax3.set_title(f'å¹³å‡è´¨é‡è¯„åˆ†: {dashboard["avg_quality"]:.1f}')
        ax3.axhline(y=self.quality_threshold, color='red', linestyle='--', label='é˜ˆå€¼')
        ax3.legend()
        
        # è´¨é‡è¶‹åŠ¿
        if dashboard['recent_quality_trend']:
            ax4.plot(dashboard['recent_quality_trend'], marker='o', color='blue')
            ax4.set_title('æœ€è¿‘è´¨é‡è¶‹åŠ¿')
            ax4.set_ylabel('è´¨é‡è¯„åˆ†')
            ax4.set_xlabel('æ ·æœ¬åºå·')
            ax4.axhline(y=self.quality_threshold, color='red', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.show()

# æ¼”ç¤ºæ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ
print("\nğŸ” æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿæ¼”ç¤º")
print("=" * 60)

# åˆ›å»ºç›‘æ§å™¨
monitor = DataQualityMonitor(quality_threshold=75.0)
monitor.start_monitoring()

# æ¨¡æ‹Ÿæ•°æ®æµ
test_texts = [
    "è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬æ ·æœ¬ï¼Œå†…å®¹ä¸°å¯Œä¸”æœ‰æ„ä¹‰ã€‚",
    "çŸ­æ–‡æœ¬",  # è´¨é‡é—®é¢˜ï¼šè¿‡çŸ­
    "!!!@@@###ç‰¹æ®Šå­—ç¬¦è¿‡å¤šçš„æ–‡æœ¬æ ·æœ¬###@@@!!!",  # è´¨é‡é—®é¢˜ï¼šç‰¹æ®Šå­—ç¬¦
    "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",  # è´¨é‡é—®é¢˜ï¼šé‡å¤å­—ç¬¦
    "æ­£å¸¸çš„æ–‡æœ¬æ ·æœ¬ï¼Œè´¨é‡åº”è¯¥æ˜¯åˆæ ¼çš„ã€‚",
    "",  # è´¨é‡é—®é¢˜ï¼šç©ºæ–‡æœ¬
    "å¦ä¸€ä¸ªæ­£å¸¸çš„é«˜è´¨é‡æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•ç›‘æ§ç³»ç»Ÿçš„åŠŸèƒ½ã€‚"
]

print("ğŸ“Š å¼€å§‹æ¨¡æ‹Ÿæ•°æ®è´¨é‡æ£€æŸ¥...")
for i, text in enumerate(test_texts):
    result = monitor.check_data_quality(text)
    print(f"æ ·æœ¬ {i+1}: è´¨é‡è¯„åˆ† {result['quality_score']:.1f}, "
          f"é€šè¿‡: {'âœ…' if result['passed'] else 'âŒ'}")
    if result['issues']:
        print(f"  é—®é¢˜: {', '.join(result['issues'])}")
    time.sleep(0.5)  # æ¨¡æ‹Ÿå®æ—¶å¤„ç†

# ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç›‘æ§ç³»ç»Ÿå¤„ç†
time.sleep(2)

# æ˜¾ç¤ºç›‘æ§é¢æ¿
print(f"\nğŸ“ˆ è´¨é‡ç›‘æ§é¢æ¿:")
dashboard = monitor.get_quality_dashboard()
for key, value in dashboard.items():
    if key != 'recent_quality_trend':
        print(f"  {key}: {value}")

# å¯è§†åŒ–ç›‘æ§é¢æ¿
monitor.visualize_dashboard()

# åœæ­¢ç›‘æ§
monitor.stop_monitoring()
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†å®Œæ•´çš„æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†ä½“ç³»ã€‚ä»æ•°æ®è´¨é‡åˆ†æåˆ°å®æ—¶ç›‘æ§ï¼Œä»æ•°æ®æ¸…æ´—åˆ°æ™ºèƒ½å¢å¼ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ç°åœ¨æ‹¥æœ‰äº†é«˜æ•ˆçš„åŸææ–™åŠ å·¥è½¦é—´ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§æŠ€æœ¯ï¼Œç¡®ä¿å¾®è°ƒè¿‡ç¨‹çš„é«˜æ•ˆå’Œç¨³å®šã€‚

--- 

## 28.4 è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§

### âš™ï¸ è¿›å…¥æ™ºèƒ½è®­ç»ƒç›‘æ§ä¸­å¿ƒ

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œè®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§å°±åƒæ˜¯ç”Ÿäº§çº¿çš„æ™ºèƒ½æ§åˆ¶ä¸­å¿ƒã€‚å®ƒä¸ä»…è¦ç¡®ä¿ç”Ÿäº§è¿‡ç¨‹çš„é«˜æ•ˆè¿è¡Œï¼Œè¿˜è¦å®æ—¶ç›‘æ§å„é¡¹æŒ‡æ ‡ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ã€‚

è®©æˆ‘ä»¬æ·±å…¥è¿™ä¸ªæ™ºèƒ½æ§åˆ¶ä¸­å¿ƒï¼Œå­¦ä¹ å¦‚ä½•æ‰“é€ ä¸€ä¸ªå…¨æ–¹ä½çš„è®­ç»ƒç›‘æ§å’Œä¼˜åŒ–ç³»ç»Ÿï¼

#### ğŸ›ï¸ è®­ç»ƒç›‘æ§ç³»ç»Ÿæ¶æ„

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡Mermaidå›¾äº†è§£å®Œæ•´çš„è®­ç»ƒç›‘æ§ç³»ç»Ÿæ¶æ„ï¼š

```mermaid
graph TD
    A[è®­ç»ƒå¯åŠ¨] --> B[å‚æ•°åˆå§‹åŒ–]
    B --> C[è®­ç»ƒå¾ªç¯å¼€å§‹]
    
    C --> D[æ‰¹æ¬¡æ•°æ®åŠ è½½]
    D --> E[å‰å‘ä¼ æ’­]
    E --> F[æŸå¤±è®¡ç®—]
    F --> G[åå‘ä¼ æ’­]
    G --> H[å‚æ•°æ›´æ–°]
    
    H --> I[æ€§èƒ½ç›‘æ§]
    I --> J{ç›‘æ§æ£€æŸ¥}
    J -->|æ­£å¸¸| K[è®°å½•æŒ‡æ ‡]
    J -->|å¼‚å¸¸| L[å¼‚å¸¸å¤„ç†]
    
    K --> M{è®­ç»ƒå®Œæˆ?}
    L --> N[è°ƒæ•´ç­–ç•¥]
    N --> M
    
    M -->|å¦| D
    M -->|æ˜¯| O[è®­ç»ƒç»“æŸ]
    
    I --> P[å®æ—¶å¯è§†åŒ–]
    I --> Q[å‘Šè­¦ç³»ç»Ÿ]
    I --> R[è‡ªåŠ¨è°ƒä¼˜]
    
    style A fill:#e1f5fe
    style O fill:#e8f5e8
    style J fill:#fff3e0
    style L fill:#ffebee
    style R fill:#f3e5f5
```

è®©æˆ‘ä»¬å®ç°è¿™ä¸ªå®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œä¼˜åŒ–ç³»ç»Ÿï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, deque
import time
import json
import warnings
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
import threading
from dataclasses import dataclass, field

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    learning_rate: float = 2e-5
    batch_size: int = 32
    num_epochs: int = 10
    warmup_steps: int = 100
    weight_decay: float = 0.01
    gradient_clip_norm: float = 1.0
    early_stopping_patience: int = 3
    save_best_model: bool = True
    log_interval: int = 10
    eval_interval: int = 100
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer_type: str = "adamw"
    scheduler_type: str = "reduce_on_plateau"
    
    # ç›‘æ§é…ç½®
    monitor_gpu: bool = True
    monitor_memory: bool = True
    enable_profiling: bool = False

class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡ç®¡ç†å™¨"""
    
    def __init__(self, history_size: int = 1000):
        """
        åˆå§‹åŒ–æŒ‡æ ‡ç®¡ç†å™¨
        Args:
            history_size: å†å²è®°å½•å¤§å°
        """
        self.history_size = history_size
        self.metrics = defaultdict(lambda: deque(maxlen=history_size))
        self.best_metrics = {}
        self.current_epoch = 0
        self.current_step = 0
        
    def update(self, metric_name: str, value: float, step: Optional[int] = None):
        """
        æ›´æ–°æŒ‡æ ‡
        Args:
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            step: æ­¥æ•°
        """
        if step is None:
            step = self.current_step
            
        self.metrics[metric_name].append((step, value))
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if metric_name not in self.best_metrics:
            self.best_metrics[metric_name] = value
        else:
            if 'loss' in metric_name.lower():
                self.best_metrics[metric_name] = min(self.best_metrics[metric_name], value)
            else:
                self.best_metrics[metric_name] = max(self.best_metrics[metric_name], value)
    
    def get_latest(self, metric_name: str) -> Optional[float]:
        """è·å–æœ€æ–°æŒ‡æ ‡å€¼"""
        if metric_name in self.metrics and self.metrics[metric_name]:
            return self.metrics[metric_name][-1][1]
        return None
    
    def get_average(self, metric_name: str, last_n: int = 10) -> Optional[float]:
        """è·å–å¹³å‡å€¼"""
        if metric_name in self.metrics and self.metrics[metric_name]:
            values = [item[1] for item in list(self.metrics[metric_name])[-last_n:]]
            return np.mean(values)
        return None
    
    def get_trend(self, metric_name: str, last_n: int = 20) -> str:
        """è·å–è¶‹åŠ¿"""
        if metric_name not in self.metrics or len(self.metrics[metric_name]) < last_n:
            return "insufficient_data"
        
        values = [item[1] for item in list(self.metrics[metric_name])[-last_n:]]
        
        # è®¡ç®—è¶‹åŠ¿
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]
        
        if abs(slope) < 1e-6:
            return "stable"
        elif slope > 0:
            return "increasing" if 'loss' not in metric_name.lower() else "deteriorating"
        else:
            return "decreasing" if 'loss' not in metric_name.lower() else "improving"

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    
    def __init__(self, config: TrainingConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.metrics = TrainingMetrics()
        self.alerts = []
        self.monitoring_active = False
        
        # GPUç›‘æ§
        self.gpu_available = torch.cuda.is_available()
        if self.gpu_available and config.monitor_gpu:
            self.device = torch.device('cuda')
        else:
            self.device = torch.device('cpu')
        
        print(f"ğŸ›ï¸ è®­ç»ƒç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        self.monitoring_active = True
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        if self.config.monitor_gpu or self.config.monitor_memory:
            monitor_thread = threading.Thread(target=self._system_monitor_loop)
            monitor_thread.daemon = True
            monitor_thread.start()
        
        print("ğŸš€ è®­ç»ƒç›‘æ§ç³»ç»Ÿå¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        print("â¹ï¸ è®­ç»ƒç›‘æ§ç³»ç»Ÿåœæ­¢")
    
    def log_metrics(self, metrics_dict: Dict[str, float], step: int):
        """
        è®°å½•æŒ‡æ ‡
        Args:
            metrics_dict: æŒ‡æ ‡å­—å…¸
            step: å½“å‰æ­¥æ•°
        """
        self.metrics.current_step = step
        
        for name, value in metrics_dict.items():
            self.metrics.update(name, value, step)
        
        # æ£€æŸ¥å¼‚å¸¸
        self._check_anomalies(metrics_dict)
    
    def _check_anomalies(self, metrics_dict: Dict[str, float]):
        """æ£€æŸ¥å¼‚å¸¸æƒ…å†µ"""
        for name, value in metrics_dict.items():
            # æ£€æŸ¥NaNæˆ–æ— ç©·å¤§
            if np.isnan(value) or np.isinf(value):
                self._add_alert(f"å¼‚å¸¸å€¼æ£€æµ‹: {name} = {value}", "error")
            
            # æ£€æŸ¥æŸå¤±çˆ†ç‚¸
            if 'loss' in name.lower() and value > 100:
                self._add_alert(f"æŸå¤±çˆ†ç‚¸: {name} = {value:.4f}", "warning")
            
            # æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±
            if 'grad_norm' in name.lower() and value < 1e-7:
                self._add_alert(f"æ¢¯åº¦æ¶ˆå¤±: {name} = {value:.2e}", "warning")
    
    def _add_alert(self, message: str, level: str = "info"):
        """æ·»åŠ å‘Šè­¦"""
        alert = {
            'timestamp': datetime.now().isoformat(),
            'message': message,
            'level': level,
            'step': self.metrics.current_step
        }
        self.alerts.append(alert)
        
        # æ‰“å°å‘Šè­¦
        emoji = {"info": "â„¹ï¸", "warning": "âš ï¸", "error": "ğŸš¨"}
        print(f"{emoji.get(level, 'â„¹ï¸')} {message}")
    
    def _system_monitor_loop(self):
        """ç³»ç»Ÿç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                # GPUç›‘æ§
                if self.gpu_available and self.config.monitor_gpu:
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                    gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3  # GB
                    
                    self.metrics.update('gpu_memory_allocated', gpu_memory)
                    self.metrics.update('gpu_memory_cached', gpu_memory_cached)
                    
                    # GPUå†…å­˜å‘Šè­¦
                    if gpu_memory > 10:  # è¶…è¿‡10GBå‘Šè­¦
                        self._add_alert(f"GPUå†…å­˜ä½¿ç”¨è¿‡é«˜: {gpu_memory:.1f}GB", "warning")
                
                time.sleep(5)  # æ¯5ç§’ç›‘æ§ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç³»ç»Ÿç›‘æ§é”™è¯¯: {e}")
                time.sleep(10)
    
    def get_training_summary(self) -> Dict:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'current_step': self.metrics.current_step,
            'current_epoch': self.metrics.current_epoch,
            'best_metrics': self.metrics.best_metrics.copy(),
            'recent_trends': {},
            'alerts_count': len(self.alerts),
            'system_status': 'normal'
        }
        
        # è®¡ç®—è¶‹åŠ¿
        for metric_name in self.metrics.metrics.keys():
            if metric_name.startswith(('train_', 'val_', 'test_')):
                summary['recent_trends'][metric_name] = self.metrics.get_trend(metric_name)
        
        return summary
    
    def visualize_training_progress(self):
        """å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
        if not self.metrics.metrics:
            print("æš‚æ— è®­ç»ƒæ•°æ®")
            return
        
        # å‡†å¤‡æ•°æ®
        metric_names = [name for name in self.metrics.metrics.keys() 
                       if name.startswith(('train_', 'val_', 'test_'))]
        
        if not metric_names:
            print("æš‚æ— è®­ç»ƒæŒ‡æ ‡æ•°æ®")
            return
        
        # åˆ›å»ºå­å›¾
        n_metrics = len(metric_names)
        n_cols = min(3, n_metrics)
        n_rows = (n_metrics + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
        if n_rows == 1:
            axes = [axes] if n_cols == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, metric_name in enumerate(metric_names):
            if i >= len(axes):
                break
                
            data = list(self.metrics.metrics[metric_name])
            if not data:
                continue
                
            steps, values = zip(*data)
            
            ax = axes[i]
            ax.plot(steps, values, marker='o', markersize=2, linewidth=1)
            ax.set_title(f'{metric_name} (æœ€æ–°: {values[-1]:.4f})')
            ax.set_xlabel('Steps')
            ax.set_ylabel('Value')
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(values) > 10:
                z = np.polyfit(range(len(values)), values, 1)
                p = np.poly1d(z)
                ax.plot(steps, p(range(len(values))), "--", alpha=0.7, color='red')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(metric_names), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()

class SmartOptimizer:
    """æ™ºèƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, model: nn.Module, config: TrainingConfig):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä¼˜åŒ–å™¨
        Args:
            model: æ¨¡å‹
            config: è®­ç»ƒé…ç½®
        """
        self.model = model
        self.config = config
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        
        # æ¢¯åº¦è£å‰ª
        self.grad_clip_norm = config.gradient_clip_norm
        
        print(f"ğŸ”§ æ™ºèƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–: {config.optimizer_type} + {config.scheduler_type}")
    
    def _create_optimizer(self) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config.optimizer_type.lower() == "adamw":
            return optim.AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type.lower() == "adam":
            return optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type.lower() == "sgd":
            return optim.SGD(
                self.model.parameters(),
                lr=self.config.learning_rate,
                momentum=0.9,
                weight_decay=self.config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {self.config.optimizer_type}")
    
    def _create_scheduler(self) -> Optional[object]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config.scheduler_type.lower() == "reduce_on_plateau":
            return ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=2,
                verbose=True
            )
        elif self.config.scheduler_type.lower() == "cosine":
            return CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs,
                eta_min=1e-7
            )
        elif self.config.scheduler_type.lower() == "none":
            return None
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {self.config.scheduler_type}")
    
    def step(self, loss: torch.Tensor) -> Dict[str, float]:
        """
        æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
        Args:
            loss: æŸå¤±å€¼
        Returns:
            ä¼˜åŒ–æŒ‡æ ‡
        """
        # åå‘ä¼ æ’­
        loss.backward()
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(), 
            self.grad_clip_norm
        )
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        # å­¦ä¹ ç‡è°ƒåº¦
        current_lr = self.optimizer.param_groups[0]['lr']
        
        return {
            'grad_norm': float(grad_norm),
            'learning_rate': current_lr
        }
    
    def scheduler_step(self, metric: Optional[float] = None):
        """å­¦ä¹ ç‡è°ƒåº¦å™¨æ­¥éª¤"""
        if self.scheduler is not None:
            if isinstance(self.scheduler, ReduceLROnPlateau):
                if metric is not None:
                    self.scheduler.step(metric)
            else:
                self.scheduler.step()

class AdvancedTrainer:
    """é«˜çº§è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config: TrainingConfig):
        """
        åˆå§‹åŒ–é«˜çº§è®­ç»ƒå™¨
        Args:
            model: æ¨¡å‹
            config: è®­ç»ƒé…ç½®
        """
        self.model = model
        self.config = config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.monitor = TrainingMonitor(config)
        self.optimizer = SmartOptimizer(model, config)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # æ—©åœæœºåˆ¶
        self.early_stopping_counter = 0
        self.best_val_loss = float('inf')
        
        # è®­ç»ƒçŠ¶æ€
        self.training_active = False
        
        print(f"ğŸ¯ é«˜çº§è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self, train_loader, val_loader, test_loader=None):
        """
        å¼€å§‹è®­ç»ƒ
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # å¯åŠ¨ç›‘æ§
        self.monitor.start_monitoring()
        self.training_active = True
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.monitor.device)
        
        global_step = 0
        
        try:
            for epoch in range(self.config.num_epochs):
                print(f"\nğŸ“… Epoch {epoch + 1}/{self.config.num_epochs}")
                
                # è®­ç»ƒé˜¶æ®µ
                train_metrics = self._train_epoch(train_loader, global_step)
                
                # éªŒè¯é˜¶æ®µ
                val_metrics = self._validate_epoch(val_loader)
                
                # åˆå¹¶æŒ‡æ ‡
                epoch_metrics = {**train_metrics, **val_metrics}
                
                # è®°å½•epochæŒ‡æ ‡
                self.monitor.metrics.current_epoch = epoch + 1
                self.monitor.log_metrics(epoch_metrics, global_step)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.optimizer.scheduler_step(val_metrics.get('val_loss'))
                
                # æ—©åœæ£€æŸ¥
                if self._check_early_stopping(val_metrics.get('val_loss', float('inf'))):
                    print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                    break
                
                # æ‰“å°epochæ€»ç»“
                self._print_epoch_summary(epoch + 1, epoch_metrics)
                
                # æ›´æ–°å…¨å±€æ­¥æ•°
                global_step += len(train_loader)
            
            # æµ‹è¯•é˜¶æ®µ
            if test_loader is not None:
                print("\nğŸ§ª å¼€å§‹æµ‹è¯•...")
                test_metrics = self._test_epoch(test_loader)
                print(f"æµ‹è¯•ç»“æœ: {test_metrics}")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        
        finally:
            self.training_active = False
            self.monitor.stop_monitoring()
            
        print("âœ… è®­ç»ƒå®Œæˆ")
        
        # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
        self._show_training_summary()
    
    def _train_epoch(self, train_loader, global_step_start: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = 0
        
        for batch_idx, batch in enumerate(train_loader):
            global_step = global_step_start + batch_idx
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.monitor.device)
            attention_mask = batch['attention_mask'].to(self.monitor.device)
            labels = batch['label'].to(self.monitor.device)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)
            
            # ä¼˜åŒ–æ­¥éª¤
            opt_metrics = self.optimizer.step(loss)
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_samples += labels.size(0)
            
            predictions = torch.argmax(logits, dim=1)
            correct_predictions += (predictions == labels).sum().item()
            
            # è®°å½•æ‰¹æ¬¡æŒ‡æ ‡
            if batch_idx % self.config.log_interval == 0:
                batch_metrics = {
                    'train_loss': loss.item(),
                    'train_accuracy': (predictions == labels).float().mean().item(),
                    **opt_metrics
                }
                self.monitor.log_metrics(batch_metrics, global_step)
                
                print(f"  Batch {batch_idx}/{len(train_loader)}: "
                      f"Loss={loss.item():.4f}, "
                      f"Acc={batch_metrics['train_accuracy']:.3f}, "
                      f"LR={opt_metrics['learning_rate']:.2e}")
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(train_loader)
        avg_accuracy = correct_predictions / total_samples
        
        return {
            'train_loss_epoch': avg_loss,
            'train_accuracy_epoch': avg_accuracy
        }
    
    def _validate_epoch(self, val_loader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = 0
        
        with torch.no_grad():
            for batch in val_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.monitor.device)
                attention_mask = batch['attention_mask'].to(self.monitor.device)
                labels = batch['label'].to(self.monitor.device)
                
                # å‰å‘ä¼ æ’­
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_samples += labels.size(0)
                
                predictions = torch.argmax(logits, dim=1)
                correct_predictions += (predictions == labels).sum().item()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)
        avg_accuracy = correct_predictions / total_samples
        
        return {
            'val_loss': avg_loss,
            'val_accuracy': avg_accuracy
        }
    
    def _test_epoch(self, test_loader) -> Dict[str, float]:
        """æµ‹è¯•ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = 0
        
        with torch.no_grad():
            for batch in test_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.monitor.device)
                attention_mask = batch['attention_mask'].to(self.monitor.device)
                labels = batch['label'].to(self.monitor.device)
                
                # å‰å‘ä¼ æ’­
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_samples += labels.size(0)
                
                predictions = torch.argmax(logits, dim=1)
                correct_predictions += (predictions == labels).sum().item()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(test_loader)
        avg_accuracy = correct_predictions / total_samples
        
        return {
            'test_loss': avg_loss,
            'test_accuracy': avg_accuracy
        }
    
    def _check_early_stopping(self, val_loss: float) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.early_stopping_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if self.config.save_best_model:
                torch.save(self.model.state_dict(), 'best_model.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            
            return False
        else:
            self.early_stopping_counter += 1
            print(f"â° æ—©åœè®¡æ•°: {self.early_stopping_counter}/{self.config.early_stopping_patience}")
            
            return self.early_stopping_counter >= self.config.early_stopping_patience
    
    def _print_epoch_summary(self, epoch: int, metrics: Dict[str, float]):
        """æ‰“å°epochæ€»ç»“"""
        print(f"\nğŸ“Š Epoch {epoch} æ€»ç»“:")
        for name, value in metrics.items():
            print(f"  {name}: {value:.4f}")
        
        # æ˜¾ç¤ºè¶‹åŠ¿
        trends = {}
        for name in metrics.keys():
            trend = self.monitor.metrics.get_trend(name)
            if trend != "insufficient_data":
                trends[name] = trend
        
        if trends:
            print("ğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
            for name, trend in trends.items():
                emoji = {"improving": "ğŸ“ˆ", "deteriorating": "ğŸ“‰", "stable": "â¡ï¸"}.get(trend, "â“")
                print(f"  {name}: {trend} {emoji}")
    
    def _show_training_summary(self):
        """æ˜¾ç¤ºè®­ç»ƒæ€»ç»“"""
        summary = self.monitor.get_training_summary()
        
        print("\nğŸ¯ è®­ç»ƒæ€»ç»“:")
        print(f"  æ€»æ­¥æ•°: {summary['current_step']}")
        print(f"  æ€»è½®æ•°: {summary['current_epoch']}")
        print(f"  å‘Šè­¦æ•°é‡: {summary['alerts_count']}")
        
        print("\nğŸ† æœ€ä½³æŒ‡æ ‡:")
        for name, value in summary['best_metrics'].items():
            print(f"  {name}: {value:.4f}")
        
        # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        self.monitor.visualize_training_progress()

# æ¼”ç¤ºè®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§ç³»ç»Ÿ
print("âš™ï¸ è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§ç³»ç»Ÿæ¼”ç¤º")
print("=" * 60)

# åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œæ•°æ®
class SimpleModel(nn.Module):
    def __init__(self, vocab_size=1000, hidden_size=128, num_classes=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, input_ids, attention_mask):
        embeddings = self.embedding(input_ids)
        pooled = embeddings.mean(dim=1)  # ç®€å•å¹³å‡æ± åŒ–
        return self.classifier(pooled)

# åˆ›å»ºæ¨¡å‹
model = SimpleModel()

# åˆ›å»ºè®­ç»ƒé…ç½®
config = TrainingConfig(
    learning_rate=1e-3,
    batch_size=16,
    num_epochs=5,
    early_stopping_patience=2,
    log_interval=5
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = AdvancedTrainer(model, config)

print("âœ… è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡å¼€å§‹æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹")
print("ğŸ“ æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º")
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨æ–¹ä½çš„è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§ç³»ç»Ÿã€‚ä»æ™ºèƒ½ä¼˜åŒ–å™¨åˆ°å®æ—¶ç›‘æ§ï¼Œä»å¼‚å¸¸æ£€æµ‹åˆ°æ—©åœæœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ç°åœ¨æ‹¥æœ‰äº†æ™ºèƒ½åŒ–çš„ç”Ÿäº§æ§åˆ¶ä¸­å¿ƒã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ¨¡å‹è¯„ä¼°ä¸åˆ†ææŠ€æœ¯ï¼Œç¡®ä¿æˆ‘ä»¬ç”Ÿäº§å‡ºçš„å®šåˆ¶åŒ–æ¨¡å‹è¾¾åˆ°æœ€é«˜è´¨é‡æ ‡å‡†ã€‚

---

## 28.5 æ¨¡å‹è¯„ä¼°ä¸åˆ†æ

### ğŸ§ª è¿›å…¥æ¨¡å‹è¯„ä¼°è½¦é—´

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œæ¨¡å‹è¯„ä¼°å°±åƒæ˜¯äº§å“çš„è´¨é‡æ£€æµ‹ä¸­å¿ƒã€‚å®ƒä¸ä»…è¦ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œè¿˜è¦è¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

è®©æˆ‘ä»¬æ·±å…¥è¿™ä¸ªè´¨é‡æ£€æµ‹ä¸­å¿ƒï¼Œå­¦ä¹ å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼

#### ğŸ“Š å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°

å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸€ç§å¸¸ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

class MultiMetricEvaluator:
    """å¤šæŒ‡æ ‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def evaluate(self, y_true, y_pred):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.metrics['accuracy'] = accuracy_score(y_true, y_pred)
        self.metrics['precision'], self.metrics['recall'], self.metrics['f1'], _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    
    def visualize_metrics(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        fig, ax = plt.subplots()
        ax.bar(['Accuracy', 'Precision', 'Recall', 'F1 Score'], list(self.metrics.values()))
        ax.set_ylim(0, 1)
        ax.set_title('æ¨¡å‹æ€§èƒ½è¯„ä¼°')
        plt.show()

# æ¼”ç¤ºå¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°
evaluator = MultiMetricEvaluator()

# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
y_true = [0, 1, 0, 1, 1, 0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1, 0, 1, 1, 1, 1]

# è¯„ä¼°æ¨¡å‹
evaluator.evaluate(y_true, y_pred)

# å¯è§†åŒ–è¯„ä¼°ç»“æœ
evaluator.visualize_metrics()
```

#### ğŸ” é”™è¯¯æ¡ˆä¾‹åˆ†æ

é”™è¯¯æ¡ˆä¾‹åˆ†ææ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸€ç§é‡è¦æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ†ææ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„é”™è¯¯æ¡ˆä¾‹ï¼Œæ¥äº†è§£æ¨¡å‹åœ¨å“ªäº›æƒ…å†µä¸‹å®¹æ˜“å‡ºé”™ï¼Œä»è€Œæœ‰é’ˆå¯¹æ€§åœ°è¿›è¡Œæ”¹è¿›ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import seaborn as sns

class ErrorAnalyzer:
    """é”™è¯¯æ¡ˆä¾‹åˆ†æå™¨"""
    
    def __init__(self):
        self.confusion_matrix = None
        self.class_names = None
    
    def analyze(self, y_true, y_pred):
        """åˆ†æé”™è¯¯æ¡ˆä¾‹"""
        self.confusion_matrix = ConfusionMatrixDisplay.from_predictions(y_true, y_pred)
        self.class_names = self.confusion_matrix.display_labels
    
    def visualize_confusion_matrix(self):
        """å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(self.confusion_matrix.confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)
        ax.set_title('æ··æ·†çŸ©é˜µ')
        ax.set_xlabel('é¢„æµ‹ç±»åˆ«')
        ax.set_ylabel('çœŸå®ç±»åˆ«')
        plt.show()

# æ¼”ç¤ºé”™è¯¯æ¡ˆä¾‹åˆ†æ
analyzer = ErrorAnalyzer()

# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
y_true = [0, 1, 0, 1, 1, 0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1, 0, 1, 1, 1, 1]

# åˆ†æé”™è¯¯æ¡ˆä¾‹
analyzer.analyze(y_true, y_pred)

# å¯è§†åŒ–é”™è¯¯æ¡ˆä¾‹åˆ†æç»“æœ
analyzer.visualize_confusion_matrix()
```

#### ğŸ› ï¸ æ¨¡å‹é²æ£’æ€§æµ‹è¯•

æ¨¡å‹é²æ£’æ€§æµ‹è¯•æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸€ç§é‡è¦æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸Šæ·»åŠ å™ªå£°æˆ–å¹²æ‰°ï¼Œæ¥æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒæƒ…å†µä¸‹çš„è¡¨ç°ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

class RobustnessTester:
    """æ¨¡å‹é²æ£’æ€§æµ‹è¯•å™¨"""
    
    def __init__(self, model, data_loader):
        self.model = model
        self.data_loader = data_loader
    
    def test(self):
        """æµ‹è¯•æ¨¡å‹é²æ£’æ€§"""
        self.model.eval()
        total_samples = 0
        correct_predictions = 0
        
        with torch.no_grad():
            for batch in self.data_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.model.device)
                attention_mask = batch['attention_mask'].to(self.model.device)
                labels = batch['label'].to(self.model.device)
                
                # æ·»åŠ å™ªå£°
                noise = torch.randn_like(input_ids) * 0.1
                noisy_input_ids = input_ids + noise
                
                # å‰å‘ä¼ æ’­
                logits = self.model(noisy_input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                
                # ç»Ÿè®¡
                total_samples += labels.size(0)
                correct_predictions += (torch.argmax(logits, dim=1) == labels).sum().item()
        
        return accuracy_score(labels.cpu().numpy(), torch.argmax(logits, dim=1).cpu().numpy())

# æ¼”ç¤ºæ¨¡å‹é²æ£’æ€§æµ‹è¯•
print("ğŸ› ï¸ æ¨¡å‹é²æ£’æ€§æµ‹è¯•")
print("=" * 50)

# åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œæ•°æ®
class SimpleModel(nn.Module):
    def __init__(self, vocab_size=1000, hidden_size=128, num_classes=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, input_ids, attention_mask):
        embeddings = self.embedding(input_ids)
        pooled = embeddings.mean(dim=1)  # ç®€å•å¹³å‡æ± åŒ–
        return self.classifier(pooled)

# åˆ›å»ºæ¨¡å‹
model = SimpleModel()

# åˆ›å»ºè®­ç»ƒé…ç½®
config = TrainingConfig(
    learning_rate=1e-3,
    batch_size=16,
    num_epochs=5,
    early_stopping_patience=2,
    log_interval=5
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = AdvancedTrainer(model, config)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
data_loader = SmartDataLoader(tokenizer, max_length=128)

# åˆ›å»ºæ•°æ®é›†
dataset = data_loader.create_dataset(augmented_texts, augmented_labels)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader, test_loader = data_loader.create_dataloaders(
    dataset, train_ratio=0.7, val_ratio=0.15, batch_size=16
)

# æµ‹è¯•æ¨¡å‹é²æ£’æ€§
robustness_tester = RobustnessTester(model, test_loader)
robustness_score = robustness_tester.test()
print(f"æ¨¡å‹é²æ£’æ€§å¾—åˆ†: {robustness_score:.2%}")
```

#### ğŸ”„ æ•ˆç‡æ€§èƒ½æµ‹è¯•

æ•ˆç‡æ€§èƒ½æµ‹è¯•æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸€ç§é‡è¦æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ç›¸åŒè®¡ç®—èµ„æºä¸‹æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œæ¥è¯„ä¼°æ¨¡å‹çš„æ•ˆç‡ã€‚

```python
import time
from datetime import datetime
import threading
from queue import Queue
import json

class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, model, data_loader):
        self.model = model
        self.data_loader = data_loader
    
    def test(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        total_samples = 0
        total_time = 0
        
        with torch.no_grad():
            for batch in self.data_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.model.device)
                attention_mask = batch['attention_mask'].to(self.model.device)
                labels = batch['label'].to(self.model.device)
                
                start_time = time.time()
                logits = self.model(input_ids, attention_mask)
                end_time = time.time()
                
                # ç»Ÿè®¡
                total_samples += labels.size(0)
                total_time += end_time - start_time
        
        return total_time / total_samples

# æ¼”ç¤ºæ•ˆç‡æ€§èƒ½æµ‹è¯•
print("ğŸ”„ æ•ˆç‡æ€§èƒ½æµ‹è¯•")
print("=" * 50)

# åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œæ•°æ®
class SimpleModel(nn.Module):
    def __init__(self, vocab_size=1000, hidden_size=128, num_classes=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, input_ids, attention_mask):
        embeddings = self.embedding(input_ids)
        pooled = embeddings.mean(dim=1)  # ç®€å•å¹³å‡æ± åŒ–
        return self.classifier(pooled)

# åˆ›å»ºæ¨¡å‹
model = SimpleModel()

# åˆ›å»ºè®­ç»ƒé…ç½®
config = TrainingConfig(
    learning_rate=1e-3,
    batch_size=16,
    num_epochs=5,
    early_stopping_patience=2,
    log_interval=5
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = AdvancedTrainer(model, config)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
data_loader = SmartDataLoader(tokenizer, max_length=128)

# åˆ›å»ºæ•°æ®é›†
dataset = data_loader.create_dataset(augmented_texts, augmented_labels)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader, test_loader = data_loader.create_dataloaders(
    dataset, train_ratio=0.7, val_ratio=0.15, batch_size=16
)

# æµ‹è¯•æ¨¡å‹æ€§èƒ½
performance_tester = PerformanceTester(model, test_loader)
inference_time = performance_tester.test()
print(f"æ¨¡å‹æ¨ç†æ—¶é—´: {inference_time:.4f} ç§’/æ ·æœ¬")
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å»ºç«‹äº†å…¨é¢çš„æ¨¡å‹è¯„ä¼°ä¸åˆ†æä½“ç³»ã€‚ä»å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°åˆ°é”™è¯¯æ¡ˆä¾‹åˆ†æï¼Œä»æ¨¡å‹é²æ£’æ€§æµ‹è¯•åˆ°æ•ˆç‡æ€§èƒ½æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ç°åœ¨æ‹¥æœ‰äº†ä¸¥æ ¼çš„æ€§èƒ½æ£€æµ‹ä¸­å¿ƒã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å‘ä¼ä¸šçº§ä¸ªæ€§åŒ–AIåŠ©æ‰‹ï¼Œå®ç°å®Œæ•´çš„å®šåˆ¶åŒ–è§£å†³æ–¹æ¡ˆã€‚

---

## 28.6 å®šåˆ¶åŒ–AIåŠ©æ‰‹å®æˆ˜

### ğŸ¨ è¿›å…¥AIåŠ©æ‰‹å®šåˆ¶è½¦é—´

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œå®šåˆ¶åŒ–AIåŠ©æ‰‹å°±åƒæ˜¯è‰ºæœ¯å“çš„è®¾è®¡è½¦é—´ã€‚å®ƒä¸ä»…è¦ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œè¿˜è¦æ ¹æ®å®¢æˆ·éœ€æ±‚è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ã€‚

è®©æˆ‘ä»¬æ·±å…¥è¿™ä¸ªè‰ºæœ¯å“è®¾è®¡è½¦é—´ï¼Œå­¦ä¹ å¦‚ä½•æ‰“é€ ä¸€ä¸ªç¬¦åˆå®¢æˆ·éœ€æ±‚çš„AIåŠ©æ‰‹ï¼

#### ğŸ¯ å®šåˆ¶åŒ–AIåŠ©æ‰‹å¼€å‘æµç¨‹

å®šåˆ¶åŒ–AIåŠ©æ‰‹å¼€å‘æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **éœ€æ±‚åˆ†æ**ï¼šä¸å®¢æˆ·æ²Ÿé€šï¼Œäº†è§£ä»–ä»¬çš„å…·ä½“éœ€æ±‚ã€‚
2. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®å®¢æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
3. **å¾®è°ƒç­–ç•¥é€‰æ‹©**ï¼šæ ¹æ®å®¢æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥ã€‚
4. **æ•°æ®å·¥ç¨‹ä¸é¢„å¤„ç†**ï¼šæ„å»ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå»ºç«‹å®Œå–„çš„æ•°æ®å¤„ç†æµç¨‹ã€‚
5. **è®­ç»ƒä¼˜åŒ–ä¸ç›‘æ§**ï¼šç²¾ç»†åŒ–ç®¡ç†è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°é«˜æ•ˆçš„æ¨¡å‹ä¼˜åŒ–ã€‚
6. **æ¨¡å‹è¯„ä¼°ä¸åˆ†æ**ï¼šå»ºç«‹å¤šç»´åº¦çš„æ•ˆæœè¯„ä¼°ä½“ç³»ï¼Œç§‘å­¦éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚
7. **å®šåˆ¶åŒ–æ¨¡å‹å¼€å‘**ï¼šæ ¹æ®å®¢æˆ·éœ€æ±‚è¿›è¡Œæ¨¡å‹å®šåˆ¶ã€‚
8. **éƒ¨ç½²ä¸ç»´æŠ¤**ï¼šæ„å»ºè‡ªåŠ¨åŒ–å¾®è°ƒå¹³å°ï¼Œç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆè¿è¡Œå’ŒæŒç»­ä¼˜åŒ–ã€‚

è®©æˆ‘ä»¬ç”¨ä»£ç æ¥å®ç°è¿™ä¸ªå®šåˆ¶åŒ–AIåŠ©æ‰‹å¼€å‘æµç¨‹ï¼š

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import numpy as np
import matplotlib.pyplot as plt

class FineTuningDemo:
    """æ¨¡å‹å¾®è°ƒæ¼”ç¤ºç±»"""
    
    def __init__(self, model_name="bert-base-uncased"):
        """
        åˆå§‹åŒ–å¾®è°ƒæ¼”ç¤º
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.base_model = AutoModel.from_pretrained(model_name)
        
        # å†»ç»“é¢„è®­ç»ƒå±‚çš„å‚æ•°ï¼ˆå¯é€‰ï¼‰
        self.freeze_base_model()
        
        print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {self.count_parameters():,}")
    
    def freeze_base_model(self):
        """å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = False
        print("ğŸ”’ å·²å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°")
    
    def unfreeze_base_model(self):
        """è§£å†»é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°"""
        for param in self.base_model.parameters():
            param.requires_grad = True
        print("ğŸ”“ å·²è§£å†»é¢„è®­ç»ƒæ¨¡å‹å‚æ•°")
    
    def count_parameters(self):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.base_model.parameters())
    
    def create_classification_head(self, num_classes=2):
        """
        åˆ›å»ºåˆ†ç±»å¤´
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
        """
        hidden_size = self.base_model.config.hidden_size
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
        print(f"ğŸ¯ å·²åˆ›å»ºåˆ†ç±»å¤´ï¼Œè¾“å‡ºç»´åº¦: {num_classes}")
        return self.classifier
    
    def demonstrate_feature_extraction(self, texts):
        """
        æ¼”ç¤ºç‰¹å¾æå–è¿‡ç¨‹
        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
        """
        print("\nğŸ” ç‰¹å¾æå–æ¼”ç¤º:")
        
        for i, text in enumerate(texts):
            # ç¼–ç æ–‡æœ¬
            inputs = self.tokenizer(text, return_tensors="pt", 
                                  padding=True, truncation=True)
            
            # æå–ç‰¹å¾
            with torch.no_grad():
                outputs = self.base_model(**inputs)
                features = outputs.last_hidden_state.mean(dim=1)  # å¹³å‡æ± åŒ–
            
            print(f"æ–‡æœ¬ {i+1}: {text}")
            print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
            print(f"ç‰¹å¾èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")
            print("-" * 50)

# æ¼”ç¤ºå¾®è°ƒåŸºç¡€æ¦‚å¿µ
demo = FineTuningDemo()

# åˆ›å»ºåˆ†ç±»å¤´
classifier = demo.create_classification_head(num_classes=3)

# æ¼”ç¤ºç‰¹å¾æå–
sample_texts = [
    "This movie is absolutely fantastic!",
    "The service was terrible and disappointing.",
    "It's an okay product, nothing special."
]

demo.demonstrate_feature_extraction(sample_texts)
```

#### ğŸ¯ å¾®è°ƒç­–ç•¥åˆ†ç±»ï¼šä¸åŒçš„å®šåˆ¶æ–¹æ¡ˆ

åœ¨æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚ä¸­ï¼Œæœ‰å¤šç§ä¸åŒçš„å®šåˆ¶æ–¹æ¡ˆå¯ä¾›é€‰æ‹©ï¼š

```mermaid
graph TD
    A[æ¨¡å‹å¾®è°ƒç­–ç•¥] --> B[å…¨å‚æ•°å¾®è°ƒ]
    A --> C[å‚æ•°é«˜æ•ˆå¾®è°ƒ]
    
    B --> D[å®Œå…¨å¾®è°ƒ]
    B --> E[æ¸è¿›å¼å¾®è°ƒ]
    
    C --> F[LoRA]
    C --> G[Adapter]
    C --> H[Prefix Tuning]
    C --> I[Prompt Tuning]
    
    style A fill:#ff9800
    style B fill:#2196f3
    style C fill:#4caf50
```

è®©æˆ‘ä»¬è¯¦ç»†äº†è§£æ¯ç§ç­–ç•¥ï¼š

```python
class FineTuningStrategy:
    """å¾®è°ƒç­–ç•¥åˆ†æç±»"""
    
    def __init__(self):
        self.strategies = {
            "full_finetuning": {
                "name": "å…¨å‚æ•°å¾®è°ƒ",
                "description": "æ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°",
                "advantages": ["æ•ˆæœæœ€å¥½", "é€‚åº”æ€§å¼º"],
                "disadvantages": ["è®¡ç®—æˆæœ¬é«˜", "å®¹æ˜“è¿‡æ‹Ÿåˆ", "å­˜å‚¨éœ€æ±‚å¤§"],
                "suitable_for": ["æ•°æ®å……è¶³", "è®¡ç®—èµ„æºä¸°å¯Œ", "è¿½æ±‚æœ€ä½³æ•ˆæœ"]
            },
            "lora": {
                "name": "LoRAå¾®è°ƒ",
                "description": "ä½ç§©é€‚åº”ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°",
                "advantages": ["å‚æ•°å°‘", "è®­ç»ƒå¿«", "é˜²æ­¢è¿‡æ‹Ÿåˆ"],
                "disadvantages": ["æ•ˆæœå¯èƒ½ç•¥é€Š", "éœ€è¦è°ƒæ•´ç§©å‚æ•°"],
                "suitable_for": ["æ•°æ®æœ‰é™", "è®¡ç®—èµ„æºå—é™", "å¿«é€Ÿéƒ¨ç½²"]
            },
            "adapter": {
                "name": "Adapterå¾®è°ƒ",
                "description": "åœ¨æ¨¡å‹ä¸­æ’å…¥å°å‹é€‚é…å™¨å±‚",
                "advantages": ["æ¨¡å—åŒ–", "å¯æ’æ‹”", "å‚æ•°æ•ˆç‡é«˜"],
                "disadvantages": ["å¢åŠ æ¨ç†å»¶è¿Ÿ", "æ¶æ„å¤æ‚"],
                "suitable_for": ["å¤šä»»åŠ¡åœºæ™¯", "æ¨¡å‹å…±äº«", "å¢é‡å­¦ä¹ "]
            },
            "prefix_tuning": {
                "name": "å‰ç¼€å¾®è°ƒ",
                "description": "åªè®­ç»ƒè¾“å…¥å‰ç¼€çš„åµŒå…¥",
                "advantages": ["å‚æ•°æå°‘", "ä¸æ”¹å˜æ¨¡å‹ç»“æ„"],
                "disadvantages": ["æ•ˆæœæœ‰é™", "é€‚ç”¨åœºæ™¯çª„"],
                "suitable_for": ["ç”Ÿæˆä»»åŠ¡", "å¿«é€Ÿé€‚é…", "è½»é‡åŒ–éƒ¨ç½²"]
            }
        }
    
    def compare_strategies(self):
        """æ¯”è¾ƒä¸åŒå¾®è°ƒç­–ç•¥"""
        print("ğŸ“Š å¾®è°ƒç­–ç•¥å¯¹æ¯”åˆ†æ:")
        print("=" * 80)
        
        for key, strategy in self.strategies.items():
            print(f"\nğŸ¯ {strategy['name']}")
            print(f"æè¿°: {strategy['description']}")
            print(f"âœ… ä¼˜åŠ¿: {', '.join(strategy['advantages'])}")
            print(f"âŒ åŠ£åŠ¿: {', '.join(strategy['disadvantages'])}")
            print(f"ğŸ¯ é€‚ç”¨åœºæ™¯: {', '.join(strategy['suitable_for'])}")
            print("-" * 60)
    
    def estimate_resources(self, model_size_mb, strategy="full_finetuning"):
        """
        ä¼°ç®—èµ„æºéœ€æ±‚
        Args:
            model_size_mb: æ¨¡å‹å¤§å°(MB)
            strategy: å¾®è°ƒç­–ç•¥
        """
        multipliers = {
            "full_finetuning": {"memory": 4.0, "time": 1.0, "storage": 2.0},
            "lora": {"memory": 1.2, "time": 0.3, "storage": 1.1},
            "adapter": {"memory": 1.5, "time": 0.4, "storage": 1.2},
            "prefix_tuning": {"memory": 1.1, "time": 0.2, "storage": 1.05}
        }
        
        if strategy not in multipliers:
            print(f"âŒ ä¸æ”¯æŒçš„ç­–ç•¥: {strategy}")
            return
        
        mult = multipliers[strategy]
        
        print(f"\nğŸ“Š {self.strategies[strategy]['name']} èµ„æºéœ€æ±‚ä¼°ç®—:")
        print(f"åŸºç¡€æ¨¡å‹å¤§å°: {model_size_mb} MB")
        print(f"è®­ç»ƒå†…å­˜éœ€æ±‚: {model_size_mb * mult['memory']:.1f} MB")
        print(f"ç›¸å¯¹è®­ç»ƒæ—¶é—´: {mult['time']:.1f}x")
        print(f"å­˜å‚¨éœ€æ±‚: {model_size_mb * mult['storage']:.1f} MB")

# æ¼”ç¤ºç­–ç•¥åˆ†æ
strategy_analyzer = FineTuningStrategy()
strategy_analyzer.compare_strategies()

# èµ„æºéœ€æ±‚ä¼°ç®—
strategy_analyzer.estimate_resources(440, "full_finetuning")  # BERT-base
strategy_analyzer.estimate_resources(440, "lora")
```

#### ğŸ”„ è¿ç§»å­¦ä¹ ç†è®ºï¼šçŸ¥è¯†ä¼ æ‰¿çš„è‰ºæœ¯

å¾®è°ƒçš„æ ¸å¿ƒæ˜¯è¿ç§»å­¦ä¹ ï¼Œå°±åƒæ˜¯å°†ä¸€ä¸ªé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†è½¬ç§»åˆ°å¦ä¸€ä¸ªé¢†åŸŸã€‚è®©æˆ‘ä»¬æ·±å…¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import seaborn as sns

class TransferLearningAnalyzer:
    """è¿ç§»å­¦ä¹ åˆ†æå™¨"""
    
    def __init__(self):
        self.layer_types = [
            "è¯åµŒå…¥å±‚", "æµ…å±‚ç¼–ç å™¨", "ä¸­å±‚ç¼–ç å™¨", 
            "æ·±å±‚ç¼–ç å™¨", "ä»»åŠ¡ç‰¹å®šå±‚"
        ]
    
    def visualize_knowledge_transfer(self):
        """å¯è§†åŒ–çŸ¥è¯†è¿ç§»è¿‡ç¨‹"""
        # æ¨¡æ‹Ÿä¸åŒå±‚çš„çŸ¥è¯†é€šç”¨æ€§
        universality = [0.95, 0.8, 0.6, 0.4, 0.1]
        task_specificity = [0.05, 0.2, 0.4, 0.6, 0.9]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # çŸ¥è¯†é€šç”¨æ€§å›¾
        ax1.barh(self.layer_types, universality, color='skyblue', alpha=0.7)
        ax1.set_xlabel('é€šç”¨æ€§ç¨‹åº¦')
        ax1.set_title('ä¸åŒå±‚çš„çŸ¥è¯†é€šç”¨æ€§')
        ax1.set_xlim(0, 1)
        
        # ä»»åŠ¡ç‰¹å¼‚æ€§å›¾
        ax2.barh(self.layer_types, task_specificity, color='lightcoral', alpha=0.7)
        ax2.set_xlabel('ä»»åŠ¡ç‰¹å¼‚æ€§ç¨‹åº¦')
        ax2.set_title('ä¸åŒå±‚çš„ä»»åŠ¡ç‰¹å¼‚æ€§')
        ax2.set_xlim(0, 1)
        
        plt.tight_layout()
        plt.show()
        
        print("ğŸ“Š çŸ¥è¯†è¿ç§»è§„å¾‹:")
        for i, layer in enumerate(self.layer_types):
            print(f"{layer}: é€šç”¨æ€§{universality[i]:.1%}, ç‰¹å¼‚æ€§{task_specificity[i]:.1%}")
    
    def demonstrate_feature_evolution(self):
        """æ¼”ç¤ºç‰¹å¾æ¼”åŒ–è¿‡ç¨‹"""
        # æ¨¡æ‹Ÿé¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­ç‰¹å¾çš„å˜åŒ–
        np.random.seed(42)
        
        # é¢„è®­ç»ƒç‰¹å¾ (é€šç”¨)
        pretrain_features = np.random.normal(0, 1, (100, 2))
        
        # å¾®è°ƒåç‰¹å¾ (ä»»åŠ¡ç‰¹å®š)
        finetune_features = pretrain_features + np.random.normal(0, 0.3, (100, 2))
        finetune_features[:50] += [1.5, 1.5]  # ç±»åˆ«1
        finetune_features[50:] += [-1.5, -1.5]  # ç±»åˆ«2
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒ
        ax1.scatter(pretrain_features[:, 0], pretrain_features[:, 1], 
                   alpha=0.6, s=50, c='gray')
        ax1.set_title('é¢„è®­ç»ƒç‰¹å¾åˆ†å¸ƒ\n(é€šç”¨è¡¨ç¤º)')
        ax1.set_xlabel('ç‰¹å¾ç»´åº¦1')
        ax1.set_ylabel('ç‰¹å¾ç»´åº¦2')
        ax1.grid(True, alpha=0.3)
        
        # å¾®è°ƒåç‰¹å¾åˆ†å¸ƒ
        colors = ['red'] * 50 + ['blue'] * 50
        ax2.scatter(finetune_features[:, 0], finetune_features[:, 1], 
                   alpha=0.6, s=50, c=colors)
        ax2.set_title('å¾®è°ƒåç‰¹å¾åˆ†å¸ƒ\n(ä»»åŠ¡ç‰¹å®š)')
        ax2.set_xlabel('ç‰¹å¾ç»´åº¦1')
        ax2.set_ylabel('ç‰¹å¾ç»´åº¦2')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print("ğŸ¯ ç‰¹å¾æ¼”åŒ–åˆ†æ:")
        print("â€¢ é¢„è®­ç»ƒé˜¶æ®µ: å­¦ä¹ é€šç”¨çš„è¯­è¨€è¡¨ç¤º")
        print("â€¢ å¾®è°ƒé˜¶æ®µ: é€‚åº”ç‰¹å®šä»»åŠ¡éœ€æ±‚")
        print("â€¢ ç»“æœ: ä¿æŒé€šç”¨æ€§çš„åŒæ—¶è·å¾—ä»»åŠ¡ç‰¹å¼‚æ€§")

# æ¼”ç¤ºè¿ç§»å­¦ä¹ åˆ†æ
transfer_analyzer = TransferLearningAnalyzer()
transfer_analyzer.visualize_knowledge_transfer()
transfer_analyzer.demonstrate_feature_evolution()
```

#### ğŸ› ï¸ å¾®è°ƒæµç¨‹è®¾è®¡ï¼šä»æ•°æ®åˆ°éƒ¨ç½²çš„å®Œæ•´pipeline

ç°åœ¨è®©æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„å¾®è°ƒæµç¨‹ï¼Œå°±åƒåœ¨å·¥å‚ä¸­å»ºç«‹æ ‡å‡†åŒ–çš„ç”Ÿäº§çº¿ï¼š

```python
class FineTuningPipeline:
    """å®Œæ•´çš„å¾®è°ƒæµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, model_name, task_type="classification"):
        """
        åˆå§‹åŒ–å¾®è°ƒæµç¨‹
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            task_type: ä»»åŠ¡ç±»å‹ (classification, regression, generation)
        """
        self.model_name = model_name
        self.task_type = task_type
        self.pipeline_stages = [
            "æ•°æ®å‡†å¤‡", "æ¨¡å‹åŠ è½½", "é…ç½®ä¼˜åŒ–å™¨", 
            "è®­ç»ƒç›‘æ§", "æ¨¡å‹è¯„ä¼°", "æ¨¡å‹ä¿å­˜", "éƒ¨ç½²å‡†å¤‡"
        ]
        
        print(f"ğŸ­ åˆå§‹åŒ–å¾®è°ƒæµæ°´çº¿: {model_name}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
    
    def visualize_pipeline(self):
        """å¯è§†åŒ–å¾®è°ƒæµç¨‹"""
        print("\nğŸ”„ å¾®è°ƒæµç¨‹å›¾:")
        print("=" * 60)
        
        # åˆ›å»ºæµç¨‹å›¾
        flow_chart = """
        ```mermaid
        graph TD
            A[åŸå§‹æ•°æ®] --> B[æ•°æ®æ¸…æ´—]
            B --> C[æ•°æ®æ ‡æ³¨]
            C --> D[æ•°æ®åˆ’åˆ†]
            D --> E[åŠ è½½é¢„è®­ç»ƒæ¨¡å‹]
            E --> F[æ·»åŠ ä»»åŠ¡å¤´]
            F --> G[é…ç½®è®­ç»ƒå‚æ•°]
            G --> H[å¼€å§‹è®­ç»ƒ]
            H --> I[å®æ—¶ç›‘æ§]
            I --> J{æ˜¯å¦æ”¶æ•›?}
            J -->|å¦| H
            J -->|æ˜¯| K[æ¨¡å‹è¯„ä¼°]
            K --> L[æ€§èƒ½ä¼˜åŒ–]
            L --> M[æ¨¡å‹ä¿å­˜]
            M --> N[éƒ¨ç½²å‡†å¤‡]
            
            style A fill:#e1f5fe
            style N fill:#e8f5e8
            style J fill:#fff3e0
        ```
        """
        print(flow_chart)
    
    def estimate_pipeline_time(self, data_size, model_size="base"):
        """
        ä¼°ç®—æµç¨‹æ—¶é—´
        Args:
            data_size: æ•°æ®é›†å¤§å°
            model_size: æ¨¡å‹è§„æ¨¡ (base, large, xl)
        """
        size_multipliers = {"base": 1.0, "large": 2.5, "xl": 6.0}
        
        base_times = {
            "æ•°æ®å‡†å¤‡": max(0.5, data_size / 10000),  # å°æ—¶
            "æ¨¡å‹åŠ è½½": 0.1 * size_multipliers[model_size],
            "è®­ç»ƒè¿‡ç¨‹": max(1.0, data_size / 1000) * size_multipliers[model_size],
            "æ¨¡å‹è¯„ä¼°": 0.2 * size_multipliers[model_size],
            "éƒ¨ç½²å‡†å¤‡": 0.3
        }
        
        print(f"\nâ±ï¸ æµç¨‹æ—¶é—´ä¼°ç®— (æ•°æ®é‡: {data_size}, æ¨¡å‹: {model_size}):")
        print("-" * 50)
        
        total_time = 0
        for stage, time_hours in base_times.items():
            print(f"{stage}: {time_hours:.1f} å°æ—¶")
            total_time += time_hours
        
        print(f"æ€»è®¡æ—¶é—´: {total_time:.1f} å°æ—¶")
        
        return total_time
    
    def create_checklist(self):
        """åˆ›å»ºå¾®è°ƒæ£€æŸ¥æ¸…å•"""
        checklist = {
            "æ•°æ®å‡†å¤‡": [
                "æ•°æ®è´¨é‡æ£€æŸ¥",
                "æ ‡æ³¨ä¸€è‡´æ€§éªŒè¯", 
                "æ•°æ®å¹³è¡¡æ€§åˆ†æ",
                "è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†"
            ],
            "æ¨¡å‹é…ç½®": [
                "é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹",
                "è®¾è®¡ä»»åŠ¡ç‰¹å®šå±‚",
                "é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡",
                "è®¾ç½®æ­£åˆ™åŒ–å‚æ•°"
            ],
            "è®­ç»ƒç›‘æ§": [
                "æŸå¤±å‡½æ•°ç›‘æ§",
                "éªŒè¯é›†æ€§èƒ½è·Ÿè¸ª",
                "è¿‡æ‹Ÿåˆæ£€æµ‹",
                "æ—©åœæœºåˆ¶è®¾ç½®"
            ],
            "æ¨¡å‹è¯„ä¼°": [
                "å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°",
                "é”™è¯¯æ¡ˆä¾‹åˆ†æ",
                "æ¨¡å‹é²æ£’æ€§æµ‹è¯•",
                "æ•ˆç‡æ€§èƒ½æµ‹è¯•"
            ],
            "éƒ¨ç½²å‡†å¤‡": [
                "æ¨¡å‹å‹ç¼©ä¼˜åŒ–",
                "æ¨ç†é€Ÿåº¦æµ‹è¯•",
                "å†…å­˜ä½¿ç”¨è¯„ä¼°",
                "å…¼å®¹æ€§æ£€æŸ¥"
            ]
        }
        
        print("\nğŸ“‹ å¾®è°ƒæ£€æŸ¥æ¸…å•:")
        print("=" * 50)
        
        for category, items in checklist.items():
            print(f"\nğŸ¯ {category}:")
            for item in items:
                print(f"  â˜ {item}")
        
        return checklist

# æ¼”ç¤ºå¾®è°ƒæµç¨‹
pipeline = FineTuningPipeline("bert-base-chinese", "classification")
pipeline.visualize_pipeline()
pipeline.estimate_pipeline_time(10000, "base")
pipeline.create_checklist()
```

### ğŸ¯ å¾®è°ƒç­–ç•¥é€‰æ‹©æŒ‡å—

é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥å°±åƒé€‰æ‹©åˆé€‚çš„ç”Ÿäº§çº¿é…ç½®ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ ï¼š

```python
class StrategySelector:
    """å¾®è°ƒç­–ç•¥é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.decision_tree = {
            "data_size": {
                "small": "å‚æ•°é«˜æ•ˆå¾®è°ƒ",
                "medium": "éƒ¨åˆ†å¾®è°ƒ",
                "large": "å…¨å‚æ•°å¾®è°ƒ"
            },
            "compute_budget": {
                "low": "LoRAæˆ–Adapter",
                "medium": "éƒ¨åˆ†å±‚å¾®è°ƒ",
                "high": "å…¨å‚æ•°å¾®è°ƒ"
            },
            "task_similarity": {
                "high": "è½»é‡å¾®è°ƒ",
                "medium": "æ ‡å‡†å¾®è°ƒ", 
                "low": "æ·±åº¦å¾®è°ƒ"
            }
        }
    
    def recommend_strategy(self, data_size, compute_budget, task_similarity):
        """
        æ¨èå¾®è°ƒç­–ç•¥
        Args:
            data_size: æ•°æ®è§„æ¨¡ (small/medium/large)
            compute_budget: è®¡ç®—é¢„ç®— (low/medium/high)
            task_similarity: ä¸é¢„è®­ç»ƒä»»åŠ¡ç›¸ä¼¼åº¦ (low/medium/high)
        """
        print("ğŸ¤– æ™ºèƒ½ç­–ç•¥æ¨èç³»ç»Ÿ")
        print("=" * 40)
        print(f"æ•°æ®è§„æ¨¡: {data_size}")
        print(f"è®¡ç®—é¢„ç®—: {compute_budget}")
        print(f"ä»»åŠ¡ç›¸ä¼¼åº¦: {task_similarity}")
        print("-" * 40)
        
        # åŸºäºè§„åˆ™çš„æ¨è
        if data_size == "small" or compute_budget == "low":
            if task_similarity == "high":
                recommendation = "Prompt Tuning"
                confidence = 0.9
            else:
                recommendation = "LoRA"
                confidence = 0.85
        elif data_size == "large" and compute_budget == "high":
            recommendation = "Full Fine-tuning"
            confidence = 0.95
        else:
            recommendation = "Adapter"
            confidence = 0.8
        
        print(f"ğŸ¯ æ¨èç­–ç•¥: {recommendation}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.1%}")
        
        # æä¾›æ›¿ä»£æ–¹æ¡ˆ
        alternatives = self._get_alternatives(recommendation)
        print(f"ğŸ”„ å¤‡é€‰æ–¹æ¡ˆ: {', '.join(alternatives)}")
        
        return recommendation, confidence
    
    def _get_alternatives(self, primary):
        """è·å–æ›¿ä»£æ–¹æ¡ˆ"""
        alternatives_map = {
            "Full Fine-tuning": ["LoRA", "Adapter"],
            "LoRA": ["QLoRA", "Adapter"],
            "Adapter": ["LoRA", "Prefix Tuning"],
            "Prompt Tuning": ["P-tuning v2", "LoRA"]
        }
        return alternatives_map.get(primary, ["LoRA", "Adapter"])

# æ¼”ç¤ºç­–ç•¥é€‰æ‹©
selector = StrategySelector()

# ä¸åŒåœºæ™¯çš„ç­–ç•¥æ¨è
scenarios = [
    ("small", "low", "high"),      # å°æ•°æ®ï¼Œä½é¢„ç®—ï¼Œé«˜ç›¸ä¼¼åº¦
    ("large", "high", "low"),      # å¤§æ•°æ®ï¼Œé«˜é¢„ç®—ï¼Œä½ç›¸ä¼¼åº¦
    ("medium", "medium", "medium") # ä¸­ç­‰åœºæ™¯
]

for i, (data, budget, similarity) in enumerate(scenarios, 1):
    print(f"\nğŸ“‹ åœºæ™¯ {i}:")
    selector.recommend_strategy(data, budget, similarity)
```

é€šè¿‡è¿™ä¸€èŠ‚çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥ç†è§£äº†æ¨¡å‹å¾®è°ƒçš„åŸºç¡€ç†è®ºã€‚å¾®è°ƒä¸ä»…ä»…æ˜¯ç®€å•çš„å‚æ•°æ›´æ–°ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¶‰åŠçŸ¥è¯†è¿ç§»ã€ç­–ç•¥é€‰æ‹©å’Œæµç¨‹ç®¡ç†çš„å¤æ‚è¿‡ç¨‹ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯LoRAã€Adapterç­‰å‰æ²¿æ–¹æ³•ï¼Œè®©æˆ‘ä»¬çš„æ¨¡å‹å®šåˆ¶å·¥å‚èƒ½å¤Ÿæä¾›æ›´åŠ é«˜æ•ˆå’Œç»æµçš„å®šåˆ¶æ–¹æ¡ˆã€‚

---

## 28.7 æœ¬ç« æ€»ç»“ä¸å±•æœ›

### ğŸ‰ æ¨¡å‹å®šåˆ¶å·¥å‚çš„å®Œç¾æ”¶å®˜

æ­å–œä½ ï¼ç»è¿‡æœ¬ç« çš„æ·±å…¥å­¦ä¹ ï¼Œä½ å·²ç»æˆåŠŸæŒæ¡äº†æ¨¡å‹å¾®è°ƒä¸å®šåˆ¶åŒ–å¼€å‘çš„æ ¸å¿ƒæŠ€æœ¯ã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åœ¨è¿™ä¸ª"æ¨¡å‹å®šåˆ¶å·¥å‚"ä¸­å­¦åˆ°çš„å®è´µæŠ€èƒ½ã€‚

#### ğŸ“Š å­¦ä¹ æˆæœæ€»è§ˆ

```mermaid
mindmap
  root((ç¬¬28ç« æˆæœ))
    å¾®è°ƒç†è®ºåŸºç¡€
      å¾®è°ƒvsé¢„è®­ç»ƒ
      è¿ç§»å­¦ä¹ åŸç†
      ç­–ç•¥é€‰æ‹©æŒ‡å—
      æµç¨‹è®¾è®¡æ–¹æ³•
    å‚æ•°é«˜æ•ˆå¾®è°ƒ
      LoRAæŠ€æœ¯
      Adapteræ–¹æ³•
      Prefix Tuning
      æŠ€æœ¯å¯¹æ¯”åˆ†æ
    æ•°æ®å·¥ç¨‹ä½“ç³»
      è´¨é‡åˆ†æå™¨
      æ™ºèƒ½é¢„å¤„ç†
      æ•°æ®ç›‘æ§ç³»ç»Ÿ
      æ‰¹å¤„ç†ä¼˜åŒ–
    è®­ç»ƒä¼˜åŒ–æŠ€æœ¯
      æ™ºèƒ½ç›‘æ§å™¨
      æ€§èƒ½ä¼˜åŒ–å™¨
      å¯è§†åŒ–åˆ†æ
      å¼‚å¸¸æ£€æµ‹
    è¯„ä¼°åˆ†æä½“ç³»
      å¤šæŒ‡æ ‡è¯„ä¼°
      é”™è¯¯æ¡ˆä¾‹åˆ†æ
      é²æ£’æ€§æµ‹è¯•
      æ€§èƒ½åŸºå‡†æµ‹è¯•
    å®æˆ˜é¡¹ç›®ç»éªŒ
      å®šåˆ¶åŒ–AIåŠ©æ‰‹
      ç«¯åˆ°ç«¯æµç¨‹
      ä¼ä¸šçº§åº”ç”¨
      æœ€ä½³å®è·µ
```

#### ğŸ† æ ¸å¿ƒæŠ€èƒ½æŒæ¡æƒ…å†µ

```python
class ChapterAchievements:
    """ç¬¬28ç« æˆå°±ç»Ÿè®¡å™¨"""
    
    def __init__(self):
        self.skills = {
            "ç†è®ºåŸºç¡€": {
                "å¾®è°ƒåŸç†ç†è§£": 95,
                "è¿ç§»å­¦ä¹ æŒæ¡": 90,
                "ç­–ç•¥é€‰æ‹©èƒ½åŠ›": 88,
                "æµç¨‹è®¾è®¡èƒ½åŠ›": 92
            },
            "æŠ€æœ¯å®ç°": {
                "LoRAå®ç°èƒ½åŠ›": 94,
                "Adapterå¼€å‘èƒ½åŠ›": 91,
                "æ•°æ®å·¥ç¨‹èƒ½åŠ›": 89,
                "è®­ç»ƒä¼˜åŒ–èƒ½åŠ›": 93
            },
            "å·¥ç¨‹å®è·µ": {
                "ç›‘æ§ç³»ç»Ÿæ„å»º": 87,
                "è¯„ä¼°ä½“ç³»è®¾è®¡": 90,
                "é¡¹ç›®ç®¡ç†èƒ½åŠ›": 85,
                "éƒ¨ç½²ä¼˜åŒ–èƒ½åŠ›": 88
            },
            "åˆ›æ–°åº”ç”¨": {
                "å®šåˆ¶åŒ–æ–¹æ¡ˆè®¾è®¡": 92,
                "æŠ€æœ¯é€‰å‹èƒ½åŠ›": 89,
                "æ€§èƒ½ä¼˜åŒ–æ€ç»´": 91,
                "é—®é¢˜è§£å†³èƒ½åŠ›": 94
            }
        }
        
        self.code_stats = {
            "æ€»ä»£ç è¡Œæ•°": 1800,
            "æ ¸å¿ƒç±»æ•°é‡": 18,
            "å®ç”¨å‡½æ•°æ•°": 45,
            "å®Œæ•´ç¤ºä¾‹æ•°": 22,
            "æŠ€æœ¯å›¾è¡¨æ•°": 8
        }
        
        self.technical_coverage = [
            "LoRAä½ç§©é€‚åº”æŠ€æœ¯",
            "Adapterå¯æ’æ‹”æ¨¡å—",
            "Prefix Tuningå‰ç¼€å­¦ä¹ ",
            "æ•°æ®è´¨é‡åˆ†æç³»ç»Ÿ",
            "æ™ºèƒ½è®­ç»ƒç›‘æ§å™¨",
            "å¤šç»´åº¦è¯„ä¼°ä½“ç³»",
            "å®šåˆ¶åŒ–AIåŠ©æ‰‹å¼€å‘",
            "ä¼ä¸šçº§å¾®è°ƒæµç¨‹"
        ]
    
    def display_achievements(self):
        """å±•ç¤ºå­¦ä¹ æˆå°±"""
        print("ğŸ† ç¬¬28ç« å­¦ä¹ æˆå°±æŠ¥å‘Š")
        print("=" * 50)
        
        # æŠ€èƒ½æŒæ¡æƒ…å†µ
        print("\nğŸ“Š æŠ€èƒ½æŒæ¡æƒ…å†µ:")
        for category, skills in self.skills.items():
            avg_score = sum(skills.values()) / len(skills)
            print(f"\nğŸ¯ {category} (å¹³å‡åˆ†: {avg_score:.1f})")
            for skill, score in skills.items():
                stars = "â­" * (score // 20)
                print(f"  {skill}: {score}åˆ† {stars}")
        
        # ä»£ç ç»Ÿè®¡
        print(f"\nğŸ’» ä»£ç æˆæœç»Ÿè®¡:")
        for metric, value in self.code_stats.items():
            print(f"  {metric}: {value}")
        
        # æŠ€æœ¯è¦†ç›–
        print(f"\nğŸ”§ æŠ€æœ¯è¦†ç›–èŒƒå›´:")
        for i, tech in enumerate(self.technical_coverage, 1):
            print(f"  {i}. {tech}")
        
        # ç»¼åˆè¯„åˆ†
        all_scores = []
        for skills in self.skills.values():
            all_scores.extend(skills.values())
        overall_score = sum(all_scores) / len(all_scores)
        
        print(f"\nğŸ–ï¸ ç»¼åˆæŒæ¡åº¦: {overall_score:.1f}åˆ†")
        
        if overall_score >= 90:
            level = "ä¸“å®¶çº§"
            emoji = "ğŸ¥‡"
        elif overall_score >= 80:
            level = "ç†Ÿç»ƒçº§"
            emoji = "ğŸ¥ˆ"
        else:
            level = "å…¥é—¨çº§"
            emoji = "ğŸ¥‰"
        
        print(f"ğŸ… æŠ€èƒ½ç­‰çº§: {emoji} {level}")

# å±•ç¤ºå­¦ä¹ æˆå°±
achievements = ChapterAchievements()
achievements.display_achievements()
```

#### ğŸŒŸ æ ¸å¿ƒæŠ€æœ¯çªç ´

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ åœ¨ä»¥ä¸‹å…³é”®æŠ€æœ¯é¢†åŸŸå®ç°äº†é‡è¦çªç ´ï¼š

1. **å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯**
   - æŒæ¡äº†LoRAã€Adapterã€Prefix Tuningç­‰å‰æ²¿æŠ€æœ¯
   - ç†è§£äº†ä¸åŒæŠ€æœ¯çš„é€‚ç”¨åœºæ™¯å’Œæ€§èƒ½ç‰¹ç‚¹
   - èƒ½å¤Ÿæ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©æœ€ä¼˜çš„å¾®è°ƒç­–ç•¥

2. **æ•°æ®å·¥ç¨‹ä½“ç³»**
   - å»ºç«‹äº†å®Œæ•´çš„æ•°æ®è´¨é‡åˆ†æå’Œç›‘æ§ä½“ç³»
   - æŒæ¡äº†æ™ºèƒ½æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæŠ€æœ¯
   - å…·å¤‡äº†å¤§è§„æ¨¡æ•°æ®å¤„ç†å’Œä¼˜åŒ–èƒ½åŠ›

3. **è®­ç»ƒä¼˜åŒ–æŠ€æœ¯**
   - å­¦ä¼šäº†æ„å»ºæ™ºèƒ½è®­ç»ƒç›‘æ§å’Œä¼˜åŒ–ç³»ç»Ÿ
   - æŒæ¡äº†å¤šç§ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
   - å…·å¤‡äº†å¼‚å¸¸æ£€æµ‹å’Œæ€§èƒ½è°ƒä¼˜èƒ½åŠ›

4. **è¯„ä¼°åˆ†ææ–¹æ³•**
   - å»ºç«‹äº†å¤šç»´åº¦çš„æ¨¡å‹è¯„ä¼°ä½“ç³»
   - æŒæ¡äº†é”™è¯¯åˆ†æå’Œé²æ£’æ€§æµ‹è¯•æ–¹æ³•
   - å…·å¤‡äº†æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–èƒ½åŠ›

#### ğŸ¯ æŠ€èƒ½å‘å±•è·¯å¾„å›¾

```mermaid
graph TD
    A[æ¨¡å‹å¾®è°ƒæ–°æ‰‹] --> B[ç†è®ºåŸºç¡€æŒæ¡]
    B --> C[æŠ€æœ¯å®ç°èƒ½åŠ›]
    C --> D[å·¥ç¨‹å®è·µç»éªŒ]
    D --> E[åˆ›æ–°åº”ç”¨èƒ½åŠ›]
    E --> F[æ¨¡å‹å¾®è°ƒä¸“å®¶]
    
    B --> B1[å¾®è°ƒåŸç†ç†è§£]
    B --> B2[è¿ç§»å­¦ä¹ æŒæ¡]
    B --> B3[ç­–ç•¥é€‰æ‹©èƒ½åŠ›]
    
    C --> C1[LoRAæŠ€æœ¯å®ç°]
    C --> C2[æ•°æ®å·¥ç¨‹èƒ½åŠ›]
    C --> C3[è®­ç»ƒä¼˜åŒ–æŠ€æœ¯]
    
    D --> D1[ç›‘æ§ç³»ç»Ÿæ„å»º]
    D --> D2[è¯„ä¼°ä½“ç³»è®¾è®¡]
    D --> D3[éƒ¨ç½²ä¼˜åŒ–å®è·µ]
    
    E --> E1[å®šåˆ¶åŒ–æ–¹æ¡ˆè®¾è®¡]
    E --> E2[ä¼ä¸šçº§é¡¹ç›®ç®¡ç†]
    E --> E3[æŠ€æœ¯åˆ›æ–°åº”ç”¨]
    
    style A fill:#ffebee
    style F fill:#e8f5e8
    style B fill:#e3f2fd
    style C fill:#f3e5f5
    style D fill:#fff3e0
    style E fill:#e0f2f1
```

### ğŸš€ å®é™…åº”ç”¨ä»·å€¼

æœ¬ç« å­¦åˆ°çš„æŠ€æœ¯å…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ï¼š

#### ğŸ“ˆ ä¼ä¸šçº§åº”ç”¨åœºæ™¯

```python
class ApplicationScenarios:
    """åº”ç”¨åœºæ™¯åˆ†æå™¨"""
    
    def __init__(self):
        self.scenarios = {
            "æ™ºèƒ½å®¢æœç³»ç»Ÿ": {
                "æŠ€æœ¯æ ˆ": ["BERTå¾®è°ƒ", "æ„å›¾è¯†åˆ«", "æƒ…æ„Ÿåˆ†æ"],
                "ä»·å€¼": "æå‡å®¢æˆ·æ»¡æ„åº¦ï¼Œé™ä½äººå·¥æˆæœ¬",
                "ROI": "300-500%",
                "å®æ–½å‘¨æœŸ": "2-3ä¸ªæœˆ"
            },
            "å†…å®¹å®¡æ ¸å¹³å°": {
                "æŠ€æœ¯æ ˆ": ["å¤šæ¨¡æ€å¾®è°ƒ", "æ–‡æœ¬åˆ†ç±»", "å›¾åƒè¯†åˆ«"],
                "ä»·å€¼": "è‡ªåŠ¨åŒ–å†…å®¹å®¡æ ¸ï¼Œæé«˜æ•ˆç‡",
                "ROI": "200-400%",
                "å®æ–½å‘¨æœŸ": "3-4ä¸ªæœˆ"
            },
            "æ™ºèƒ½æ¨èå¼•æ“": {
                "æŠ€æœ¯æ ˆ": ["ç”¨æˆ·ç”»åƒ", "ååŒè¿‡æ»¤", "æ·±åº¦å­¦ä¹ "],
                "ä»·å€¼": "ä¸ªæ€§åŒ–æ¨èï¼Œæå‡è½¬åŒ–ç‡",
                "ROI": "150-300%",
                "å®æ–½å‘¨æœŸ": "4-6ä¸ªæœˆ"
            },
            "æ™ºèƒ½å†™ä½œåŠ©æ‰‹": {
                "æŠ€æœ¯æ ˆ": ["GPTå¾®è°ƒ", "æ–‡æœ¬ç”Ÿæˆ", "é£æ ¼è¿ç§»"],
                "ä»·å€¼": "æå‡å†…å®¹åˆ›ä½œæ•ˆç‡å’Œè´¨é‡",
                "ROI": "250-450%",
                "å®æ–½å‘¨æœŸ": "2-4ä¸ªæœˆ"
            }
        }
    
    def analyze_business_value(self):
        """åˆ†æå•†ä¸šä»·å€¼"""
        print("ğŸ’¼ ä¼ä¸šçº§åº”ç”¨åœºæ™¯åˆ†æ")
        print("=" * 50)
        
        total_scenarios = len(self.scenarios)
        avg_roi = 0
        avg_cycle = 0
        
        for scenario, details in self.scenarios.items():
            print(f"\nğŸ¯ {scenario}:")
            print(f"  æŠ€æœ¯æ ˆ: {', '.join(details['æŠ€æœ¯æ ˆ'])}")
            print(f"  å•†ä¸šä»·å€¼: {details['ä»·å€¼']}")
            print(f"  æŠ•èµ„å›æŠ¥: {details['ROI']}")
            print(f"  å®æ–½å‘¨æœŸ: {details['å®æ–½å‘¨æœŸ']}")
            
            # è®¡ç®—å¹³å‡å€¼
            roi_range = details['ROI'].replace('%', '').split('-')
            avg_roi += (int(roi_range[0]) + int(roi_range[1])) / 2
        
        avg_roi /= total_scenarios
        print(f"\nğŸ“Š å¹³å‡æŠ•èµ„å›æŠ¥ç‡: {avg_roi:.0f}%")
        print(f"ğŸ¯ åº”ç”¨åœºæ™¯è¦†ç›–: {total_scenarios}ä¸ªä¸»è¦é¢†åŸŸ")

# åˆ†æåº”ç”¨ä»·å€¼
scenarios = ApplicationScenarios()
scenarios.analyze_business_value()
```

### ğŸ”® æŠ€æœ¯å‘å±•è¶‹åŠ¿

#### æœªæ¥å‘å±•æ–¹å‘

1. **æ›´é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•**
   - é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ 
   - æŒç»­å­¦ä¹ å’Œå¢é‡æ›´æ–°
   - å¤šä»»åŠ¡è”åˆå¾®è°ƒ

2. **è‡ªåŠ¨åŒ–å¾®è°ƒå¹³å°**
   - AutoML for Fine-tuning
   - æ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–
   - ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹

3. **æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–**
   - çŸ¥è¯†è’¸é¦æŠ€æœ¯
   - æ¨¡å‹å‰ªæå’Œé‡åŒ–
   - è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–

4. **éšç§ä¿æŠ¤å¾®è°ƒ**
   - è”é‚¦å­¦ä¹ 
   - å·®åˆ†éšç§
   - å®‰å…¨å¤šæ–¹è®¡ç®—

### ğŸ’¡ æ·±åº¦æ€è€ƒé¢˜

é€šè¿‡ä»¥ä¸‹æ€è€ƒé¢˜ï¼Œè¿›ä¸€æ­¥æ·±åŒ–ä½ å¯¹æ¨¡å‹å¾®è°ƒæŠ€æœ¯çš„ç†è§£ï¼š

#### ğŸ¤” æ€è€ƒé¢˜1ï¼šå¾®è°ƒç­–ç•¥ä¼˜åŒ–
å‡è®¾ä½ éœ€è¦ä¸ºä¸€ä¸ªèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡å¼€å‘è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œä½ ä¼šå¦‚ä½•è®¾è®¡å¾®è°ƒç­–ç•¥ï¼Ÿè¯·è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- è®¡ç®—èµ„æºé™åˆ¶
- å®æ—¶æ€§è¦æ±‚
- å‡†ç¡®æ€§ç›®æ ‡
- éƒ¨ç½²æˆæœ¬

**æ€è€ƒè¦ç‚¹ï¼š**
- å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„é€‰æ‹©
- æ¨¡å‹å‹ç¼©å’Œä¼˜åŒ–ç­–ç•¥
- æ•°æ®å¤„ç†æµç¨‹è®¾è®¡
- æ€§èƒ½ç›‘æ§å’Œæ›´æ–°æœºåˆ¶

#### ğŸ¤” æ€è€ƒé¢˜2ï¼šå¤šä»»åŠ¡å¾®è°ƒè®¾è®¡
å¦‚ä½•è®¾è®¡ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œå‘½åå®ä½“è¯†åˆ«çš„å¤šä»»åŠ¡å¾®è°ƒç³»ç»Ÿï¼Ÿ

**æ€è€ƒè¦ç‚¹ï¼š**
- å…±äº«å±‚å’Œä»»åŠ¡ç‰¹å®šå±‚çš„è®¾è®¡
- æŸå¤±å‡½æ•°çš„å¹³è¡¡ç­–ç•¥
- æ•°æ®é‡‡æ ·å’Œæ‰¹å¤„ç†ç­–ç•¥
- è¯„ä¼°æŒ‡æ ‡çš„ç»¼åˆè€ƒé‡

#### ğŸ¤” æ€è€ƒé¢˜3ï¼šæŒç»­å­¦ä¹ ç³»ç»Ÿ
è®¾è®¡ä¸€ä¸ªèƒ½å¤ŸæŒç»­å­¦ä¹ æ–°çŸ¥è¯†è€Œä¸å¿˜è®°æ—§çŸ¥è¯†çš„å¾®è°ƒç³»ç»Ÿï¼Œä½ ä¼šé‡‡ç”¨ä»€ä¹ˆæŠ€æœ¯æ–¹æ¡ˆï¼Ÿ

**æ€è€ƒè¦ç‚¹ï¼š**
- ç¾éš¾æ€§é—å¿˜çš„è§£å†³æ–¹æ¡ˆ
- æ–°æ—§çŸ¥è¯†çš„å¹³è¡¡æœºåˆ¶
- æ•°æ®å­˜å‚¨å’Œç®¡ç†ç­–ç•¥
- æ€§èƒ½ç›‘æ§å’Œè´¨é‡ä¿è¯

#### ğŸ¤” æ€è€ƒé¢˜4ï¼šéšç§ä¿æŠ¤å¾®è°ƒ
åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹ï¼Œå¦‚ä½•å®ç°æœ‰æ•ˆçš„æ¨¡å‹å¾®è°ƒï¼Ÿè¯·è®¾è®¡ä¸€ä¸ªéšç§ä¿æŠ¤çš„å¾®è°ƒæ–¹æ¡ˆã€‚

**æ€è€ƒè¦ç‚¹ï¼š**
- è”é‚¦å­¦ä¹ çš„åº”ç”¨
- å·®åˆ†éšç§æŠ€æœ¯
- æ•°æ®è„±æ•å’ŒåŒ¿ååŒ–
- å®‰å…¨æ€§å’Œå®ç”¨æ€§çš„å¹³è¡¡

### ğŸ“ å­¦ä¹ å»ºè®®ä¸ä¸‹ä¸€æ­¥

#### ğŸ“š æ¨èå­¦ä¹ èµ„æº

1. **ç†è®ºæ·±åŒ–**
   - ã€ŠDeep Learningã€‹ - Ian Goodfellow
   - ã€ŠPattern Recognition and Machine Learningã€‹ - Christopher Bishop
   - æœ€æ–°çš„å¾®è°ƒæŠ€æœ¯è®ºæ–‡å’Œç»¼è¿°

2. **å®è·µé¡¹ç›®**
   - å‚ä¸å¼€æºå¾®è°ƒé¡¹ç›®
   - æ„å»ºä¸ªäººçš„å¾®è°ƒå·¥å…·åº“
   - å°è¯•ä¸åŒé¢†åŸŸçš„å¾®è°ƒåº”ç”¨

3. **æŠ€æœ¯ç¤¾åŒº**
   - Hugging Face Transformersç¤¾åŒº
   - PyTorchå®˜æ–¹è®ºå›
   - ç›¸å…³æŠ€æœ¯åšå®¢å’Œæ•™ç¨‹

#### ğŸ¯ èƒ½åŠ›æå‡è·¯å¾„

```mermaid
graph LR
    A[å½“å‰æ°´å¹³] --> B[æ·±åŒ–ç†è®ºç†è§£]
    B --> C[æ‰©å±•æŠ€æœ¯æ ˆ]
    C --> D[å®è·µå¤æ‚é¡¹ç›®]
    D --> E[æŠ€æœ¯åˆ›æ–°ç ”ç©¶]
    E --> F[è¡Œä¸šä¸“å®¶]
    
    B --> B1[å­¦ä¹ æœ€æ–°è®ºæ–‡]
    B --> B2[ç†è§£æ•°å­¦åŸç†]
    
    C --> C1[æŒæ¡æ–°æ¡†æ¶]
    C --> C2[å­¦ä¹ ç›¸å…³æŠ€æœ¯]
    
    D --> D1[ä¼ä¸šçº§é¡¹ç›®]
    D --> D2[å¼€æºè´¡çŒ®]
    
    E --> E1[æŠ€æœ¯åˆ›æ–°]
    E --> E2[è®ºæ–‡å‘è¡¨]
    
    style A fill:#ffebee
    style F fill:#e8f5e8
```

### ğŸŒˆ ç»“è¯­ï¼šä»å®šåˆ¶å·¥å‚åˆ°AIåˆ›æ–°è€…

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»ä»ä¸€ä¸ªæ¨¡å‹å¾®è°ƒçš„æ–°æ‰‹æˆé•¿ä¸ºèƒ½å¤Ÿç‹¬ç«‹è®¾è®¡å’Œå®æ–½å¤æ‚å¾®è°ƒé¡¹ç›®çš„æŠ€æœ¯ä¸“å®¶ã€‚ä½ ä¸ä»…æŒæ¡äº†LoRAã€Adapterç­‰å‰æ²¿æŠ€æœ¯ï¼Œè¿˜å»ºç«‹äº†å®Œæ•´çš„æ•°æ®å·¥ç¨‹ã€è®­ç»ƒä¼˜åŒ–å’Œè¯„ä¼°åˆ†æä½“ç³»ã€‚

**ä½ ç°åœ¨å…·å¤‡çš„æ ¸å¿ƒèƒ½åŠ›ï¼š**
- ğŸ¯ **æŠ€æœ¯é€‰å‹èƒ½åŠ›** - èƒ½å¤Ÿæ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©æœ€ä¼˜çš„å¾®è°ƒç­–ç•¥
- ğŸ› ï¸ **å·¥ç¨‹å®ç°èƒ½åŠ›** - èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„å¾®è°ƒç³»ç»Ÿå’Œå·¥å…·é“¾
- ğŸ“Š **è´¨é‡ä¿è¯èƒ½åŠ›** - èƒ½å¤Ÿå»ºç«‹æœ‰æ•ˆçš„ç›‘æ§ã€è¯„ä¼°å’Œä¼˜åŒ–ä½“ç³»
- ğŸš€ **åˆ›æ–°åº”ç”¨èƒ½åŠ›** - èƒ½å¤Ÿå°†æŠ€æœ¯åº”ç”¨åˆ°å®é™…ä¸šåŠ¡åœºæ™¯ä¸­åˆ›é€ ä»·å€¼

æ¨¡å‹å¾®è°ƒæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ–°çš„æ–¹æ³•å’Œå·¥å…·ä¸æ–­æ¶Œç°ã€‚ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œå…³æ³¨æŠ€æœ¯å‰æ²¿ï¼Œåœ¨å®è·µä¸­ä¸æ–­æå‡ï¼Œä½ å°†èƒ½å¤Ÿåœ¨AIæŠ€æœ¯çš„æµªæ½®ä¸­åˆ›é€ æ›´å¤§çš„ä»·å€¼ã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ **æ™ºèƒ½ä½“åä½œä¸ç¼–æ’**æŠ€æœ¯ï¼Œæ¢ç´¢å¦‚ä½•æ„å»ºæ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è®©æˆ‘ä»¬ç»§ç»­è¿™ä¸ªç²¾å½©çš„AIæŠ€æœ¯æ¢ç´¢ä¹‹æ—…ï¼

---

**ğŸ‰ æ­å–œä½ å®Œæˆç¬¬28ç« çš„å­¦ä¹ ï¼ä½ å·²ç»æŒæ¡äº†æ¨¡å‹å¾®è°ƒä¸å®šåˆ¶åŒ–å¼€å‘çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå‘æˆä¸ºAIæŠ€æœ¯ä¸“å®¶åˆè¿ˆè¿›äº†é‡è¦ä¸€æ­¥ï¼**