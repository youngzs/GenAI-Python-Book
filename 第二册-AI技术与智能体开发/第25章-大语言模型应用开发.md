# ç¬¬25ç« ï¼šå¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### ğŸ“š çŸ¥è¯†ç›®æ ‡
- æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹(LLM)çš„å·¥ä½œåŸç†å’Œåº”ç”¨åœºæ™¯
- æŒæ¡ä¸»æµLLMæœåŠ¡(GPTã€Claudeã€æ–‡å¿ƒä¸€è¨€ç­‰)çš„APIä½¿ç”¨
- å­¦ä¹ Promptå·¥ç¨‹çš„æ ¸å¿ƒæŠ€æœ¯å’Œæœ€ä½³å®è·µ
- ç†è§£LLMåº”ç”¨å¼€å‘çš„æ¶æ„è®¾è®¡å’Œæ€§èƒ½ä¼˜åŒ–

### ğŸ› ï¸ æŠ€èƒ½ç›®æ ‡  
- èƒ½å¤Ÿç†Ÿç»ƒä½¿ç”¨å„ç§LLM APIå¼€å‘å®é™…åº”ç”¨
- æŒæ¡LangChainç­‰ä¸»æµLLMåº”ç”¨å¼€å‘æ¡†æ¶
- å…·å¤‡è®¾è®¡å’Œä¼˜åŒ–Promptçš„ä¸“ä¸šèƒ½åŠ›
- å­¦ä¼šæ„å»ºä¼ä¸šçº§LLMåº”ç”¨çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

### ğŸ’¡ ç´ å…»ç›®æ ‡
- å»ºç«‹å¤§æ¨¡å‹æ—¶ä»£çš„AIåº”ç”¨å¼€å‘æ€ç»´
- åŸ¹å…»å¯¹LLMèƒ½åŠ›è¾¹ç•Œå’Œå±€é™æ€§çš„å‡†ç¡®è®¤çŸ¥
- å½¢æˆè´Ÿè´£ä»»çš„AIåº”ç”¨å¼€å‘ç†å¿µ
- æå‡å¯¹å‰æ²¿AIæŠ€æœ¯çš„æ•æ„Ÿåº¦å’Œåº”ç”¨èƒ½åŠ›

## ğŸ¢ å¤§æ¨¡å‹åº”ç”¨ä¸­å¿ƒæ¬¢è¿è¾

æ¬¢è¿æ¥åˆ°å¤§æ¨¡å‹åº”ç”¨ä¸­å¿ƒï¼ä»æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶é™¢çš„ç†è®ºæ¢ç´¢ï¼Œæˆ‘ä»¬ç°åœ¨è¿›å…¥äº†ä¸€ä¸ªæ›´åŠ å®ç”¨çš„é¢†åŸŸâ€”â€”å¤§è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨å¼€å‘ã€‚

### ğŸŒŸ åº”ç”¨ä¸­å¿ƒçš„ä½¿å‘½
åœ¨è¿™ä¸ªåº”ç”¨ä¸­å¿ƒé‡Œï¼Œæˆ‘ä»¬ä¸“æ³¨äºå°†å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå®é™…çš„å•†ä¸šä»·å€¼ã€‚è¿™é‡Œå°±åƒä¸€ä¸ªæ™ºèƒ½æœåŠ¡è°ƒåº¦å¹³å°ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„ä¸šåŠ¡éœ€æ±‚ï¼Œè°ƒç”¨æœ€åˆé€‚çš„AIæ¨¡å‹æ¥æä¾›æœåŠ¡ã€‚

### ğŸš€ å¤§æ¨¡å‹æ—¶ä»£çš„åˆ°æ¥
2022å¹´ChatGPTçš„å‘å¸ƒæ ‡å¿—ç€å¤§æ¨¡å‹æ—¶ä»£çš„æ­£å¼åˆ°æ¥ã€‚è¿™äº›æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œè¿˜å±•ç°å‡ºäº†ä»¤äººæƒŠå¹çš„æ¨ç†ã€åˆ›ä½œå’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä¼ ç»Ÿçš„è½¯ä»¶å¼€å‘å°±åƒæ­å»ºç§¯æœ¨ï¼Œè€Œå¤§æ¨¡å‹åº”ç”¨å¼€å‘å°±åƒæŒ‡æŒ¥ä¸€ä¸ªè¶…çº§æ™ºèƒ½çš„åŠ©æ‰‹ã€‚ä½ åªéœ€è¦ç”¨è‡ªç„¶è¯­è¨€æè¿°éœ€æ±‚ï¼ŒAIå°±èƒ½å¸®ä½ å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚è¿™å°±æ˜¯å¤§æ¨¡å‹åº”ç”¨çš„é­”åŠ›ï¼

### ğŸ—ï¸ åº”ç”¨ä¸­å¿ƒçš„æ¶æ„è®¾è®¡

```mermaid
graph TD
    A[å¤§æ¨¡å‹åº”ç”¨ä¸­å¿ƒ] --> B[æœåŠ¡è°ƒåº¦å±‚]
    A --> C[æ¨¡å‹ç®¡ç†å±‚]
    A --> D[åº”ç”¨å¼€å‘å±‚]
    A --> E[è´¨é‡ç›‘æ§å±‚]
    
    B --> B1[æ™ºèƒ½è·¯ç”±]
    B --> B2[è´Ÿè½½å‡è¡¡]
    B --> B3[æ•…éšœè½¬ç§»]
    
    C --> C1[OpenAI GPT]
    C --> C2[Anthropic Claude]
    C --> C3[ç™¾åº¦æ–‡å¿ƒä¸€è¨€]
    C --> C4[é˜¿é‡Œé€šä¹‰åƒé—®]
    
    D --> D1[Promptå·¥ç¨‹]
    D --> D2[åº”ç”¨æ¡†æ¶]
    D --> D3[ä¸šåŠ¡é›†æˆ]
    
    E --> E1[æ€§èƒ½ç›‘æ§]
    E --> E2[æˆæœ¬æ§åˆ¶]
    E --> E3[è´¨é‡è¯„ä¼°]
```

## ğŸ§  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€åŸç†

### ğŸ’¡ ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

å¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)æ˜¯åŸºäºTransformeræ¶æ„ï¼Œåœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å­¦ä¹ äººç±»è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ï¼Œè·å¾—äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚

#### ğŸ” LLMçš„æ ¸å¿ƒç‰¹å¾
1. **è§„æ¨¡åºå¤§**: å‚æ•°é‡ä»æ•°åäº¿åˆ°æ•°åƒäº¿ä¸ç­‰
2. **é€šç”¨èƒ½åŠ›**: èƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€ä»»åŠ¡
3. **æ¶Œç°èƒ½åŠ›**: åœ¨è¾¾åˆ°ä¸€å®šè§„æ¨¡åå±•ç°å‡ºæ„æƒ³ä¸åˆ°çš„èƒ½åŠ›
4. **ä¸Šä¸‹æ–‡å­¦ä¹ **: èƒ½å¤Ÿä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ æ–°ä»»åŠ¡

### ğŸ“ ä»Transformeråˆ°LLMçš„æ¼”è¿›

åŸºäºç¬¬24ç« å­¦ä¹ çš„Transformeræ¶æ„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹LLMæ˜¯å¦‚ä½•å‘å±•çš„ï¼š

```python
# LLMæ¼”è¿›æ—¶é—´çº¿
llm_evolution = {
    "2017å¹´": {
        "æ¨¡å‹": "Transformer",
        "å‚æ•°": "65M",
        "çªç ´": "æ³¨æ„åŠ›æœºåˆ¶é©å‘½"
    },
    "2018å¹´": {
        "æ¨¡å‹": "GPT-1",
        "å‚æ•°": "117M", 
        "çªç ´": "æ— ç›‘ç£é¢„è®­ç»ƒ"
    },
    "2019å¹´": {
        "æ¨¡å‹": "GPT-2",
        "å‚æ•°": "1.5B",
        "çªç ´": "ç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æå‡"
    },
    "2020å¹´": {
        "æ¨¡å‹": "GPT-3",
        "å‚æ•°": "175B",
        "çªç ´": "æ¶Œç°èƒ½åŠ›å‡ºç°"
    },
    "2022å¹´": {
        "æ¨¡å‹": "ChatGPT",
        "å‚æ•°": "175B+",
        "çªç ´": "å¯¹è¯èƒ½åŠ›é©å‘½"
    },
    "2023å¹´": {
        "æ¨¡å‹": "GPT-4",
        "å‚æ•°": "1.7T+(ä¼°è®¡)",
        "çªç ´": "å¤šæ¨¡æ€èƒ½åŠ›"
    }
}

def show_llm_evolution():
    """å±•ç¤ºLLMæ¼”è¿›å†ç¨‹"""
    for year, info in llm_evolution.items():
        print(f"{year}: {info['æ¨¡å‹']} ({info['å‚æ•°']}) - {info['çªç ´']}")

show_llm_evolution()
```

### ğŸ¯ LLMçš„å·¥ä½œåŸç†

```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥] --> B[Tokenization]
    B --> C[ä½ç½®ç¼–ç ]
    C --> D[Transformerå±‚1]
    D --> E[Transformerå±‚2]
    E --> F[...]
    F --> G[Transformerå±‚N]
    
    G --> H[è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ]
    H --> I[é‡‡æ ·ç­–ç•¥]
    I --> J[ç”ŸæˆToken]
    
    J --> K{æ˜¯å¦ç»“æŸ?}
    K -->|å¦| L[æ·»åŠ åˆ°åºåˆ—]
    L --> D
    K -->|æ˜¯| M[æœ€ç»ˆè¾“å‡º]
    
    style A fill:#e1f5fe
    style M fill:#c8e6c9
    style I fill:#fff3e0
```

## ğŸ”§ ä¸»æµLLMæœåŠ¡è¯¦è§£

### ğŸ¤– OpenAI GPTç³»åˆ—

OpenAIçš„GPTç³»åˆ—æ˜¯ç›®å‰æœ€çŸ¥åçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼š

```python
import openai
import os
from typing import List, Dict
import json

class OpenAIService:
    """OpenAI GPTæœåŠ¡å°è£…"""
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–OpenAIæœåŠ¡
        
        Args:
            api_key: OpenAI APIå¯†é’¥
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        openai.api_key = self.api_key
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "gpt-4": {
                "max_tokens": 8192,
                "cost_per_1k": {"input": 0.03, "output": 0.06},
                "best_for": ["å¤æ‚æ¨ç†", "ä»£ç ç”Ÿæˆ", "åˆ›æ„å†™ä½œ"]
            },
            "gpt-3.5-turbo": {
                "max_tokens": 4096,
                "cost_per_1k": {"input": 0.001, "output": 0.002},
                "best_for": ["å¯¹è¯", "æ–‡æœ¬å¤„ç†", "å¿«é€Ÿå“åº”"]
            }
        }
    
    def chat_completion(self, messages: List[Dict], 
                       model: str = "gpt-3.5-turbo",
                       temperature: float = 0.7,
                       max_tokens: int = 1000) -> Dict:
        """
        èŠå¤©è¡¥å…¨APIè°ƒç”¨
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹
            temperature: éšæœºæ€§æ§åˆ¶(0-1)
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            APIå“åº”ç»“æœ
        """
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=False
            )
            
            return {
                "success": True,
                "content": response.choices[0].message.content,
                "usage": response.usage,
                "model": response.model
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

# ä½¿ç”¨ç¤ºä¾‹
def demo_openai_service():
    """OpenAIæœåŠ¡æ¼”ç¤º"""
    # åˆå§‹åŒ–æœåŠ¡
    service = OpenAIService()
    
    # æ„å»ºå¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå®ç”¨çš„ä¾‹å­ã€‚"}
    ]
    
    # æ™®é€šè°ƒç”¨
    result = service.chat_completion(messages)
    if result["success"]:
        print("AIå›å¤:", result["content"])
        print("Tokenä½¿ç”¨:", result["usage"])

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_openai_service()
```

## ğŸ¨ Promptå·¥ç¨‹æ ¸å¿ƒæŠ€æœ¯

### ğŸ’¡ ä»€ä¹ˆæ˜¯Promptå·¥ç¨‹ï¼Ÿ

Promptå·¥ç¨‹æ˜¯è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥æç¤ºçš„è‰ºæœ¯ä¸ç§‘å­¦ï¼Œå®ƒå†³å®šäº†LLMè¾“å‡ºçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚ä¸€ä¸ªå¥½çš„Promptå°±åƒä¸€ä¸ªç²¾ç¡®çš„æŒ‡ä»¤ï¼Œèƒ½å¤Ÿå¼•å¯¼AIäº§ç”ŸæœŸæœ›çš„ç»“æœã€‚

### ğŸ”§ Promptè®¾è®¡çš„åŸºæœ¬åŸåˆ™

```python
class PromptEngineer:
    """Promptå·¥ç¨‹å¸ˆå·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–Promptå·¥ç¨‹å·¥å…·"""
        self.principles = {
            "clarity": "æ¸…æ™°æ˜ç¡®çš„æŒ‡ä»¤",
            "context": "å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯", 
            "examples": "æ°å½“çš„ç¤ºä¾‹å¼•å¯¼",
            "constraints": "å¿…è¦çš„çº¦æŸæ¡ä»¶",
            "format": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼"
        }
    
    def create_basic_prompt(self, task: str, context: str = "", 
                           examples: List[str] = None,
                           constraints: List[str] = None,
                           output_format: str = "") -> str:
        """
        åˆ›å»ºåŸºç¡€Prompt
        
        Args:
            task: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            examples: ç¤ºä¾‹åˆ—è¡¨
            constraints: çº¦æŸæ¡ä»¶
            output_format: è¾“å‡ºæ ¼å¼
            
        Returns:
            å®Œæ•´çš„Prompt
        """
        prompt_parts = []
        
        # ä»»åŠ¡æè¿°
        prompt_parts.append(f"ä»»åŠ¡: {task}")
        
        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context:
            prompt_parts.append(f"\nèƒŒæ™¯: {context}")
        
        # çº¦æŸæ¡ä»¶
        if constraints:
            prompt_parts.append("\nè¦æ±‚:")
            for constraint in constraints:
                prompt_parts.append(f"- {constraint}")
        
        # ç¤ºä¾‹
        if examples:
            prompt_parts.append("\nç¤ºä¾‹:")
            for i, example in enumerate(examples, 1):
                prompt_parts.append(f"{i}. {example}")
        
        # è¾“å‡ºæ ¼å¼
        if output_format:
            prompt_parts.append(f"\nè¾“å‡ºæ ¼å¼: {output_format}")
        
        return "".join(prompt_parts)
    
    def few_shot_prompt(self, task: str, examples: List[Dict]) -> str:
        """
        åˆ›å»ºFew-shot Prompt
        
        Args:
            task: ä»»åŠ¡æè¿°
            examples: è¾“å…¥è¾“å‡ºç¤ºä¾‹å¯¹
            
        Returns:
            Few-shot Prompt
        """
        prompt = f"ä»»åŠ¡: {task}\n\n"
        
        # æ·»åŠ ç¤ºä¾‹
        for i, example in enumerate(examples, 1):
            prompt += f"ç¤ºä¾‹ {i}:\n"
            prompt += f"è¾“å…¥: {example['input']}\n"
            prompt += f"è¾“å‡º: {example['output']}\n\n"
        
        prompt += "ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹è¾“å…¥:\nè¾“å…¥: "
        
        return prompt

# Promptå·¥ç¨‹å®æˆ˜æ¼”ç¤º
def demo_prompt_engineering():
    """Promptå·¥ç¨‹æ¼”ç¤º"""
    engineer = PromptEngineer()
    
    # åŸºç¡€Promptç¤ºä¾‹
    basic_prompt = engineer.create_basic_prompt(
        task="åˆ†æä¸€æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
        context="è¿™æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œéœ€è¦åˆ¤æ–­æ–‡æœ¬æ˜¯æ­£é¢ã€è´Ÿé¢è¿˜æ˜¯ä¸­æ€§",
        constraints=["åªè¿”å›æƒ…æ„Ÿæ ‡ç­¾", "ç»™å‡ºç½®ä¿¡åº¦", "è§£é‡Šåˆ¤æ–­ç†ç”±"],
        output_format="JSONæ ¼å¼ï¼š{\"sentiment\": \"æ­£é¢/è´Ÿé¢/ä¸­æ€§\", \"confidence\": 0.95, \"reason\": \"åˆ¤æ–­ç†ç”±\"}"
    )
    print("=== åŸºç¡€Prompt ===")
    print(basic_prompt)

# è¿è¡Œæ¼”ç¤º
demo_prompt_engineering()
```
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=False
            )
            
            return {
                "success": True,
                "content": response.choices[0].message.content,
                "usage": response.usage,
                "model": response.model
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def stream_completion(self, messages: List[Dict],
                         model: str = "gpt-3.5-turbo"):
        """
        æµå¼å“åº”ç”Ÿæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Yields:
            æµå¼å“åº”å†…å®¹
        """
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                stream=True
            )
            
            for chunk in response:
                if chunk.choices[0].delta.get("content"):
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Error: {str(e)}"

# ä½¿ç”¨ç¤ºä¾‹
def demo_openai_service():
    """OpenAIæœåŠ¡æ¼”ç¤º"""
    # åˆå§‹åŒ–æœåŠ¡
    service = OpenAIService()
    
    # æ„å»ºå¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå®ç”¨çš„ä¾‹å­ã€‚"}
    ]
    
    # æ™®é€šè°ƒç”¨
    result = service.chat_completion(messages)
    if result["success"]:
        print("AIå›å¤:", result["content"])
        print("Tokenä½¿ç”¨:", result["usage"])
    
    # æµå¼è°ƒç”¨æ¼”ç¤º
    print("\næµå¼å“åº”:")
    for chunk in service.stream_completion(messages):
        print(chunk, end="", flush=True)

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_openai_service()
```

### ğŸ­ Anthropic Claude

Claudeæ˜¯Anthropicå¼€å‘çš„AIåŠ©æ‰‹ï¼Œä»¥å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§è‘—ç§°ï¼š

```python
import anthropic
from typing import List, Dict

class ClaudeService:
    """Anthropic ClaudeæœåŠ¡å°è£…"""
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–ClaudeæœåŠ¡
        
        Args:
            api_key: Anthropic APIå¯†é’¥
        """
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.client = anthropic.Anthropic(api_key=self.api_key)
        
        # æ¨¡å‹é…ç½®
        self.models = {
            "claude-3-opus": {
                "max_tokens": 200000,
                "best_for": ["å¤æ‚åˆ†æ", "åˆ›æ„å†™ä½œ", "ä»£ç å®¡æŸ¥"]
            },
            "claude-3-sonnet": {
                "max_tokens": 200000,
                "best_for": ["å¹³è¡¡æ€§èƒ½", "æ—¥å¸¸å¯¹è¯", "æ–‡æ¡£å¤„ç†"]
            },
            "claude-3-haiku": {
                "max_tokens": 200000,
                "best_for": ["å¿«é€Ÿå“åº”", "ç®€å•ä»»åŠ¡", "æˆæœ¬æ§åˆ¶"]
            }
        }
    
    def create_message(self, content: str,
                      system_prompt: str = None,
                      model: str = "claude-3-sonnet-20240229",
                      max_tokens: int = 1000) -> Dict:
        """
        åˆ›å»ºæ¶ˆæ¯
        
        Args:
            content: ç”¨æˆ·è¾“å…¥å†…å®¹
            system_prompt: ç³»ç»Ÿæç¤º
            model: ä½¿ç”¨çš„æ¨¡å‹
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            APIå“åº”ç»“æœ
        """
        try:
            message = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": content}
                ]
            )
            
            return {
                "success": True,
                "content": message.content[0].text,
                "usage": message.usage,
                "model": message.model
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def analyze_text(self, text: str, analysis_type: str = "sentiment") -> Dict:
        """
        æ–‡æœ¬åˆ†æåŠŸèƒ½
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            analysis_type: åˆ†æç±»å‹
            
        Returns:
            åˆ†æç»“æœ
        """
        analysis_prompts = {
            "sentiment": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œç»™å‡ºæ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§çš„åˆ¤æ–­ï¼Œå¹¶è§£é‡ŠåŸå› ï¼š",
            "summary": "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼š",
            "keywords": "è¯·æå–ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®è¯ï¼š",
            "topics": "è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦è¯é¢˜ï¼š"
        }
        
        prompt = analysis_prompts.get(analysis_type, analysis_prompts["sentiment"])
        full_prompt = f"{prompt}\n\n{text}"
        
        return self.create_message(full_prompt)

# ä½¿ç”¨ç¤ºä¾‹
def demo_claude_service():
    """ClaudeæœåŠ¡æ¼”ç¤º"""
    service = ClaudeService()
    
    # åŸºç¡€å¯¹è¯
    result = service.create_message(
        content="è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜å®ƒåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„åº”ç”¨ã€‚",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ•™è‚²ä¸“å®¶ï¼Œå–„äºç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"
    )
    
    if result["success"]:
        print("Claudeå›å¤:", result["content"])
    
    # æ–‡æœ¬åˆ†æ
    sample_text = "ä»Šå¤©çš„å¤©æ°”çœŸæ˜¯å¤ªå¥½äº†ï¼é˜³å…‰æ˜åªšï¼Œå¾®é£è½»æ‹‚ï¼Œè®©äººå¿ƒæƒ…æ„‰æ‚¦ã€‚"
    analysis = service.analyze_text(sample_text, "sentiment")
    
    if analysis["success"]:
        print("\næƒ…æ„Ÿåˆ†æç»“æœ:", analysis["content"])

# è¿è¡Œæ¼”ç¤º
demo_claude_service()
```

### ğŸ‡¨ğŸ‡³ å›½äº§å¤§æ¨¡å‹æœåŠ¡

è®©æˆ‘ä»¬ä¹Ÿå­¦ä¹ å¦‚ä½•ä½¿ç”¨å›½äº§çš„ä¼˜ç§€å¤§æ¨¡å‹ï¼š

```python
import requests
import json
from typing import Dict, List

class ChineseLLMService:
    """å›½äº§å¤§æ¨¡å‹æœåŠ¡é›†æˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å›½äº§LLMæœåŠ¡"""
        self.services = {
            "wenxin": {
                "name": "ç™¾åº¦æ–‡å¿ƒä¸€è¨€",
                "api_base": "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop",
                "models": ["ernie-bot", "ernie-bot-turbo"]
            },
            "tongyi": {
                "name": "é˜¿é‡Œé€šä¹‰åƒé—®", 
                "api_base": "https://dashscope.aliyuncs.com/api/v1",
                "models": ["qwen-turbo", "qwen-plus", "qwen-max"]
            },
            "chatglm": {
                "name": "æ™ºè°±ChatGLM",
                "api_base": "https://open.bigmodel.cn/api/paas/v4",
                "models": ["glm-4", "glm-3-turbo"]
            }
        }
    
    def call_wenxin(self, prompt: str, access_token: str) -> Dict:
        """
        è°ƒç”¨ç™¾åº¦æ–‡å¿ƒä¸€è¨€
        
        Args:
            prompt: è¾“å…¥æç¤º
            access_token: è®¿é—®ä»¤ç‰Œ
            
        Returns:
            APIå“åº”ç»“æœ
        """
        url = f"{self.services['wenxin']['api_base']}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        data = {
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        try:
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            return {
                "success": True,
                "content": result.get("result", ""),
                "service": "æ–‡å¿ƒä¸€è¨€"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "service": "æ–‡å¿ƒä¸€è¨€"
            }
    
    def call_tongyi(self, prompt: str, api_key: str) -> Dict:
        """
        è°ƒç”¨é˜¿é‡Œé€šä¹‰åƒé—®
        
        Args:
            prompt: è¾“å…¥æç¤º
            api_key: APIå¯†é’¥
            
        Returns:
            APIå“åº”ç»“æœ
        """
        url = f"{self.services['tongyi']['api_base']}/services/aigc/text-generation/generation"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "qwen-turbo",
            "input": {
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            },
            "parameters": {
                "temperature": 0.7,
                "max_tokens": 1000
            }
        }
        
        try:
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            
            return {
                "success": True,
                "content": result["output"]["text"],
                "service": "é€šä¹‰åƒé—®"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "service": "é€šä¹‰åƒé—®"
            }
    
    def compare_models(self, prompt: str, credentials: Dict) -> Dict:
        """
        å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å“åº”
        
        Args:
            prompt: æµ‹è¯•æç¤º
            credentials: å„æœåŠ¡çš„è®¤è¯ä¿¡æ¯
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        results = {}
        
        # è°ƒç”¨æ–‡å¿ƒä¸€è¨€
        if "wenxin_token" in credentials:
            results["wenxin"] = self.call_wenxin(prompt, credentials["wenxin_token"])
        
        # è°ƒç”¨é€šä¹‰åƒé—®
        if "tongyi_key" in credentials:
            results["tongyi"] = self.call_tongyi(prompt, credentials["tongyi_key"])
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
def demo_chinese_llm():
    """å›½äº§å¤§æ¨¡å‹æ¼”ç¤º"""
    service = ChineseLLMService()
    
    # æ¨¡æ‹Ÿå‡­è¯ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰
    credentials = {
        "wenxin_token": "your_wenxin_access_token",
        "tongyi_key": "your_tongyi_api_key"
    }
    
    test_prompt = "è¯·ç”¨ä¸­æ–‡è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Œå¹¶åˆ†æå®ƒå¯¹æœªæ¥ç¤¾ä¼šçš„å½±å“ã€‚"
    
    # å¯¹æ¯”ä¸åŒæ¨¡å‹
    results = service.compare_models(test_prompt, credentials)
    
    for service_name, result in results.items():
        print(f"\n=== {result.get('service', service_name)} ===")
        if result["success"]:
            print(result["content"])
        else:
            print(f"è°ƒç”¨å¤±è´¥: {result['error']}")

# è¿è¡Œæ¼”ç¤º
demo_chinese_llm()
```

## ğŸ¨ Promptå·¥ç¨‹æ ¸å¿ƒæŠ€æœ¯

### ğŸ’¡ ä»€ä¹ˆæ˜¯Promptå·¥ç¨‹ï¼Ÿ

Promptå·¥ç¨‹æ˜¯è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥æç¤ºçš„è‰ºæœ¯ä¸ç§‘å­¦ï¼Œå®ƒå†³å®šäº†LLMè¾“å‡ºçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚ä¸€ä¸ªå¥½çš„Promptå°±åƒä¸€ä¸ªç²¾ç¡®çš„æŒ‡ä»¤ï¼Œèƒ½å¤Ÿå¼•å¯¼AIäº§ç”ŸæœŸæœ›çš„ç»“æœã€‚

### ğŸ”§ Promptè®¾è®¡çš„åŸºæœ¬åŸåˆ™

```python
class PromptEngineer:
    """Promptå·¥ç¨‹å¸ˆå·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–Promptå·¥ç¨‹å·¥å…·"""
        self.principles = {
            "clarity": "æ¸…æ™°æ˜ç¡®çš„æŒ‡ä»¤",
            "context": "å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯", 
            "examples": "æ°å½“çš„ç¤ºä¾‹å¼•å¯¼",
            "constraints": "å¿…è¦çš„çº¦æŸæ¡ä»¶",
            "format": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼"
        }
    
    def create_basic_prompt(self, task: str, context: str = "", 
                           examples: List[str] = None,
                           constraints: List[str] = None,
                           output_format: str = "") -> str:
        """
        åˆ›å»ºåŸºç¡€Prompt
        
        Args:
            task: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            examples: ç¤ºä¾‹åˆ—è¡¨
            constraints: çº¦æŸæ¡ä»¶
            output_format: è¾“å‡ºæ ¼å¼
            
        Returns:
            å®Œæ•´çš„Prompt
        """
        prompt_parts = []
        
        # ä»»åŠ¡æè¿°
        prompt_parts.append(f"ä»»åŠ¡: {task}")
        
        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context:
            prompt_parts.append(f"\nèƒŒæ™¯: {context}")
        
        # çº¦æŸæ¡ä»¶
        if constraints:
            prompt_parts.append("\nè¦æ±‚:")
            for constraint in constraints:
                prompt_parts.append(f"- {constraint}")
        
        # ç¤ºä¾‹
        if examples:
            prompt_parts.append("\nç¤ºä¾‹:")
            for i, example in enumerate(examples, 1):
                prompt_parts.append(f"{i}. {example}")
        
        # è¾“å‡ºæ ¼å¼
        if output_format:
            prompt_parts.append(f"\nè¾“å‡ºæ ¼å¼: {output_format}")
        
        return "".join(prompt_parts)
    
    def few_shot_prompt(self, task: str, examples: List[Dict]) -> str:
        """
        åˆ›å»ºFew-shot Prompt
        
        Args:
            task: ä»»åŠ¡æè¿°
            examples: è¾“å…¥è¾“å‡ºç¤ºä¾‹å¯¹
            
        Returns:
            Few-shot Prompt
        """
        prompt = f"ä»»åŠ¡: {task}\n\n"
        
        # æ·»åŠ ç¤ºä¾‹
        for i, example in enumerate(examples, 1):
            prompt += f"ç¤ºä¾‹ {i}:\n"
            prompt += f"è¾“å…¥: {example['input']}\n"
            prompt += f"è¾“å‡º: {example['output']}\n\n"
        
        prompt += "ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹è¾“å…¥:\nè¾“å…¥: "
        
        return prompt
    
    def chain_of_thought_prompt(self, problem: str) -> str:
        """
        åˆ›å»ºæ€ç»´é“¾Prompt
        
        Args:
            problem: é—®é¢˜æè¿°
            
        Returns:
            æ€ç»´é“¾Prompt
        """
        prompt = f"""
è¯·è§£å†³ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶è¯¦ç»†å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

é—®é¢˜: {problem}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š
1. ç†è§£é—®é¢˜ï¼šé¦–å…ˆåˆ†æé—®é¢˜çš„å…³é”®ä¿¡æ¯
2. åˆ¶å®šç­–ç•¥ï¼šç¡®å®šè§£å†³é—®é¢˜çš„æ–¹æ³•
3. é€æ­¥æ¨ç†ï¼šä¸€æ­¥æ­¥å±•ç¤ºæ¨ç†è¿‡ç¨‹
4. å¾—å‡ºç»“è®ºï¼šç»™å‡ºæœ€ç»ˆç­”æ¡ˆ

è®©æˆ‘ä»¬å¼€å§‹ï¼š
"""
        return prompt
    
    def role_based_prompt(self, role: str, task: str, 
                         expertise: List[str] = None) -> str:
        """
        åˆ›å»ºè§’è‰²æ‰®æ¼”Prompt
        
        Args:
            role: è§’è‰²å®šä¹‰
            task: å…·ä½“ä»»åŠ¡
            expertise: ä¸“ä¸šæŠ€èƒ½
            
        Returns:
            è§’è‰²æ‰®æ¼”Prompt
        """
        prompt = f"ä½ æ˜¯ä¸€ä¸ª{role}ã€‚"
        
        if expertise:
            prompt += f"ä½ çš„ä¸“ä¸šæŠ€èƒ½åŒ…æ‹¬ï¼š{', '.join(expertise)}ã€‚"
        
        prompt += f"\n\nç°åœ¨ï¼Œè¯·ä»¥{role}çš„èº«ä»½å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n{task}"
        
        return prompt

# Promptå·¥ç¨‹å®æˆ˜æ¼”ç¤º
def demo_prompt_engineering():
    """Promptå·¥ç¨‹æ¼”ç¤º"""
    engineer = PromptEngineer()
    
    # 1. åŸºç¡€Promptç¤ºä¾‹
    basic_prompt = engineer.create_basic_prompt(
        task="åˆ†æä¸€æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
        context="è¿™æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œéœ€è¦åˆ¤æ–­æ–‡æœ¬æ˜¯æ­£é¢ã€è´Ÿé¢è¿˜æ˜¯ä¸­æ€§",
        constraints=["åªè¿”å›æƒ…æ„Ÿæ ‡ç­¾", "ç»™å‡ºç½®ä¿¡åº¦", "è§£é‡Šåˆ¤æ–­ç†ç”±"],
        output_format="JSONæ ¼å¼ï¼š{\"sentiment\": \"æ­£é¢/è´Ÿé¢/ä¸­æ€§\", \"confidence\": 0.95, \"reason\": \"åˆ¤æ–­ç†ç”±\"}"
    )
    print("=== åŸºç¡€Prompt ===")
    print(basic_prompt)
    
    # 2. Few-shot Promptç¤ºä¾‹
    few_shot_examples = [
        {"input": "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼", "output": "{\"sentiment\": \"æ­£é¢\", \"confidence\": 0.9}"},
        {"input": "è¿™ä¸ªäº§å“è´¨é‡å¤ªå·®äº†", "output": "{\"sentiment\": \"è´Ÿé¢\", \"confidence\": 0.95}"},
        {"input": "ä»Šå¤©æ˜¯æ˜ŸæœŸä¸‰", "output": "{\"sentiment\": \"ä¸­æ€§\", \"confidence\": 0.8}"}
    ]
    
    few_shot = engineer.few_shot_prompt("æƒ…æ„Ÿåˆ†æ", few_shot_examples)
    print("\n=== Few-shot Prompt ===")
    print(few_shot)
    
    # 3. æ€ç»´é“¾Promptç¤ºä¾‹
    cot_prompt = engineer.chain_of_thought_prompt(
        "å¦‚æœä¸€ä¸ªç­çº§æœ‰30ä¸ªå­¦ç”Ÿï¼Œå…¶ä¸­60%æ˜¯å¥³ç”Ÿï¼Œé‚£ä¹ˆç”·ç”Ÿæœ‰å¤šå°‘äººï¼Ÿ"
    )
    print("\n=== æ€ç»´é“¾Prompt ===")
    print(cot_prompt)
    
    # 4. è§’è‰²æ‰®æ¼”Promptç¤ºä¾‹
    role_prompt = engineer.role_based_prompt(
        role="èµ„æ·±Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
        task="è§£é‡ŠPythonè£…é¥°å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶æä¾›ä¸€ä¸ªå®é™…åº”ç”¨çš„ä¾‹å­",
        expertise=["Pythonç¼–ç¨‹", "è½¯ä»¶æ¶æ„", "ä»£ç ä¼˜åŒ–"]
    )
    print("\n=== è§’è‰²æ‰®æ¼”Prompt ===")
    print(role_prompt)

# è¿è¡Œæ¼”ç¤º
demo_prompt_engineering()
```

### ğŸ§ª é«˜çº§PromptæŠ€æœ¯

```python
class AdvancedPromptTechniques:
    """é«˜çº§PromptæŠ€æœ¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–é«˜çº§æŠ€æœ¯å·¥å…·"""
        self.techniques = [
            "Tree of Thought",
            "Self-Consistency", 
            "Program-aided Language Models",
            "ReAct (Reasoning + Acting)"
        ]
    
    def tree_of_thought_prompt(self, problem: str, num_paths: int = 3) -> str:
        """
        æ€ç»´æ ‘Prompt
        
        Args:
            problem: é—®é¢˜æè¿°
            num_paths: æ€è€ƒè·¯å¾„æ•°é‡
            
        Returns:
            æ€ç»´æ ‘Prompt
        """
        prompt = f"""
é—®é¢˜: {problem}

è¯·ä½¿ç”¨æ€ç»´æ ‘æ–¹æ³•è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

1. é¦–å…ˆï¼Œç”Ÿæˆ{num_paths}ä¸ªä¸åŒçš„è§£é¢˜æ€è·¯
2. å¯¹æ¯ä¸ªæ€è·¯è¿›è¡Œè¯„ä¼°ï¼Œåˆ†æå…¶ä¼˜ç¼ºç‚¹
3. é€‰æ‹©æœ€æœ‰å¸Œæœ›çš„æ€è·¯ç»§ç»­æ·±å…¥
4. åœ¨æ¯ä¸€æ­¥éƒ½è€ƒè™‘å¤šç§å¯èƒ½æ€§
5. æœ€ç»ˆç»™å‡ºæœ€ä½³è§£å†³æ–¹æ¡ˆ

å¼€å§‹æ€è€ƒï¼š
"""
        return prompt
    
    def self_consistency_prompt(self, problem: str) -> str:
        """
        è‡ªä¸€è‡´æ€§Prompt
        
        Args:
            problem: é—®é¢˜æè¿°
            
        Returns:
            è‡ªä¸€è‡´æ€§Prompt
        """
        prompt = f"""
è¯·ç”¨å¤šç§ä¸åŒçš„æ–¹æ³•è§£å†³ä»¥ä¸‹é—®é¢˜ï¼Œç„¶åæ¯”è¾ƒè¿™äº›æ–¹æ³•çš„ç»“æœï¼š

é—®é¢˜: {problem}

æ–¹æ³•1: ç›´æ¥æ¨ç†
æ–¹æ³•2: é€†å‘æ€è€ƒ
æ–¹æ³•3: ç±»æ¯”æ¨ç†

è¯·åˆ†åˆ«ä½¿ç”¨è¿™ä¸‰ç§æ–¹æ³•ï¼Œç„¶åæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸€è‡´ã€‚å¦‚æœä¸ä¸€è‡´ï¼Œè¯·åˆ†æåŸå› å¹¶ç»™å‡ºæœ€å¯èƒ½æ­£ç¡®çš„ç­”æ¡ˆã€‚
"""
        return prompt
    
    def react_prompt(self, task: str) -> str:
        """
        ReAct (æ¨ç†+è¡ŒåŠ¨) Prompt
        
        Args:
            task: ä»»åŠ¡æè¿°
            
        Returns:
            ReAct Prompt
        """
        prompt = f"""
ä»»åŠ¡: {task}

è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼æ¥å®Œæˆä»»åŠ¡ï¼š

æ€è€ƒ: [åˆ†æå½“å‰æƒ…å†µï¼Œåˆ¶å®šä¸‹ä¸€æ­¥è®¡åˆ’]
è¡ŒåŠ¨: [æ‰§è¡Œå…·ä½“çš„æ“ä½œ]
è§‚å¯Ÿ: [è§‚å¯Ÿè¡ŒåŠ¨çš„ç»“æœ]

é‡å¤"æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿ"çš„å¾ªç¯ï¼Œç›´åˆ°å®Œæˆä»»åŠ¡ã€‚

å¼€å§‹ï¼š
æ€è€ƒ: æˆ‘éœ€è¦ç†è§£è¿™ä¸ªä»»åŠ¡çš„è¦æ±‚...
"""
        return prompt
    
    def prompt_optimization(self, original_prompt: str, 
                           optimization_goals: List[str]) -> str:
        """
        Promptä¼˜åŒ–
        
        Args:
            original_prompt: åŸå§‹Prompt
            optimization_goals: ä¼˜åŒ–ç›®æ ‡
            
        Returns:
            ä¼˜åŒ–åçš„Prompt
        """
        optimization_prompt = f"""
è¯·å¸®æˆ‘ä¼˜åŒ–ä»¥ä¸‹Promptï¼Œä¼˜åŒ–ç›®æ ‡ï¼š{', '.join(optimization_goals)}

åŸå§‹Prompt:
{original_prompt}

ä¼˜åŒ–è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰åŠŸèƒ½çš„åŒæ—¶æé«˜æ•ˆæœ
2. ä½¿æŒ‡ä»¤æ›´åŠ æ¸…æ™°æ˜ç¡®
3. å¢åŠ å¿…è¦çš„çº¦æŸå’Œç¤ºä¾‹
4. ä¼˜åŒ–è¾“å‡ºæ ¼å¼

è¯·æä¾›ä¼˜åŒ–åçš„Promptï¼š
"""
        return optimization_prompt

# é«˜çº§æŠ€æœ¯æ¼”ç¤º
def demo_advanced_techniques():
    """é«˜çº§PromptæŠ€æœ¯æ¼”ç¤º"""
    advanced = AdvancedPromptTechniques()
    
    # æ€ç»´æ ‘ç¤ºä¾‹
    tot_prompt = advanced.tree_of_thought_prompt(
        "è®¾è®¡ä¸€ä¸ªæé«˜åœ¨çº¿å­¦ä¹ æ•ˆæœçš„åˆ›æ–°æ–¹æ¡ˆ"
    )
    print("=== æ€ç»´æ ‘Prompt ===")
    print(tot_prompt)
    
    # è‡ªä¸€è‡´æ€§ç¤ºä¾‹
    consistency_prompt = advanced.self_consistency_prompt(
        "ä¸€ä¸ªæ°´æ± å¯ä»¥è£…100å‡æ°´ï¼Œç°åœ¨æœ‰ä¸¤ä¸ªæ°´é¾™å¤´ï¼Œå¤§æ°´é¾™å¤´æ¯åˆ†é’Ÿæµ10å‡ï¼Œå°æ°´é¾™å¤´æ¯åˆ†é’Ÿæµ5å‡ã€‚å¦‚æœåŒæ—¶æ‰“å¼€ä¸¤ä¸ªæ°´é¾™å¤´ï¼Œå¤šé•¿æ—¶é—´èƒ½è£…æ»¡æ°´æ± ï¼Ÿ"
    )
    print("\n=== è‡ªä¸€è‡´æ€§Prompt ===")
    print(consistency_prompt)
    
    # ReActç¤ºä¾‹
    react_prompt = advanced.react_prompt(
        "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªä¸ºæœŸä¸€å‘¨çš„Pythonå­¦ä¹ è®¡åˆ’"
    )
    print("\n=== ReAct Prompt ===")
    print(react_prompt)

# è¿è¡Œæ¼”ç¤º
demo_advanced_techniques()
```

## ğŸš€ æ ¸å¿ƒé¡¹ç›®ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

ç°åœ¨è®©æˆ‘ä»¬è¿ç”¨æ‰€å­¦çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§æ™ºèƒ½å®¢æœç³»ç»Ÿã€‚è¿™ä¸ªç³»ç»Ÿå°†å±•ç¤ºå¦‚ä½•å°†å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åˆ°å®é™…ä¸šåŠ¡åœºæ™¯ä¸­ã€‚

### ğŸ¯ é¡¹ç›®éœ€æ±‚åˆ†æ

æˆ‘ä»¬è¦æ„å»ºçš„æ™ºèƒ½å®¢æœç³»ç»Ÿéœ€è¦å…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š
1. **å¤šè½®å¯¹è¯ç®¡ç†**: ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡å’Œå†å²è®°å½•
2. **æ„å›¾è¯†åˆ«**: ç†è§£ç”¨æˆ·é—®é¢˜çš„çœŸå®æ„å›¾
3. **çŸ¥è¯†åº“æŸ¥è¯¢**: ä»ä¼ä¸šçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
4. **æƒ…æ„Ÿåˆ†æ**: è¯†åˆ«ç”¨æˆ·æƒ…ç»ªï¼Œæä¾›ä¸ªæ€§åŒ–æœåŠ¡
5. **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒä¸åŒLLMæœåŠ¡çš„åˆ‡æ¢å’Œå¯¹æ¯”
6. **æœåŠ¡è´¨é‡ç›‘æ§**: è®°å½•å’Œåˆ†ææœåŠ¡è´¨é‡æŒ‡æ ‡

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥] --> B[å¯¹è¯ç®¡ç†å™¨]
    B --> C[æ„å›¾è¯†åˆ«]
    C --> D[æƒ…æ„Ÿåˆ†æ]
    D --> E[çŸ¥è¯†åº“æŸ¥è¯¢]
    E --> F[å“åº”ç”Ÿæˆ]
    F --> G[è´¨é‡è¯„ä¼°]
    G --> H[ç”¨æˆ·åé¦ˆ]
    
    B --> I[å¯¹è¯å†å²]
    E --> J[ä¼ä¸šçŸ¥è¯†åº“]
    F --> K[å¤šæ¨¡å‹è°ƒåº¦]
    G --> L[ç›‘æ§ç³»ç»Ÿ]
    
    style A fill:#e1f5fe
    style H fill:#c8e6c9
    style K fill:#fff3e0
```

### ğŸ’» ç³»ç»Ÿæ ¸å¿ƒå®ç°

```python
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
import sqlite3
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Message:
    """æ¶ˆæ¯æ•°æ®ç»“æ„"""
    user_id: str
    content: str
    timestamp: datetime
    message_type: str  # 'user' or 'assistant'
    intent: Optional[str] = None
    sentiment: Optional[str] = None
    confidence: Optional[float] = None

class CustomerServiceSystem:
    """æ™ºèƒ½å®¢æœç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, llm_service):
        """
        åˆå§‹åŒ–å®¢æœç³»ç»Ÿ
        
        Args:
            llm_service: LLMæœåŠ¡å®ä¾‹
        """
        self.llm_service = llm_service
        self.active_sessions = {}
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.knowledge_base = {
            "product_inquiry": [
                {
                    "question": "äº§å“æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ",
                    "answer": "æˆ‘ä»¬çš„äº§å“ä¸»è¦åŒ…æ‹¬ï¼š1) æ™ºèƒ½æ•°æ®åˆ†æ 2) è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ 3) å®æ—¶ç›‘æ§é¢„è­¦ 4) å¤šå¹³å°é›†æˆæ”¯æŒã€‚"
                }
            ],
            "technical_support": [
                {
                    "question": "ç³»ç»Ÿç™»å½•ä¸äº†æ€ä¹ˆåŠï¼Ÿ",
                    "answer": "è¯·å°è¯•ï¼š1) æ£€æŸ¥ç”¨æˆ·åå¯†ç  2) æ¸…é™¤æµè§ˆå™¨ç¼“å­˜ 3) ä½¿ç”¨å…¶ä»–æµè§ˆå™¨ 4) æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
                }
            ],
            "billing_question": [
                {
                    "question": "å¦‚ä½•æŸ¥çœ‹è´¦å•ï¼Ÿ",
                    "answer": "æ‚¨å¯ä»¥ï¼š1) ç™»å½•ç”¨æˆ·ä¸­å¿ƒæŸ¥çœ‹ 2) ä½¿ç”¨ç§»åŠ¨åº”ç”¨ 3) è”ç³»å®¢æœè·å–è¯¦æƒ…ã€‚"
                }
            ]
        }
    
    def start_conversation(self, user_id: str) -> str:
        """å¼€å§‹æ–°å¯¹è¯"""
        session_id = f"{user_id}_{int(time.time())}"
        self.active_sessions[session_id] = {
            "user_id": user_id,
            "messages": [],
            "start_time": datetime.now()
        }
        return session_id
    
    def classify_intent(self, message: str) -> Dict:
        """æ„å›¾è¯†åˆ«"""
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯çš„æ„å›¾ï¼Œä»è¿™äº›ç±»åˆ«ä¸­é€‰æ‹©ï¼š
- product_inquiry: äº§å“å’¨è¯¢
- technical_support: æŠ€æœ¯æ”¯æŒ
- billing_question: è´¦å•é—®é¢˜
- complaint: æŠ•è¯‰å»ºè®®
- general_chat: é—²èŠ

ç”¨æˆ·æ¶ˆæ¯ï¼š{message}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{"intent": "ç±»åˆ«", "confidence": 0.95, "reason": "åˆ¤æ–­ç†ç”±"}}
"""
        
        try:
            result = self.llm_service.chat_completion([
                {"role": "user", "content": prompt}
            ])
            
            if result["success"]:
                response = result["content"]
                # ç®€å•çš„JSONæå–
                start = response.find('{')
                end = response.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = response[start:end]
                    return json.loads(json_str)
            
            return {"intent": "general_chat", "confidence": 0.5, "reason": "æ— æ³•è¯†åˆ«"}
            
        except Exception as e:
            logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return {"intent": "general_chat", "confidence": 0.3, "reason": "è¯†åˆ«å‡ºé”™"}
    
    def analyze_sentiment(self, message: str) -> Dict:
        """æƒ…æ„Ÿåˆ†æ"""
        prompt = f"""
åˆ†æä»¥ä¸‹æ¶ˆæ¯çš„æƒ…æ„Ÿï¼š
ç”¨æˆ·æ¶ˆæ¯ï¼š{message}

é€‰æ‹©ï¼špositive(æ­£é¢)ã€negative(è´Ÿé¢)ã€neutral(ä¸­æ€§)

è¿”å›JSONæ ¼å¼ï¼š
{{"sentiment": "ç±»åˆ«", "confidence": 0.95, "suggestion": "åº”å¯¹å»ºè®®"}}
"""
        
        try:
            result = self.llm_service.chat_completion([
                {"role": "user", "content": prompt}
            ])
            
            if result["success"]:
                response = result["content"]
                start = response.find('{')
                end = response.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = response[start:end]
                    return json.loads(json_str)
            
            return {"sentiment": "neutral", "confidence": 0.5, "suggestion": "æ­£å¸¸å›åº”"}
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {"sentiment": "neutral", "confidence": 0.3, "suggestion": "åˆ†æå‡ºé”™"}
    
    def search_knowledge(self, query: str, intent: str) -> List[Dict]:
        """æœç´¢çŸ¥è¯†åº“"""
        if intent in self.knowledge_base:
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            results = []
            for item in self.knowledge_base[intent]:
                if any(keyword in query.lower() for keyword in item["question"].lower().split()):
                    results.append(item)
            return results[:2]  # è¿”å›æœ€å¤š2ä¸ªç»“æœ
        return []
    
    def generate_response(self, user_message: str, intent: Dict, 
                         sentiment: Dict, knowledge_results: List[Dict],
                         conversation_history: List[Dict] = None) -> str:
        """ç”Ÿæˆå®¢æœå“åº”"""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        if conversation_history:
            recent_history = conversation_history[-2:]
            history_str = "\n".join([
                f"{msg['type']}: {msg['content']}" 
                for msg in recent_history
            ])
            context_parts.append(f"å¯¹è¯å†å²ï¼š\n{history_str}")
        
        context_parts.append(f"ç”¨æˆ·æ„å›¾ï¼š{intent['intent']}")
        context_parts.append(f"ç”¨æˆ·æƒ…æ„Ÿï¼š{sentiment['sentiment']}")
        
        if knowledge_results:
            kb_str = "\n".join([
                f"Q: {kb['question']}\nA: {kb['answer']}"
                for kb in knowledge_results
            ])
            context_parts.append(f"ç›¸å…³çŸ¥è¯†ï¼š\n{kb_str}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context}

ç”¨æˆ·é—®é¢˜ï¼š{user_message}

è¯·éµå¾ªï¼š
1. ä¿æŒå‹å¥½ä¸“ä¸šçš„è¯­è°ƒ
2. æ ¹æ®ç”¨æˆ·æƒ…æ„Ÿè°ƒæ•´å›åº”æ–¹å¼
3. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯
4. å›ç­”ç®€æ´æ˜äº†
5. å¿…è¦æ—¶è¯¢é—®æ›´å¤šç»†èŠ‚

è¯·ç”Ÿæˆå®¢æœå›å¤ï¼š
"""
        
        try:
            result = self.llm_service.chat_completion([
                {"role": "user", "content": prompt}
            ], temperature=0.7)
            
            if result["success"]:
                return result["content"].strip()
            else:
                return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
                
        except Exception as e:
            logger.error(f"å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"
    
    def process_message(self, session_id: str, user_message: str) -> Dict:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        start_time = time.time()
        
        try:
            if session_id not in self.active_sessions:
                return {"success": False, "error": "ä¼šè¯ä¸å­˜åœ¨"}
            
            session = self.active_sessions[session_id]
            
            # 1. æ„å›¾è¯†åˆ«
            intent_result = self.classify_intent(user_message)
            
            # 2. æƒ…æ„Ÿåˆ†æ
            sentiment_result = self.analyze_sentiment(user_message)
            
            # 3. çŸ¥è¯†åº“æœç´¢
            knowledge_results = self.search_knowledge(
                user_message, intent_result["intent"]
            )
            
            # 4. ç”Ÿæˆå“åº”
            assistant_response = self.generate_response(
                user_message=user_message,
                intent=intent_result,
                sentiment=sentiment_result,
                knowledge_results=knowledge_results,
                conversation_history=session["messages"]
            )
            
            # ä¿å­˜æ¶ˆæ¯
            session["messages"].extend([
                {"type": "user", "content": user_message, "timestamp": datetime.now()},
                {"type": "assistant", "content": assistant_response, "timestamp": datetime.now()}
            ])
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "response": assistant_response,
                "intent": intent_result,
                "sentiment": sentiment_result,
                "knowledge_matches": len(knowledge_results),
                "processing_time": round(processing_time, 2)
            }
            
        except Exception as e:
            logger.error(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
                "response": "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ã€‚"
            }

# ç³»ç»Ÿæ¼”ç¤º
def demo_customer_service_system():
    """æ™ºèƒ½å®¢æœç³»ç»Ÿæ¼”ç¤º"""
    print("ğŸš€ æ™ºèƒ½å®¢æœç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # æ¨¡æ‹ŸLLMæœåŠ¡
    class MockLLMService:
        def chat_completion(self, messages, temperature=0.7):
            # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å“åº”
            user_content = messages[0]["content"].lower()
            
            if "æ„å›¾" in user_content and "äº§å“" in user_content:
                return {
                    "success": True,
                    "content": '{"intent": "product_inquiry", "confidence": 0.9, "reason": "ç”¨æˆ·è¯¢é—®äº§å“ç›¸å…³ä¿¡æ¯"}'
                }
            elif "æƒ…æ„Ÿ" in user_content and "å¥½" in user_content:
                return {
                    "success": True,
                    "content": '{"sentiment": "positive", "confidence": 0.85, "suggestion": "ç»§ç»­ä¿æŒå‹å¥½æœåŠ¡"}'
                }
            elif "æƒ…æ„Ÿ" in user_content and "å·®" in user_content:
                return {
                    "success": True,
                    "content": '{"sentiment": "negative", "confidence": 0.9, "suggestion": "éœ€è¦æ›´åŠ è€å¿ƒå’Œç†è§£"}'
                }
            else:
                return {
                    "success": True,
                    "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹ä¿¡æ¯å’Œå»ºè®®ã€‚å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–ç–‘é—®ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚"
                }
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    mock_llm = MockLLMService()
    cs_system = CustomerServiceSystem(mock_llm)
    
    # å¼€å§‹å¯¹è¯
    user_id = "demo_user_001"
    session_id = cs_system.start_conversation(user_id)
    print(f"ä¼šè¯å¼€å§‹ï¼Œä¼šè¯ID: {session_id}")
    
    # æ¨¡æ‹Ÿç”¨æˆ·å¯¹è¯
    test_messages = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ ä»¬çš„äº§å“åŠŸèƒ½",
        "æˆ‘çš„ç³»ç»Ÿç™»å½•ä¸äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ",
        "ä½ ä»¬çš„æœåŠ¡å¤ªå·®äº†ï¼Œå¾ˆä¸æ»¡æ„ï¼",
        "è°¢è°¢ä½ çš„å¸®åŠ©"
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\n--- ç¬¬{i}è½®å¯¹è¯ ---")
        print(f"ç”¨æˆ·: {message}")
        
        result = cs_system.process_message(session_id, message)
        
        if result["success"]:
            print(f"å®¢æœ: {result['response']}")
            print(f"è¯†åˆ«æ„å›¾: {result['intent']['intent']} (ç½®ä¿¡åº¦: {result['intent']['confidence']})")
            print(f"æƒ…æ„Ÿåˆ†æ: {result['sentiment']['sentiment']}")
            print(f"å¤„ç†æ—¶é—´: {result['processing_time']}ç§’")
            print(f"çŸ¥è¯†åº“åŒ¹é…: {result['knowledge_matches']}æ¡")
        else:
            print(f"é”™è¯¯: {result['error']}")
    
    print(f"\n--- å¯¹è¯ç»Ÿè®¡ ---")
    session = cs_system.active_sessions[session_id]
    print(f"æ€»æ¶ˆæ¯æ•°: {len(session['messages'])}")
    print(f"å¯¹è¯æ—¶é•¿: {datetime.now() - session['start_time']}")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_customer_service_system()
```

### ğŸ¯ é¡¹ç›®ç‰¹ç‚¹ä¸åˆ›æ–°

1. **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªåŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **å¤šæ¨¡å‹æ”¯æŒ**: å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„LLMæœåŠ¡
3. **ä¸Šä¸‹æ–‡ç®¡ç†**: ç»´æŠ¤å®Œæ•´çš„å¯¹è¯å†å²å’Œç”¨æˆ·ç”»åƒ
4. **æ™ºèƒ½è·¯ç”±**: æ ¹æ®æ„å›¾å’Œæƒ…æ„Ÿæ™ºèƒ½é€‰æ‹©å¤„ç†ç­–ç•¥
5. **å®æ—¶ç›‘æ§**: å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œè´¨é‡è¯„ä¼°
6. **å¯æ‰©å±•æ€§**: æ”¯æŒæ·»åŠ æ–°çš„åŠŸèƒ½æ¨¡å—å’ŒçŸ¥è¯†æº

## ğŸ” LangChainæ¡†æ¶åº”ç”¨

### ğŸŒŸ LangChainç®€ä»‹

LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚å®ƒæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶å’Œå·¥å…·ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°æ„å»ºå¤æ‚çš„LLMåº”ç”¨ã€‚

```python
# LangChainæ ¸å¿ƒç»„ä»¶ç¤ºä¾‹
from typing import Dict, List

class LangChainIntegration:
    """LangChainé›†æˆç¤ºä¾‹"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–LangChainç»„ä»¶
        
        Args:
            api_key: OpenAI APIå¯†é’¥
        """
        self.api_key = api_key
        self.conversation_history = []
        
        # æ¨¡æ‹ŸLangChainç»„ä»¶
        self.prompt_templates = {
            "customer_service": """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å¯¹è¯å†å²ï¼š
{chat_history}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

è¯·æä¾›ä¸“ä¸šã€å‹å¥½çš„å›ç­”ï¼š
""",
            "technical_support": """
ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ”¯æŒä¸“å®¶ã€‚è¯·æ ¹æ®æŠ€æœ¯æ–‡æ¡£å’Œç”¨æˆ·é—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆã€‚

æŠ€æœ¯æ–‡æ¡£ï¼š
{documentation}

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

è¯·æä¾›è¯¦ç»†çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼š
"""
        }
    
    def create_chain(self, template_name: str) -> str:
        """
        åˆ›å»ºå¤„ç†é“¾
        
        Args:
            template_name: æ¨¡æ¿åç§°
            
        Returns:
            å¤„ç†é“¾æ ‡è¯†
        """
        if template_name in self.prompt_templates:
            return f"chain_{template_name}_{len(self.conversation_history)}"
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡æ¿: {template_name}")
    
    def run_chain(self, chain_id: str, inputs: Dict) -> str:
        """
        è¿è¡Œå¤„ç†é“¾
        
        Args:
            chain_id: é“¾æ ‡è¯†
            inputs: è¾“å…¥å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        # æ¨¡æ‹ŸLangChainé“¾æ‰§è¡Œ
        template_name = chain_id.split("_")[1]
        template = self.prompt_templates[template_name]
        
        # æ ¼å¼åŒ–æ¨¡æ¿
        try:
            formatted_prompt = template.format(**inputs)
            
            # æ¨¡æ‹ŸLLMè°ƒç”¨
            response = f"åŸºäº{template_name}æ¨¡æ¿çš„å“åº”ï¼šæ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ..."
            
            # ä¿å­˜åˆ°å†å²
            self.conversation_history.append({
                "input": inputs,
                "output": response,
                "chain_id": chain_id
            })
            
            return response
            
        except KeyError as e:
            return f"æ¨¡æ¿å‚æ•°ç¼ºå¤±: {e}"
    
    def get_memory(self) -> List[Dict]:
        """è·å–å¯¹è¯è®°å¿†"""
        return self.conversation_history[-5:]  # è¿”å›æœ€è¿‘5æ¡è®°å½•

# LangChainåº”ç”¨ç¤ºä¾‹
class LangChainCustomerService:
    """åŸºäºLangChainçš„å®¢æœç³»ç»Ÿ"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.langchain = LangChainIntegration(api_key)
        self.customer_service_chain = self.langchain.create_chain("customer_service")
        self.technical_support_chain = self.langchain.create_chain("technical_support")
    
    def handle_customer_inquiry(self, user_input: str, context: str = "") -> str:
        """å¤„ç†å®¢æˆ·å’¨è¯¢"""
        inputs = {
            "user_input": user_input,
            "context": context,
            "chat_history": self.format_chat_history()
        }
        
        return self.langchain.run_chain(self.customer_service_chain, inputs)
    
    def handle_technical_support(self, user_input: str, documentation: str = "") -> str:
        """å¤„ç†æŠ€æœ¯æ”¯æŒ"""
        inputs = {
            "user_input": user_input,
            "documentation": documentation or "ç³»ç»Ÿæ–‡æ¡£ï¼šè¯·å‚è€ƒå®˜æ–¹æŠ€æœ¯æ–‡æ¡£è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
        }
        
        return self.langchain.run_chain(self.technical_support_chain, inputs)
    
    def format_chat_history(self) -> str:
        """æ ¼å¼åŒ–èŠå¤©å†å²"""
        memory = self.langchain.get_memory()
        if not memory:
            return "è¿™æ˜¯æ–°çš„å¯¹è¯"
        
        history_lines = []
        for item in memory:
            user_input = item["input"].get("user_input", "")
            output = item["output"]
            history_lines.append(f"ç”¨æˆ·: {user_input}")
            history_lines.append(f"åŠ©æ‰‹: {output}")
        
        return "\n".join(history_lines)

# ä½¿ç”¨ç¤ºä¾‹
def demo_langchain_integration():
    """LangChainé›†æˆæ¼”ç¤º"""
    print("ğŸ” LangChainæ¡†æ¶åº”ç”¨æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    cs_system = LangChainCustomerService("demo-api-key")
    
    # å®¢æˆ·å’¨è¯¢ç¤ºä¾‹
    print("\n--- å®¢æˆ·å’¨è¯¢ ---")
    inquiry_response = cs_system.handle_customer_inquiry(
        "æˆ‘æƒ³äº†è§£ä½ ä»¬çš„äº§å“ä»·æ ¼",
        "äº§å“ä»·æ ¼ï¼šåŸºç¡€ç‰ˆ199å…ƒ/æœˆï¼Œä¸“ä¸šç‰ˆ399å…ƒ/æœˆï¼Œä¼ä¸šç‰ˆ999å…ƒ/æœˆ"
    )
    print(f"å®¢æœå›å¤: {inquiry_response}")
    
    # æŠ€æœ¯æ”¯æŒç¤ºä¾‹
    print("\n--- æŠ€æœ¯æ”¯æŒ ---")
    tech_response = cs_system.handle_technical_support(
        "APIè°ƒç”¨è¿”å›404é”™è¯¯",
        "APIæ–‡æ¡£ï¼šç¡®ä¿è¯·æ±‚URLæ­£ç¡®ï¼Œæ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆï¼ŒéªŒè¯è¯·æ±‚æ–¹æ³•æ˜¯å¦æ­£ç¡®ã€‚"
    )
    print(f"æŠ€æœ¯æ”¯æŒ: {tech_response}")
    
    # æ˜¾ç¤ºå¯¹è¯è®°å¿†
    print("\n--- å¯¹è¯è®°å¿† ---")
    memory = cs_system.langchain.get_memory()
    for i, item in enumerate(memory, 1):
        print(f"{i}. é“¾ID: {item['chain_id']}")
        print(f"   è¾“å…¥: {item['input'].get('user_input', 'N/A')}")
        print(f"   è¾“å‡º: {item['output'][:50]}...")

demo_langchain_integration()
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import hashlib
import time

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨"""
        self.cache = {}
        self.metrics = {
            "total_requests": 0,
            "cache_hits": 0,
            "average_response_time": 0,
            "error_count": 0
        }
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    def generate_cache_key(self, prompt: str, model: str = "default") -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            prompt: æç¤ºå†…å®¹
            model: æ¨¡å‹åç§°
            
        Returns:
            ç¼“å­˜é”®
        """
        content = f"{prompt}_{model}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def cache_response(self, key: str, response: str, ttl: int = 3600):
        """
        ç¼“å­˜å“åº”ç»“æœ
        
        Args:
            key: ç¼“å­˜é”®
            response: å“åº”å†…å®¹
            ttl: ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.cache[key] = {
            "response": response,
            "timestamp": time.time(),
            "ttl": ttl
        }
    
    def get_cached_response(self, key: str) -> str:
        """
        è·å–ç¼“å­˜çš„å“åº”
        
        Args:
            key: ç¼“å­˜é”®
            
        Returns:
            ç¼“å­˜çš„å“åº”å†…å®¹æˆ–None
        """
        if key in self.cache:
            cached = self.cache[key]
            if time.time() - cached["timestamp"] < cached["ttl"]:
                self.metrics["cache_hits"] += 1
                return cached["response"]
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[key]
        
        return None
    
    def batch_process_requests(self, requests: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†è¯·æ±‚
        
        Args:
            requests: è¯·æ±‚åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            
            for request in requests:
                future = executor.submit(self._process_single_request, request)
                futures.append(future)
            
            for future in futures:
                try:
                    result = future.result(timeout=30)
                    results.append(result)
                except Exception as e:
                    results.append({
                        "success": False,
                        "error": str(e)
                    })
        
        return results
    
    def _process_single_request(self, request: Dict) -> Dict:
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.generate_cache_key(
                request.get("prompt", ""),
                request.get("model", "default")
            )
            
            cached_response = self.get_cached_response(cache_key)
            if cached_response:
                return {
                    "success": True,
                    "response": cached_response,
                    "cached": True,
                    "processing_time": 0
                }
            
            # æ¨¡æ‹ŸLLMè°ƒç”¨
            time.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            response = f"å¤„ç†è¯·æ±‚: {request.get('prompt', '')[:50]}..."
            
            # ç¼“å­˜ç»“æœ
            self.cache_response(cache_key, response)
            
            processing_time = time.time() - start_time
            self.metrics["total_requests"] += 1
            self.metrics["average_response_time"] = (
                (self.metrics["average_response_time"] * (self.metrics["total_requests"] - 1) + processing_time) 
                / self.metrics["total_requests"]
            )
            
            return {
                "success": True,
                "response": response,
                "cached": False,
                "processing_time": processing_time
            }
            
        except Exception as e:
            self.metrics["error_count"] += 1
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        cache_hit_rate = (
            self.metrics["cache_hits"] / max(self.metrics["total_requests"], 1)
        )
        
        return {
            "æ€»è¯·æ±‚æ•°": self.metrics["total_requests"],
            "ç¼“å­˜å‘½ä¸­æ•°": self.metrics["cache_hits"],
            "ç¼“å­˜å‘½ä¸­ç‡": f"{cache_hit_rate:.2%}",
            "å¹³å‡å“åº”æ—¶é—´": f"{self.metrics['average_response_time']:.3f}ç§’",
            "é”™è¯¯æ•°": self.metrics["error_count"],
            "æˆåŠŸç‡": f"{(1 - self.metrics['error_count'] / max(self.metrics['total_requests'], 1)):.2%}"
        }

# ç›‘æ§ç³»ç»Ÿ
class MonitoringSystem:
    """ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ"""
        self.alerts = []
        self.thresholds = {
            "response_time": 2.0,  # å“åº”æ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰
            "error_rate": 0.05,    # é”™è¯¯ç‡é˜ˆå€¼ï¼ˆ5%ï¼‰
            "cache_hit_rate": 0.7  # ç¼“å­˜å‘½ä¸­ç‡é˜ˆå€¼ï¼ˆ70%ï¼‰
        }
    
    def check_performance(self, stats: Dict) -> List[str]:
        """
        æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        
        Args:
            stats: æ€§èƒ½ç»Ÿè®¡æ•°æ®
            
        Returns:
            å‘Šè­¦ä¿¡æ¯åˆ—è¡¨
        """
        alerts = []
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        avg_time = float(stats["å¹³å‡å“åº”æ—¶é—´"].replace("ç§’", ""))
        if avg_time > self.thresholds["response_time"]:
            alerts.append(f"å“åº”æ—¶é—´è¿‡é•¿: {avg_time:.3f}ç§’ > {self.thresholds['response_time']}ç§’")
        
        # æ£€æŸ¥é”™è¯¯ç‡
        success_rate = float(stats["æˆåŠŸç‡"].replace("%", "")) / 100
        if success_rate < (1 - self.thresholds["error_rate"]):
            alerts.append(f"é”™è¯¯ç‡è¿‡é«˜: {(1-success_rate):.2%} > {self.thresholds['error_rate']:.2%}")
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        cache_rate = float(stats["ç¼“å­˜å‘½ä¸­ç‡"].replace("%", "")) / 100
        if cache_rate < self.thresholds["cache_hit_rate"]:
            alerts.append(f"ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {cache_rate:.2%} < {self.thresholds['cache_hit_rate']:.2%}")
        
        return alerts
    
    def generate_report(self, stats: Dict) -> str:
        """
        ç”Ÿæˆç›‘æ§æŠ¥å‘Š
        
        Args:
            stats: æ€§èƒ½ç»Ÿè®¡æ•°æ®
            
        Returns:
            ç›‘æ§æŠ¥å‘Š
        """
        alerts = self.check_performance(stats)
        
        report = f"""
ğŸ“Š ç³»ç»Ÿæ€§èƒ½ç›‘æ§æŠ¥å‘Š
{'='*40}

ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ï¼š
- æ€»è¯·æ±‚æ•°: {stats['æ€»è¯·æ±‚æ•°']}
- ç¼“å­˜å‘½ä¸­æ•°: {stats['ç¼“å­˜å‘½ä¸­æ•°']}
- ç¼“å­˜å‘½ä¸­ç‡: {stats['ç¼“å­˜å‘½ä¸­ç‡']}
- å¹³å‡å“åº”æ—¶é—´: {stats['å¹³å‡å“åº”æ—¶é—´']}
- é”™è¯¯æ•°: {stats['é”™è¯¯æ•°']}
- æˆåŠŸç‡: {stats['æˆåŠŸç‡']}

âš ï¸ å‘Šè­¦ä¿¡æ¯ï¼š
"""
        
        if alerts:
            for alert in alerts:
                report += f"- {alert}\n"
        else:
            report += "- æ— å‘Šè­¦ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸\n"
        
        return report

# æ€§èƒ½æµ‹è¯•æ¼”ç¤º
def demo_performance_optimization():
    """æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º"""
    print("ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œç›‘æ§ç³»ç»Ÿ
    optimizer = PerformanceOptimizer()
    monitor = MonitoringSystem()
    
    # æ¨¡æ‹Ÿæ‰¹é‡è¯·æ±‚
    test_requests = [
        {"prompt": "åˆ†æç”¨æˆ·æƒ…æ„Ÿï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½", "model": "gpt-3.5"},
        {"prompt": "åˆ†æç”¨æˆ·æƒ…æ„Ÿï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½", "model": "gpt-3.5"},  # é‡å¤è¯·æ±‚ï¼Œæµ‹è¯•ç¼“å­˜
        {"prompt": "ç¿»è¯‘ï¼šHello World", "model": "gpt-4"},
        {"prompt": "æ€»ç»“æ–‡æ¡£ï¼šè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£", "model": "gpt-3.5"},
        {"prompt": "åˆ†æç”¨æˆ·æƒ…æ„Ÿï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½", "model": "gpt-3.5"},  # å†æ¬¡é‡å¤
    ]
    
    print("å¤„ç†æ‰¹é‡è¯·æ±‚...")
    results = optimizer.batch_process_requests(test_requests)
    
    # æ˜¾ç¤ºç»“æœ
    for i, result in enumerate(results, 1):
        print(f"\nè¯·æ±‚ {i}:")
        if result["success"]:
            print(f"  å“åº”: {result['response'][:50]}...")
            print(f"  ç¼“å­˜: {'æ˜¯' if result['cached'] else 'å¦'}")
            print(f"  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’")
        else:
            print(f"  é”™è¯¯: {result['error']}")
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = optimizer.get_performance_stats()
    print(f"\n--- æ€§èƒ½ç»Ÿè®¡ ---")
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
    report = monitor.generate_report(stats)
    print(f"\n{report}")

# è¿è¡Œæ¼”ç¤º
demo_performance_optimization()
```

## ğŸ¯ ç« èŠ‚æ€»ç»“

### ğŸ† å­¦ä¹ æˆæœå›é¡¾

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸæŒæ¡äº†ï¼š

1. **å¤§è¯­è¨€æ¨¡å‹åŸºç¡€**: æ·±å…¥ç†è§£äº†LLMçš„å·¥ä½œåŸç†å’Œå‘å±•å†ç¨‹
2. **APIæœåŠ¡é›†æˆ**: å­¦ä¼šäº†ä½¿ç”¨OpenAIã€Claudeã€å›½äº§å¤§æ¨¡å‹ç­‰ä¸»æµæœåŠ¡
3. **Promptå·¥ç¨‹**: æŒæ¡äº†è®¾è®¡é«˜æ•ˆPromptçš„æ ¸å¿ƒæŠ€æœ¯å’Œé«˜çº§æŠ€å·§
4. **ä¼ä¸šçº§åº”ç”¨**: æ„å»ºäº†å®Œæ•´çš„æ™ºèƒ½å®¢æœç³»ç»Ÿï¼Œå…·å¤‡å®é™…å•†ä¸šä»·å€¼
5. **æ¡†æ¶åº”ç”¨**: äº†è§£äº†LangChainç­‰ä¸»æµå¼€å‘æ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•
6. **æ€§èƒ½ä¼˜åŒ–**: å­¦ä¼šäº†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§çš„æœ€ä½³å®è·µ

### ğŸ’¡ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

- **LLMæœåŠ¡è°ƒç”¨**: ç»Ÿä¸€çš„APIå°è£…å’Œé”™è¯¯å¤„ç†æœºåˆ¶
- **æ„å›¾è¯†åˆ«**: åŸºäºPromptçš„ç”¨æˆ·æ„å›¾åˆ†ç±»æŠ€æœ¯
- **æƒ…æ„Ÿåˆ†æ**: ç”¨æˆ·æƒ…ç»ªè¯†åˆ«å’Œä¸ªæ€§åŒ–æœåŠ¡ç­–ç•¥
- **çŸ¥è¯†åº“é›†æˆ**: ä¼ä¸šçŸ¥è¯†ä¸AIèƒ½åŠ›çš„æœ‰æœºç»“åˆ
- **å¯¹è¯ç®¡ç†**: å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ç»´æŠ¤å’ŒçŠ¶æ€ç®¡ç†
- **æ€§èƒ½ç›‘æ§**: å…¨æ–¹ä½çš„ç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–ç­–ç•¥

### ğŸš€ å®æˆ˜é¡¹ç›®ä»·å€¼

æˆ‘ä»¬æ„å»ºçš„æ™ºèƒ½å®¢æœç³»ç»Ÿå…·æœ‰ä»¥ä¸‹å•†ä¸šä»·å€¼ï¼š
- **é™æœ¬å¢æ•ˆ**: è‡ªåŠ¨åŒ–å¤„ç†å¤§é‡å®¢æˆ·å’¨è¯¢ï¼Œå‡å°‘äººå·¥æˆæœ¬
- **7Ã—24æœåŠ¡**: æä¾›å…¨å¤©å€™ä¸é—´æ–­çš„å®¢æˆ·æœåŠ¡
- **ä¸ªæ€§åŒ–ä½“éªŒ**: æ ¹æ®ç”¨æˆ·æƒ…æ„Ÿå’Œå†å²è®°å½•æä¾›ä¸ªæ€§åŒ–æœåŠ¡
- **æ•°æ®é©±åŠ¨**: å®Œæ•´çš„ç”¨æˆ·è¡Œä¸ºåˆ†æå’ŒæœåŠ¡è´¨é‡ç›‘æ§
- **å¯æ‰©å±•æ€§**: æ”¯æŒå¿«é€Ÿæ‰©å±•åˆ°ä¸åŒä¸šåŠ¡åœºæ™¯

### ğŸ”® æœªæ¥å‘å±•æ–¹å‘

å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘çš„æœªæ¥è¶‹åŠ¿ï¼š
1. **å¤šæ¨¡æ€èåˆ**: æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³çš„ç»Ÿä¸€å¤„ç†
2. **ä¸“ä¸šåŒ–å®šåˆ¶**: é’ˆå¯¹ç‰¹å®šè¡Œä¸šçš„ä¸“ä¸šæ¨¡å‹
3. **è¾¹ç¼˜è®¡ç®—**: æœ¬åœ°åŒ–éƒ¨ç½²å’Œéšç§ä¿æŠ¤
4. **æ™ºèƒ½ä½“åä½œ**: å¤šä¸ªAIæ™ºèƒ½ä½“çš„ååŒå·¥ä½œ
5. **äººæœºåä½œ**: AIè¾…åŠ©è€Œéæ›¿ä»£äººç±»å·¥ä½œ

### ğŸ¤” æ·±åº¦æ€è€ƒé¢˜

1. **æ¶æ„è®¾è®¡é¢˜**: å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒç™¾ä¸‡çº§å¹¶å‘ç”¨æˆ·çš„å¤§è§„æ¨¡æ™ºèƒ½å®¢æœç³»ç»Ÿï¼Ÿéœ€è¦è€ƒè™‘å“ªäº›æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ

2. **Promptä¼˜åŒ–é¢˜**: é’ˆå¯¹ç”µå•†å®¢æœåœºæ™¯ï¼Œè®¾è®¡ä¸€å¥—å®Œæ•´çš„Promptæ¨¡æ¿ä½“ç³»ï¼ŒåŒ…æ‹¬å•†å“å’¨è¯¢ã€è®¢å•å¤„ç†ã€å”®åæœåŠ¡ç­‰ä¸åŒåœºæ™¯ã€‚

3. **æ€§èƒ½ä¼˜åŒ–é¢˜**: åœ¨ä¿è¯æœåŠ¡è´¨é‡çš„å‰æä¸‹ï¼Œå¦‚ä½•æœ€å¤§åŒ–é™ä½LLM APIè°ƒç”¨æˆæœ¬ï¼Ÿå¯ä»¥é‡‡ç”¨å“ªäº›æŠ€æœ¯æ‰‹æ®µï¼Ÿ

4. **åˆ›æ–°åº”ç”¨é¢˜**: åŸºäºå¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œè®¾è®¡ä¸€ä¸ªå…¨æ–°çš„AIåº”ç”¨åœºæ™¯ï¼Œè¯´æ˜å…¶æŠ€æœ¯å®ç°æ–¹æ¡ˆå’Œå•†ä¸šä»·å€¼ã€‚

---

## ğŸ‰ æ­å–œä½ ï¼

ä½ å·²ç»æˆåŠŸå®Œæˆäº†å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘çš„å­¦ä¹ ï¼ä»ç†è®ºåŸºç¡€åˆ°å®æˆ˜é¡¹ç›®ï¼Œä»åŸºç¡€APIè°ƒç”¨åˆ°ä¼ä¸šçº§ç³»ç»Ÿæ¶æ„ï¼Œä½ ç°åœ¨å·²ç»å…·å¤‡äº†æ„å»ºä¸“ä¸šLLMåº”ç”¨çš„èƒ½åŠ›ã€‚

åœ¨ä¸‹ä¸€ç« ã€Šç¬¬26ç« ï¼šAIæ™ºèƒ½ä½“æ¶æ„è®¾è®¡ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿›å…¥æ›´åŠ å‰æ²¿çš„é¢†åŸŸâ€”â€”æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°ã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ„å»ºèƒ½å¤Ÿè‡ªä¸»å†³ç­–ã€å¤šæ™ºèƒ½ä½“åä½œçš„å¤æ‚AIç³»ç»Ÿã€‚

ç»§ç»­ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢AIæŠ€æœ¯çš„æ— é™å¯èƒ½ï¼ğŸš€
