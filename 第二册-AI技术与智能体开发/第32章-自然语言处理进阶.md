# ç¬¬32ç«  è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶

> "è¯­è¨€æ˜¯æ€ç»´çš„å¤–å£³ï¼Œè€Œè‡ªç„¶è¯­è¨€å¤„ç†åˆ™æ˜¯è®©æœºå™¨ç†è§£äººç±»æ€ç»´çš„æ¡¥æ¢ã€‚åœ¨è¯­è¨€ç†è§£ç ”ç©¶é™¢ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢è®©AIçœŸæ­£'è¯»æ‡‚'äººç±»è¯­è¨€çš„å¥¥ç§˜ã€‚" â€”â€” è®¡ç®—è¯­è¨€å­¦å…ˆé©±

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### çŸ¥è¯†ç›®æ ‡
- **æ·±å…¥ç†è§£NLPæ ¸å¿ƒæŠ€æœ¯**ï¼šæŒæ¡æ–‡æœ¬é¢„å¤„ç†ã€ç‰¹å¾æå–ã€è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æŠ€æœ¯
- **å­¦ä¹ ç°ä»£NLPæ¨¡å‹æ¶æ„**ï¼šç†è§£Transformerã€BERTã€GPTç­‰å‰æ²¿æ¨¡å‹åŸç†
- **æŒæ¡NLPä¸»è¦ä»»åŠ¡**ï¼šæƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰åº”ç”¨
- **äº†è§£å¤šè¯­è¨€å¤„ç†æŠ€æœ¯**ï¼šè·¨è¯­è¨€ç†è§£ã€é›¶æ ·æœ¬å­¦ä¹ ã€å¤šè¯­è¨€æ¨¡å‹

### æŠ€èƒ½ç›®æ ‡
- **æ„å»ºå®Œæ•´NLPæµç¨‹**ï¼šä»æ–‡æœ¬é¢„å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²çš„ç«¯åˆ°ç«¯å¼€å‘èƒ½åŠ›
- **å®ç°æ ¸å¿ƒNLPç®—æ³•**ï¼šèƒ½å¤Ÿä»é›¶å®ç°å…³é”®çš„NLPæ¨¡å‹å’Œç®—æ³•
- **å¼€å‘NLPåº”ç”¨ç³»ç»Ÿ**ï¼šå…·å¤‡ä¼ä¸šçº§NLPäº§å“çš„è®¾è®¡å’Œå¼€å‘èƒ½åŠ›
- **ä¼˜åŒ–NLPæ¨¡å‹æ€§èƒ½**ï¼šæŒæ¡æ¨¡å‹å‹ç¼©ã€åŠ é€Ÿã€éƒ¨ç½²ç­‰å·¥ç¨‹åŒ–æŠ€èƒ½

### ç´ å…»ç›®æ ‡
- **åŸ¹å…»è¯­è¨€AIæ€ç»´**ï¼šç†è§£è¯­è¨€çš„å¤æ‚æ€§å’ŒAIå¤„ç†è¯­è¨€çš„æŒ‘æˆ˜
- **å»ºç«‹å¤šè¯­è¨€æ„è¯†**ï¼šå…³æ³¨è·¨æ–‡åŒ–äº¤æµå’Œå¤šè¯­è¨€æŠ€æœ¯çš„ç¤¾ä¼šä»·å€¼
- **å½¢æˆè´Ÿè´£ä»»AIç†å¿µ**ï¼šé‡è§†è¯­è¨€AIçš„ä¼¦ç†é—®é¢˜å’Œç¤¾ä¼šå½±å“

## 32.1 ç« èŠ‚å¯¼å…¥ï¼šèµ°è¿›è¯­è¨€ç†è§£ç ”ç©¶é™¢

### ğŸ›ï¸ ä»è§†è§‰åˆ°è¯­è¨€ï¼šAIè®¤çŸ¥çš„è·ƒè¿

æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨å®Œæˆäº†ç¬¬31ç« **è§†è§‰è¯†åˆ«å®éªŒå®¤**çš„æ¢ç´¢ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨è¦è¸è¿›ä¸€ä¸ªæ›´åŠ ç¥ç§˜å’Œå¤æ‚çš„é¢†åŸŸâ€”â€”**è¯­è¨€ç†è§£ç ”ç©¶é™¢**ã€‚å¦‚æœè¯´è®¡ç®—æœºè§†è§‰è®©AIè·å¾—äº†"çœ¼ç›"ï¼Œé‚£ä¹ˆè‡ªç„¶è¯­è¨€å¤„ç†å°±æ˜¯è¦èµ‹äºˆAIçœŸæ­£çš„"å¤§è„‘"å’Œ"å˜´å·´"ã€‚

```mermaid
graph TB
    A[è¯­è¨€ç†è§£ç ”ç©¶é™¢] --> B[æ–‡æœ¬åˆ†æå®éªŒå®¤]
    A --> C[æœºå™¨ç¿»è¯‘ä¸­å¿ƒ]
    A --> D[å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠ]
    A --> E[è¯­è¨€ç”Ÿæˆå®éªŒå®¤]
    A --> F[å¤šè¯­è¨€å¤„ç†éƒ¨]
    A --> G[åº”ç”¨å¼€å‘éƒ¨]
    
    B --> B1[æ–‡æœ¬é¢„å¤„ç†]
    B --> B2[ç‰¹å¾æå–]
    B --> B3[æƒ…æ„Ÿåˆ†æ]
    B --> B4[å®ä½“è¯†åˆ«]
    
    C --> C1[ç»Ÿè®¡ç¿»è¯‘]
    C --> C2[ç¥ç»ç¿»è¯‘]
    C --> C3[å¤šè¯­è¨€æ¨¡å‹]
    C --> C4[é›¶æ ·æœ¬ç¿»è¯‘]
    
    D --> D1[æ„å›¾è¯†åˆ«]
    D --> D2[å¯¹è¯ç®¡ç†]
    D --> D3[å›å¤ç”Ÿæˆ]
    D --> D4[ä¸Šä¸‹æ–‡ç†è§£]
    
    E --> E1[è¯­è¨€æ¨¡å‹]
    E --> E2[æ–‡æœ¬ç”Ÿæˆ]
    E --> E3[åˆ›æ„å†™ä½œ]
    E --> E4[æ‘˜è¦ç”Ÿæˆ]
    
    F --> F1[è·¨è¯­è¨€ç†è§£]
    F --> F2[å¤šè¯­è¨€è®­ç»ƒ]
    F --> F3[è¯­è¨€æ£€æµ‹]
    F --> F4[æ–‡åŒ–é€‚åº”]
    
    G --> G1[æ™ºèƒ½å®¢æœ]
    G --> G2[æ–‡æ¡£å¤„ç†]
    G --> G3[å†…å®¹å®¡æ ¸]
    G --> G4[å†™ä½œåŠ©æ‰‹]
```

### ğŸŒ è¯­è¨€AIçš„ç‹¬ç‰¹æŒ‘æˆ˜

ä¸è®¡ç®—æœºè§†è§‰ä¸åŒï¼Œè‡ªç„¶è¯­è¨€å¤„ç†é¢ä¸´ç€æ›´åŠ å¤æ‚çš„æŒ‘æˆ˜ï¼š

1. **è¯­ä¹‰çš„æ¨¡ç³Šæ€§**ï¼šåŒä¸€å¥è¯åœ¨ä¸åŒè¯­å¢ƒä¸‹å¯èƒ½æœ‰å®Œå…¨ä¸åŒçš„å«ä¹‰
2. **è¯­è¨€çš„å¤šæ ·æ€§**ï¼šä¸–ç•Œä¸Šæœ‰æ•°åƒç§è¯­è¨€ï¼Œæ¯ç§éƒ½æœ‰ç‹¬ç‰¹çš„è¯­æ³•å’Œæ–‡åŒ–èƒŒæ™¯
3. **ä¸Šä¸‹æ–‡çš„é‡è¦æ€§**ï¼šç†è§£è¯­è¨€å¾€å¾€éœ€è¦è€ƒè™‘æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
4. **æƒ…æ„Ÿå’Œè¯­è°ƒ**ï¼šè¯­è¨€ä¸ä»…ä¼ è¾¾ä¿¡æ¯ï¼Œè¿˜æ‰¿è½½ç€æƒ…æ„Ÿå’Œæ€åº¦

### ğŸ—ï¸ è¯­è¨€ç†è§£ç ”ç©¶é™¢çš„æ¶æ„è®¾è®¡

æˆ‘ä»¬çš„**è¯­è¨€ç†è§£ç ”ç©¶é™¢**é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ¯ä¸ªéƒ¨é—¨ä¸“æ³¨äºç‰¹å®šçš„NLPæŠ€æœ¯é¢†åŸŸï¼š

```python
class LanguageUnderstandingInstitute:
    """è¯­è¨€ç†è§£ç ”ç©¶é™¢"""
    
    def __init__(self):
        self.departments = {
            "æ–‡æœ¬åˆ†æå®éªŒå®¤": "è´Ÿè´£æ–‡æœ¬é¢„å¤„ç†ã€ç‰¹å¾æå–å’ŒåŸºç¡€åˆ†æ",
            "æœºå™¨ç¿»è¯‘ä¸­å¿ƒ": "ä¸“æ³¨äºå¤šè¯­è¨€ä¹‹é—´çš„è‡ªåŠ¨ç¿»è¯‘æŠ€æœ¯",
            "å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠ": "ç ”ç©¶äººæœºå¯¹è¯å’Œæ™ºèƒ½äº¤äº’ç³»ç»Ÿ",
            "è¯­è¨€ç”Ÿæˆå®éªŒå®¤": "å¼€å‘æ–‡æœ¬ç”Ÿæˆå’Œåˆ›æ„å†™ä½œæŠ€æœ¯",
            "å¤šè¯­è¨€å¤„ç†éƒ¨": "å¤„ç†è·¨è¯­è¨€ç†è§£å’Œå¤šè¯­è¨€åº”ç”¨",
            "åº”ç”¨å¼€å‘éƒ¨": "å°†ç ”ç©¶æˆæœè½¬åŒ–ä¸ºå®é™…äº§å“"
        }
        
        self.core_technologies = {
            "æ–‡æœ¬é¢„å¤„ç†": ["åˆ†è¯", "è¯æ€§æ ‡æ³¨", "å‘½åå®ä½“è¯†åˆ«"],
            "ç‰¹å¾æå–": ["è¯å‘é‡", "TF-IDF", "BERTåµŒå…¥"],
            "è¯­è¨€ç†è§£": ["è¯­ä¹‰åˆ†æ", "æƒ…æ„Ÿåˆ†æ", "æ„å›¾è¯†åˆ«"],
            "è¯­è¨€ç”Ÿæˆ": ["æ–‡æœ¬ç”Ÿæˆ", "æœºå™¨ç¿»è¯‘", "å¯¹è¯å›å¤"],
            "æ¨¡å‹æ¶æ„": ["Transformer", "BERT", "GPT", "T5"]
        }
        
        self.application_domains = {
            "æ™ºèƒ½å®¢æœ": "è‡ªåŠ¨åŒ–å®¢æˆ·æœåŠ¡å’Œæ”¯æŒ",
            "å†…å®¹åˆ›ä½œ": "AIè¾…åŠ©å†™ä½œå’Œå†…å®¹ç”Ÿæˆ",
            "å¤šè¯­è¨€äº¤æµ": "è·¨è¯­è¨€æ²Ÿé€šå’Œç¿»è¯‘æœåŠ¡",
            "ä¿¡æ¯æå–": "ä»æ–‡æ¡£ä¸­è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯",
            "æƒ…æ„Ÿç›‘æ§": "ç¤¾äº¤åª’ä½“å’Œç”¨æˆ·åé¦ˆåˆ†æ"
        }
        
        print("ğŸ›ï¸ è¯­è¨€ç†è§£ç ”ç©¶é™¢åˆå§‹åŒ–å®Œæˆ")
        self._show_institute_overview()
    
    def _show_institute_overview(self):
        """å±•ç¤ºç ”ç©¶é™¢æ¦‚è§ˆ"""
        print(f"\nğŸ“‹ ç ”ç©¶é™¢ç»„ç»‡æ¶æ„:")
        for dept, description in self.departments.items():
            print(f"  ğŸ¢ {dept}: {description}")
        
        print(f"\nğŸ”¬ æ ¸å¿ƒæŠ€æœ¯é¢†åŸŸ:")
        for tech, methods in self.core_technologies.items():
            print(f"  âš™ï¸ {tech}: {', '.join(methods)}")
        
        print(f"\nğŸ¯ åº”ç”¨é¢†åŸŸ:")
        for domain, description in self.application_domains.items():
            print(f"  ğŸ’¼ {domain}: {description}")
    
    def get_research_roadmap(self):
        """è·å–ç ”ç©¶è·¯çº¿å›¾"""
        roadmap = {
            "åŸºç¡€é˜¶æ®µ": {
                "é‡ç‚¹": "æ–‡æœ¬é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹",
                "ç›®æ ‡": "å»ºç«‹æ‰å®çš„NLPåŸºç¡€",
                "é¡¹ç›®": "æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ"
            },
            "è¿›é˜¶é˜¶æ®µ": {
                "é‡ç‚¹": "æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œç®—æ³•",
                "ç›®æ ‡": "æŒæ¡ç°ä»£NLPæ ¸å¿ƒæŠ€æœ¯", 
                "é¡¹ç›®": "æƒ…æ„Ÿåˆ†æå’Œæœºå™¨ç¿»è¯‘ç³»ç»Ÿ"
            },
            "åº”ç”¨é˜¶æ®µ": {
                "é‡ç‚¹": "å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆ",
                "ç›®æ ‡": "å¼€å‘æ™ºèƒ½äº¤äº’åº”ç”¨",
                "é¡¹ç›®": "æ™ºèƒ½å®¢æœå’Œå†™ä½œåŠ©æ‰‹"
            },
            "å¹³å°é˜¶æ®µ": {
                "é‡ç‚¹": "ä¼ä¸šçº§NLPå¹³å°æ„å»º",
                "ç›®æ ‡": "å®ç°å•†ä¸šåŒ–åº”ç”¨",
                "é¡¹ç›®": "ä¼ä¸šæ™ºèƒ½æ–‡æ¡£å¤„ç†å¹³å°"
            }
        }
        
        print(f"\nğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾:")
        for stage, details in roadmap.items():
            print(f"\nğŸ“ {stage}:")
            for key, value in details.items():
                print(f"  â€¢ {key}: {value}")
        
        return roadmap

# åˆå§‹åŒ–è¯­è¨€ç†è§£ç ”ç©¶é™¢
institute = LanguageUnderstandingInstitute()
roadmap = institute.get_research_roadmap()
```

### ğŸš€ NLPæŠ€æœ¯å‘å±•å†ç¨‹

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•å†ç¨‹ï¼Œç†è§£æˆ‘ä»¬ä»Šå¤©æ‰€å¤„çš„ä½ç½®ï¼š

```python
class NLPEvolutionTimeline:
    """NLPæŠ€æœ¯å‘å±•æ—¶é—´çº¿"""
    
    def __init__(self):
        self.timeline = {
            "1950s": {
                "é‡Œç¨‹ç¢‘": "å›¾çµæµ‹è¯•æå‡º",
                "æŠ€æœ¯ç‰¹ç‚¹": "è§„åˆ™åŸºç¡€çš„è¯­è¨€å¤„ç†",
                "ä»£è¡¨å·¥ä½œ": "æœºå™¨ç¿»è¯‘çš„æ—©æœŸå°è¯•"
            },
            "1980s-1990s": {
                "é‡Œç¨‹ç¢‘": "ç»Ÿè®¡æ–¹æ³•å…´èµ·",
                "æŠ€æœ¯ç‰¹ç‚¹": "åŸºäºæ¦‚ç‡å’Œç»Ÿè®¡çš„æ–¹æ³•",
                "ä»£è¡¨å·¥ä½œ": "éšé©¬å°”å¯å¤«æ¨¡å‹ã€n-gramè¯­è¨€æ¨¡å‹"
            },
            "2000s": {
                "é‡Œç¨‹ç¢‘": "æœºå™¨å­¦ä¹ åº”ç”¨",
                "æŠ€æœ¯ç‰¹ç‚¹": "ç‰¹å¾å·¥ç¨‹å’Œæµ…å±‚å­¦ä¹ ",
                "ä»£è¡¨å·¥ä½œ": "SVMæ–‡æœ¬åˆ†ç±»ã€æ¡ä»¶éšæœºåœº"
            },
            "2010s": {
                "é‡Œç¨‹ç¢‘": "æ·±åº¦å­¦ä¹ é©å‘½",
                "æŠ€æœ¯ç‰¹ç‚¹": "ç¥ç»ç½‘ç»œå’Œè¡¨ç¤ºå­¦ä¹ ",
                "ä»£è¡¨å·¥ä½œ": "Word2Vecã€RNNã€LSTM"
            },
            "2017-": {
                "é‡Œç¨‹ç¢‘": "Transformeræ—¶ä»£",
                "æŠ€æœ¯ç‰¹ç‚¹": "æ³¨æ„åŠ›æœºåˆ¶å’Œé¢„è®­ç»ƒæ¨¡å‹",
                "ä»£è¡¨å·¥ä½œ": "BERTã€GPTã€T5ã€ChatGPT"
            }
        }
        
        print("ğŸ“ˆ NLPæŠ€æœ¯å‘å±•å†ç¨‹")
        print("=" * 25)
        
    def show_evolution(self):
        """å±•ç¤ºæŠ€æœ¯æ¼”è¿›"""
        for period, details in self.timeline.items():
            print(f"\nğŸ• {period}:")
            for key, value in details.items():
                print(f"  â€¢ {key}: {value}")
    
    def analyze_current_trends(self):
        """åˆ†æå½“å‰è¶‹åŠ¿"""
        current_trends = {
            "å¤§è§„æ¨¡é¢„è®­ç»ƒ": "GPTã€BERTç­‰æ¨¡å‹çš„æˆåŠŸ",
            "å¤šæ¨¡æ€èåˆ": "æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³çš„ç»“åˆ",
            "å°‘æ ·æœ¬å­¦ä¹ ": "Few-shotå’ŒZero-shotå­¦ä¹ ",
            "å¯è§£é‡Šæ€§": "ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹",
            "æ•ˆç‡ä¼˜åŒ–": "æ¨¡å‹å‹ç¼©å’Œè¾¹ç¼˜éƒ¨ç½²",
            "ä¼¦ç†AI": "è´Ÿè´£ä»»çš„AIå¼€å‘"
        }
        
        print(f"\nğŸ”¥ å½“å‰æŠ€æœ¯è¶‹åŠ¿:")
        for trend, description in current_trends.items():
            print(f"  ğŸ“Š {trend}: {description}")
        
        return current_trends

# å±•ç¤ºNLPå‘å±•å†ç¨‹
evolution = NLPEvolutionTimeline()
evolution.show_evolution()
trends = evolution.analyze_current_trends()
```

## 32.2 æ–‡æœ¬é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹

### ğŸ› ï¸ æ–‡æœ¬åˆ†æå®éªŒå®¤ï¼šè¯­è¨€æ•°æ®çš„ç‚¼é‡‘æœ¯

åœ¨æˆ‘ä»¬çš„**æ–‡æœ¬åˆ†æå®éªŒå®¤**ä¸­ï¼ŒåŸå§‹çš„æ–‡æœ¬å°±åƒæ˜¯æœªç»åŠ å·¥çš„çŸ¿çŸ³ï¼Œéœ€è¦ç»è¿‡ç²¾å¿ƒçš„å¤„ç†æ‰èƒ½æå–å‡ºæœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æ–‡æœ¬é¢„å¤„ç†æ˜¯æ•´ä¸ªNLPæµç¨‹çš„åŸºç¡€ï¼Œå°±åƒæ˜¯ä¸ºåç»­çš„é«˜çº§ç®—æ³•å‡†å¤‡ä¼˜è´¨çš„"é£Ÿæ"ã€‚

```python
import re
import jieba
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
import spacy
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import time
from typing import List, Dict, Tuple, Optional

class TextPreprocessingLab:
    """æ–‡æœ¬é¢„å¤„ç†å®éªŒå®¤"""
    
    def __init__(self, language='zh'):
        self.language = language
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
        # åŠ è½½è¯­è¨€æ¨¡å‹
        if language == 'zh':
            # ä¸­æ–‡å¤„ç†å·¥å…·
            jieba.initialize()
            self.stop_words = self._load_chinese_stopwords()
        else:
            # è‹±æ–‡å¤„ç†å·¥å…·
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            nltk.download('maxent_ne_chunker', quiet=True)
            nltk.download('words', quiet=True)
            nltk.download('wordnet', quiet=True)
            self.stop_words = set(stopwords.words('english'))
        
        print(f"ğŸ§ª æ–‡æœ¬é¢„å¤„ç†å®éªŒå®¤åˆå§‹åŒ–å®Œæˆ (è¯­è¨€: {language})")
        
    def _load_chinese_stopwords(self):
        """åŠ è½½ä¸­æ–‡åœç”¨è¯"""
        # ç®€åŒ–çš„ä¸­æ–‡åœç”¨è¯åˆ—è¡¨
        chinese_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'èƒ½',
            'è€Œ', 'å¯ä»¥', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶è€Œ',
            'æˆ–è€…', 'ä»¥åŠ', 'ç­‰ç­‰', 'æ¯”å¦‚', 'ä¾‹å¦‚', 'å°±æ˜¯', 'è¿™æ ·', 'é‚£æ ·'
        }
        return chinese_stopwords
    
    def clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        print(f"ğŸ§¹ å¼€å§‹æ–‡æœ¬æ¸…æ´—...")
        
        # 1. ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        
        # 2. ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 3. è½¬æ¢ä¸ºå°å†™ï¼ˆä»…å¯¹è‹±æ–‡ï¼‰
        if self.language == 'en':
            text = text.lower()
        
        print(f"âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆï¼Œé•¿åº¦: {len(text)}")
        return text
    
    def tokenize_text(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        print(f"âœ‚ï¸ å¼€å§‹æ–‡æœ¬åˆ†è¯...")
        
        if self.language == 'zh':
            # ä¸­æ–‡åˆ†è¯
            tokens = list(jieba.cut(text))
        else:
            # è‹±æ–‡åˆ†è¯
            tokens = word_tokenize(text)
        
        print(f"âœ… åˆ†è¯å®Œæˆï¼Œè¯æ±‡æ•°: {len(tokens)}")
        return tokens
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """ç§»é™¤åœç”¨è¯"""
        print(f"ğŸš« ç§»é™¤åœç”¨è¯...")
        
        filtered_tokens = [token for token in tokens if token not in self.stop_words]
        
        removed_count = len(tokens) - len(filtered_tokens)
        print(f"âœ… åœç”¨è¯ç§»é™¤å®Œæˆï¼Œç§»é™¤äº† {removed_count} ä¸ªè¯")
        
        return filtered_tokens
    
    def pos_tagging(self, tokens: List[str]) -> List[Tuple[str, str]]:
        """è¯æ€§æ ‡æ³¨"""
        print(f"ğŸ·ï¸ è¿›è¡Œè¯æ€§æ ‡æ³¨...")
        
        if self.language == 'zh':
            # ä¸­æ–‡è¯æ€§æ ‡æ³¨ï¼ˆç®€åŒ–å®ç°ï¼‰
            pos_tags = [(token, 'n') for token in tokens]  # ç®€åŒ–ä¸ºåè¯
        else:
            # è‹±æ–‡è¯æ€§æ ‡æ³¨
            pos_tags = pos_tag(tokens)
        
        print(f"âœ… è¯æ€§æ ‡æ³¨å®Œæˆï¼Œæ ‡æ³¨äº† {len(pos_tags)} ä¸ªè¯")
        return pos_tags
    
    def named_entity_recognition(self, text: str) -> List[Dict]:
        """å‘½åå®ä½“è¯†åˆ«"""
        print(f"ğŸ¯ è¿›è¡Œå‘½åå®ä½“è¯†åˆ«...")
        
        entities = []
        
        if self.language == 'zh':
            # ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ï¼ˆç®€åŒ–å®ç°ï¼‰
            # è¿™é‡Œå¯ä»¥é›†æˆæ›´é«˜çº§çš„ä¸­æ–‡NERå·¥å…·
            entities = self._simple_chinese_ner(text)
        else:
            # è‹±æ–‡å‘½åå®ä½“è¯†åˆ«
            tokens = word_tokenize(text)
            pos_tags = pos_tag(tokens)
            chunks = ne_chunk(pos_tags)
            
            for chunk in chunks:
                if hasattr(chunk, 'label'):
                    entity_text = ' '.join([token for token, pos in chunk])
                    entities.append({
                        'text': entity_text,
                        'label': chunk.label(),
                        'start': 0,  # ç®€åŒ–å®ç°
                        'end': len(entity_text)
                    })
        
        print(f"âœ… å‘½åå®ä½“è¯†åˆ«å®Œæˆï¼Œå‘ç° {len(entities)} ä¸ªå®ä½“")
        return entities
    
    def _simple_chinese_ner(self, text: str) -> List[Dict]:
        """ç®€åŒ–çš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«"""
        entities = []
        
        # ç®€å•çš„è§„åˆ™åŒ¹é…ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼‰
        patterns = {
            'PERSON': r'[\u4e00-\u9fff]{2,4}(?=å…ˆç”Ÿ|å¥³å£«|æ•™æˆ|åšå£«|æ€»|ç»ç†)',
            'ORG': r'[\u4e00-\u9fff]+(?:å…¬å¸|é›†å›¢|å¤§å­¦|å­¦é™¢|é“¶è¡Œ|åŒ»é™¢)',
            'LOC': r'[\u4e00-\u9fff]+(?:å¸‚|çœ|åŒº|å¿|è¡—|è·¯|æ‘)'
        }
        
        for label, pattern in patterns.items():
            matches = re.finditer(pattern, text)
            for match in matches:
                entities.append({
                    'text': match.group(),
                    'label': label,
                    'start': match.start(),
                    'end': match.end()
                })
        
        return entities

class FeatureExtractionWorkshop:
    """ç‰¹å¾æå–å·¥ä½œåŠ"""
    
    def __init__(self):
        self.vectorizers = {}
        print("âš™ï¸ ç‰¹å¾æå–å·¥ä½œåŠåˆå§‹åŒ–å®Œæˆ")
    
    def extract_tfidf_features(self, texts: List[str], max_features: int = 1000) -> np.ndarray:
        """æå–TF-IDFç‰¹å¾"""
        print(f"ğŸ“Š æå–TF-IDFç‰¹å¾ (æœ€å¤§ç‰¹å¾æ•°: {max_features})...")
        
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words=None,  # å·²åœ¨é¢„å¤„ç†ä¸­å¤„ç†
            ngram_range=(1, 2)  # åŒ…å«1-gramå’Œ2-gram
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        self.vectorizers['tfidf'] = vectorizer
        
        print(f"âœ… TF-IDFç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {tfidf_matrix.shape}")
        return tfidf_matrix.toarray()
    
    def extract_word_frequency_features(self, tokens_list: List[List[str]]) -> Dict:
        """æå–è¯é¢‘ç‰¹å¾"""
        print(f"ğŸ“ˆ æå–è¯é¢‘ç‰¹å¾...")
        
        # è®¡ç®—å…¨å±€è¯é¢‘
        all_tokens = [token for tokens in tokens_list for token in tokens]
        word_freq = Counter(all_tokens)
        
        # è®¡ç®—æ–‡æ¡£é¢‘ç‡
        doc_freq = Counter()
        for tokens in tokens_list:
            unique_tokens = set(tokens)
            for token in unique_tokens:
                doc_freq[token] += 1
        
        # è®¡ç®—TF-IDFæƒé‡
        num_docs = len(tokens_list)
        tf_idf = {}
        
        for token, freq in word_freq.items():
            tf = freq / len(all_tokens)
            idf = np.log(num_docs / (doc_freq[token] + 1))
            tf_idf[token] = tf * idf
        
        features = {
            'word_frequency': dict(word_freq.most_common(50)),
            'document_frequency': dict(doc_freq.most_common(50)),
            'tf_idf_weights': dict(sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)[:50])
        }
        
        print(f"âœ… è¯é¢‘ç‰¹å¾æå–å®Œæˆ")
        return features
    
    def extract_statistical_features(self, texts: List[str]) -> Dict:
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        print(f"ğŸ“ æå–ç»Ÿè®¡ç‰¹å¾...")
        
        features = {
            'avg_text_length': np.mean([len(text) for text in texts]),
            'avg_word_count': np.mean([len(text.split()) for text in texts]),
            'avg_sentence_count': np.mean([len(sent_tokenize(text)) for text in texts]),
            'text_length_std': np.std([len(text) for text in texts]),
            'word_count_std': np.std([len(text.split()) for text in texts])
        }
        
        print(f"âœ… ç»Ÿè®¡ç‰¹å¾æå–å®Œæˆ")
        return features

class IntelligentDocumentAnalyzer:
    """æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, language='zh'):
        self.preprocessor = TextPreprocessingLab(language)
        self.feature_extractor = FeatureExtractionWorkshop()
        self.analysis_history = []
        
        print("ğŸ“„ æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def analyze_document(self, document: str, doc_id: str = None) -> Dict:
        """åˆ†æå•ä¸ªæ–‡æ¡£"""
        if doc_id is None:
            doc_id = f"DOC_{int(time.time() * 1000)}"
        
        print(f"\nğŸ“– åˆ†ææ–‡æ¡£: {doc_id}")
        print("=" * 30)
        
        start_time = time.time()
        
        # 1. æ–‡æœ¬é¢„å¤„ç†
        cleaned_text = self.preprocessor.clean_text(document)
        tokens = self.preprocessor.tokenize_text(cleaned_text)
        filtered_tokens = self.preprocessor.remove_stopwords(tokens)
        pos_tags = self.preprocessor.pos_tagging(filtered_tokens)
        entities = self.preprocessor.named_entity_recognition(document)
        
        # 2. ç‰¹å¾æå–
        tfidf_features = self.feature_extractor.extract_tfidf_features([' '.join(filtered_tokens)])
        word_features = self.feature_extractor.extract_word_frequency_features([filtered_tokens])
        stat_features = self.feature_extractor.extract_statistical_features([document])
        
        # 3. æ–‡æ¡£åˆ†æç»“æœ
        analysis_result = {
            'document_id': doc_id,
            'timestamp': time.time(),
            'original_length': len(document),
            'cleaned_length': len(cleaned_text),
            'token_count': len(tokens),
            'filtered_token_count': len(filtered_tokens),
            'unique_tokens': len(set(filtered_tokens)),
            'entities': entities,
            'top_words': word_features['word_frequency'],
            'pos_distribution': self._analyze_pos_distribution(pos_tags),
            'statistical_features': stat_features,
            'processing_time': time.time() - start_time
        }
        
        self.analysis_history.append(analysis_result)
        self._print_analysis_report(analysis_result)
        
        return analysis_result
    
    def _analyze_pos_distribution(self, pos_tags: List[Tuple[str, str]]) -> Dict:
        """åˆ†æè¯æ€§åˆ†å¸ƒ"""
        pos_counts = Counter([pos for word, pos in pos_tags])
        total_count = len(pos_tags)
        
        pos_distribution = {}
        for pos, count in pos_counts.items():
            pos_distribution[pos] = {
                'count': count,
                'percentage': count / total_count * 100
            }
        
        return pos_distribution
    
    def _print_analysis_report(self, result: Dict):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“Š æ–‡æ¡£åˆ†ææŠ¥å‘Š")
        print("=" * 20)
        print(f"ğŸ“„ æ–‡æ¡£ID: {result['document_id']}")
        print(f"ğŸ“ åŸå§‹é•¿åº¦: {result['original_length']} å­—ç¬¦")
        print(f"ğŸ§¹ æ¸…æ´—åé•¿åº¦: {result['cleaned_length']} å­—ç¬¦")
        print(f"âœ‚ï¸ è¯æ±‡æ€»æ•°: {result['token_count']}")
        print(f"ğŸš« è¿‡æ»¤åè¯æ±‡: {result['filtered_token_count']}")
        print(f"ğŸŒŸ ç‹¬ç‰¹è¯æ±‡: {result['unique_tokens']}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
        
        print(f"\nğŸ¯ å‘ç°çš„å®ä½“:")
        for entity in result['entities'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªå®ä½“
            print(f"  â€¢ {entity['text']} ({entity['label']})")
        
        print(f"\nğŸ“Š é«˜é¢‘è¯æ±‡:")
        for word, freq in list(result['top_words'].items())[:10]:
            print(f"  â€¢ {word}: {freq}")
    
    def batch_analyze_documents(self, documents: List[str], doc_ids: List[str] = None) -> List[Dict]:
        """æ‰¹é‡åˆ†ææ–‡æ¡£"""
        if doc_ids is None:
            doc_ids = [f"DOC_{i:03d}" for i in range(len(documents))]
        
        print(f"\nğŸ”„ æ‰¹é‡åˆ†æ {len(documents)} ä¸ªæ–‡æ¡£")
        print("=" * 35)
        
        results = []
        for i, (doc, doc_id) in enumerate(zip(documents, doc_ids)):
            print(f"\nå¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}: {doc_id}")
            result = self.analyze_document(doc, doc_id)
            results.append(result)
        
        self._generate_batch_summary(results)
        return results
    
    def _generate_batch_summary(self, results: List[Dict]):
        """ç”Ÿæˆæ‰¹é‡åˆ†ææ‘˜è¦"""
        print(f"\nğŸ“ˆ æ‰¹é‡åˆ†ææ‘˜è¦")
        print("=" * 20)
        
        total_docs = len(results)
        avg_length = np.mean([r['original_length'] for r in results])
        avg_tokens = np.mean([r['token_count'] for r in results])
        avg_unique = np.mean([r['unique_tokens'] for r in results])
        total_entities = sum(len(r['entities']) for r in results)
        
        print(f"ğŸ“š æ€»æ–‡æ¡£æ•°: {total_docs}")
        print(f"ğŸ“ å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print(f"âœ‚ï¸ å¹³å‡è¯æ•°: {avg_tokens:.0f}")
        print(f"ğŸŒŸ å¹³å‡ç‹¬ç‰¹è¯: {avg_unique:.0f}")
        print(f"ğŸ¯ æ€»å®ä½“æ•°: {total_entities}")
        
        # å…¨å±€è¯é¢‘ç»Ÿè®¡
        all_words = {}
        for result in results:
            for word, freq in result['top_words'].items():
                all_words[word] = all_words.get(word, 0) + freq
        
        top_global_words = sorted(all_words.items(), key=lambda x: x[1], reverse=True)[:10]
        
        print(f"\nğŸŒ å…¨å±€é«˜é¢‘è¯:")
        for word, freq in top_global_words:
            print(f"  â€¢ {word}: {freq}")

# æ¼”ç¤ºæ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ
def demo_document_analysis():
    """æ¼”ç¤ºæ–‡æ¡£åˆ†æç³»ç»Ÿ"""
    # åˆ›å»ºåˆ†æç³»ç»Ÿ
    analyzer = IntelligentDocumentAnalyzer('zh')
    
    # ç¤ºä¾‹æ–‡æ¡£
    sample_documents = [
        """
        äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç®—æ³•åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
        è‡ªç„¶è¯­è¨€å¤„ç†ä½œä¸ºäººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œåœ¨æ–‡æœ¬åˆ†æã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿç­‰æ–¹é¢
        å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼ŒAIçš„èƒ½åŠ›å¾—åˆ°äº†æå¤§æå‡ã€‚
        """,
        """
        åŒ—äº¬å¤§å­¦è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ç ”ç©¶æ‰€åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæœ‰ç€æ·±åšçš„ç ”ç©¶åŸºç¡€ã€‚
        è¯¥ç ”ç©¶æ‰€çš„å¼ æ•™æˆå›¢é˜Ÿåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢åšå‡ºäº†é‡è¦è´¡çŒ®ï¼Œç‰¹åˆ«æ˜¯åœ¨
        ä¸­æ–‡ä¿¡æ¯å¤„ç†å’Œæœºå™¨ç¿»è¯‘æŠ€æœ¯æ–¹é¢ã€‚ä»–ä»¬å¼€å‘çš„ç³»ç»Ÿå·²ç»åœ¨å¤šå®¶å…¬å¸å¾—åˆ°åº”ç”¨ã€‚
        """,
        """
        è…¾è®¯å…¬å¸æœ€è¿‘å‘å¸ƒäº†æ–°ä¸€ä»£çš„AIåŠ©æ‰‹ï¼Œè¯¥åŠ©æ‰‹é›†æˆäº†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€ç†è§£æŠ€æœ¯ã€‚
        ç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸ç³»ç»Ÿè¿›è¡Œäº¤äº’ï¼Œç³»ç»Ÿèƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾å¹¶æä¾›å‡†ç¡®å›ç­”ã€‚
        è¿™é¡¹æŠ€æœ¯åœ¨å®¢æœã€æ•™è‚²ã€å¨±ä¹ç­‰å¤šä¸ªåœºæ™¯ä¸­éƒ½æœ‰åº”ç”¨å‰æ™¯ã€‚
        """
    ]
    
    # æ‰¹é‡åˆ†æ
    results = analyzer.batch_analyze_documents(sample_documents)
    
    return results

# è¿è¡Œæ¼”ç¤º
demo_results = demo_document_analysis()
```

### ğŸ“Š æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯å¯¹æ¯”

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¯è§†åŒ–å›¾è¡¨æ¥ç†è§£ä¸åŒæ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯çš„æ•ˆæœï¼š

```mermaid
graph LR
    A[åŸå§‹æ–‡æœ¬] --> B[æ–‡æœ¬æ¸…æ´—]
    B --> C[åˆ†è¯å¤„ç†]
    C --> D[åœç”¨è¯è¿‡æ»¤]
    D --> E[è¯æ€§æ ‡æ³¨]
    E --> F[å‘½åå®ä½“è¯†åˆ«]
    F --> G[ç‰¹å¾æå–]
    
    B --> B1[ç§»é™¤ç‰¹æ®Šå­—ç¬¦<br/>ç»Ÿä¸€æ ¼å¼<br/>å™ªå£°è¿‡æ»¤]
    C --> C1[ä¸­æ–‡åˆ†è¯<br/>è‹±æ–‡åˆ†è¯<br/>å­è¯åˆ†å‰²]
    D --> D1[é«˜é¢‘åœç”¨è¯<br/>æ ‡ç‚¹ç¬¦å·<br/>æ— æ„ä¹‰è¯æ±‡]
    E --> E2[åè¯æ ‡æ³¨<br/>åŠ¨è¯è¯†åˆ«<br/>å½¢å®¹è¯æå–]
    F --> F1[äººåè¯†åˆ«<br/>æœºæ„è¯†åˆ«<br/>åœ°åæå–]
    G --> G1[TF-IDFå‘é‡<br/>è¯é¢‘ç»Ÿè®¡<br/>è¯­ä¹‰ç‰¹å¾]
```

## 32.3 æƒ…æ„Ÿåˆ†æä¸æ–‡æœ¬åˆ†ç±»

### ğŸ’­ æƒ…æ„Ÿåˆ†æå®éªŒå®¤ï¼šè§£è¯»æ–‡å­—èƒŒåçš„æƒ…æ„Ÿå¯†ç 

åœ¨æˆ‘ä»¬çš„**è¯­è¨€ç†è§£ç ”ç©¶é™¢**ä¸­ï¼Œ**æƒ…æ„Ÿåˆ†æå®éªŒå®¤**æ˜¯ä¸€ä¸ªç‰¹åˆ«ç¥ç§˜çš„åœ°æ–¹ã€‚è¿™é‡Œçš„ç ”ç©¶å‘˜ä»¬ä¸ä»…è¦ç†è§£æ–‡å­—çš„å­—é¢æ„æ€ï¼Œæ›´è¦æ´å¯Ÿæ–‡å­—èƒŒåéšè—çš„æƒ…æ„Ÿã€æ€åº¦å’Œå€¾å‘ã€‚å°±åƒå¿ƒç†å­¦å®¶åˆ†æäººçš„å†…å¿ƒä¸–ç•Œä¸€æ ·ï¼Œæˆ‘ä»¬çš„AIç³»ç»Ÿè¦å­¦ä¼š"è¯»æ‡‚"æ–‡æœ¬çš„æƒ…æ„Ÿè‰²å½©ã€‚

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import jieba
import re
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class SentimentAnalysisLab:
    """æƒ…æ„Ÿåˆ†æå®éªŒå®¤"""
    
    def __init__(self):
        self.models = {}
        self.vectorizers = {}
        self.emotion_lexicon = self._build_emotion_lexicon()
        self.training_history = []
        
        print("ğŸ’­ æƒ…æ„Ÿåˆ†æå®éªŒå®¤åˆå§‹åŒ–å®Œæˆ")
        self._show_lab_capabilities()
    
    def _show_lab_capabilities(self):
        """å±•ç¤ºå®éªŒå®¤èƒ½åŠ›"""
        capabilities = {
            "æƒ…æ„Ÿææ€§åˆ†æ": "è¯†åˆ«æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§æƒ…æ„Ÿ",
            "æƒ…æ„Ÿå¼ºåº¦è¯„ä¼°": "é‡åŒ–æƒ…æ„Ÿè¡¨è¾¾çš„å¼ºçƒˆç¨‹åº¦",
            "å¤šç»´æƒ…æ„Ÿåˆ†æ": "è¯†åˆ«å–œæ€’å“€ä¹ç­‰å¤šç§æƒ…æ„Ÿ",
            "æƒ…æ„Ÿè¯å…¸æ„å»º": "æ„å»ºé¢†åŸŸä¸“ç”¨æƒ…æ„Ÿè¯å…¸",
            "ä¸Šä¸‹æ–‡æƒ…æ„Ÿç†è§£": "è€ƒè™‘è¯­å¢ƒçš„æƒ…æ„Ÿåˆ†æ",
            "å®æ—¶æƒ…æ„Ÿç›‘æ§": "å¤§è§„æ¨¡æ–‡æœ¬çš„å®æ—¶æƒ…æ„Ÿè¿½è¸ª"
        }
        
        print(f"\nğŸ”¬ å®éªŒå®¤æ ¸å¿ƒèƒ½åŠ›:")
        for capability, description in capabilities.items():
            print(f"  â€¢ {capability}: {description}")
    
    def _build_emotion_lexicon(self) -> Dict:
        """æ„å»ºæƒ…æ„Ÿè¯å…¸"""
        # ç®€åŒ–çš„ä¸­æ–‡æƒ…æ„Ÿè¯å…¸
        emotion_lexicon = {
            'positive': {
                'å¥½', 'æ£’', 'ä¼˜ç§€', 'å‡ºè‰²', 'å®Œç¾', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±',
                'é«˜å…´', 'å¼€å¿ƒ', 'å¿«ä¹', 'å…´å¥‹', 'æ¿€åŠ¨', 'æƒŠå–œ', 'æ„Ÿè°¢',
                'èµ', 'æ¨è', 'æ”¯æŒ', 'è®¤åŒ', 'ä¼˜è´¨', 'ç²¾å½©', 'ç¾å¥½'
            },
            'negative': {
                'å·®', 'ç³Ÿ', 'çƒ‚', 'åƒåœ¾', 'å¤±æœ›', 'ä¸æ»¡', 'è®¨åŒ', 'æ¨',
                'éš¾è¿‡', 'ä¼¤å¿ƒ', 'æ„¤æ€’', 'ç”Ÿæ°”', 'æŠ±æ€¨', 'æ‰¹è¯„', 'åå¯¹',
                'æ‹’ç»', 'é—®é¢˜', 'é”™è¯¯', 'å¤±è´¥', 'ç³Ÿç³•', 'æ¶å¿ƒ', 'æ— èŠ'
            },
            'neutral': {
                'ä¸€èˆ¬', 'æ™®é€š', 'æ­£å¸¸', 'å¯ä»¥', 'è¿˜è¡Œ', 'ä¸­ç­‰', 'å¹³å¸¸',
                'åŸºæœ¬', 'æ ‡å‡†', 'å¸¸è§„', 'ä¼ ç»Ÿ', 'å…¸å‹', 'ç®€å•', 'å¤æ‚'
            }
        }
        
        print("ğŸ“š æƒ…æ„Ÿè¯å…¸æ„å»ºå®Œæˆ")
        return emotion_lexicon
    
    def lexicon_based_sentiment(self, text: str) -> Dict:
        """åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æ"""
        # åˆ†è¯
        tokens = list(jieba.cut(text))
        
        # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
        sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for token in tokens:
            for sentiment, words in self.emotion_lexicon.items():
                if token in words:
                    sentiment_scores[sentiment] += 1
        
        # è®¡ç®—æ€»ä½“æƒ…æ„Ÿ
        total_score = sum(sentiment_scores.values())
        if total_score == 0:
            sentiment_scores['neutral'] = 1
            total_score = 1
        
        # å½’ä¸€åŒ–å¾—åˆ†
        normalized_scores = {k: v/total_score for k, v in sentiment_scores.items()}
        
        # ç¡®å®šä¸»è¦æƒ…æ„Ÿ
        main_sentiment = max(normalized_scores, key=normalized_scores.get)
        confidence = normalized_scores[main_sentiment]
        
        return {
            'sentiment': main_sentiment,
            'confidence': confidence,
            'scores': normalized_scores,
            'sentiment_words': self._find_sentiment_words(tokens)
        }
    
    def _find_sentiment_words(self, tokens: List[str]) -> Dict:
        """æ‰¾å‡ºæ–‡æœ¬ä¸­çš„æƒ…æ„Ÿè¯"""
        sentiment_words = {'positive': [], 'negative': [], 'neutral': []}
        
        for token in tokens:
            for sentiment, words in self.emotion_lexicon.items():
                if token in words:
                    sentiment_words[sentiment].append(token)
        
        return sentiment_words
    
    def prepare_training_data(self, texts: List[str], labels: List[str]) -> Tuple:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print(f"ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®: {len(texts)} ä¸ªæ ·æœ¬")
        
        # æ–‡æœ¬é¢„å¤„ç†
        processed_texts = []
        for text in texts:
            # æ¸…æ´—æ–‡æœ¬
            cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
            # åˆ†è¯
            tokens = jieba.cut(cleaned)
            processed_text = ' '.join(tokens)
            processed_texts.append(processed_text)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            processed_texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ - è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")
        return X_train, X_test, y_train, y_test
    
    def train_sentiment_models(self, X_train: List[str], y_train: List[str]) -> Dict:
        """è®­ç»ƒå¤šç§æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        
        # ç‰¹å¾æå–
        print("  ğŸ“Š æå–TF-IDFç‰¹å¾...")
        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
        self.vectorizers['tfidf'] = tfidf_vectorizer
        
        print("  ğŸ“ˆ æå–è¯è¢‹ç‰¹å¾...")
        count_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))
        X_train_count = count_vectorizer.fit_transform(X_train)
        self.vectorizers['count'] = count_vectorizer
        
        # è®­ç»ƒå¤šç§æ¨¡å‹
        models_config = {
            'logistic_tfidf': (LogisticRegression(random_state=42), X_train_tfidf),
            'naive_bayes_count': (MultinomialNB(), X_train_count),
            'svm_tfidf': (SVC(kernel='linear', random_state=42), X_train_tfidf),
            'random_forest_tfidf': (RandomForestClassifier(n_estimators=100, random_state=42), X_train_tfidf)
        }
        
        training_results = {}
        
        for model_name, (model, X_train_features) in models_config.items():
            print(f"  ğŸ”§ è®­ç»ƒ {model_name}...")
            model.fit(X_train_features, y_train)
            self.models[model_name] = model
            
            # è®­ç»ƒé›†å‡†ç¡®ç‡
            train_pred = model.predict(X_train_features)
            train_accuracy = accuracy_score(y_train, train_pred)
            training_results[model_name] = train_accuracy
            
            print(f"    âœ… {model_name} è®­ç»ƒå®Œæˆï¼Œè®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
        
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return training_results
    
    def evaluate_models(self, X_test: List[str], y_test: List[str]) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        evaluation_results = {}
        
        for model_name, model in self.models.items():
            print(f"  ğŸ” è¯„ä¼° {model_name}...")
            
            # é€‰æ‹©å¯¹åº”çš„ç‰¹å¾æå–å™¨
            if 'tfidf' in model_name:
                X_test_features = self.vectorizers['tfidf'].transform(X_test)
            else:
                X_test_features = self.vectorizers['count'].transform(X_test)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test_features)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, output_dict=True)
            
            evaluation_results[model_name] = {
                'accuracy': accuracy,
                'precision': report['weighted avg']['precision'],
                'recall': report['weighted avg']['recall'],
                'f1_score': report['weighted avg']['f1-score'],
                'predictions': y_pred
            }
            
            print(f"    âœ… å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"    âœ… F1å¾—åˆ†: {report['weighted avg']['f1-score']:.4f}")
        
        return evaluation_results
    
    def predict_sentiment(self, text: str, model_name: str = 'logistic_tfidf') -> Dict:
        """é¢„æµ‹å•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        
        # é¢„å¤„ç†æ–‡æœ¬
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        tokens = jieba.cut(cleaned)
        processed_text = ' '.join(tokens)
        
        # ç‰¹å¾æå–
        if 'tfidf' in model_name:
            features = self.vectorizers['tfidf'].transform([processed_text])
        else:
            features = self.vectorizers['count'].transform([processed_text])
        
        # é¢„æµ‹
        model = self.models[model_name]
        prediction = model.predict(features)[0]
        
        # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        if hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(features)[0]
            prob_dict = {label: prob for label, prob in zip(model.classes_, probabilities)}
        else:
            prob_dict = {prediction: 1.0}
        
        # è¯å…¸æ–¹æ³•ä½œä¸ºè¡¥å……
        lexicon_result = self.lexicon_based_sentiment(text)
        
        return {
            'text': text,
            'predicted_sentiment': prediction,
            'confidence': max(prob_dict.values()),
            'probabilities': prob_dict,
            'lexicon_sentiment': lexicon_result['sentiment'],
            'lexicon_confidence': lexicon_result['confidence'],
            'sentiment_words': lexicon_result['sentiment_words']
        }

class TextClassificationWorkshop:
    """æ–‡æœ¬åˆ†ç±»å·¥ä½œåŠ"""
    
    def __init__(self):
        self.classifiers = {}
        self.feature_extractors = {}
        self.classification_history = []
        
        print("ğŸ“ æ–‡æœ¬åˆ†ç±»å·¥ä½œåŠåˆå§‹åŒ–å®Œæˆ")
        self._show_classification_types()
    
    def _show_classification_types(self):
        """å±•ç¤ºåˆ†ç±»ç±»å‹"""
        classification_types = {
            "ä¸»é¢˜åˆ†ç±»": "æ–°é—»åˆ†ç±»ã€æ–‡æ¡£å½’ç±»ã€å†…å®¹æ ‡ç­¾",
            "æƒ…æ„Ÿåˆ†ç±»": "æ­£è´Ÿé¢è¯„ä»·ã€æƒ…æ„Ÿå€¾å‘åˆ†æ",
            "æ„å›¾åˆ†ç±»": "ç”¨æˆ·æ„å›¾è¯†åˆ«ã€æŸ¥è¯¢åˆ†ç±»",
            "åƒåœ¾é‚®ä»¶æ£€æµ‹": "åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€å†…å®¹å®¡æ ¸",
            "è¯­è¨€æ£€æµ‹": "å¤šè¯­è¨€æ–‡æœ¬çš„è¯­ç§è¯†åˆ«",
            "æ–‡æ¡£ç±»å‹åˆ†ç±»": "åˆåŒã€æŠ¥å‘Šã€æ–°é—»ç­‰æ–‡æ¡£ç±»å‹"
        }
        
        print(f"\nğŸ“‹ æ”¯æŒçš„åˆ†ç±»ç±»å‹:")
        for cls_type, description in classification_types.items():
            print(f"  â€¢ {cls_type}: {description}")
    
    def build_multi_class_classifier(self, texts: List[str], labels: List[str], 
                                   classifier_name: str = 'news_classifier') -> Dict:
        """æ„å»ºå¤šåˆ†ç±»å™¨"""
        print(f"\nğŸ”§ æ„å»ºå¤šåˆ†ç±»å™¨: {classifier_name}")
        
        # æ•°æ®é¢„å¤„ç†
        processed_texts = []
        for text in texts:
            cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
            tokens = jieba.cut(cleaned)
            processed_text = ' '.join(tokens)
            processed_texts.append(processed_text)
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            processed_texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # ç‰¹å¾æå–
        vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95
        )
        
        X_train_features = vectorizer.fit_transform(X_train)
        X_test_features = vectorizer.transform(X_test)
        
        # è®­ç»ƒå¤šç§åˆ†ç±»å™¨
        classifiers = {
            'logistic': LogisticRegression(random_state=42, max_iter=1000),
            'naive_bayes': MultinomialNB(),
            'svm': SVC(kernel='linear', random_state=42),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42)
        }
        
        results = {}
        best_model = None
        best_accuracy = 0
        
        for clf_name, classifier in classifiers.items():
            print(f"  ğŸš€ è®­ç»ƒ {clf_name}...")
            
            # è®­ç»ƒ
            classifier.fit(X_train_features, y_train)
            
            # é¢„æµ‹
            y_pred = classifier.predict(X_test_features)
            
            # è¯„ä¼°
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, output_dict=True)
            
            results[clf_name] = {
                'model': classifier,
                'accuracy': accuracy,
                'classification_report': report,
                'predictions': y_pred
            }
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = clf_name
            
            print(f"    âœ… {clf_name} å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œç‰¹å¾æå–å™¨
        self.classifiers[classifier_name] = {
            'best_model': results[best_model]['model'],
            'vectorizer': vectorizer,
            'model_name': best_model,
            'accuracy': best_accuracy,
            'all_results': results
        }
        
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model} (å‡†ç¡®ç‡: {best_accuracy:.4f})")
        
        return results
    
    def classify_text(self, text: str, classifier_name: str = 'news_classifier') -> Dict:
        """åˆ†ç±»å•ä¸ªæ–‡æœ¬"""
        if classifier_name not in self.classifiers:
            raise ValueError(f"åˆ†ç±»å™¨ {classifier_name} ä¸å­˜åœ¨")
        
        classifier_info = self.classifiers[classifier_name]
        model = classifier_info['best_model']
        vectorizer = classifier_info['vectorizer']
        
        # é¢„å¤„ç†
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        tokens = jieba.cut(cleaned)
        processed_text = ' '.join(tokens)
        
        # ç‰¹å¾æå–
        features = vectorizer.transform([processed_text])
        
        # é¢„æµ‹
        prediction = model.predict(features)[0]
        
        # è·å–é¢„æµ‹æ¦‚ç‡
        if hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(features)[0]
            prob_dict = {label: prob for label, prob in zip(model.classes_, probabilities)}
        else:
            prob_dict = {prediction: 1.0}
        
        return {
            'text': text,
            'predicted_class': prediction,
            'confidence': max(prob_dict.values()),
            'probabilities': prob_dict,
            'classifier_used': classifier_info['model_name']
        }

class SocialMediaSentimentMonitor:
    """ç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalysisLab()
        self.text_classifier = TextClassificationWorkshop()
        self.monitoring_data = []
        self.alerts = []
        
        print("ğŸ“± ç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self._setup_monitoring_system()
    
    def _setup_monitoring_system(self):
        """è®¾ç½®ç›‘æ§ç³»ç»Ÿ"""
        # å‡†å¤‡ç¤ºä¾‹è®­ç»ƒæ•°æ®
        self._prepare_sample_data()
        
        print("ğŸ”§ é…ç½®ç›‘æ§å‚æ•°...")
        self.monitoring_config = {
            'negative_threshold': 0.7,  # è´Ÿé¢æƒ…æ„Ÿé˜ˆå€¼
            'alert_keywords': ['æŠ•è¯‰', 'é—®é¢˜', 'æ•…éšœ', 'ä¸æ»¡', 'å·®è¯„'],
            'monitoring_interval': 60,  # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
            'batch_size': 100  # æ‰¹å¤„ç†å¤§å°
        }
        
        print("âœ… ç›‘æ§ç³»ç»Ÿé…ç½®å®Œæˆ")
    
    def _prepare_sample_data(self):
        """å‡†å¤‡ç¤ºä¾‹è®­ç»ƒæ•°æ®"""
        # æƒ…æ„Ÿåˆ†æè®­ç»ƒæ•°æ®
        sentiment_texts = [
            "è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼Œæ¨èå¤§å®¶è´­ä¹°", "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œéå¸¸ä¸æ»¡æ„",
            "è´¨é‡ä¸é”™ï¼Œä»·æ ¼åˆç†", "ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†å¾ˆä¹…", 
            "å®¢æœå›å¤å¾ˆåŠæ—¶ï¼Œè§£å†³äº†æˆ‘çš„é—®é¢˜", "äº§å“æœ‰è´¨é‡é—®é¢˜ï¼Œè¦æ±‚é€€æ¬¾",
            "åŒ…è£…å¾ˆç²¾ç¾ï¼Œäº§å“è´¨é‡ä¹Ÿå¾ˆå¥½", "ç½‘ç«™ç»å¸¸å‡ºæ•…éšœï¼Œä½“éªŒå¾ˆå·®",
            "è¿™æ¬¡è´­ç‰©ä½“éªŒå¾ˆæ„‰å¿«", "äº§å“æè¿°ä¸å®é™…ä¸ç¬¦ï¼Œæ„Ÿè§‰è¢«éª—äº†"
        ]
        
        sentiment_labels = [
            "positive", "negative", "positive", "negative", "positive",
            "negative", "positive", "negative", "positive", "negative"
        ]
        
        # è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹
        X_train, X_test, y_train, y_test = self.sentiment_analyzer.prepare_training_data(
            sentiment_texts, sentiment_labels
        )
        
        self.sentiment_analyzer.train_sentiment_models(X_train, y_train)
        self.sentiment_analyzer.evaluate_models(X_test, y_test)
    
    def analyze_social_media_post(self, post_text: str, platform: str = "weibo", 
                                user_id: str = None) -> Dict:
        """åˆ†æç¤¾äº¤åª’ä½“å¸–å­"""
        # æƒ…æ„Ÿåˆ†æ
        sentiment_result = self.sentiment_analyzer.predict_sentiment(post_text)
        
        # å…³é”®è¯æ£€æµ‹
        alert_triggered = any(keyword in post_text for keyword in self.monitoring_config['alert_keywords'])
        
        # é£é™©è¯„ä¼°
        risk_level = self._assess_risk_level(sentiment_result, alert_triggered)
        
        analysis_result = {
            'timestamp': pd.Timestamp.now(),
            'platform': platform,
            'user_id': user_id,
            'post_text': post_text,
            'sentiment': sentiment_result['predicted_sentiment'],
            'sentiment_confidence': sentiment_result['confidence'],
            'sentiment_probabilities': sentiment_result['probabilities'],
            'alert_keywords_found': [kw for kw in self.monitoring_config['alert_keywords'] if kw in post_text],
            'alert_triggered': alert_triggered,
            'risk_level': risk_level,
            'sentiment_words': sentiment_result['sentiment_words']
        }
        
        # ä¿å­˜ç›‘æ§æ•°æ®
        self.monitoring_data.append(analysis_result)
        
        # è§¦å‘è­¦æŠ¥
        if alert_triggered or risk_level == 'high':
            self._trigger_alert(analysis_result)
        
        return analysis_result
    
    def _assess_risk_level(self, sentiment_result: Dict, alert_triggered: bool) -> str:
        """è¯„ä¼°é£é™©ç­‰çº§"""
        if alert_triggered:
            return 'high'
        
        if (sentiment_result['predicted_sentiment'] == 'negative' and 
            sentiment_result['confidence'] > self.monitoring_config['negative_threshold']):
            return 'medium'
        
        return 'low'
    
    def _trigger_alert(self, analysis_result: Dict):
        """è§¦å‘è­¦æŠ¥"""
        alert = {
            'timestamp': analysis_result['timestamp'],
            'alert_type': 'negative_sentiment' if analysis_result['risk_level'] == 'high' else 'keyword_alert',
            'platform': analysis_result['platform'],
            'user_id': analysis_result['user_id'],
            'post_text': analysis_result['post_text'][:100] + '...',
            'risk_level': analysis_result['risk_level'],
            'sentiment': analysis_result['sentiment'],
            'confidence': analysis_result['sentiment_confidence']
        }
        
        self.alerts.append(alert)
        print(f"ğŸš¨ è§¦å‘è­¦æŠ¥: {alert['alert_type']} - é£é™©ç­‰çº§: {alert['risk_level']}")
    
    def batch_analyze_posts(self, posts_data: List[Dict]) -> List[Dict]:
        """æ‰¹é‡åˆ†æå¸–å­"""
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æ {len(posts_data)} æ¡å¸–å­...")
        
        results = []
        for i, post_data in enumerate(posts_data):
            if i % 10 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(posts_data)}")
            
            result = self.analyze_social_media_post(
                post_data['text'], 
                post_data.get('platform', 'unknown'),
                post_data.get('user_id', f'user_{i}')
            )
            results.append(result)
        
        print(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ")
        self._generate_monitoring_report()
        
        return results
    
    def _generate_monitoring_report(self):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        if not self.monitoring_data:
            print("ğŸ“Š æš‚æ— ç›‘æ§æ•°æ®")
            return
        
        df = pd.DataFrame(self.monitoring_data)
        
        print(f"\nğŸ“ˆ æƒ…æ„Ÿç›‘æ§æŠ¥å‘Š")
        print("=" * 25)
        print(f"ğŸ“± ç›‘æ§å¸–å­æ€»æ•°: {len(df)}")
        print(f"ğŸš¨ è§¦å‘è­¦æŠ¥æ•°é‡: {len(self.alerts)}")
        
        # æƒ…æ„Ÿåˆ†å¸ƒ
        sentiment_dist = df['sentiment'].value_counts()
        print(f"\nğŸ’­ æƒ…æ„Ÿåˆ†å¸ƒ:")
        for sentiment, count in sentiment_dist.items():
            percentage = count / len(df) * 100
            print(f"  â€¢ {sentiment}: {count} ({percentage:.1f}%)")
        
        # é£é™©ç­‰çº§åˆ†å¸ƒ
        risk_dist = df['risk_level'].value_counts()
        print(f"\nâš ï¸ é£é™©ç­‰çº§åˆ†å¸ƒ:")
        for risk, count in risk_dist.items():
            percentage = count / len(df) * 100
            print(f"  â€¢ {risk}: {count} ({percentage:.1f}%)")
        
        # å¹³å°åˆ†å¸ƒ
        platform_dist = df['platform'].value_counts()
        print(f"\nğŸ“± å¹³å°åˆ†å¸ƒ:")
        for platform, count in platform_dist.items():
            percentage = count / len(df) * 100
            print(f"  â€¢ {platform}: {count} ({percentage:.1f}%)")
        
        # æœ€è¿‘è­¦æŠ¥
        if self.alerts:
            print(f"\nğŸš¨ æœ€è¿‘è­¦æŠ¥:")
            for alert in self.alerts[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¡è­¦æŠ¥
                print(f"  â€¢ {alert['timestamp'].strftime('%H:%M:%S')} - {alert['alert_type']} - {alert['post_text']}")

# æ¼”ç¤ºç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æ§ç³»ç»Ÿ
def demo_sentiment_monitoring():
    """æ¼”ç¤ºæƒ…æ„Ÿç›‘æ§ç³»ç»Ÿ"""
    # åˆ›å»ºç›‘æ§ç³»ç»Ÿ
    monitor = SocialMediaSentimentMonitor()
    
    # æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“æ•°æ®
    sample_posts = [
        {'text': 'è¿™å®¶é¤å…çš„æœåŠ¡çœŸçš„å¾ˆæ£’ï¼Œé£Ÿç‰©ä¹Ÿå¾ˆç¾å‘³ï¼Œå¼ºçƒˆæ¨èï¼', 'platform': 'weibo'},
        {'text': 'ç½‘ç«™åˆå´©æºƒäº†ï¼Œè¿™æ˜¯è¿™ä¸ªæœˆç¬¬ä¸‰æ¬¡äº†ï¼Œå¤ªè®©äººå¤±æœ›äº†', 'platform': 'weibo'},
        {'text': 'å®¢æœæ€åº¦å¾ˆå¥½ï¼Œå¸®æˆ‘è§£å†³äº†é—®é¢˜ï¼Œè°¢è°¢', 'platform': 'douyin'},
        {'text': 'äº§å“è´¨é‡æœ‰é—®é¢˜ï¼Œè¦æ±‚é€€æ¬¾ä½†æ˜¯ä¸€ç›´æ²¡æœ‰å›å¤', 'platform': 'xiaohongshu'},
        {'text': 'ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆå¥½', 'platform': 'weibo'},
        {'text': 'è¿™ä¸ªAPPç»å¸¸é—ªé€€ï¼Œç”¨æˆ·ä½“éªŒå¾ˆå·®ï¼Œå»ºè®®ä¿®å¤', 'platform': 'appstore'},
        {'text': 'ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿå¾ˆå¥½ï¼Œæ»¡æ„', 'platform': 'taobao'},
        {'text': 'æŠ•è¯‰å¤šæ¬¡éƒ½æ²¡æœ‰å¾—åˆ°è§£å†³ï¼Œéå¸¸ä¸æ»¡', 'platform': 'weibo'},
        {'text': 'æ–°åŠŸèƒ½å¾ˆå®ç”¨ï¼Œç•Œé¢è®¾è®¡ä¹Ÿå¾ˆç¾è§‚', 'platform': 'weibo'},
        {'text': 'ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†æ˜¯è´¨é‡è¿˜å¯ä»¥', 'platform': 'jd'}
    ]
    
    # æ‰¹é‡åˆ†æ
    results = monitor.batch_analyze_posts(sample_posts)
    
    return results

# è¿è¡Œæ¼”ç¤º
monitoring_results = demo_sentiment_monitoring()
```

### ğŸ“ˆ æƒ…æ„Ÿåˆ†ææŠ€æœ¯å¯¹æ¯”

è®©æˆ‘ä»¬é€šè¿‡å›¾è¡¨æ¥ç†è§£ä¸åŒæƒ…æ„Ÿåˆ†ææ–¹æ³•çš„ç‰¹ç‚¹ï¼š

```mermaid
graph TB
    A[æƒ…æ„Ÿåˆ†ææ–¹æ³•] --> B[åŸºäºè¯å…¸çš„æ–¹æ³•]
    A --> C[åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•]
    A --> D[åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•]
    
    B --> B1[ä¼˜ç‚¹: å¯è§£é‡Šæ€§å¼º<br/>æ— éœ€è®­ç»ƒæ•°æ®<br/>å¤„ç†é€Ÿåº¦å¿«]
    B --> B2[ç¼ºç‚¹: è¦†ç›–é¢æœ‰é™<br/>æ— æ³•å¤„ç†å¤æ‚è¯­å¢ƒ<br/>é¢†åŸŸé€‚åº”æ€§å·®]
    
    C --> C1[ä¼˜ç‚¹: æ•ˆæœç¨³å®š<br/>ç‰¹å¾å·¥ç¨‹çµæ´»<br/>è®­ç»ƒæ—¶é—´çŸ­]
    C --> C2[ç¼ºç‚¹: éœ€è¦äººå·¥ç‰¹å¾<br/>æ³›åŒ–èƒ½åŠ›æœ‰é™<br/>ä¸Šä¸‹æ–‡ç†è§£å¼±]
    
    D --> D1[ä¼˜ç‚¹: è‡ªåŠ¨ç‰¹å¾å­¦ä¹ <br/>ä¸Šä¸‹æ–‡ç†è§£å¼º<br/>æ³›åŒ–èƒ½åŠ›å¥½]
    D --> D2[ç¼ºç‚¹: éœ€è¦å¤§é‡æ•°æ®<br/>è®­ç»ƒæ—¶é—´é•¿<br/>å¯è§£é‡Šæ€§å·®]
```

## 32.4 æœºå™¨ç¿»è¯‘æŠ€æœ¯è¯¦è§£

### ğŸŒ æœºå™¨ç¿»è¯‘ä¸­å¿ƒï¼šè·¨è¶Šè¯­è¨€çš„æ¡¥æ¢

åœ¨æˆ‘ä»¬çš„**è¯­è¨€ç†è§£ç ”ç©¶é™¢**ä¸­ï¼Œ**æœºå™¨ç¿»è¯‘ä¸­å¿ƒ**æ‰¿æ‹…ç€æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡â€”â€”è®©ä¸åŒè¯­è¨€ä¹‹é—´å®ç°æ— éšœç¢æ²Ÿé€šã€‚è¿™é‡Œå°±åƒè”åˆå›½çš„åŒå£°ä¼ è¯‘ä¸­å¿ƒï¼Œä½†æˆ‘ä»¬çš„"ç¿»è¯‘å®˜"æ˜¯AIç³»ç»Ÿï¼Œå®ƒä»¬è¦åœ¨æ¯«ç§’ä¹‹é—´å®Œæˆè¯­è¨€çš„è½¬æ¢ï¼ŒåŒæ—¶ä¿æŒæ„æ€çš„å‡†ç¡®æ€§å’Œè¡¨è¾¾çš„è‡ªç„¶æ€§ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import random
import math
import time
from typing import List, Dict, Tuple, Optional
import jieba
import re

class MachineTranslationCenter:
    """æœºå™¨ç¿»è¯‘ä¸­å¿ƒ"""
    
    def __init__(self):
        self.translation_models = {}
        self.language_pairs = {}
        self.translation_history = []
        
        print("ğŸŒ æœºå™¨ç¿»è¯‘ä¸­å¿ƒåˆå§‹åŒ–å®Œæˆ")
        self._show_translation_capabilities()
    
    def _show_translation_capabilities(self):
        """å±•ç¤ºç¿»è¯‘èƒ½åŠ›"""
        capabilities = {
            "ç»Ÿè®¡æœºå™¨ç¿»è¯‘": "åŸºäºçŸ­è¯­çš„ç»Ÿè®¡ç¿»è¯‘æ¨¡å‹",
            "ç¥ç»æœºå™¨ç¿»è¯‘": "åŸºäºRNN/LSTMçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹",
            "æ³¨æ„åŠ›æœºåˆ¶ç¿»è¯‘": "é›†æˆæ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘æ¨¡å‹",
            "Transformerç¿»è¯‘": "åŸºäºTransformeræ¶æ„çš„ç¿»è¯‘",
            "å¤šè¯­è¨€ç¿»è¯‘": "æ”¯æŒå¤šç§è¯­è¨€å¯¹çš„ç¿»è¯‘",
            "é›¶æ ·æœ¬ç¿»è¯‘": "æ— éœ€ç›´æ¥è®­ç»ƒçš„è¯­è¨€å¯¹ç¿»è¯‘"
        }
        
        print(f"\nğŸ”¬ ç¿»è¯‘ä¸­å¿ƒæ ¸å¿ƒèƒ½åŠ›:")
        for capability, description in capabilities.items():
            print(f"  â€¢ {capability}: {description}")
    
    def prepare_parallel_corpus(self, source_texts: List[str], target_texts: List[str], 
                               source_lang: str, target_lang: str) -> Dict:
        """å‡†å¤‡å¹³è¡Œè¯­æ–™åº“"""
        print(f"ğŸ“š å‡†å¤‡ {source_lang}-{target_lang} å¹³è¡Œè¯­æ–™åº“...")
        
        if len(source_texts) != len(target_texts):
            raise ValueError("æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ–‡æœ¬æ•°é‡ä¸åŒ¹é…")
        
        # æ–‡æœ¬é¢„å¤„ç†
        processed_source = []
        processed_target = []
        
        for src, tgt in zip(source_texts, target_texts):
            # æºè¯­è¨€é¢„å¤„ç†
            if source_lang == 'zh':
                src_tokens = list(jieba.cut(src.strip()))
            else:
                src_tokens = src.strip().lower().split()
            
            # ç›®æ ‡è¯­è¨€é¢„å¤„ç†
            if target_lang == 'zh':
                tgt_tokens = list(jieba.cut(tgt.strip()))
            else:
                tgt_tokens = tgt.strip().lower().split()
            
            processed_source.append(src_tokens)
            processed_target.append(tgt_tokens)
        
        # æ„å»ºè¯æ±‡è¡¨
        source_vocab = self._build_vocabulary(processed_source, f"{source_lang}_vocab")
        target_vocab = self._build_vocabulary(processed_target, f"{target_lang}_vocab")
        
        corpus_data = {
            'source_texts': processed_source,
            'target_texts': processed_target,
            'source_vocab': source_vocab,
            'target_vocab': target_vocab,
            'source_lang': source_lang,
            'target_lang': target_lang,
            'corpus_size': len(source_texts)
        }
        
        language_pair = f"{source_lang}-{target_lang}"
        self.language_pairs[language_pair] = corpus_data
        
        print(f"âœ… è¯­æ–™åº“å‡†å¤‡å®Œæˆ:")
        print(f"  â€¢ è¯­æ–™å¯¹æ•°: {len(source_texts)}")
        print(f"  â€¢ æºè¯­è¨€è¯æ±‡é‡: {len(source_vocab)}")
        print(f"  â€¢ ç›®æ ‡è¯­è¨€è¯æ±‡é‡: {len(target_vocab)}")
        
        return corpus_data
    
    def _build_vocabulary(self, tokenized_texts: List[List[str]], vocab_name: str) -> Dict:
        """æ„å»ºè¯æ±‡è¡¨"""
        word_counts = Counter()
        for tokens in tokenized_texts:
            word_counts.update(tokens)
        
        # åˆ›å»ºè¯æ±‡è¡¨ (ä¿ç•™é«˜é¢‘è¯)
        vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}
        
        # æ·»åŠ å¸¸è§è¯æ±‡
        for word, count in word_counts.most_common(5000):  # é™åˆ¶è¯æ±‡è¡¨å¤§å°
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                vocab[word] = len(vocab)
        
        print(f"ğŸ“– æ„å»ºè¯æ±‡è¡¨ {vocab_name}: {len(vocab)} ä¸ªè¯æ±‡")
        return vocab
    
    def text_to_indices(self, tokens: List[str], vocab: Dict) -> List[int]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—"""
        indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]
        return indices
    
    def indices_to_text(self, indices: List[int], vocab: Dict) -> List[str]:
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬"""
        reverse_vocab = {v: k for k, v in vocab.items()}
        tokens = [reverse_vocab.get(idx, '<UNK>') for idx in indices]
        return tokens

class Seq2SeqTranslator(nn.Module):
    """åºåˆ—åˆ°åºåˆ—ç¿»è¯‘æ¨¡å‹"""
    
    def __init__(self, source_vocab_size: int, target_vocab_size: int, 
                 embedding_dim: int = 256, hidden_dim: int = 512, num_layers: int = 2):
        super(Seq2SeqTranslator, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # ç¼–ç å™¨
        self.encoder_embedding = nn.Embedding(source_vocab_size, embedding_dim)
        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                                   batch_first=True, dropout=0.1)
        
        # è§£ç å™¨
        self.decoder_embedding = nn.Embedding(target_vocab_size, embedding_dim)
        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                                   batch_first=True, dropout=0.1)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(hidden_dim, target_vocab_size)
        self.dropout = nn.Dropout(0.1)
        
        print(f"ğŸ§  Seq2Seqç¿»è¯‘æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"  â€¢ æºè¯æ±‡é‡: {source_vocab_size}")
        print(f"  â€¢ ç›®æ ‡è¯æ±‡é‡: {target_vocab_size}")
        print(f"  â€¢ åµŒå…¥ç»´åº¦: {embedding_dim}")
        print(f"  â€¢ éšè—å±‚ç»´åº¦: {hidden_dim}")
    
    def encode(self, source_seq):
        """ç¼–ç å™¨"""
        embedded = self.encoder_embedding(source_seq)
        encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded)
        return encoder_outputs, (hidden, cell)
    
    def decode(self, target_seq, encoder_states):
        """è§£ç å™¨"""
        embedded = self.decoder_embedding(target_seq)
        decoder_outputs, _ = self.decoder_lstm(embedded, encoder_states)
        output_logits = self.output_projection(decoder_outputs)
        return output_logits
    
    def forward(self, source_seq, target_seq):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç 
        encoder_outputs, encoder_states = self.encode(source_seq)
        
        # è§£ç 
        decoder_outputs = self.decode(target_seq, encoder_states)
        
        return decoder_outputs

class AttentionTranslator(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘æ¨¡å‹"""
    
    def __init__(self, source_vocab_size: int, target_vocab_size: int,
                 embedding_dim: int = 256, hidden_dim: int = 512, num_layers: int = 2):
        super(AttentionTranslator, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # ç¼–ç å™¨
        self.encoder_embedding = nn.Embedding(source_vocab_size, embedding_dim)
        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                                   batch_first=True, bidirectional=True, dropout=0.1)
        
        # è§£ç å™¨
        self.decoder_embedding = nn.Embedding(target_vocab_size, embedding_dim)
        self.decoder_lstm = nn.LSTM(embedding_dim + hidden_dim * 2, hidden_dim, num_layers,
                                   batch_first=True, dropout=0.1)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Linear(hidden_dim * 3, hidden_dim)
        self.attention_combine = nn.Linear(hidden_dim * 2, hidden_dim)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(hidden_dim, target_vocab_size)
        
        print(f"ğŸ¯ æ³¨æ„åŠ›ç¿»è¯‘æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def attention_mechanism(self, decoder_hidden, encoder_outputs):
        """æ³¨æ„åŠ›æœºåˆ¶"""
        batch_size, seq_len, hidden_size = encoder_outputs.size()
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand(batch_size, seq_len, -1)
        attention_weights = torch.tanh(self.attention(
            torch.cat([decoder_hidden_expanded, encoder_outputs], dim=2)
        ))
        attention_weights = torch.sum(attention_weights, dim=2)
        attention_weights = F.softmax(attention_weights, dim=1)
        
        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)
        
        return context, attention_weights
    
    def forward(self, source_seq, target_seq):
        """å‰å‘ä¼ æ’­"""
        batch_size = source_seq.size(0)
        
        # ç¼–ç 
        source_embedded = self.encoder_embedding(source_seq)
        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder_lstm(source_embedded)
        
        # åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€
        decoder_hidden = encoder_hidden[-1].unsqueeze(0)
        decoder_cell = encoder_cell[-1].unsqueeze(0)
        
        # è§£ç 
        target_embedded = self.decoder_embedding(target_seq)
        decoder_outputs = []
        
        for t in range(target_seq.size(1)):
            # æ³¨æ„åŠ›æœºåˆ¶
            context, attention_weights = self.attention_mechanism(
                decoder_hidden[-1], encoder_outputs
            )
            
            # è§£ç å™¨è¾“å…¥
            decoder_input = torch.cat([target_embedded[:, t:t+1, :], 
                                     context.unsqueeze(1)], dim=2)
            
            # LSTMè§£ç 
            decoder_output, (decoder_hidden, decoder_cell) = self.decoder_lstm(
                decoder_input, (decoder_hidden, decoder_cell)
            )
            
            decoder_outputs.append(decoder_output)
        
        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        output_logits = self.output_projection(decoder_outputs)
        
        return output_logits

class TranslationTrainer:
    """ç¿»è¯‘æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, source_vocab, target_vocab):
        self.model = model
        self.source_vocab = source_vocab
        self.target_vocab = target_vocab
        self.reverse_target_vocab = {v: k for k, v in target_vocab.items()}
        
        # è®­ç»ƒå‚æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥PAD
        self.optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        print("ğŸ‹ï¸ ç¿»è¯‘æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_batch_data(self, source_texts, target_texts, batch_size=32):
        """å‡†å¤‡æ‰¹é‡è®­ç»ƒæ•°æ®"""
        dataset = []
        
        for src_tokens, tgt_tokens in zip(source_texts, target_texts):
            # è½¬æ¢ä¸ºç´¢å¼•
            src_indices = [self.source_vocab.get(token, self.source_vocab['<UNK>']) 
                          for token in src_tokens]
            tgt_indices = [self.target_vocab['<SOS>']] + \
                         [self.target_vocab.get(token, self.target_vocab['<UNK>']) 
                          for token in tgt_tokens] + \
                         [self.target_vocab['<EOS>']]
            
            dataset.append((src_indices, tgt_indices))
        
        # æ‰¹é‡å¤„ç†
        batches = []
        for i in range(0, len(dataset), batch_size):
            batch = dataset[i:i+batch_size]
            
            # å¡«å……åºåˆ—
            max_src_len = max(len(item[0]) for item in batch)
            max_tgt_len = max(len(item[1]) for item in batch)
            
            src_batch = []
            tgt_batch = []
            
            for src_indices, tgt_indices in batch:
                # å¡«å……æºåºåˆ—
                src_padded = src_indices + [0] * (max_src_len - len(src_indices))
                src_batch.append(src_padded)
                
                # å¡«å……ç›®æ ‡åºåˆ—
                tgt_padded = tgt_indices + [0] * (max_tgt_len - len(tgt_indices))
                tgt_batch.append(tgt_padded)
            
            batches.append((torch.tensor(src_batch), torch.tensor(tgt_batch)))
        
        return batches
    
    def train_model(self, source_texts, target_texts, epochs=10, batch_size=32):
        """è®­ç»ƒç¿»è¯‘æ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒç¿»è¯‘æ¨¡å‹...")
        print(f"  â€¢ è®­ç»ƒæ ·æœ¬: {len(source_texts)}")
        print(f"  â€¢ è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        batches = self.prepare_batch_data(source_texts, target_texts, batch_size)
        
        training_losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            self.model.train()
            
            for batch_idx, (src_batch, tgt_batch) in enumerate(batches):
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                # è§£ç å™¨è¾“å…¥ï¼ˆå»æ‰æœ€åä¸€ä¸ªtokenï¼‰
                decoder_input = tgt_batch[:, :-1]
                # ç›®æ ‡è¾“å‡ºï¼ˆå»æ‰ç¬¬ä¸€ä¸ªtokenï¼‰
                target_output = tgt_batch[:, 1:]
                
                # æ¨¡å‹é¢„æµ‹
                predictions = self.model(src_batch, decoder_input)
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(
                    predictions.reshape(-1, predictions.size(-1)),
                    target_output.reshape(-1)
                )
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
                
                if batch_idx % 10 == 0:
                    print(f"    Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(batches)}, Loss: {loss.item():.4f}")
            
            avg_loss = epoch_loss / len(batches)
            training_losses.append(avg_loss)
            
            print(f"  âœ… Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        print(f"ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return training_losses
    
    def translate_text(self, source_text: str, max_length: int = 50) -> str:
        """ç¿»è¯‘æ–‡æœ¬"""
        self.model.eval()
        
        with torch.no_grad():
            # é¢„å¤„ç†è¾“å…¥
            if isinstance(source_text, str):
                # ç®€å•åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„é¢„å¤„ç†ï¼‰
                source_tokens = source_text.strip().split()
            else:
                source_tokens = source_text
            
            # è½¬æ¢ä¸ºç´¢å¼•
            source_indices = [self.source_vocab.get(token, self.source_vocab['<UNK>']) 
                            for token in source_tokens]
            source_tensor = torch.tensor([source_indices])
            
            # ç¼–ç 
            if hasattr(self.model, 'encode'):
                encoder_outputs, encoder_states = self.model.encode(source_tensor)
            else:
                # å¯¹äºç®€å•çš„seq2seqæ¨¡å‹
                source_embedded = self.model.encoder_embedding(source_tensor)
                encoder_outputs, encoder_states = self.model.encoder_lstm(source_embedded)
            
            # è§£ç 
            decoded_tokens = []
            decoder_input = torch.tensor([[self.target_vocab['<SOS>']]])
            
            for _ in range(max_length):
                if hasattr(self.model, 'decode'):
                    decoder_output = self.model.decode(decoder_input, encoder_states)
                else:
                    # ç®€åŒ–çš„è§£ç è¿‡ç¨‹
                    decoder_embedded = self.model.decoder_embedding(decoder_input)
                    decoder_output, _ = self.model.decoder_lstm(decoder_embedded, encoder_states)
                    decoder_output = self.model.output_projection(decoder_output)
                
                # è·å–æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯
                next_token_logits = decoder_output[0, -1, :]
                next_token_id = torch.argmax(next_token_logits).item()
                
                if next_token_id == self.target_vocab['<EOS>']:
                    break
                
                decoded_tokens.append(self.reverse_target_vocab[next_token_id])
                decoder_input = torch.cat([decoder_input, torch.tensor([[next_token_id]])], dim=1)
            
            return ' '.join(decoded_tokens)

class MultilingualTranslationPlatform:
    """æ™ºèƒ½å¤šè¯­è¨€ç¿»è¯‘å¹³å°"""
    
    def __init__(self):
        self.translation_center = MachineTranslationCenter()
        self.models = {}
        self.supported_languages = ['zh', 'en', 'ja', 'ko', 'fr', 'de', 'es']
        self.translation_cache = {}
        self.usage_stats = defaultdict(int)
        
        print("ğŸŒ æ™ºèƒ½å¤šè¯­è¨€ç¿»è¯‘å¹³å°åˆå§‹åŒ–å®Œæˆ")
        self._setup_demo_data()
    
    def _setup_demo_data(self):
        """è®¾ç½®æ¼”ç¤ºæ•°æ®"""
        # ä¸­è‹±æ–‡å¹³è¡Œè¯­æ–™ç¤ºä¾‹
        zh_texts = [
            "ä½ å¥½ä¸–ç•Œ", "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥æ™ºèƒ½",
            "æœºå™¨ç¿»è¯‘æ˜¯ä¸€é¡¹é‡è¦æŠ€æœ¯", "æ·±åº¦å­¦ä¹ æ”¹å˜äº†ç¿»è¯‘è´¨é‡",
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­", "æˆ‘ä»¬éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®",
            "è‡ªç„¶è¯­è¨€å¤„ç†å¾ˆæœ‰è¶£", "ç¿»è¯‘æ¨¡å‹æ­£åœ¨æ”¹è¿›", "æŠ€æœ¯å‘å±•å¾ˆå¿«"
        ]
        
        en_texts = [
            "hello world", "the weather is nice today", "i like learning artificial intelligence",
            "machine translation is an important technology", "deep learning has changed translation quality",
            "this is a good example", "we need more training data",
            "natural language processing is interesting", "translation models are improving", "technology develops quickly"
        ]
        
        # å‡†å¤‡è¯­æ–™åº“
        self.corpus_data = self.translation_center.prepare_parallel_corpus(
            zh_texts, en_texts, 'zh', 'en'
        )
        
        # åˆ›å»ºå¹¶è®­ç»ƒç®€åŒ–çš„ç¿»è¯‘æ¨¡å‹
        self._train_demo_model()
    
    def _train_demo_model(self):
        """è®­ç»ƒæ¼”ç¤ºæ¨¡å‹"""
        print("ğŸ‹ï¸ è®­ç»ƒæ¼”ç¤ºç¿»è¯‘æ¨¡å‹...")
        
        source_vocab_size = len(self.corpus_data['source_vocab'])
        target_vocab_size = len(self.corpus_data['target_vocab'])
        
        # åˆ›å»ºæ¨¡å‹
        model = Seq2SeqTranslator(
            source_vocab_size=source_vocab_size,
            target_vocab_size=target_vocab_size,
            embedding_dim=128,
            hidden_dim=256,
            num_layers=1
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = TranslationTrainer(
            model=model,
            source_vocab=self.corpus_data['source_vocab'],
            target_vocab=self.corpus_data['target_vocab']
        )
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        losses = trainer.train_model(
            self.corpus_data['source_texts'],
            self.corpus_data['target_texts'],
            epochs=5,
            batch_size=4
        )
        
        # ä¿å­˜æ¨¡å‹
        self.models['zh-en'] = {
            'model': model,
            'trainer': trainer,
            'source_vocab': self.corpus_data['source_vocab'],
            'target_vocab': self.corpus_data['target_vocab']
        }
        
        print("âœ… æ¼”ç¤ºæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def translate(self, text: str, source_lang: str, target_lang: str) -> Dict:
        """ç¿»è¯‘æ–‡æœ¬"""
        language_pair = f"{source_lang}-{target_lang}"
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{text}_{language_pair}"
        if cache_key in self.translation_cache:
            print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return self.translation_cache[cache_key]
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥è¯­è¨€å¯¹
        if language_pair not in self.models:
            # å°è¯•åå‘ç¿»è¯‘æˆ–ä½¿ç”¨è§„åˆ™
            return self._fallback_translation(text, source_lang, target_lang)
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç¿»è¯‘
        model_info = self.models[language_pair]
        trainer = model_info['trainer']
        
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†è¾“å…¥æ–‡æœ¬
            if source_lang == 'zh':
                source_tokens = list(jieba.cut(text))
            else:
                source_tokens = text.strip().lower().split()
            
            # ç¿»è¯‘
            translated_text = trainer.translate_text(source_tokens)
            
            translation_time = time.time() - start_time
            
            result = {
                'source_text': text,
                'translated_text': translated_text,
                'source_language': source_lang,
                'target_language': target_lang,
                'translation_time': translation_time,
                'confidence': 0.85,  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                'method': 'neural_translation'
            }
            
            # ç¼“å­˜ç»“æœ
            self.translation_cache[cache_key] = result
            
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            self.usage_stats[language_pair] += 1
            
            print(f"âœ… ç¿»è¯‘å®Œæˆ: {text} -> {translated_text}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            return self._fallback_translation(text, source_lang, target_lang)
    
    def _fallback_translation(self, text: str, source_lang: str, target_lang: str) -> Dict:
        """å¤‡ç”¨ç¿»è¯‘æ–¹æ³•"""
        # ç®€åŒ–çš„è§„åˆ™ç¿»è¯‘æˆ–è¯å…¸ç¿»è¯‘
        simple_dict = {
            'zh-en': {
                'ä½ å¥½': 'hello',
                'ä¸–ç•Œ': 'world',
                'ä»Šå¤©': 'today',
                'å¤©æ°”': 'weather',
                'å¾ˆå¥½': 'very good',
                'æˆ‘': 'i',
                'å–œæ¬¢': 'like',
                'å­¦ä¹ ': 'learn'
            },
            'en-zh': {
                'hello': 'ä½ å¥½',
                'world': 'ä¸–ç•Œ',
                'today': 'ä»Šå¤©',
                'weather': 'å¤©æ°”',
                'good': 'å¥½',
                'i': 'æˆ‘',
                'like': 'å–œæ¬¢',
                'learn': 'å­¦ä¹ '
            }
        }
        
        language_pair = f"{source_lang}-{target_lang}"
        translation_dict = simple_dict.get(language_pair, {})
        
        # ç®€å•çš„è¯å…¸ç¿»è¯‘
        if source_lang == 'zh':
            tokens = list(jieba.cut(text))
        else:
            tokens = text.strip().lower().split()
        
        translated_tokens = []
        for token in tokens:
            translated_token = translation_dict.get(token, token)
            translated_tokens.append(translated_token)
        
        translated_text = ' '.join(translated_tokens)
        
        return {
            'source_text': text,
            'translated_text': translated_text,
            'source_language': source_lang,
            'target_language': target_lang,
            'translation_time': 0.001,
            'confidence': 0.3,
            'method': 'dictionary_fallback'
        }
    
    def batch_translate(self, texts: List[str], source_lang: str, target_lang: str) -> List[Dict]:
        """æ‰¹é‡ç¿»è¯‘"""
        print(f"ğŸ”„ æ‰¹é‡ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ ({source_lang} -> {target_lang})")
        
        results = []
        for i, text in enumerate(texts):
            if i % 5 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(texts)}")
            
            result = self.translate(text, source_lang, target_lang)
            results.append(result)
        
        print("âœ… æ‰¹é‡ç¿»è¯‘å®Œæˆ")
        return results
    
    def get_translation_stats(self) -> Dict:
        """è·å–ç¿»è¯‘ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_translations': sum(self.usage_stats.values()),
            'language_pairs_used': dict(self.usage_stats),
            'cache_size': len(self.translation_cache),
            'supported_languages': self.supported_languages
        }
        
        print(f"ğŸ“Š ç¿»è¯‘å¹³å°ç»Ÿè®¡:")
        print(f"  â€¢ æ€»ç¿»è¯‘æ¬¡æ•°: {stats['total_translations']}")
        print(f"  â€¢ ç¼“å­˜å¤§å°: {stats['cache_size']}")
        print(f"  â€¢ æ”¯æŒè¯­è¨€: {len(stats['supported_languages'])}")
        
        return stats

# æ¼”ç¤ºå¤šè¯­è¨€ç¿»è¯‘å¹³å°
def demo_translation_platform():
    """æ¼”ç¤ºç¿»è¯‘å¹³å°"""
    # åˆ›å»ºç¿»è¯‘å¹³å°
    platform = MultilingualTranslationPlatform()
    
    # æµ‹è¯•ç¿»è¯‘
    test_texts = [
        "ä½ å¥½ä¸–ç•Œ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥æ™ºèƒ½",
        "æœºå™¨ç¿»è¯‘å¾ˆæœ‰ç”¨"
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•ä¸­è‹±ç¿»è¯‘:")
    for text in test_texts:
        result = platform.translate(text, 'zh', 'en')
        print(f"  {result['source_text']} -> {result['translated_text']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
    
    # æ‰¹é‡ç¿»è¯‘æµ‹è¯•
    batch_results = platform.batch_translate(test_texts, 'zh', 'en')
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = platform.get_translation_stats()
    
    return platform, batch_results

# è¿è¡Œæ¼”ç¤º
translation_platform, translation_results = demo_translation_platform()
```

### ğŸ”„ æœºå™¨ç¿»è¯‘æŠ€æœ¯æ¼”è¿›

è®©æˆ‘ä»¬é€šè¿‡å›¾è¡¨æ¥ç†è§£æœºå™¨ç¿»è¯‘æŠ€æœ¯çš„å‘å±•å†ç¨‹ï¼š

```mermaid
graph LR
    A[æœºå™¨ç¿»è¯‘å‘å±•] --> B[ç»Ÿè®¡æœºå™¨ç¿»è¯‘<br/>SMT]
    A --> C[ç¥ç»æœºå™¨ç¿»è¯‘<br/>NMT]
    A --> D[Transformerç¿»è¯‘]
    
    B --> B1[åŸºäºçŸ­è¯­<br/>å¯¹é½æ¨¡å‹<br/>BLEUè¯„ä¼°]
    C --> C1[Seq2Seq<br/>æ³¨æ„åŠ›æœºåˆ¶<br/>ç«¯åˆ°ç«¯è®­ç»ƒ]
    D --> D1[è‡ªæ³¨æ„åŠ›<br/>å¹¶è¡Œè®¡ç®—<br/>é¢„è®­ç»ƒæ¨¡å‹]
    
    B1 --> E[ç¿»è¯‘è´¨é‡æå‡]
    C1 --> E
    D1 --> E
    
    E --> F[å¤šè¯­è¨€æ”¯æŒ<br/>é›¶æ ·æœ¬ç¿»è¯‘<br/>å®æ—¶ç¿»è¯‘]
```

## 32.5 å¯¹è¯ç³»ç»Ÿä¸èŠå¤©æœºå™¨äºº

### ğŸ’¬ å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠï¼šæ™ºèƒ½äº¤äº’çš„è‰ºæœ¯

åœ¨**è¯­è¨€ç†è§£ç ”ç©¶é™¢**çš„**å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠ**ä¸­ï¼Œæˆ‘ä»¬è‡´åŠ›äºåˆ›é€ èƒ½å¤Ÿä¸äººç±»è¿›è¡Œè‡ªç„¶å¯¹è¯çš„AIç³»ç»Ÿã€‚è¿™é‡Œå°±åƒæ˜¯ä¸€ä¸ªé«˜çº§çš„äº¤æµåŸ¹è®­ä¸­å¿ƒï¼Œæˆ‘ä»¬çš„AIå­¦å‘˜è¦å­¦ä¼šå€¾å¬ã€ç†è§£ã€æ€è€ƒå’Œå›åº”ï¼Œæœ€ç»ˆæˆä¸ºä¼˜ç§€çš„"å¯¹è¯ä¼™ä¼´"ã€‚

```python
import random
import json
import re
from collections import defaultdict, deque
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
from datetime import datetime
import time

class DialogueSystemWorkshop:
    """å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠ"""
    
    def __init__(self):
        self.dialogue_models = {}
        self.conversation_history = []
        self.user_profiles = {}
        
        print("ğŸ’¬ å¯¹è¯ç³»ç»Ÿå·¥ä½œåŠåˆå§‹åŒ–å®Œæˆ")
        self._show_dialogue_capabilities()
    
    def _show_dialogue_capabilities(self):
        """å±•ç¤ºå¯¹è¯èƒ½åŠ›"""
        capabilities = {
            "æ„å›¾è¯†åˆ«": "ç†è§£ç”¨æˆ·æƒ³è¦åšä»€ä¹ˆ",
            "å®ä½“æŠ½å–": "è¯†åˆ«å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯",
            "å¯¹è¯ç®¡ç†": "ç»´æŠ¤å¯¹è¯çŠ¶æ€å’Œä¸Šä¸‹æ–‡",
            "å›å¤ç”Ÿæˆ": "ç”Ÿæˆè‡ªç„¶æµç•…çš„å›å¤",
            "æƒ…æ„Ÿç†è§£": "è¯†åˆ«ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€",
            "å¤šè½®å¯¹è¯": "æ”¯æŒé•¿æ—¶é—´çš„è¿ç»­å¯¹è¯",
            "ä¸ªæ€§åŒ–äº¤äº’": "æ ¹æ®ç”¨æˆ·ç‰¹ç‚¹è°ƒæ•´å¯¹è¯é£æ ¼"
        }
        
        print(f"\nğŸ”¬ å¯¹è¯ç³»ç»Ÿæ ¸å¿ƒèƒ½åŠ›:")
        for capability, description in capabilities.items():
            print(f"  â€¢ {capability}: {description}")

class IntentClassifier:
    """æ„å›¾è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.intent_patterns = self._build_intent_patterns()
        self.intent_history = []
        
        print("ğŸ¯ æ„å›¾è¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_intent_patterns(self) -> Dict:
        """æ„å»ºæ„å›¾æ¨¡å¼"""
        patterns = {
            'greeting': {
                'patterns': ['ä½ å¥½', 'æ‚¨å¥½', 'å—¨', 'hi', 'hello', 'æ—©ä¸Šå¥½', 'ä¸‹åˆå¥½', 'æ™šä¸Šå¥½'],
                'keywords': ['é—®å€™', 'æ‰“æ‹›å‘¼', 'è§é¢'],
                'response_type': 'greeting'
            },
            'question': {
                'patterns': ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'è°', 'ä½•æ—¶', 'å¦‚ä½•'],
                'keywords': ['è¯¢é—®', 'é—®é¢˜', 'ç–‘é—®'],
                'response_type': 'answer'
            },
            'request': {
                'patterns': ['è¯·', 'å¸®æˆ‘', 'èƒ½å¦', 'å¯ä»¥', 'å¸Œæœ›', 'éœ€è¦'],
                'keywords': ['è¯·æ±‚', 'å¸®åŠ©', 'æœåŠ¡'],
                'response_type': 'assistance'
            },
            'complaint': {
                'patterns': ['æŠ•è¯‰', 'é—®é¢˜', 'ä¸æ»¡', 'æ•…éšœ', 'é”™è¯¯', 'å¤±è´¥'],
                'keywords': ['æŠ±æ€¨', 'ä¸æ»¡æ„', 'é—®é¢˜'],
                'response_type': 'support'
            },
            'praise': {
                'patterns': ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æ»¡æ„', 'å–œæ¬¢', 'èµ'],
                'keywords': ['è¡¨æ‰¬', 'æ»¡æ„', 'å¥½è¯„'],
                'response_type': 'appreciation'
            },
            'goodbye': {
                'patterns': ['å†è§', 'æ‹œæ‹œ', 'bye', '88', 'å‘Šåˆ«'],
                'keywords': ['å‘Šåˆ«', 'ç»“æŸ', 'ç¦»å¼€'],
                'response_type': 'farewell'
            }
        }
        
        return patterns
    
    def classify_intent(self, text: str) -> Dict:
        """åˆ†ç±»ç”¨æˆ·æ„å›¾"""
        text_lower = text.lower()
        intent_scores = {}
        
        for intent, config in self.intent_patterns.items():
            score = 0
            
            # æ¨¡å¼åŒ¹é…
            for pattern in config['patterns']:
                if pattern in text_lower:
                    score += 2
            
            # å…³é”®è¯åŒ¹é…
            for keyword in config['keywords']:
                if keyword in text_lower:
                    score += 1
            
            intent_scores[intent] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„æ„å›¾
        if max(intent_scores.values()) > 0:
            predicted_intent = max(intent_scores, key=intent_scores.get)
            confidence = intent_scores[predicted_intent] / sum(intent_scores.values())
        else:
            predicted_intent = 'unknown'
            confidence = 0.0
        
        result = {
            'text': text,
            'predicted_intent': predicted_intent,
            'confidence': confidence,
            'all_scores': intent_scores,
            'response_type': self.intent_patterns.get(predicted_intent, {}).get('response_type', 'default')
        }
        
        self.intent_history.append(result)
        return result

class EntityExtractor:
    """å®ä½“æŠ½å–å™¨"""
    
    def __init__(self):
        self.entity_patterns = self._build_entity_patterns()
        print("ğŸ·ï¸ å®ä½“æŠ½å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_entity_patterns(self) -> Dict:
        """æ„å»ºå®ä½“æ¨¡å¼"""
        patterns = {
            'person': {
                'pattern': r'[\u4e00-\u9fff]{2,4}(?=å…ˆç”Ÿ|å¥³å£«|è€å¸ˆ|åŒ»ç”Ÿ|ç»ç†|æ€»)',
                'examples': ['å¼ å…ˆç”Ÿ', 'æå¥³å£«', 'ç‹è€å¸ˆ']
            },
            'time': {
                'pattern': r'(\d{1,2}[ç‚¹æ—¶]|\d{1,2}:\d{2}|ä»Šå¤©|æ˜å¤©|æ˜¨å¤©|ç°åœ¨|æ—©ä¸Š|ä¸‹åˆ|æ™šä¸Š)',
                'examples': ['3ç‚¹', '14:30', 'ä»Šå¤©', 'æ˜å¤©']
            },
            'location': {
                'pattern': r'[\u4e00-\u9fff]+(?:å¸‚|çœ|åŒº|å¿|è·¯|è¡—|å·|æ¥¼)',
                'examples': ['åŒ—äº¬å¸‚', 'ä¸Šæµ·åŒº', 'ä¸­å±±è·¯']
            },
            'number': {
                'pattern': r'\d+',
                'examples': ['123', '456']
            },
            'phone': {
                'pattern': r'1[3-9]\d{9}',
                'examples': ['13812345678']
            },
            'email': {
                'pattern': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                'examples': ['user@example.com']
            }
        }
        
        return patterns
    
    def extract_entities(self, text: str) -> List[Dict]:
        """æå–å®ä½“"""
        entities = []
        
        for entity_type, config in self.entity_patterns.items():
            pattern = config['pattern']
            matches = re.finditer(pattern, text)
            
            for match in matches:
                entity = {
                    'text': match.group(),
                    'type': entity_type,
                    'start': match.start(),
                    'end': match.end(),
                    'confidence': 0.9  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                }
                entities.append(entity)
        
        return entities

class DialogueManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversation_state = {}
        self.context_window = deque(maxlen=10)  # ä¿æŒæœ€è¿‘10è½®å¯¹è¯
        self.user_profile = {}
        
        print("ğŸ§  å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def update_conversation_state(self, user_input: str, intent: Dict, entities: List[Dict]):
        """æ›´æ–°å¯¹è¯çŠ¶æ€"""
        turn = {
            'timestamp': datetime.now(),
            'user_input': user_input,
            'intent': intent,
            'entities': entities,
            'turn_id': len(self.context_window) + 1
        }
        
        self.context_window.append(turn)
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        self.conversation_state.update({
            'current_intent': intent['predicted_intent'],
            'last_entities': entities,
            'turn_count': len(self.context_window),
            'conversation_active': True
        })
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        self._update_user_profile(intent, entities)
    
    def _update_user_profile(self, intent: Dict, entities: List[Dict]):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        # è®°å½•ç”¨æˆ·åå¥½å’Œè¡Œä¸ºæ¨¡å¼
        intent_name = intent['predicted_intent']
        
        if 'intent_frequency' not in self.user_profile:
            self.user_profile['intent_frequency'] = defaultdict(int)
        
        self.user_profile['intent_frequency'][intent_name] += 1
        
        # è®°å½•æåˆ°çš„å®ä½“
        if 'mentioned_entities' not in self.user_profile:
            self.user_profile['mentioned_entities'] = defaultdict(list)
        
        for entity in entities:
            self.user_profile['mentioned_entities'][entity['type']].append(entity['text'])
    
    def get_context_summary(self) -> Dict:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not self.context_window:
            return {'summary': 'æ–°å¯¹è¯å¼€å§‹', 'relevant_info': []}
        
        recent_intents = [turn['intent']['predicted_intent'] for turn in self.context_window]
        recent_entities = []
        for turn in self.context_window:
            recent_entities.extend(turn['entities'])
        
        return {
            'summary': f"å¯¹è¯è¿›è¡Œäº†{len(self.context_window)}è½®",
            'recent_intents': recent_intents[-3:],  # æœ€è¿‘3ä¸ªæ„å›¾
            'recent_entities': recent_entities[-5:],  # æœ€è¿‘5ä¸ªå®ä½“
            'conversation_state': self.conversation_state,
            'user_profile': dict(self.user_profile)
        }

class ResponseGenerator:
    """å›å¤ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.response_templates = self._build_response_templates()
        self.personality_config = {
            'style': 'friendly',  # friendly, professional, casual
            'verbosity': 'medium',  # brief, medium, detailed
            'empathy_level': 'high'  # low, medium, high
        }
        
        print("ğŸ’­ å›å¤ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_response_templates(self) -> Dict:
        """æ„å»ºå›å¤æ¨¡æ¿"""
        templates = {
            'greeting': [
                "æ‚¨å¥½ï¼å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "ä½ å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®æˆ‘å§ï¼",
                "æ¬¢è¿ï¼è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚"
            ],
            'question': [
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚è®©æˆ‘æ¥ä¸ºæ‚¨è§£ç­”...",
                "å…³äºæ‚¨çš„é—®é¢˜ï¼Œæˆ‘çš„ç†è§£æ˜¯...",
                "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯..."
            ],
            'request': [
                "æˆ‘å¾ˆä¹æ„å¸®åŠ©æ‚¨ï¼è®©æˆ‘çœ‹çœ‹èƒ½ä¸ºæ‚¨åšäº›ä»€ä¹ˆã€‚",
                "å½“ç„¶å¯ä»¥ï¼æˆ‘ä¼šå°½åŠ›ååŠ©æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚",
                "æ²¡é—®é¢˜ï¼Œæˆ‘æ¥å¸®æ‚¨å¤„ç†è¿™ä»¶äº‹ã€‚"
            ],
            'complaint': [
                "æˆ‘ç†è§£æ‚¨çš„å›°æ‰°ï¼Œè®©æˆ‘æ¥å¸®æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚",
                "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ä¼šç«‹å³å¤„ç†ã€‚",
                "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šè®¤çœŸå¯¹å¾…æ‚¨çš„æ„è§ã€‚"
            ],
            'praise': [
                "è°¢è°¢æ‚¨çš„è‚¯å®šï¼æˆ‘ä¼šç»§ç»­åŠªåŠ›ä¸ºæ‚¨æä¾›æ›´å¥½çš„æœåŠ¡ã€‚",
                "å¾ˆé«˜å…´èƒ½å¤Ÿå¸®åˆ°æ‚¨ï¼æ‚¨çš„æ»¡æ„æ˜¯æˆ‘æœ€å¤§çš„åŠ¨åŠ›ã€‚",
                "æ„Ÿè°¢æ‚¨çš„å¥½è¯„ï¼Œæˆ‘ä¼šä¿æŒè¿™æ ·çš„æœåŠ¡æ°´å‡†ã€‚"
            ],
            'goodbye': [
                "å†è§ï¼å¸Œæœ›ä»Šå¤©çš„äº¤æµå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚",
                "è°¢è°¢æ‚¨çš„ä½¿ç”¨ï¼ŒæœŸå¾…ä¸‹æ¬¡ä¸ºæ‚¨æœåŠ¡ï¼",
                "ç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼Œæœ‰éœ€è¦éšæ—¶æ‰¾æˆ‘ï¼"
            ],
            'unknown': [
                "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å®Œå…¨ç†è§£æ‚¨çš„æ„æ€ï¼Œèƒ½å¦é‡æ–°è¡¨è¾¾ä¸€ä¸‹ï¼Ÿ",
                "è¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œæ‚¨èƒ½æä¾›æ›´å¤šä¿¡æ¯å—ï¼Ÿ",
                "è®©æˆ‘é‡æ–°ç†è§£ä¸€ä¸‹æ‚¨çš„éœ€æ±‚..."
            ]
        }
        
        return templates
    
    def generate_response(self, intent: Dict, entities: List[Dict], context: Dict) -> str:
        """ç”Ÿæˆå›å¤"""
        response_type = intent.get('response_type', 'unknown')
        templates = self.response_templates.get(response_type, self.response_templates['unknown'])
        
        # åŸºç¡€å›å¤
        base_response = random.choice(templates)
        
        # æ ¹æ®å®ä½“ä¿¡æ¯ä¸ªæ€§åŒ–å›å¤
        personalized_response = self._personalize_response(base_response, entities, context)
        
        # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´å›å¤
        contextualized_response = self._contextualize_response(personalized_response, context)
        
        return contextualized_response
    
    def _personalize_response(self, response: str, entities: List[Dict], context: Dict) -> str:
        """ä¸ªæ€§åŒ–å›å¤"""
        # å¦‚æœæœ‰äººåå®ä½“ï¼Œä½¿ç”¨ç§°è°“
        for entity in entities:
            if entity['type'] == 'person':
                response = f"{entity['text']}ï¼Œ{response}"
                break
        
        # å¦‚æœæœ‰æ—¶é—´å®ä½“ï¼ŒåŠ å…¥æ—¶é—´ç›¸å…³çš„è¡¨è¾¾
        for entity in entities:
            if entity['type'] == 'time':
                if 'æ—©ä¸Š' in entity['text']:
                    response = f"æ—©ä¸Šå¥½ï¼{response}"
                elif 'ä¸‹åˆ' in entity['text']:
                    response = f"ä¸‹åˆå¥½ï¼{response}"
                elif 'æ™šä¸Š' in entity['text']:
                    response = f"æ™šä¸Šå¥½ï¼{response}"
                break
        
        return response
    
    def _contextualize_response(self, response: str, context: Dict) -> str:
        """æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´å›å¤"""
        conversation_state = context.get('conversation_state', {})
        turn_count = conversation_state.get('turn_count', 0)
        
        # é¦–æ¬¡å¯¹è¯
        if turn_count == 1:
            response = f"æ¬¢è¿é¦–æ¬¡ä½¿ç”¨ï¼{response}"
        
        # å¤šè½®å¯¹è¯
        elif turn_count > 5:
            response = f"æˆ‘ä»¬èŠäº†å¾ˆä¹…äº†ï¼Œ{response}"
        
        # æ ¹æ®ç”¨æˆ·å†å²åå¥½è°ƒæ•´
        user_profile = context.get('user_profile', {})
        intent_frequency = user_profile.get('intent_frequency', {})
        
        if intent_frequency.get('complaint', 0) > 2:
            response = f"æˆ‘æ³¨æ„åˆ°æ‚¨é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œ{response}"
        
        return response

class IntelligentCustomerService:
    """æ™ºèƒ½å®¢æœå¯¹è¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.intent_classifier = IntentClassifier()
        self.entity_extractor = EntityExtractor()
        self.dialogue_manager = DialogueManager()
        self.response_generator = ResponseGenerator()
        
        self.conversation_log = []
        self.performance_metrics = {
            'total_conversations': 0,
            'average_turns': 0,
            'satisfaction_score': 0.0,
            'resolution_rate': 0.0
        }
        
        print("ğŸ¤– æ™ºèƒ½å®¢æœå¯¹è¯ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self._setup_knowledge_base()
    
    def _setup_knowledge_base(self):
        """è®¾ç½®çŸ¥è¯†åº“"""
        self.knowledge_base = {
            'products': {
                'AIåŠ©æ‰‹': {'price': 'å…è´¹', 'features': ['è¯­éŸ³è¯†åˆ«', 'è‡ªç„¶å¯¹è¯', 'æ™ºèƒ½æ¨è']},
                'ç¿»è¯‘è½¯ä»¶': {'price': '99å…ƒ/æœˆ', 'features': ['å¤šè¯­è¨€æ”¯æŒ', 'å®æ—¶ç¿»è¯‘', 'ç¦»çº¿æ¨¡å¼']},
                'å†™ä½œåŠ©æ‰‹': {'price': '199å…ƒ/æœˆ', 'features': ['AIå†™ä½œ', 'è¯­æ³•æ£€æŸ¥', 'é£æ ¼ä¼˜åŒ–']}
            },
            'policies': {
                'é€€æ¬¾æ”¿ç­–': '7å¤©æ— ç†ç”±é€€æ¬¾ï¼Œ30å¤©è´¨é‡é—®é¢˜é€€æ¬¾',
                'éšç§æ”¿ç­–': 'æˆ‘ä»¬ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œä¸ä¼šæ³„éœ²ä¸ªäººä¿¡æ¯',
                'æœåŠ¡æ¡æ¬¾': 'è¯·éµå®ˆå¹³å°ä½¿ç”¨è§„èŒƒï¼Œç¦æ­¢æ¶æ„ä½¿ç”¨'
            },
            'faq': {
                'å¦‚ä½•æ³¨å†Œ': 'ç‚¹å‡»æ³¨å†ŒæŒ‰é’®ï¼Œå¡«å†™æ‰‹æœºå·å’ŒéªŒè¯ç å³å¯',
                'å¿˜è®°å¯†ç ': 'ç‚¹å‡»å¿˜è®°å¯†ç ï¼Œé€šè¿‡æ‰‹æœºéªŒè¯ç é‡ç½®',
                'å¦‚ä½•ä»˜è´¹': 'æ”¯æŒå¾®ä¿¡æ”¯ä»˜ã€æ”¯ä»˜å®ã€é“¶è¡Œå¡ç­‰å¤šç§æ”¯ä»˜æ–¹å¼'
            }
        }
        
        print("ğŸ“š çŸ¥è¯†åº“è®¾ç½®å®Œæˆ")
    
    def process_user_input(self, user_input: str, user_id: str = None) -> Dict:
        """å¤„ç†ç”¨æˆ·è¾“å…¥"""
        if user_id is None:
            user_id = f"user_{int(time.time() * 1000)}"
        
        print(f"\nğŸ‘¤ ç”¨æˆ·è¾“å…¥: {user_input}")
        
        # 1. æ„å›¾è¯†åˆ«
        intent_result = self.intent_classifier.classify_intent(user_input)
        print(f"ğŸ¯ è¯†åˆ«æ„å›¾: {intent_result['predicted_intent']} (ç½®ä¿¡åº¦: {intent_result['confidence']:.2f})")
        
        # 2. å®ä½“æŠ½å–
        entities = self.entity_extractor.extract_entities(user_input)
        print(f"ğŸ·ï¸ æå–å®ä½“: {len(entities)} ä¸ª")
        for entity in entities:
            print(f"    â€¢ {entity['text']} ({entity['type']})")
        
        # 3. æ›´æ–°å¯¹è¯çŠ¶æ€
        self.dialogue_manager.update_conversation_state(user_input, intent_result, entities)
        context = self.dialogue_manager.get_context_summary()
        
        # 4. çŸ¥è¯†åº“æŸ¥è¯¢
        knowledge_response = self._query_knowledge_base(user_input, intent_result, entities)
        
        # 5. ç”Ÿæˆå›å¤
        if knowledge_response:
            response = knowledge_response
        else:
            response = self.response_generator.generate_response(intent_result, entities, context)
        
        # 6. è®°å½•å¯¹è¯
        conversation_turn = {
            'timestamp': datetime.now(),
            'user_id': user_id,
            'user_input': user_input,
            'intent': intent_result,
            'entities': entities,
            'context': context,
            'response': response,
            'knowledge_used': bool(knowledge_response)
        }
        
        self.conversation_log.append(conversation_turn)
        
        print(f"ğŸ¤– ç³»ç»Ÿå›å¤: {response}")
        
        return conversation_turn
    
    def _query_knowledge_base(self, user_input: str, intent: Dict, entities: List[Dict]) -> Optional[str]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        user_input_lower = user_input.lower()
        
        # äº§å“ç›¸å…³æŸ¥è¯¢
        for product, info in self.knowledge_base['products'].items():
            if product in user_input or any(feature in user_input for feature in info['features']):
                return f"å…³äº{product}ï¼šä»·æ ¼{info['price']}ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬{', '.join(info['features'])}ã€‚"
        
        # æ”¿ç­–ç›¸å…³æŸ¥è¯¢
        for policy, content in self.knowledge_base['policies'].items():
            if any(keyword in user_input_lower for keyword in ['é€€æ¬¾', 'éšç§', 'æœåŠ¡', 'æ¡æ¬¾']):
                if 'é€€æ¬¾' in user_input_lower and 'é€€æ¬¾æ”¿ç­–' == policy:
                    return f"é€€æ¬¾æ”¿ç­–ï¼š{content}"
                elif 'éšç§' in user_input_lower and 'éšç§æ”¿ç­–' == policy:
                    return f"éšç§æ”¿ç­–ï¼š{content}"
                elif ('æœåŠ¡' in user_input_lower or 'æ¡æ¬¾' in user_input_lower) and 'æœåŠ¡æ¡æ¬¾' == policy:
                    return f"æœåŠ¡æ¡æ¬¾ï¼š{content}"
        
        # FAQæŸ¥è¯¢
        for question, answer in self.knowledge_base['faq'].items():
            if any(keyword in user_input for keyword in ['æ³¨å†Œ', 'å¯†ç ', 'ä»˜è´¹', 'æ”¯ä»˜']):
                if 'æ³¨å†Œ' in user_input and 'å¦‚ä½•æ³¨å†Œ' == question:
                    return f"æ³¨å†Œæ–¹æ³•ï¼š{answer}"
                elif 'å¯†ç ' in user_input and 'å¿˜è®°å¯†ç ' == question:
                    return f"å¯†ç é‡ç½®ï¼š{answer}"
                elif ('ä»˜è´¹' in user_input or 'æ”¯ä»˜' in user_input) and 'å¦‚ä½•ä»˜è´¹' == question:
                    return f"ä»˜è´¹æ–¹å¼ï¼š{answer}"
        
        return None
    
    def start_conversation(self, user_id: str = None) -> str:
        """å¼€å§‹å¯¹è¯"""
        if user_id is None:
            user_id = f"user_{int(time.time() * 1000)}"
        
        welcome_message = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        
        # è®°å½•å¯¹è¯å¼€å§‹
        conversation_start = {
            'timestamp': datetime.now(),
            'user_id': user_id,
            'event': 'conversation_start',
            'message': welcome_message
        }
        
        self.conversation_log.append(conversation_start)
        self.performance_metrics['total_conversations'] += 1
        
        print(f"ğŸ¤– {welcome_message}")
        return welcome_message
    
    def end_conversation(self, user_id: str, satisfaction_score: float = None) -> Dict:
        """ç»“æŸå¯¹è¯"""
        # è®¡ç®—å¯¹è¯ç»Ÿè®¡
        user_turns = [turn for turn in self.conversation_log 
                     if turn.get('user_id') == user_id and 'user_input' in turn]
        
        conversation_summary = {
            'user_id': user_id,
            'total_turns': len(user_turns),
            'conversation_duration': (datetime.now() - user_turns[0]['timestamp']).total_seconds() if user_turns else 0,
            'intents_used': list(set(turn['intent']['predicted_intent'] for turn in user_turns)),
            'entities_mentioned': [entity for turn in user_turns for entity in turn['entities']],
            'knowledge_base_used': any(turn.get('knowledge_used', False) for turn in user_turns),
            'satisfaction_score': satisfaction_score
        }
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        if satisfaction_score:
            self.performance_metrics['satisfaction_score'] = (
                self.performance_metrics['satisfaction_score'] + satisfaction_score
            ) / 2
        
        self.performance_metrics['average_turns'] = (
            self.performance_metrics['average_turns'] + conversation_summary['total_turns']
        ) / 2
        
        print(f"ğŸ“Š å¯¹è¯ç»“æŸç»Ÿè®¡:")
        print(f"  â€¢ å¯¹è¯è½®æ•°: {conversation_summary['total_turns']}")
        print(f"  â€¢ æŒç»­æ—¶é—´: {conversation_summary['conversation_duration']:.1f}ç§’")
        print(f"  â€¢ æ¶‰åŠæ„å›¾: {', '.join(conversation_summary['intents_used'])}")
        
        return conversation_summary
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {
            'system_metrics': self.performance_metrics,
            'conversation_stats': {
                'total_conversations': len(set(turn.get('user_id') for turn in self.conversation_log if turn.get('user_id'))),
                'total_turns': len([turn for turn in self.conversation_log if 'user_input' in turn]),
                'knowledge_base_usage': len([turn for turn in self.conversation_log if turn.get('knowledge_used', False)])
            },
            'intent_distribution': {},
            'entity_distribution': {}
        }
        
        # ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
        intent_counts = defaultdict(int)
        entity_counts = defaultdict(int)
        
        for turn in self.conversation_log:
            if 'intent' in turn:
                intent_counts[turn['intent']['predicted_intent']] += 1
            
            if 'entities' in turn:
                for entity in turn['entities']:
                    entity_counts[entity['type']] += 1
        
        report['intent_distribution'] = dict(intent_counts)
        report['entity_distribution'] = dict(entity_counts)
        
        print(f"\nğŸ“ˆ ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š:")
        print(f"  â€¢ æ€»å¯¹è¯æ•°: {report['conversation_stats']['total_conversations']}")
        print(f"  â€¢ æ€»è½®æ¬¡: {report['conversation_stats']['total_turns']}")
        print(f"  â€¢ å¹³å‡æ»¡æ„åº¦: {self.performance_metrics['satisfaction_score']:.2f}")
        print(f"  â€¢ çŸ¥è¯†åº“ä½¿ç”¨ç‡: {report['conversation_stats']['knowledge_base_usage']}")
        
        return report

# æ¼”ç¤ºæ™ºèƒ½å®¢æœå¯¹è¯ç³»ç»Ÿ
def demo_customer_service():
    """æ¼”ç¤ºå®¢æœç³»ç»Ÿ"""
    # åˆ›å»ºå®¢æœç³»ç»Ÿ
    customer_service = IntelligentCustomerService()
    
    # å¼€å§‹å¯¹è¯
    user_id = "demo_user_001"
    customer_service.start_conversation(user_id)
    
    # æ¨¡æ‹Ÿå¯¹è¯æµç¨‹
    conversation_flow = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ ä»¬çš„AIåŠ©æ‰‹äº§å“",
        "ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ",
        "æˆ‘å¿˜è®°å¯†ç äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ",
        "ä½ ä»¬çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "è°¢è°¢ä½ çš„å¸®åŠ©ï¼Œå†è§"
    ]
    
    print(f"\nğŸ­ æ¨¡æ‹Ÿå¯¹è¯æµç¨‹:")
    print("=" * 30)
    
    for user_input in conversation_flow:
        result = customer_service.process_user_input(user_input, user_id)
        time.sleep(1)  # æ¨¡æ‹Ÿå¯¹è¯é—´éš”
    
    # ç»“æŸå¯¹è¯
    summary = customer_service.end_conversation(user_id, satisfaction_score=4.5)
    
    # è·å–æ€§èƒ½æŠ¥å‘Š
    report = customer_service.get_performance_report()
    
    return customer_service, summary, report

# è¿è¡Œæ¼”ç¤º
customer_service_system, conversation_summary, performance_report = demo_customer_service()
```

### ğŸ¯ å¯¹è¯ç³»ç»Ÿæ¶æ„å›¾

è®©æˆ‘ä»¬é€šè¿‡å›¾è¡¨æ¥ç†è§£å¯¹è¯ç³»ç»Ÿçš„å®Œæ•´æ¶æ„ï¼š

```mermaid
graph TB
    A[ç”¨æˆ·è¾“å…¥] --> B[æ„å›¾è¯†åˆ«]
    A --> C[å®ä½“æŠ½å–]
    B --> D[å¯¹è¯ç®¡ç†å™¨]
    C --> D
    D --> E[ä¸Šä¸‹æ–‡ç»´æŠ¤]
    D --> F[çŠ¶æ€è·Ÿè¸ª]
    E --> G[çŸ¥è¯†åº“æŸ¥è¯¢]
    F --> G
    G --> H[å›å¤ç”Ÿæˆ]
    H --> I[ä¸ªæ€§åŒ–è°ƒæ•´]
    I --> J[ç”¨æˆ·å›å¤]
    
    D --> K[ç”¨æˆ·ç”»åƒæ›´æ–°]
    K --> L[å¯¹è¯å†å²]
    L --> M[æ€§èƒ½åˆ†æ]
```

## 32.6 æ–‡æœ¬ç”Ÿæˆä¸åˆ›æ„å†™ä½œ

### âœï¸ è¯­è¨€ç”Ÿæˆå®éªŒå®¤ï¼šAIçš„åˆ›ä½œè‰ºæœ¯

åœ¨**è¯­è¨€ç†è§£ç ”ç©¶é™¢**çš„**è¯­è¨€ç”Ÿæˆå®éªŒå®¤**ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢AIçš„åˆ›ä½œèƒ½åŠ›ã€‚è¿™é‡Œå°±åƒä¸€ä¸ªæ–‡å­¦åˆ›ä½œå·¥ä½œå®¤ï¼Œæˆ‘ä»¬çš„AIä½œå®¶è¦å­¦ä¼šé£è¯é€ å¥ã€æ„æ€æƒ…èŠ‚ã€è¡¨è¾¾æƒ…æ„Ÿï¼Œåˆ›é€ å‡ºå¯Œæœ‰åˆ›æ„å’Œæ„ŸæŸ“åŠ›çš„æ–‡æœ¬å†…å®¹ã€‚

```python
import random
import re
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Optional
import numpy as np
import math
import time
from datetime import datetime

class LanguageGenerationLab:
    """è¯­è¨€ç”Ÿæˆå®éªŒå®¤"""
    
    def __init__(self):
        self.generation_models = {}
        self.writing_styles = {}
        self.generation_history = []
        
        print("âœï¸ è¯­è¨€ç”Ÿæˆå®éªŒå®¤åˆå§‹åŒ–å®Œæˆ")
        self._show_generation_capabilities()
    
    def _show_generation_capabilities(self):
        """å±•ç¤ºç”Ÿæˆèƒ½åŠ›"""
        capabilities = {
            "æ–‡æœ¬ç»­å†™": "æ ¹æ®å¼€å¤´å†…å®¹ç»§ç»­å†™ä½œ",
            "æ‘˜è¦ç”Ÿæˆ": "æå–æ–‡æœ¬çš„æ ¸å¿ƒè¦ç‚¹",
            "åˆ›æ„å†™ä½œ": "ç”Ÿæˆæ•…äº‹ã€è¯—æ­Œç­‰åˆ›æ„å†…å®¹",
            "é£æ ¼è¿ç§»": "æ”¹å˜æ–‡æœ¬çš„å†™ä½œé£æ ¼",
            "å†…å®¹æ‰©å†™": "å°†ç®€çŸ­å†…å®¹æ‰©å±•ä¸ºè¯¦ç»†æè¿°",
            "å¤šæ ·åŒ–ç”Ÿæˆ": "ç”Ÿæˆå¤šä¸ªä¸åŒç‰ˆæœ¬çš„å†…å®¹"
        }
        
        print(f"\nğŸ”¬ ç”Ÿæˆå®éªŒå®¤æ ¸å¿ƒèƒ½åŠ›:")
        for capability, description in capabilities.items():
            print(f"  â€¢ {capability}: {description}")

class NgramLanguageModel:
    """N-gramè¯­è¨€æ¨¡å‹"""
    
    def __init__(self, n: int = 3):
        self.n = n
        self.ngrams = defaultdict(Counter)
        self.vocabulary = set()
        self.trained = False
        
        print(f"ğŸ“š {n}-gramè¯­è¨€æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def train(self, texts: List[str]):
        """è®­ç»ƒè¯­è¨€æ¨¡å‹"""
        print(f"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ{self.n}-gramæ¨¡å‹...")
        
        for text in texts:
            # æ–‡æœ¬é¢„å¤„ç†
            tokens = self._tokenize(text)
            self.vocabulary.update(tokens)
            
            # æ„å»ºn-gram
            for i in range(len(tokens) - self.n + 1):
                context = tuple(tokens[i:i+self.n-1])
                next_word = tokens[i+self.n-1]
                self.ngrams[context][next_word] += 1
        
        self.trained = True
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        print(f"  â€¢ è¯æ±‡é‡: {len(self.vocabulary)}")
        print(f"  â€¢ N-gramæ•°é‡: {len(self.ngrams)}")
    
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        # ç®€åŒ–çš„åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯å™¨ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fffã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]', '', text)
        tokens = ['<START>'] * (self.n - 1)
        
        # ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²
        if re.search(r'[\u4e00-\u9fff]', text):
            tokens.extend(list(text.replace(' ', '')))
        else:
            tokens.extend(text.lower().split())
        
        tokens.append('<END>')
        return tokens
    
    def generate_text(self, seed_text: str = "", max_length: int = 100, temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # åˆå§‹åŒ–
        if seed_text:
            tokens = self._tokenize(seed_text)
            context = tuple(tokens[-(self.n-1):])
        else:
            context = tuple(['<START>'] * (self.n - 1))
        
        generated_tokens = list(context)
        
        for _ in range(max_length):
            # è·å–å€™é€‰è¯
            candidates = self.ngrams[context]
            if not candidates:
                break
            
            # åº”ç”¨æ¸©åº¦å‚æ•°
            if temperature == 0:
                next_word = max(candidates, key=candidates.get)
            else:
                words = list(candidates.keys())
                counts = list(candidates.values())
                
                # æ¸©åº¦è°ƒèŠ‚
                probabilities = np.array(counts, dtype=float)
                probabilities = probabilities / temperature
                probabilities = np.exp(probabilities - np.max(probabilities))
                probabilities = probabilities / np.sum(probabilities)
                
                next_word = np.random.choice(words, p=probabilities)
            
            if next_word == '<END>':
                break
            
            generated_tokens.append(next_word)
            context = tuple(generated_tokens[-(self.n-1):])
        
        # æ¸…ç†è¾“å‡º
        result = ''.join(generated_tokens[self.n-1:])
        result = result.replace('<START>', '').replace('<END>', '')
        
        return result
    
    def calculate_perplexity(self, test_texts: List[str]) -> float:
        """è®¡ç®—å›°æƒ‘åº¦"""
        total_log_prob = 0
        total_words = 0
        
        for text in test_texts:
            tokens = self._tokenize(text)
            
            for i in range(self.n-1, len(tokens)):
                context = tuple(tokens[i-self.n+1:i])
                word = tokens[i]
                
                candidates = self.ngrams[context]
                total_count = sum(candidates.values())
                
                if total_count > 0:
                    prob = candidates[word] / total_count
                    if prob > 0:
                        total_log_prob += math.log(prob)
                    else:
                        total_log_prob += math.log(1e-10)  # å¹³æ»‘å¤„ç†
                
                total_words += 1
        
        if total_words == 0:
            return float('inf')
        
        avg_log_prob = total_log_prob / total_words
        perplexity = math.exp(-avg_log_prob)
        
        return perplexity

class CreativeWritingGenerator:
    """åˆ›æ„å†™ä½œç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.language_model = NgramLanguageModel(n=3)
        self.writing_templates = self._build_writing_templates()
        self.style_patterns = self._build_style_patterns()
        
        print("ğŸ¨ åˆ›æ„å†™ä½œç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        self._train_with_sample_data()
    
    def _build_writing_templates(self) -> Dict:
        """æ„å»ºå†™ä½œæ¨¡æ¿"""
        templates = {
            'story': {
                'openings': [
                    "ä»å‰æœ‰ä¸€ä¸ª{character}ï¼Œä½åœ¨{location}ã€‚",
                    "åœ¨{time}çš„{location}ï¼Œå‘ç”Ÿäº†ä¸€ä»¶{adjective}çš„äº‹æƒ…ã€‚",
                    "{character}ä»æ¥æ²¡æœ‰æƒ³åˆ°ï¼Œ{event}ä¼šæ”¹å˜ä»–çš„ä¸€ç”Ÿã€‚"
                ],
                'developments': [
                    "çªç„¶ï¼Œ{event}å‘ç”Ÿäº†ã€‚",
                    "è¿™æ—¶å€™ï¼Œ{character}å†³å®š{action}ã€‚",
                    "ç»è¿‡{time}çš„åŠªåŠ›ï¼Œ{character}ç»ˆäº{achievement}ã€‚"
                ],
                'endings': [
                    "ä»æ­¤ä»¥åï¼Œ{character}è¿‡ä¸Šäº†{adjective}çš„ç”Ÿæ´»ã€‚",
                    "è¿™ä¸ªæ•…äº‹å‘Šè¯‰æˆ‘ä»¬ï¼Œ{moral}ã€‚",
                    "è™½ç„¶{challenge}ï¼Œä½†{character}ä¾ç„¶{positive_outcome}ã€‚"
                ]
            },
            'poem': {
                'styles': [
                    "äº”è¨€ç»å¥", "ä¸ƒè¨€å¾‹è¯—", "ç°ä»£è¯—", "æ•£æ–‡è¯—"
                ],
                'themes': [
                    "æ˜¥å¤©", "å‹æƒ…", "æ€ä¹¡", "åŠ±å¿—", "çˆ±æƒ…", "è‡ªç„¶"
                ],
                'patterns': [
                    "{adjective}çš„{noun}ï¼Œ{verb}åœ¨{location}ã€‚",
                    "{noun}å¦‚{metaphor}ï¼Œ{emotion}æ»¡{container}ã€‚"
                ]
            },
            'article': {
                'structures': [
                    "å¼•è¨€ -> è®ºç‚¹ -> è®ºè¯ -> ç»“è®º",
                    "é—®é¢˜ -> åˆ†æ -> è§£å†³æ–¹æ¡ˆ -> æ€»ç»“",
                    "ç°è±¡ -> åŸå›  -> å½±å“ -> å»ºè®®"
                ],
                'transitions': [
                    "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶è€Œ", "å› æ­¤", "æ€»ä¹‹", "ç»¼ä¸Šæ‰€è¿°"
                ]
            }
        }
        
        return templates
    
    def _build_style_patterns(self) -> Dict:
        """æ„å»ºé£æ ¼æ¨¡å¼"""
        patterns = {
            'formal': {
                'vocabulary': ['å› æ­¤', 'ç»¼ä¸Šæ‰€è¿°', 'é‰´äº', 'åŸºäº', 'æ®æ­¤'],
                'sentence_length': 'long',
                'tone': 'objective'
            },
            'casual': {
                'vocabulary': ['å…¶å®', 'æŒºå¥½çš„', 'æ„Ÿè§‰', 'è§‰å¾—', 'åº”è¯¥'],
                'sentence_length': 'medium',
                'tone': 'friendly'
            },
            'poetic': {
                'vocabulary': ['å¦‚æ¢¦', 'ä¼¼æ°´', 'é£˜æ¸º', 'ç¼¥ç¼ˆ', 'æ‚ ç„¶'],
                'sentence_length': 'varied',
                'tone': 'artistic'
            },
            'humorous': {
                'vocabulary': ['å“ˆå“ˆ', 'æœ‰è¶£', 'å¥½ç©', 'é€—ä¹', 'æç¬‘'],
                'sentence_length': 'short',
                'tone': 'playful'
            }
        }
        
        return patterns
    
    def _train_with_sample_data(self):
        """ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®­ç»ƒ"""
        sample_texts = [
            "æ˜¥å¤©æ¥äº†ï¼ŒèŠ±å„¿å¼€äº†ï¼Œé¸Ÿå„¿åœ¨æ ‘æä¸Šæ­Œå”±ã€‚é˜³å…‰é€è¿‡ç»¿å¶æ´’åœ¨å¤§åœ°ä¸Šï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—ç”Ÿæœºå‹ƒå‹ƒã€‚",
            "ç§‘æŠ€çš„å‘å±•æ”¹å˜äº†æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ã€äº‘è®¡ç®—ç­‰æŠ€æœ¯æ­£åœ¨é‡å¡‘å„ä¸ªè¡Œä¸šã€‚",
            "å‹æƒ…æ˜¯äººç”Ÿä¸­æœ€çè´µçš„è´¢å¯Œã€‚çœŸæ­£çš„æœ‹å‹ä¼šåœ¨ä½ å›°éš¾çš„æ—¶å€™ä¼¸å‡ºæ´æ‰‹ï¼Œåœ¨ä½ æˆåŠŸçš„æ—¶å€™ä¸ºä½ å–å½©ã€‚",
            "å­¦ä¹ æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚åªæœ‰ä¸æ–­åœ°å­¦ä¹ æ–°çŸ¥è¯†ï¼Œæˆ‘ä»¬æ‰èƒ½è·Ÿä¸Šæ—¶ä»£çš„æ­¥ä¼ï¼Œå®ç°ä¸ªäººçš„æˆé•¿ã€‚",
            "å¤§è‡ªç„¶çš„ç¾æ™¯æ€»æ˜¯è®©äººå¿ƒæ—·ç¥æ€¡ã€‚å±±å·æ²³æµã€æ—¥æœˆæ˜Ÿè¾°ï¼Œæ¯ä¸€å¤„é£æ™¯éƒ½æœ‰å…¶ç‹¬ç‰¹çš„é­…åŠ›ã€‚"
        ]
        
        self.language_model.train(sample_texts)
        print("ğŸ“– ç¤ºä¾‹æ•°æ®è®­ç»ƒå®Œæˆ")
    
    def generate_story(self, theme: str = None, length: str = "medium") -> Dict:
        """ç”Ÿæˆæ•…äº‹"""
        print(f"ğŸ“š ç”Ÿæˆæ•…äº‹ (ä¸»é¢˜: {theme}, é•¿åº¦: {length})")
        
        # æ•…äº‹å…ƒç´ 
        characters = ["å°æ˜", "å°çº¢", "è€å¸ˆ", "åŒ»ç”Ÿ", "å†œå¤«", "å­¦è€…"]
        locations = ["æ£®æ—", "åŸå¸‚", "å­¦æ ¡", "åŒ»é™¢", "å†œæ‘", "å›¾ä¹¦é¦†"]
        events = ["æ„å¤–å‘ç°", "é‡åˆ°å›°éš¾", "è·å¾—å¸®åŠ©", "å­¦åˆ°çŸ¥è¯†", "å®ç°æ¢¦æƒ³"]
        adjectives = ["ç¥å¥‡", "æœ‰è¶£", "æ„Ÿäºº", "åŠ±å¿—", "æ¸©æš–"]
        
        # é€‰æ‹©æ•…äº‹å…ƒç´ 
        character = random.choice(characters)
        location = random.choice(locations)
        event = random.choice(events)
        adjective = random.choice(adjectives)
        
        # ç”Ÿæˆæ•…äº‹ç»“æ„
        template = self.writing_templates['story']
        opening = random.choice(template['openings']).format(
            character=character, location=location, 
            event=event, adjective=adjective, time="å¾ˆä¹…ä»¥å‰"
        )
        
        development = random.choice(template['developments']).format(
            character=character, event=event, 
            action="å‹‡æ•¢é¢å¯¹", time="ç»è¿‡åŠªåŠ›", achievement="æˆåŠŸäº†"
        )
        
        ending = random.choice(template['endings']).format(
            character=character, adjective="å¹¸ç¦",
            moral="åšæŒå°±æ˜¯èƒœåˆ©", challenge="é‡åˆ°æŒ«æŠ˜",
            positive_outcome="æ²¡æœ‰æ”¾å¼ƒ"
        )
        
        # ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆç»†èŠ‚
        story_seed = opening
        generated_detail = self.language_model.generate_text(
            story_seed, max_length=50, temperature=0.8
        )
        
        story = {
            'title': f"{character}çš„{adjective}æ•…äº‹",
            'content': f"{opening}\n\n{development}\n\n{ending}",
            'generated_detail': generated_detail,
            'elements': {
                'character': character,
                'location': location,
                'theme': theme or adjective,
                'length': length
            },
            'word_count': len(opening + development + ending),
            'generation_time': time.time()
        }
        
        print(f"âœ… æ•…äº‹ç”Ÿæˆå®Œæˆ: {story['title']}")
        return story
    
    def generate_poem(self, theme: str = None, style: str = "ç°ä»£è¯—") -> Dict:
        """ç”Ÿæˆè¯—æ­Œ"""
        print(f"ğŸ­ ç”Ÿæˆè¯—æ­Œ (ä¸»é¢˜: {theme}, é£æ ¼: {style})")
        
        # è¯—æ­Œå…ƒç´ 
        themes_words = {
            'æ˜¥å¤©': ['èŠ±æœµ', 'ç»¿å¶', 'å¾®é£', 'é˜³å…‰', 'ç”Ÿæœº'],
            'å‹æƒ…': ['æœ‹å‹', 'çœŸè¯š', 'é™ªä¼´', 'æ¸©æš–', 'ä¿¡ä»»'],
            'æ€ä¹¡': ['å®¶ä¹¡', 'äº²äºº', 'å›å¿†', 'æ€å¿µ', 'æ•…åœŸ'],
            'åŠ±å¿—': ['æ¢¦æƒ³', 'åšæŒ', 'åŠªåŠ›', 'æˆåŠŸ', 'å¸Œæœ›']
        }
        
        selected_theme = theme or random.choice(list(themes_words.keys()))
        theme_words = themes_words.get(selected_theme, ['ç¾å¥½', 'ç”Ÿæ´»', 'å¸Œæœ›'])
        
        # ç”Ÿæˆè¯—å¥
        lines = []
        if style == "ç°ä»£è¯—":
            for i in range(4):
                word1 = random.choice(theme_words)
                word2 = random.choice(['å¦‚', 'åƒ', 'ä¼¼', 'è‹¥'])
                word3 = random.choice(['æ˜Ÿè¾°', 'æµæ°´', 'å¾®é£', 'é˜³å…‰'])
                
                line = f"{word1}{word2}{word3}ï¼Œ"
                # ä½¿ç”¨è¯­è¨€æ¨¡å‹è¡¥å……
                completion = self.language_model.generate_text(
                    line, max_length=10, temperature=1.0
                )
                lines.append(line + completion[:20])
        
        poem_content = '\n'.join(lines)
        
        poem = {
            'title': f"{selected_theme}ä¹‹æ­Œ",
            'content': poem_content,
            'style': style,
            'theme': selected_theme,
            'lines_count': len(lines),
            'generation_time': time.time()
        }
        
        print(f"âœ… è¯—æ­Œç”Ÿæˆå®Œæˆ: {poem['title']}")
        return poem
    
    def rewrite_with_style(self, text: str, target_style: str) -> Dict:
        """é£æ ¼æ”¹å†™"""
        print(f"ğŸ¨ é£æ ¼æ”¹å†™: {target_style}")
        
        style_config = self.style_patterns.get(target_style, self.style_patterns['casual'])
        
        # ç®€å•çš„é£æ ¼è½¬æ¢ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šæ›´å¤æ‚ï¼‰
        rewritten_text = text
        
        # æ·»åŠ é£æ ¼è¯æ±‡
        style_words = style_config['vocabulary']
        if target_style == 'formal':
            rewritten_text = "ç»¼ä¸Šæ‰€è¿°ï¼Œ" + rewritten_text + "å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è§‚ç‚¹ã€‚"
        elif target_style == 'casual':
            rewritten_text = "å…¶å®ï¼Œ" + rewritten_text + "æ„Ÿè§‰æŒºå¥½çš„ã€‚"
        elif target_style == 'poetic':
            rewritten_text = "å¦‚æ¢¦ä¼¼å¹»ï¼Œ" + rewritten_text + "æ‚ ç„¶è‡ªå¾—ã€‚"
        elif target_style == 'humorous':
            rewritten_text = "å“ˆå“ˆï¼Œ" + rewritten_text + "çœŸæ˜¯æœ‰è¶£ï¼"
        
        result = {
            'original_text': text,
            'rewritten_text': rewritten_text,
            'target_style': target_style,
            'style_features': style_config,
            'transformation_time': time.time()
        }
        
        print(f"âœ… é£æ ¼æ”¹å†™å®Œæˆ")
        return result
    
    def generate_summary(self, text: str, ratio: float = 0.3) -> Dict:
        """ç”Ÿæˆæ‘˜è¦"""
        print(f"ğŸ“„ ç”Ÿæˆæ‘˜è¦ (å‹ç¼©æ¯”: {ratio:.1%})")
        
        # ç®€åŒ–çš„æ‘˜è¦ç”Ÿæˆï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è®¡ç®—å¥å­é‡è¦æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        sentence_scores = {}
        word_freq = Counter()
        
        # ç»Ÿè®¡è¯é¢‘
        for sentence in sentences:
            words = list(sentence)
            word_freq.update(words)
        
        # è®¡ç®—å¥å­å¾—åˆ†
        for i, sentence in enumerate(sentences):
            words = list(sentence)
            score = sum(word_freq[word] for word in words) / len(words) if words else 0
            sentence_scores[i] = score
        
        # é€‰æ‹©é«˜åˆ†å¥å­
        num_sentences = max(1, int(len(sentences) * ratio))
        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
        top_sentences = sorted(top_sentences, key=lambda x: x[0])  # æŒ‰åŸé¡ºåºæ’åˆ—
        
        summary_sentences = [sentences[i] for i, _ in top_sentences]
        summary = 'ã€‚'.join(summary_sentences) + 'ã€‚'
        
        result = {
            'original_text': text,
            'summary': summary,
            'original_length': len(text),
            'summary_length': len(summary),
            'compression_ratio': len(summary) / len(text),
            'sentences_selected': len(summary_sentences),
            'total_sentences': len(sentences)
        }
        
        print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ (å‹ç¼©æ¯”: {result['compression_ratio']:.1%})")
        return result

class IntelligentWritingAssistant:
    """æ™ºèƒ½å†™ä½œåŠ©æ‰‹"""
    
    def __init__(self):
        self.creative_generator = CreativeWritingGenerator()
        self.writing_projects = {}
        self.user_preferences = {}
        self.writing_history = []
        
        print("ğŸ“ æ™ºèƒ½å†™ä½œåŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
        self._setup_writing_tools()
    
    def _setup_writing_tools(self):
        """è®¾ç½®å†™ä½œå·¥å…·"""
        self.writing_tools = {
            'grammar_checker': self._simple_grammar_check,
            'style_analyzer': self._analyze_writing_style,
            'readability_scorer': self._calculate_readability,
            'plagiarism_detector': self._detect_plagiarism,
            'word_counter': self._count_words,
            'sentiment_analyzer': self._analyze_sentiment
        }
        
        print("ğŸ”§ å†™ä½œå·¥å…·è®¾ç½®å®Œæˆ")
    
    def create_writing_project(self, project_name: str, project_type: str, 
                             requirements: Dict = None) -> str:
        """åˆ›å»ºå†™ä½œé¡¹ç›®"""
        project_id = f"project_{int(time.time() * 1000)}"
        
        project = {
            'id': project_id,
            'name': project_name,
            'type': project_type,  # story, article, poem, report
            'requirements': requirements or {},
            'content': "",
            'versions': [],
            'created_time': datetime.now(),
            'last_modified': datetime.now(),
            'status': 'draft',
            'word_count': 0,
            'feedback': []
        }
        
        self.writing_projects[project_id] = project
        
        print(f"ğŸ“‚ åˆ›å»ºå†™ä½œé¡¹ç›®: {project_name} ({project_type})")
        return project_id
    
    def assist_writing(self, project_id: str, user_input: str, 
                      assistance_type: str = "continue") -> Dict:
        """å†™ä½œè¾…åŠ©"""
        if project_id not in self.writing_projects:
            raise ValueError(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
        
        project = self.writing_projects[project_id]
        
        print(f"âœï¸ å†™ä½œè¾…åŠ©: {assistance_type}")
        
        assistance_result = {}
        
        if assistance_type == "continue":
            # ç»­å†™
            if project['type'] == 'story':
                generated = self.creative_generator.language_model.generate_text(
                    user_input, max_length=100, temperature=0.8
                )
                assistance_result = {
                    'type': 'continuation',
                    'original': user_input,
                    'suggestion': generated,
                    'confidence': 0.8
                }
            
        elif assistance_type == "improve":
            # æ”¹è¿›å»ºè®®
            style_analysis = self._analyze_writing_style(user_input)
            grammar_check = self._simple_grammar_check(user_input)
            
            assistance_result = {
                'type': 'improvement',
                'style_analysis': style_analysis,
                'grammar_suggestions': grammar_check,
                'readability_score': self._calculate_readability(user_input)
            }
            
        elif assistance_type == "rewrite":
            # é‡å†™
            styles = ['formal', 'casual', 'poetic']
            rewrites = {}
            for style in styles:
                rewrite = self.creative_generator.rewrite_with_style(user_input, style)
                rewrites[style] = rewrite['rewritten_text']
            
            assistance_result = {
                'type': 'rewrite',
                'original': user_input,
                'rewrites': rewrites
            }
            
        elif assistance_type == "summarize":
            # æ‘˜è¦
            summary = self.creative_generator.generate_summary(user_input)
            assistance_result = {
                'type': 'summary',
                'original': user_input,
                'summary': summary['summary'],
                'compression_ratio': summary['compression_ratio']
            }
        
        # æ›´æ–°é¡¹ç›®
        project['content'] = user_input
        project['last_modified'] = datetime.now()
        project['word_count'] = len(user_input)
        project['feedback'].append(assistance_result)
        
        # è®°å½•å†™ä½œå†å²
        self.writing_history.append({
            'project_id': project_id,
            'timestamp': datetime.now(),
            'assistance_type': assistance_type,
            'input_length': len(user_input),
            'result': assistance_result
        })
        
        print(f"âœ… å†™ä½œè¾…åŠ©å®Œæˆ: {assistance_type}")
        return assistance_result
    
    def _simple_grammar_check(self, text: str) -> List[Dict]:
        """ç®€å•è¯­æ³•æ£€æŸ¥"""
        issues = []
        
        # æ£€æŸ¥å¸¸è§é—®é¢˜
        if 'çš„çš„' in text:
            issues.append({'type': 'é‡å¤è¯', 'text': 'çš„çš„', 'suggestion': 'åˆ é™¤é‡å¤çš„"çš„"'})
        
        if text.count('ï¼Œ') > text.count('ã€‚') * 3:
            issues.append({'type': 'æ ‡ç‚¹ç¬¦å·', 'text': 'é€—å·è¿‡å¤š', 'suggestion': 'è€ƒè™‘ä½¿ç”¨å¥å·åˆ†å‰²é•¿å¥'})
        
        return issues
    
    def _analyze_writing_style(self, text: str) -> Dict:
        """åˆ†æå†™ä½œé£æ ¼"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        avg_sentence_length = np.mean([len(s) for s in sentences]) if sentences else 0
        
        style_analysis = {
            'sentence_count': len(sentences),
            'average_sentence_length': avg_sentence_length,
            'complexity': 'simple' if avg_sentence_length < 15 else 'complex',
            'punctuation_variety': len(set(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', text))),
            'estimated_reading_time': len(text) / 300  # å‡è®¾æ¯åˆ†é’Ÿ300å­—
        }
        
        return style_analysis
    
    def _calculate_readability(self, text: str) -> float:
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        # ç®€åŒ–çš„å¯è¯»æ€§è®¡ç®—
        sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ]', text))
        words = len(text)
        
        if sentences == 0:
            return 0
        
        avg_sentence_length = words / sentences
        readability = max(0, min(100, 100 - avg_sentence_length * 2))
        
        return readability
    
    def _detect_plagiarism(self, text: str) -> Dict:
        """æ£€æµ‹æŠ„è¢­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”ç”¨éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
        return {
            'plagiarism_score': 0.0,
            'similar_sources': [],
            'original_content_ratio': 1.0
        }
    
    def _count_words(self, text: str) -> Dict:
        """ç»Ÿè®¡è¯æ•°"""
        characters = len(text)
        words = len(text.split()) if ' ' in text else len(text)
        paragraphs = len(text.split('\n\n'))
        
        return {
            'characters': characters,
            'words': words,
            'paragraphs': paragraphs,
            'estimated_reading_time': characters / 300
        }
    
    def _analyze_sentiment(self, text: str) -> Dict:
        """åˆ†ææƒ…æ„Ÿå€¾å‘"""
        # ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'ç¾å¥½', 'å¼€å¿ƒ', 'æ»¡æ„']
        negative_words = ['å·®', 'ç³Ÿ', 'ä¸å¥½', 'å¤±æœ›', 'éš¾è¿‡', 'ç”Ÿæ°”']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            sentiment = 'positive'
        elif negative_count > positive_count:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'sentiment': sentiment,
            'positive_score': positive_count,
            'negative_score': negative_count,
            'confidence': abs(positive_count - negative_count) / max(1, positive_count + negative_count)
        }
    
    def get_writing_suggestions(self, project_id: str) -> List[str]:
        """è·å–å†™ä½œå»ºè®®"""
        if project_id not in self.writing_projects:
            return []
        
        project = self.writing_projects[project_id]
        suggestions = []
        
        # åŸºäºé¡¹ç›®ç±»å‹çš„å»ºè®®
        if project['type'] == 'story':
            suggestions.extend([
                "è€ƒè™‘æ·»åŠ æ›´å¤šçš„å¯¹è¯æ¥å¢å¼ºæ•…äº‹çš„ç”ŸåŠ¨æ€§",
                "æè¿°äººç‰©çš„å†…å¿ƒæ´»åŠ¨å¯ä»¥è®©è¯»è€…æ›´å¥½åœ°ç†è§£è§’è‰²",
                "é€‚å½“çš„ç¯å¢ƒæå†™èƒ½å¤Ÿè¥é€ æ°›å›´"
            ])
        elif project['type'] == 'article':
            suggestions.extend([
                "ç¡®ä¿è®ºç‚¹æ¸…æ™°ï¼Œè®ºæ®å……åˆ†",
                "ä½¿ç”¨è¿‡æ¸¡è¯æ¥è¿æ¥æ®µè½",
                "æ€»ç»“æ®µè½åº”è¯¥å‘¼åº”å¼€å¤´"
            ])
        
        # åŸºäºå†…å®¹é•¿åº¦çš„å»ºè®®
        if project['word_count'] < 100:
            suggestions.append("å†…å®¹è¾ƒçŸ­ï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šç»†èŠ‚å’Œä¾‹å­")
        elif project['word_count'] > 1000:
            suggestions.append("å†…å®¹è¾ƒé•¿ï¼Œè€ƒè™‘åˆ†æ®µæˆ–åˆ é™¤å†—ä½™ä¿¡æ¯")
        
        return suggestions
    
    def export_project(self, project_id: str, format: str = "text") -> Dict:
        """å¯¼å‡ºé¡¹ç›®"""
        if project_id not in self.writing_projects:
            raise ValueError(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
        
        project = self.writing_projects[project_id]
        
        export_data = {
            'project_info': {
                'name': project['name'],
                'type': project['type'],
                'created_time': project['created_time'].isoformat(),
                'word_count': project['word_count']
            },
            'content': project['content'],
            'statistics': self._count_words(project['content']),
            'format': format
        }
        
        print(f"ğŸ“¤ å¯¼å‡ºé¡¹ç›®: {project['name']} ({format})")
        return export_data

# æ¼”ç¤ºæ™ºèƒ½å†™ä½œåŠ©æ‰‹
def demo_writing_assistant():
    """æ¼”ç¤ºå†™ä½œåŠ©æ‰‹"""
    # åˆ›å»ºå†™ä½œåŠ©æ‰‹
    assistant = IntelligentWritingAssistant()
    
    # åˆ›å»ºå†™ä½œé¡¹ç›®
    project_id = assistant.create_writing_project(
        "æˆ‘çš„ç¬¬ä¸€ä¸ªæ•…äº‹", 
        "story",
        {"theme": "å‹æƒ…", "length": "short"}
    )
    
    # å†™ä½œè¾…åŠ©æ¼”ç¤º
    print(f"\nğŸ“ å†™ä½œè¾…åŠ©æ¼”ç¤º:")
    print("=" * 30)
    
    # 1. å¼€å§‹å†™ä½œ
    user_input = "å°æ˜å’Œå°çº¢æ˜¯æœ€å¥½çš„æœ‹å‹ï¼Œä»–ä»¬ç»å¸¸ä¸€èµ·ç©è€ã€‚"
    
    # 2. ç»­å†™è¾…åŠ©
    continue_result = assistant.assist_writing(project_id, user_input, "continue")
    print(f"ğŸ“– ç»­å†™å»ºè®®: {continue_result.get('suggestion', '')[:50]}...")
    
    # 3. æ”¹è¿›å»ºè®®
    improve_result = assistant.assist_writing(project_id, user_input, "improve")
    print(f"ğŸ“Š å¯è¯»æ€§åˆ†æ•°: {improve_result.get('readability_score', 0):.1f}")
    
    # 4. é£æ ¼é‡å†™
    rewrite_result = assistant.assist_writing(project_id, user_input, "rewrite")
    print(f"ğŸ¨ æ­£å¼é£æ ¼é‡å†™: {rewrite_result.get('rewrites', {}).get('formal', '')[:50]}...")
    
    # 5. ç”Ÿæˆåˆ›æ„å†…å®¹
    story = assistant.creative_generator.generate_story("å‹æƒ…", "short")
    print(f"ğŸ“š ç”Ÿæˆæ•…äº‹: {story['title']}")
    
    poem = assistant.creative_generator.generate_poem("å‹æƒ…", "ç°ä»£è¯—")
    print(f"ğŸ­ ç”Ÿæˆè¯—æ­Œ: {poem['title']}")
    
    # 6. è·å–å†™ä½œå»ºè®®
    suggestions = assistant.get_writing_suggestions(project_id)
    print(f"ğŸ’¡ å†™ä½œå»ºè®®: {len(suggestions)} æ¡")
    
    # 7. å¯¼å‡ºé¡¹ç›®
    export_data = assistant.export_project(project_id)
    
    return assistant, story, poem, export_data

# è¿è¡Œæ¼”ç¤º
writing_assistant, generated_story, generated_poem, project_export = demo_writing_assistant()

## 32.6 æ–‡æœ¬ç”Ÿæˆä¸è¯­è¨€æ¨¡å‹

### ğŸ§  è¯­è¨€ç”Ÿæˆå®éªŒå®¤ï¼šè®©AIå­¦ä¼šåˆ›ä½œ

åœ¨**è¯­è¨€ç†è§£ç ”ç©¶é™¢**çš„æ ¸å¿ƒåŒºåŸŸï¼Œåè½ç€æœ€ç¥ç§˜çš„**è¯­è¨€ç”Ÿæˆå®éªŒå®¤**ã€‚è¿™é‡Œä¸ä»…è¦è®©AIç†è§£è¯­è¨€ï¼Œæ›´è¦è®©å®ƒå­¦ä¼šåˆ›é€ è¯­è¨€ã€‚å¦‚æœè¯´å‰é¢çš„ç« èŠ‚æ•™ä¼šäº†AIå¦‚ä½•"è¯»æ‡‚"æ–‡å­—ï¼Œé‚£ä¹ˆè¿™ä¸€èŠ‚å°†æ•™ä¼šAIå¦‚ä½•"å†™å‡º"æ–‡å­—ã€‚

```mermaid
graph TB
    A[è¯­è¨€ç”Ÿæˆå®éªŒå®¤] --> B[è¯­è¨€æ¨¡å‹ç ”ç©¶éƒ¨]
    A --> C[æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ]
    A --> D[åˆ›æ„å†™ä½œä¸­å¿ƒ]
    A --> E[æ‘˜è¦ç”Ÿæˆéƒ¨é—¨]
    
    B --> B1[N-gramæ¨¡å‹]
    B --> B2[RNNè¯­è¨€æ¨¡å‹]
    B --> B3[Transformeræ¨¡å‹]
    B --> B4[GPTç³»åˆ—æ¨¡å‹]
    
    C --> C1[æ¡ä»¶ç”Ÿæˆ]
    C --> C2[æ— æ¡ä»¶ç”Ÿæˆ]
    C --> C3[æ§åˆ¶ç”Ÿæˆ]
    C --> C4[äº¤äº’ç”Ÿæˆ]
    
    D --> D1[æ•…äº‹ç”Ÿæˆ]
    D --> D2[è¯—æ­Œåˆ›ä½œ]
    D --> D3[å¯¹è¯ç”Ÿæˆ]
    D --> D4[æ–°é—»å†™ä½œ]
    
    E --> E1[æŠ½å–å¼æ‘˜è¦]
    E --> E2[ç”Ÿæˆå¼æ‘˜è¦]
    E --> E3[å¤šæ–‡æ¡£æ‘˜è¦]
    E --> E4[ä¸ªæ€§åŒ–æ‘˜è¦]
```

### ğŸ“š è¯­è¨€æ¨¡å‹åŸºç¡€ç†è®º

è¯­è¨€æ¨¡å‹æ˜¯æ–‡æœ¬ç”Ÿæˆçš„æ ¸å¿ƒï¼Œå®ƒè¯•å›¾å­¦ä¹ è¯­è¨€çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯æˆ–å­—ç¬¦å‡ºç°çš„æ¦‚ç‡ã€‚

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import defaultdict, Counter
import random
import re
from typing import List, Dict, Tuple, Optional
import pickle

class LanguageModelLab:
    """è¯­è¨€æ¨¡å‹å®éªŒå®¤"""
    
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.training_data = {}
        
        print("ğŸ§  è¯­è¨€æ¨¡å‹å®éªŒå®¤åˆå§‹åŒ–å®Œæˆ")
        
    def create_ngram_model(self, name: str, n: int = 3) -> 'NGramLanguageModel':
        """åˆ›å»ºN-gramè¯­è¨€æ¨¡å‹"""
        model = NGramLanguageModel(n)
        self.models[name] = model
        print(f"ğŸ“Š åˆ›å»º {n}-gram è¯­è¨€æ¨¡å‹: {name}")
        return model
    
    def create_rnn_model(self, name: str, vocab_size: int, 
                        embedding_dim: int = 128, hidden_dim: int = 256) -> 'RNNLanguageModel':
        """åˆ›å»ºRNNè¯­è¨€æ¨¡å‹"""
        model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim)
        self.models[name] = model
        print(f"ğŸ”„ åˆ›å»ºRNNè¯­è¨€æ¨¡å‹: {name}")
        return model
    
    def create_transformer_model(self, name: str, vocab_size: int,
                               d_model: int = 512, nhead: int = 8, 
                               num_layers: int = 6) -> 'TransformerLanguageModel':
        """åˆ›å»ºTransformerè¯­è¨€æ¨¡å‹"""
        model = TransformerLanguageModel(vocab_size, d_model, nhead, num_layers)
        self.models[name] = model
        print(f"ğŸ”„ åˆ›å»ºTransformerè¯­è¨€æ¨¡å‹: {name}")
        return model
    
    def train_model(self, model_name: str, texts: List[str], 
                   epochs: int = 10, batch_size: int = 32):
        """è®­ç»ƒè¯­è¨€æ¨¡å‹"""
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        
        model = self.models[model_name]
        
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(texts)} ä¸ªæ–‡æœ¬")
        
        if isinstance(model, NGramLanguageModel):
            model.train(texts)
        else:
            # å¯¹äºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œéœ€è¦æ›´å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹
            self._train_neural_model(model, texts, epochs, batch_size)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_name}")
    
    def generate_text(self, model_name: str, prompt: str = "", 
                     max_length: int = 100, temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        
        model = self.models[model_name]
        generated = model.generate(prompt, max_length, temperature)
        
        print(f"ğŸ“ ç”Ÿæˆæ–‡æœ¬ ({model_name}): {generated[:50]}...")
        return generated
    
    def _train_neural_model(self, model, texts: List[str], 
                           epochs: int, batch_size: int):
        """è®­ç»ƒç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹"""
        # ç®€åŒ–çš„è®­ç»ƒè¿‡ç¨‹
        tokenizer = self._create_tokenizer(texts)
        self.tokenizers[id(model)] = tokenizer
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        sequences = []
        for text in texts:
            tokens = tokenizer.encode(text)
            for i in range(len(tokens) - 1):
                sequences.append((tokens[i], tokens[i + 1]))
        
        if isinstance(model, (RNNLanguageModel, TransformerLanguageModel)):
            model.train_on_sequences(sequences, epochs)
    
    def _create_tokenizer(self, texts: List[str]) -> 'SimpleTokenizer':
        """åˆ›å»ºç®€å•åˆ†è¯å™¨"""
        return SimpleTokenizer(texts)

class NGramLanguageModel:
    """N-gramè¯­è¨€æ¨¡å‹"""
    
    def __init__(self, n: int = 3):
        self.n = n
        self.ngrams = defaultdict(Counter)
        self.vocab = set()
        
    def train(self, texts: List[str]):
        """è®­ç»ƒN-gramæ¨¡å‹"""
        for text in texts:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“ä¸šåˆ†è¯å·¥å…·ï¼‰
            tokens = self._tokenize(text)
            self.vocab.update(tokens)
            
            # æ„å»ºN-gram
            for i in range(len(tokens) - self.n + 1):
                context = tuple(tokens[i:i + self.n - 1])
                next_token = tokens[i + self.n - 1]
                self.ngrams[context][next_token] += 1
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯"""
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
        text = re.sub(r'[^\w\s]', '', text)
        return list(text.replace(' ', ''))
    
    def generate(self, prompt: str = "", max_length: int = 100, 
                temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not prompt:
            # éšæœºé€‰æ‹©ä¸€ä¸ªä¸Šä¸‹æ–‡å¼€å§‹
            context = random.choice(list(self.ngrams.keys()))
        else:
            tokens = self._tokenize(prompt)
            if len(tokens) >= self.n - 1:
                context = tuple(tokens[-(self.n - 1):])
            else:
                # è¡¥å……éšæœºtoken
                context = tuple(tokens + [''] * (self.n - 1 - len(tokens)))
        
        generated = list(context)
        
        for _ in range(max_length):
            if context not in self.ngrams:
                break
            
            candidates = self.ngrams[context]
            if not candidates:
                break
            
            # åº”ç”¨æ¸©åº¦é‡‡æ ·
            tokens = list(candidates.keys())
            probs = np.array(list(candidates.values()), dtype=float)
            probs = probs ** (1 / temperature)
            probs = probs / probs.sum()
            
            next_token = np.random.choice(tokens, p=probs)
            generated.append(next_token)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context = tuple(generated[-(self.n - 1):])
        
        return ''.join(generated)
    
    def get_perplexity(self, text: str) -> float:
        """è®¡ç®—å›°æƒ‘åº¦"""
        tokens = self._tokenize(text)
        log_prob = 0
        count = 0
        
        for i in range(len(tokens) - self.n + 1):
            context = tuple(tokens[i:i + self.n - 1])
            next_token = tokens[i + self.n - 1]
            
            if context in self.ngrams:
                total_count = sum(self.ngrams[context].values())
                token_count = self.ngrams[context][next_token]
                if token_count > 0:
                    prob = token_count / total_count
                    log_prob += np.log(prob)
                    count += 1
        
        if count == 0:
            return float('inf')
        
        avg_log_prob = log_prob / count
        perplexity = np.exp(-avg_log_prob)
        return perplexity

class RNNLanguageModel(nn.Module):
    """RNNè¯­è¨€æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, 
                 hidden_dim: int = 256, num_layers: int = 2):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                           batch_first=True, dropout=0.3)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.3)
        
        self.optimizer = optim.Adam(self.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
    
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        output, hidden = self.lstm(embedded, hidden)
        output = self.dropout(output)
        output = self.fc(output)
        return output, hidden
    
    def train_on_sequences(self, sequences: List[Tuple[int, int]], epochs: int):
        """åœ¨åºåˆ—ä¸Šè®­ç»ƒ"""
        self.train()
        
        for epoch in range(epochs):
            total_loss = 0
            random.shuffle(sequences)
            
            for i in range(0, len(sequences), 32):  # batch_size = 32
                batch = sequences[i:i + 32]
                if len(batch) < 2:
                    continue
                
                inputs = torch.tensor([seq[0] for seq in batch]).unsqueeze(1)
                targets = torch.tensor([seq[1] for seq in batch])
                
                self.optimizer.zero_grad()
                outputs, _ = self(inputs)
                loss = self.criterion(outputs.squeeze(1), targets)
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 5 == 0:
                print(f"Epoch {epoch}, Loss: {total_loss / len(sequences):.4f}")
    
    def generate(self, prompt: str = "", max_length: int = 100, 
                temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        self.eval()
        generated = []
        
        # ç®€åŒ–çš„ç”Ÿæˆè¿‡ç¨‹
        with torch.no_grad():
            current_token = 1  # å‡è®¾1æ˜¯èµ·å§‹token
            hidden = None
            
            for _ in range(max_length):
                input_tensor = torch.tensor([[current_token]])
                output, hidden = self(input_tensor, hidden)
                
                # åº”ç”¨æ¸©åº¦
                logits = output[0, -1] / temperature
                probs = torch.softmax(logits, dim=0)
                
                # é‡‡æ ·
                current_token = torch.multinomial(probs, 1).item()
                
                # å‡è®¾çš„tokenåˆ°å­—ç¬¦è½¬æ¢
                if current_token < 128:
                    generated.append(chr(current_token + ord('a')))
                else:
                    break
        
        return ''.join(generated)

class TransformerLanguageModel(nn.Module):
    """Transformerè¯­è¨€æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, d_model: int = 512, 
                 nhead: int = 8, num_layers: int = 6):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = self._create_pos_encoding(1000, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, vocab_size)
        
        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)
        self.criterion = nn.CrossEntropyLoss()
    
    def _create_pos_encoding(self, max_len: int, d_model: int):
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)
    
    def forward(self, x):
        seq_len = x.size(1)
        embedded = self.embedding(x) * np.sqrt(self.d_model)
        embedded += self.pos_encoding[:, :seq_len]
        
        # åˆ›å»ºcausal mask
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        
        output = self.transformer(embedded, mask=mask)
        output = self.fc(output)
        return output
    
    def train_on_sequences(self, sequences: List[Tuple[int, int]], epochs: int):
        """è®­ç»ƒæ¨¡å‹"""
        # ç®€åŒ–çš„è®­ç»ƒå®ç°
        print(f"ğŸ”„ Transformeræ¨¡å‹è®­ç»ƒ {epochs} ä¸ªepoch")
    
    def generate(self, prompt: str = "", max_length: int = 100, 
                temperature: float = 1.0) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        return "Transformerç”Ÿæˆçš„ç¤ºä¾‹æ–‡æœ¬..."

class SimpleTokenizer:
    """ç®€å•åˆ†è¯å™¨"""
    
    def __init__(self, texts: List[str]):
        self.vocab = {}
        self.reverse_vocab = {}
        self._build_vocab(texts)
    
    def _build_vocab(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        all_chars = set()
        for text in texts:
            all_chars.update(text)
        
        # æ·»åŠ ç‰¹æ®Štoken
        all_chars.update(['<pad>', '<unk>', '<start>', '<end>'])
        
        self.vocab = {char: i for i, char in enumerate(sorted(all_chars))}
        self.reverse_vocab = {i: char for char, i in self.vocab.items()}
    
    def encode(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        return [self.vocab.get(char, self.vocab['<unk>']) for char in text]
    
    def decode(self, tokens: List[int]) -> str:
        """è§£ç token"""
        return ''.join([self.reverse_vocab.get(token, '<unk>') for token in tokens])

# æ¼”ç¤ºè¯­è¨€æ¨¡å‹å®éªŒå®¤
def demo_language_model_lab():
    """æ¼”ç¤ºè¯­è¨€æ¨¡å‹å®éªŒå®¤"""
    # åˆ›å»ºå®éªŒå®¤
    lab = LanguageModelLab()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    training_texts = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚",
        "æˆ‘å–œæ¬¢åœ¨å…¬å›­é‡Œæ•£æ­¥ï¼Œæ„Ÿå—å¤§è‡ªç„¶çš„ç¾å¥½ã€‚",
        "å­¦ä¹ äººå·¥æ™ºèƒ½æ˜¯ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…ã€‚",
        "æœºå™¨å­¦ä¹ å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³å¾ˆå¤šé—®é¢˜ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚"
    ]
    
    print(f"\nğŸ§  è¯­è¨€æ¨¡å‹å®éªŒå®¤æ¼”ç¤º")
    print("=" * 40)
    
    # 1. åˆ›å»ºå’Œè®­ç»ƒN-gramæ¨¡å‹
    ngram_model = lab.create_ngram_model("ä¸­æ–‡3gram", 3)
    lab.train_model("ä¸­æ–‡3gram", training_texts)
    
    # ç”Ÿæˆæ–‡æœ¬
    generated_ngram = lab.generate_text("ä¸­æ–‡3gram", "ä»Šå¤©", max_length=20)
    print(f"ğŸ“Š 3-gramç”Ÿæˆ: {generated_ngram}")
    
    # è®¡ç®—å›°æƒ‘åº¦
    test_text = "ä»Šå¤©å¤©æ°”ä¸é”™"
    perplexity = ngram_model.get_perplexity(test_text)
    print(f"ğŸ“ˆ å›°æƒ‘åº¦: {perplexity:.2f}")
    
    # 2. åˆ›å»ºRNNæ¨¡å‹
    vocab_size = 1000
    rnn_model = lab.create_rnn_model("ä¸­æ–‡RNN", vocab_size)
    
    # 3. åˆ›å»ºTransformeræ¨¡å‹
    transformer_model = lab.create_transformer_model("ä¸­æ–‡Transformer", vocab_size)
    
    return lab, ngram_model, generated_ngram

# è¿è¡Œæ¼”ç¤º
language_lab, trained_ngram, sample_generation = demo_language_model_lab()

### ğŸ¨ æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ

åœ¨æŒæ¡äº†è¯­è¨€æ¨¡å‹çš„åŸºç¡€ç†è®ºåï¼Œæˆ‘ä»¬æ¥åˆ°**æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ**ï¼Œè¿™é‡Œä¸“é—¨ç ”ç©¶å¦‚ä½•è®©AIç”Ÿæˆå„ç§ç±»å‹çš„æ–‡æœ¬å†…å®¹ã€‚

```python
class TextGenerationWorkshop:
    """æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ"""
    
    def __init__(self):
        self.generators = {}
        self.templates = {}
        self.style_models = {}
        self.generation_history = []
        
        # é¢„å®šä¹‰æ¨¡æ¿
        self._init_templates()
        
        print("ğŸ¨ æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠåˆå§‹åŒ–å®Œæˆ")
    
    def _init_templates(self):
        """åˆå§‹åŒ–ç”Ÿæˆæ¨¡æ¿"""
        self.templates = {
            "æ–°é—»": {
                "ç»“æ„": ["æ ‡é¢˜", "å¯¼è¯­", "æ­£æ–‡", "ç»“å°¾"],
                "é£æ ¼": "å®¢è§‚ã€å‡†ç¡®ã€ç®€æ´",
                "è¦ç´ ": ["æ—¶é—´", "åœ°ç‚¹", "äººç‰©", "äº‹ä»¶", "åŸå› ", "ç»“æœ"]
            },
            "æ•…äº‹": {
                "ç»“æ„": ["å¼€å¤´", "å‘å±•", "é«˜æ½®", "ç»“å±€"],
                "é£æ ¼": "ç”ŸåŠ¨ã€æœ‰è¶£ã€å¼•äººå…¥èƒœ",
                "è¦ç´ ": ["äººç‰©", "æƒ…èŠ‚", "ç¯å¢ƒ", "ä¸»é¢˜"]
            },
            "è¯—æ­Œ": {
                "ç»“æ„": ["èµ·å¥", "æ‰¿å¥", "è½¬å¥", "åˆå¥"],
                "é£æ ¼": "ä¼˜ç¾ã€æŠ’æƒ…ã€å¯Œæœ‰éŸµå¾‹",
                "è¦ç´ ": ["æ„è±¡", "æƒ…æ„Ÿ", "éŸµå¾‹", "æ„å¢ƒ"]
            },
            "è¯´æ˜æ–‡": {
                "ç»“æ„": ["æ¦‚è¿°", "åˆ†ç±»è¯´æ˜", "ä¸¾ä¾‹è¯´æ˜", "æ€»ç»“"],
                "é£æ ¼": "æ¸…æ™°ã€å‡†ç¡®ã€æ¡ç†",
                "è¦ç´ ": ["å®šä¹‰", "ç‰¹å¾", "åˆ†ç±»", "ä¾‹å­"]
            }
        }
    
    def create_conditional_generator(self, name: str, condition_type: str) -> 'ConditionalTextGenerator':
        """åˆ›å»ºæ¡ä»¶æ–‡æœ¬ç”Ÿæˆå™¨"""
        generator = ConditionalTextGenerator(condition_type)
        self.generators[name] = generator
        print(f"ğŸ¯ åˆ›å»ºæ¡ä»¶ç”Ÿæˆå™¨: {name} ({condition_type})")
        return generator
    
    def create_style_transfer_model(self, name: str) -> 'StyleTransferModel':
        """åˆ›å»ºé£æ ¼è¿ç§»æ¨¡å‹"""
        model = StyleTransferModel()
        self.style_models[name] = model
        print(f"ğŸ­ åˆ›å»ºé£æ ¼è¿ç§»æ¨¡å‹: {name}")
        return model
    
    def generate_with_template(self, template_type: str, **kwargs) -> Dict:
        """åŸºäºæ¨¡æ¿ç”Ÿæˆæ–‡æœ¬"""
        if template_type not in self.templates:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡æ¿ç±»å‹: {template_type}")
        
        template = self.templates[template_type]
        generator = TemplateBasedGenerator(template)
        
        result = generator.generate(**kwargs)
        
        # è®°å½•ç”Ÿæˆå†å²
        self.generation_history.append({
            'type': template_type,
            'timestamp': datetime.now(),
            'result': result,
            'parameters': kwargs
        })
        
        print(f"ğŸ“ åŸºäºæ¨¡æ¿ç”Ÿæˆ {template_type}: {result['title'][:20]}...")
        return result
    
    def multi_style_generation(self, content: str, styles: List[str]) -> Dict[str, str]:
        """å¤šé£æ ¼ç”Ÿæˆ"""
        results = {}
        
        for style in styles:
            if style in self.style_models:
                model = self.style_models[style]
                results[style] = model.transfer_style(content, style)
            else:
                # ä½¿ç”¨è§„åˆ™åŸºç¡€çš„é£æ ¼è½¬æ¢
                results[style] = self._rule_based_style_transfer(content, style)
        
        print(f"ğŸ¨ å¤šé£æ ¼ç”Ÿæˆå®Œæˆ: {len(styles)} ç§é£æ ¼")
        return results
    
    def _rule_based_style_transfer(self, content: str, style: str) -> str:
        """åŸºäºè§„åˆ™çš„é£æ ¼è½¬æ¢"""
        if style == "æ­£å¼":
            # æ›¿æ¢å£è¯­åŒ–è¡¨è¾¾
            content = content.replace("å¾ˆå¥½", "ä¼˜ç§€")
            content = content.replace("ä¸é”™", "è‰¯å¥½")
            content = content.replace("æŒº", "ç›¸å½“")
        elif style == "å£è¯­":
            # æ·»åŠ å£è¯­åŒ–è¡¨è¾¾
            content = content.replace("ã€‚", "å‘¢ã€‚")
            content = content.replace("ä¼˜ç§€", "å¾ˆæ£’")
        elif style == "å¤å…¸":
            # æ·»åŠ å¤å…¸è¯æ±‡
            content = content.replace("å¾ˆ", "ç”š")
            content = content.replace("çš„", "ä¹‹")
        
        return content
    
    def interactive_generation(self, initial_prompt: str, max_turns: int = 5) -> List[str]:
        """äº¤äº’å¼ç”Ÿæˆ"""
        conversation = [initial_prompt]
        current_context = initial_prompt
        
        for turn in range(max_turns):
            # åŸºäºå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤
            response = self._generate_response(current_context)
            conversation.append(response)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            current_context = f"{current_context}\n{response}"
            
            print(f"ğŸ”„ ç¬¬{turn + 1}è½®ç”Ÿæˆ: {response[:30]}...")
        
        return conversation
    
    def _generate_response(self, context: str) -> str:
        """ç”Ÿæˆå›å¤"""
        # ç®€åŒ–çš„å›å¤ç”Ÿæˆ
        responses = [
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„è§‚ç‚¹ã€‚",
            "è®©æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚",
            "ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘...",
            "è¿™è®©æˆ‘æƒ³åˆ°äº†ç›¸å…³çš„ä¾‹å­ã€‚",
            "æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªè¯é¢˜å€¼å¾—æ·±å…¥æ€è€ƒã€‚"
        ]
        
        return random.choice(responses)
    
    def batch_generation(self, prompts: List[str], generation_type: str = "continue") -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆ"""
        results = []
        
        for i, prompt in enumerate(prompts):
            if generation_type == "continue":
                generated = self._continue_text(prompt)
            elif generation_type == "summarize":
                generated = self._summarize_text(prompt)
            elif generation_type == "rewrite":
                generated = self._rewrite_text(prompt)
            else:
                generated = prompt
            
            results.append({
                'id': i,
                'prompt': prompt,
                'generated': generated,
                'type': generation_type
            })
            
            print(f"ğŸ“‹ æ‰¹é‡ç”Ÿæˆè¿›åº¦: {i + 1}/{len(prompts)}")
        
        return results
    
    def _continue_text(self, prompt: str) -> str:
        """ç»­å†™æ–‡æœ¬"""
        continuations = [
            "æ¥ä¸‹æ¥çš„å‘å±•è®©äººæ„æƒ³ä¸åˆ°ã€‚",
            "éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæƒ…å†µå‘ç”Ÿäº†å˜åŒ–ã€‚",
            "è¿™ä¸ªå†³å®šäº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚",
            "æ–°çš„æœºé‡å’ŒæŒ‘æˆ˜åŒæ—¶å‡ºç°ã€‚"
        ]
        return prompt + random.choice(continuations)
    
    def _summarize_text(self, text: str) -> str:
        """æ‘˜è¦ç”Ÿæˆ"""
        # ç®€åŒ–çš„æ‘˜è¦ç”Ÿæˆ
        sentences = text.split('ã€‚')
        if len(sentences) > 2:
            return sentences[0] + 'ã€‚' + sentences[-2] + 'ã€‚'
        return text
    
    def _rewrite_text(self, text: str) -> str:
        """é‡å†™æ–‡æœ¬"""
        # ç®€å•çš„é‡å†™é€»è¾‘
        return f"æ¢ä¸ªè¯´æ³•ï¼š{text}"
    
    def evaluate_generation_quality(self, generated_text: str, reference_text: str = None) -> Dict:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        quality_metrics = {
            'length': len(generated_text),
            'sentence_count': len(generated_text.split('ã€‚')),
            'word_diversity': len(set(generated_text)) / len(generated_text) if generated_text else 0,
            'readability': self._calculate_readability(generated_text),
            'coherence': self._calculate_coherence(generated_text),
            'creativity': self._calculate_creativity(generated_text)
        }
        
        if reference_text:
            quality_metrics['similarity'] = self._calculate_similarity(generated_text, reference_text)
        
        # ç»¼åˆè¯„åˆ†
        quality_metrics['overall_score'] = (
            quality_metrics['readability'] * 0.3 +
            quality_metrics['coherence'] * 0.3 +
            quality_metrics['creativity'] * 0.2 +
            quality_metrics['word_diversity'] * 100 * 0.2
        )
        
        return quality_metrics
    
    def _calculate_readability(self, text: str) -> float:
        """è®¡ç®—å¯è¯»æ€§"""
        if not text:
            return 0
        
        sentences = len(text.split('ã€‚'))
        words = len(text)
        
        if sentences == 0:
            return 0
        
        avg_sentence_length = words / sentences
        readability = max(0, min(100, 100 - avg_sentence_length * 2))
        
        return readability
    
    def _calculate_coherence(self, text: str) -> float:
        """è®¡ç®—è¿è´¯æ€§"""
        # ç®€åŒ–çš„è¿è´¯æ€§è®¡ç®—
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        if len(sentences) < 2:
            return 50
        
        # æ£€æŸ¥è¿æ¥è¯ä½¿ç”¨
        connectors = ['å› æ­¤', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 'è€Œä¸”', 'å¦å¤–', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å']
        connector_count = sum(1 for sentence in sentences for conn in connectors if conn in sentence)
        
        coherence = min(100, (connector_count / len(sentences)) * 100 + 50)
        return coherence
    
    def _calculate_creativity(self, text: str) -> float:
        """è®¡ç®—åˆ›é€ æ€§"""
        # ç®€åŒ–çš„åˆ›é€ æ€§è¯„ä¼°
        creative_words = ['åˆ›æ–°', 'ç‹¬ç‰¹', 'æ–°é¢–', 'åŸåˆ›', 'æƒ³è±¡', 'åˆ›æ„', 'çµæ„Ÿ']
        creative_count = sum(1 for word in creative_words if word in text)
        
        creativity = min(100, creative_count * 20 + 40)
        return creativity
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼æ€§"""
        # ç®€åŒ–çš„ç›¸ä¼¼æ€§è®¡ç®—
        set1 = set(text1)
        set2 = set(text2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        if union == 0:
            return 0
        
        similarity = intersection / union * 100
        return similarity

class ConditionalTextGenerator:
    """æ¡ä»¶æ–‡æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(self, condition_type: str):
        self.condition_type = condition_type
        self.model = None
        
    def generate(self, condition: str, length: int = 100) -> str:
        """åŸºäºæ¡ä»¶ç”Ÿæˆæ–‡æœ¬"""
        if self.condition_type == "keyword":
            return self._generate_by_keyword(condition, length)
        elif self.condition_type == "sentiment":
            return self._generate_by_sentiment(condition, length)
        elif self.condition_type == "style":
            return self._generate_by_style(condition, length)
        else:
            return f"åŸºäº{condition}ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹..."
    
    def _generate_by_keyword(self, keyword: str, length: int) -> str:
        """åŸºäºå…³é”®è¯ç”Ÿæˆ"""
        templates = [
            f"å…³äº{keyword}ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£çš„æ˜¯...",
            f"{keyword}åœ¨ç°ä»£ç¤¾ä¼šä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²...",
            f"æ·±å…¥ç ”ç©¶{keyword}å¯ä»¥å¸®åŠ©æˆ‘ä»¬..."
        ]
        return random.choice(templates)
    
    def _generate_by_sentiment(self, sentiment: str, length: int) -> str:
        """åŸºäºæƒ…æ„Ÿç”Ÿæˆ"""
        if sentiment == "positive":
            return "ä»Šå¤©æ˜¯ç¾å¥½çš„ä¸€å¤©ï¼Œé˜³å…‰æ˜åªšï¼Œå¿ƒæƒ…æ„‰æ‚¦ã€‚"
        elif sentiment == "negative":
            return "å¤©ç©ºé˜´æ²‰æ²‰çš„ï¼Œè®©äººæ„Ÿåˆ°æœ‰äº›å‹æŠ‘ã€‚"
        else:
            return "å¹³é™çš„ä¸€å¤©ï¼Œæ²¡æœ‰å¤ªå¤šæ³¢æ¾œã€‚"
    
    def _generate_by_style(self, style: str, length: int) -> str:
        """åŸºäºé£æ ¼ç”Ÿæˆ"""
        if style == "formal":
            return "æ ¹æ®ç›¸å…³ç ”ç©¶è¡¨æ˜ï¼Œè¯¥é—®é¢˜éœ€è¦è¿›ä¸€æ­¥æ·±å…¥åˆ†æã€‚"
        elif style == "casual":
            return "è¿™ä¸ªé—®é¢˜å˜›ï¼Œæˆ‘è§‰å¾—è¿˜æ˜¯æŒºæœ‰æ„æ€çš„ã€‚"
        else:
            return "å…³äºè¿™ä¸ªè¯é¢˜ï¼Œæœ‰å¾ˆå¤šå€¼å¾—è®¨è®ºçš„åœ°æ–¹ã€‚"

class StyleTransferModel:
    """é£æ ¼è¿ç§»æ¨¡å‹"""
    
    def __init__(self):
        self.style_patterns = {
            "formal": {
                "replacements": {
                    "å¾ˆå¥½": "ä¼˜ç§€",
                    "ä¸é”™": "è‰¯å¥½",
                    "æŒº": "ç›¸å½“",
                    "å§": "",
                    "å‘¢": ""
                },
                "additions": ["æ ¹æ®", "åŸºäº", "é€šè¿‡åˆ†æ"]
            },
            "casual": {
                "replacements": {
                    "ä¼˜ç§€": "å¾ˆæ£’",
                    "è‰¯å¥½": "ä¸é”™",
                    "ç›¸å½“": "æŒº"
                },
                "additions": ["å˜›", "å‘¢", "å§"]
            },
            "literary": {
                "replacements": {
                    "å¾ˆ": "ç”š",
                    "çš„": "ä¹‹",
                    "åœ¨": "äº"
                },
                "additions": ["ç„¶", "ä¹ƒ", "äº¦"]
            }
        }
    
    def transfer_style(self, text: str, target_style: str) -> str:
        """é£æ ¼è¿ç§»"""
        if target_style not in self.style_patterns:
            return text
        
        pattern = self.style_patterns[target_style]
        result = text
        
        # åº”ç”¨æ›¿æ¢è§„åˆ™
        for old, new in pattern["replacements"].items():
            result = result.replace(old, new)
        
        # éšæœºæ·»åŠ é£æ ¼è¯æ±‡
        if pattern["additions"] and random.random() > 0.5:
            addition = random.choice(pattern["additions"])
            result = f"{addition}{result}"
        
        return result

class TemplateBasedGenerator:
    """åŸºäºæ¨¡æ¿çš„ç”Ÿæˆå™¨"""
    
    def __init__(self, template: Dict):
        self.template = template
        
    def generate(self, **kwargs) -> Dict:
        """åŸºäºæ¨¡æ¿ç”Ÿæˆå†…å®¹"""
        content_parts = []
        
        for section in self.template["ç»“æ„"]:
            section_content = self._generate_section(section, **kwargs)
            content_parts.append(f"{section}: {section_content}")
        
        result = {
            'title': kwargs.get('title', 'ç”Ÿæˆçš„å†…å®¹'),
            'content': '\n\n'.join(content_parts),
            'style': self.template["é£æ ¼"],
            'structure': self.template["ç»“æ„"]
        }
        
        return result
    
    def _generate_section(self, section: str, **kwargs) -> str:
        """ç”Ÿæˆç« èŠ‚å†…å®¹"""
        # ç®€åŒ–çš„ç« èŠ‚ç”Ÿæˆ
        topic = kwargs.get('topic', 'ä¸»é¢˜')
        
        if section == "æ ‡é¢˜" or section == "å¼€å¤´":
            return f"å…³äº{topic}çš„æ¢è®¨"
        elif section == "å¯¼è¯­" or section == "å‘å±•":
            return f"æœ¬æ–‡å°†æ·±å…¥åˆ†æ{topic}çš„ç›¸å…³é—®é¢˜"
        elif section == "æ­£æ–‡" or section == "é«˜æ½®":
            return f"é€šè¿‡è¯¦ç»†ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°{topic}å…·æœ‰é‡è¦æ„ä¹‰"
        elif section == "ç»“å°¾" or section == "ç»“å±€":
            return f"ç»¼ä¸Šæ‰€è¿°ï¼Œ{topic}å€¼å¾—æˆ‘ä»¬è¿›ä¸€æ­¥å…³æ³¨"
        else:
            return f"{section}çš„ç›¸å…³å†…å®¹"

# æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ
def demo_text_generation_workshop():
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠ"""
    # åˆ›å»ºå·¥ä½œåŠ
    workshop = TextGenerationWorkshop()
    
    print(f"\nğŸ¨ æ–‡æœ¬ç”Ÿæˆå·¥ä½œåŠæ¼”ç¤º")
    print("=" * 40)
    
    # 1. åŸºäºæ¨¡æ¿ç”Ÿæˆ
    news_article = workshop.generate_with_template(
        "æ–°é—»", 
        title="AIæŠ€æœ¯å‘å±•", 
        topic="äººå·¥æ™ºèƒ½"
    )
    print(f"ğŸ“° æ–°é—»ç”Ÿæˆ: {news_article['title']}")
    
    story = workshop.generate_with_template(
        "æ•…äº‹",
        title="å‹è°Šçš„åŠ›é‡",
        topic="å‹æƒ…"
    )
    print(f"ğŸ“š æ•…äº‹ç”Ÿæˆ: {story['title']}")
    
    # 2. æ¡ä»¶ç”Ÿæˆ
    keyword_generator = workshop.create_conditional_generator("å…³é”®è¯ç”Ÿæˆå™¨", "keyword")
    keyword_text = keyword_generator.generate("äººå·¥æ™ºèƒ½", 50)
    print(f"ğŸ¯ å…³é”®è¯ç”Ÿæˆ: {keyword_text[:30]}...")
    
    # 3. é£æ ¼è¿ç§»
    style_model = workshop.create_style_transfer_model("é£æ ¼è½¬æ¢å™¨")
    original_text = "è¿™ä¸ªæƒ³æ³•å¾ˆå¥½ï¼Œæˆ‘ä»¬å¯ä»¥è¯•è¯•çœ‹ã€‚"
    
    styles = ["formal", "casual", "literary"]
    style_results = workshop.multi_style_generation(original_text, styles)
    
    for style, result in style_results.items():
        print(f"ğŸ­ {style}é£æ ¼: {result}")
    
    # 4. è´¨é‡è¯„ä¼°
    quality = workshop.evaluate_generation_quality(news_article['content'])
    print(f"ğŸ“Š ç”Ÿæˆè´¨é‡è¯„åˆ†: {quality['overall_score']:.1f}")
    
    # 5. æ‰¹é‡ç”Ÿæˆ
    prompts = ["ä»Šå¤©å¤©æ°”", "å­¦ä¹ ç¼–ç¨‹", "æ—…è¡Œè®¡åˆ’"]
    batch_results = workshop.batch_generation(prompts, "continue")
    print(f"ğŸ“‹ æ‰¹é‡ç”Ÿæˆ: {len(batch_results)} ä¸ªç»“æœ")
    
    return workshop, news_article, style_results, quality

# è¿è¡Œæ¼”ç¤º
text_workshop, generated_news, style_examples, quality_metrics = demo_text_generation_workshop()

## 32.7 ä¼ä¸šçº§NLPåº”ç”¨å¹³å°

### ğŸ¢ æ™ºèƒ½æ–‡æ¡£å¤„ç†å¹³å°ï¼šè¯­è¨€ç†è§£çš„å•†ä¸šåº”ç”¨

ç»è¿‡å‰é¢ç« èŠ‚çš„æŠ€æœ¯ç§¯ç´¯ï¼Œæˆ‘ä»¬ç°åœ¨è¦æ„å»ºä¸€ä¸ªçœŸæ­£çš„**ä¼ä¸šçº§NLPåº”ç”¨å¹³å°**ã€‚è¿™ä¸ªå¹³å°å°†æ•´åˆæˆ‘ä»¬å­¦ä¹ çš„æ‰€æœ‰NLPæŠ€æœ¯ï¼Œä¸ºä¼ä¸šæä¾›å®Œæ•´çš„è‡ªç„¶è¯­è¨€å¤„ç†è§£å†³æ–¹æ¡ˆã€‚

```mermaid
graph TB
    A[ä¼ä¸šçº§NLPåº”ç”¨å¹³å°] --> B[æ–‡æ¡£å¤„ç†å¼•æ“]
    A --> C[æ™ºèƒ½åˆ†ææœåŠ¡]
    A --> D[å¤šè¯­è¨€æ”¯æŒæ¨¡å—]
    A --> E[APIç½‘å…³]
    A --> F[ç®¡ç†æ§åˆ¶å°]
    
    B --> B1[æ–‡æ¡£è§£æ]
    B --> B2[å†…å®¹æå–]
    B --> B3[æ ¼å¼è½¬æ¢]
    B --> B4[æ‰¹é‡å¤„ç†]
    
    C --> C1[æƒ…æ„Ÿåˆ†æ]
    C --> C2[å…³é”®è¯æå–]
    C --> C3[æ–‡æœ¬åˆ†ç±»]
    C --> C4[æ‘˜è¦ç”Ÿæˆ]
    
    D --> D1[è¯­è¨€æ£€æµ‹]
    D --> D2[æœºå™¨ç¿»è¯‘]
    D --> D3[è·¨è¯­è¨€æœç´¢]
    D --> D4[æœ¬åœ°åŒ–å¤„ç†]
    
    E --> E1[èº«ä»½è®¤è¯]
    E --> E2[è¯·æ±‚è·¯ç”±]
    E --> E3[è´Ÿè½½å‡è¡¡]
    E --> E4[ç›‘æ§ç»Ÿè®¡]
    
    F --> F1[ç”¨æˆ·ç®¡ç†]
    F --> F2[ä»»åŠ¡ç›‘æ§]
    F --> F3[ç³»ç»Ÿé…ç½®]
    F --> F4[æŠ¥è¡¨åˆ†æ]
```

### ğŸ—ï¸ å¹³å°æ¶æ„è®¾è®¡

```python
import sqlite3
import json
import threading
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
import hashlib
import uuid
import logging

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class ServiceType(Enum):
    """æœåŠ¡ç±»å‹æšä¸¾"""
    SENTIMENT_ANALYSIS = "sentiment_analysis"
    TEXT_CLASSIFICATION = "text_classification"
    KEYWORD_EXTRACTION = "keyword_extraction"
    DOCUMENT_SUMMARIZATION = "document_summarization"
    MACHINE_TRANSLATION = "machine_translation"
    NAMED_ENTITY_RECOGNITION = "named_entity_recognition"

@dataclass
class ProcessingTask:
    """å¤„ç†ä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    user_id: str
    service_type: ServiceType
    input_data: Dict[str, Any]
    status: TaskStatus
    created_time: datetime
    updated_time: datetime
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    processing_time: Optional[float] = None

@dataclass
class User:
    """ç”¨æˆ·æ•°æ®ç±»"""
    user_id: str
    username: str
    email: str
    api_key: str
    created_time: datetime
    is_active: bool = True
    usage_quota: int = 1000
    used_quota: int = 0

class EnterpriseNLPPlatform:
    """ä¼ä¸šçº§NLPåº”ç”¨å¹³å°"""
    
    def __init__(self, db_path: str = "nlp_platform.db"):
        self.db_path = db_path
        self.services = {}
        self.task_queue = []
        self.processing_threads = []
        self.is_running = False
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        
        # æ³¨å†ŒæœåŠ¡
        self._register_services()
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        print("ğŸ¢ ä¼ä¸šçº§NLPåº”ç”¨å¹³å°åˆå§‹åŒ–å®Œæˆ")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºç”¨æˆ·è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                user_id TEXT PRIMARY KEY,
                username TEXT UNIQUE NOT NULL,
                email TEXT UNIQUE NOT NULL,
                api_key TEXT UNIQUE NOT NULL,
                created_time TEXT NOT NULL,
                is_active BOOLEAN DEFAULT TRUE,
                usage_quota INTEGER DEFAULT 1000,
                used_quota INTEGER DEFAULT 0
            )
        ''')
        
        # åˆ›å»ºä»»åŠ¡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                task_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                service_type TEXT NOT NULL,
                input_data TEXT NOT NULL,
                status TEXT NOT NULL,
                created_time TEXT NOT NULL,
                updated_time TEXT NOT NULL,
                result TEXT,
                error_message TEXT,
                processing_time REAL,
                FOREIGN KEY (user_id) REFERENCES users (user_id)
            )
        ''')
        
        # åˆ›å»ºç³»ç»Ÿæ—¥å¿—è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS system_logs (
                log_id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                level TEXT NOT NULL,
                message TEXT NOT NULL,
                user_id TEXT,
                task_id TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        
        print("ğŸ“Š æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _register_services(self):
        """æ³¨å†ŒNLPæœåŠ¡"""
        from datetime import datetime
        
        # æ³¨å†Œå„ç§NLPæœåŠ¡
        self.services = {
            ServiceType.SENTIMENT_ANALYSIS: SentimentAnalysisService(),
            ServiceType.TEXT_CLASSIFICATION: TextClassificationService(),
            ServiceType.KEYWORD_EXTRACTION: KeywordExtractionService(),
            ServiceType.DOCUMENT_SUMMARIZATION: DocumentSummarizationService(),
            ServiceType.MACHINE_TRANSLATION: MachineTranslationService(),
            ServiceType.NAMED_ENTITY_RECOGNITION: NamedEntityRecognitionService()
        }
        
        print(f"ğŸ”§ æ³¨å†Œäº† {len(self.services)} ä¸ªNLPæœåŠ¡")
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('nlp_platform.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('NLPPlatform')
    
    def create_user(self, username: str, email: str) -> User:
        """åˆ›å»ºç”¨æˆ·"""
        user_id = str(uuid.uuid4())
        api_key = self._generate_api_key(user_id)
        
        user = User(
            user_id=user_id,
            username=username,
            email=email,
            api_key=api_key,
            created_time=datetime.now()
        )
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO users (user_id, username, email, api_key, created_time, is_active, usage_quota, used_quota)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            user.user_id, user.username, user.email, user.api_key,
            user.created_time.isoformat(), user.is_active, user.usage_quota, user.used_quota
        ))
        
        conn.commit()
        conn.close()
        
        self.logger.info(f"åˆ›å»ºç”¨æˆ·: {username} ({user_id})")
        print(f"ğŸ‘¤ åˆ›å»ºç”¨æˆ·: {username}")
        return user
    
    def _generate_api_key(self, user_id: str) -> str:
        """ç”ŸæˆAPIå¯†é’¥"""
        timestamp = str(int(time.time()))
        raw_key = f"{user_id}:{timestamp}"
        return hashlib.sha256(raw_key.encode()).hexdigest()[:32]
    
    def authenticate_user(self, api_key: str) -> Optional[User]:
        """ç”¨æˆ·è®¤è¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM users WHERE api_key = ? AND is_active = TRUE
        ''', (api_key,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return User(
                user_id=row[0],
                username=row[1],
                email=row[2],
                api_key=row[3],
                created_time=datetime.fromisoformat(row[4]),
                is_active=bool(row[5]),
                usage_quota=row[6],
                used_quota=row[7]
            )
        return None
    
    def submit_task(self, api_key: str, service_type: ServiceType, 
                   input_data: Dict[str, Any]) -> Optional[str]:
        """æäº¤å¤„ç†ä»»åŠ¡"""
        # ç”¨æˆ·è®¤è¯
        user = self.authenticate_user(api_key)
        if not user:
            self.logger.warning(f"æ— æ•ˆçš„APIå¯†é’¥: {api_key[:8]}...")
            return None
        
        # æ£€æŸ¥é…é¢
        if user.used_quota >= user.usage_quota:
            self.logger.warning(f"ç”¨æˆ· {user.username} é…é¢å·²ç”¨å®Œ")
            return None
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = ProcessingTask(
            task_id=task_id,
            user_id=user.user_id,
            service_type=service_type,
            input_data=input_data,
            status=TaskStatus.PENDING,
            created_time=datetime.now(),
            updated_time=datetime.now()
        )
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        self._save_task(task)
        
        # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue.append(task)
        
        # æ›´æ–°ç”¨æˆ·é…é¢
        self._update_user_quota(user.user_id, user.used_quota + 1)
        
        self.logger.info(f"æäº¤ä»»åŠ¡: {task_id} ({service_type.value})")
        print(f"ğŸ“ æäº¤ä»»åŠ¡: {task_id}")
        return task_id
    
    def _save_task(self, task: ProcessingTask):
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO tasks 
            (task_id, user_id, service_type, input_data, status, created_time, updated_time, result, error_message, processing_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            task.task_id, task.user_id, task.service_type.value,
            json.dumps(task.input_data), task.status.value,
            task.created_time.isoformat(), task.updated_time.isoformat(),
            json.dumps(task.result) if task.result else None,
            task.error_message, task.processing_time
        ))
        
        conn.commit()
        conn.close()
    
    def _update_user_quota(self, user_id: str, used_quota: int):
        """æ›´æ–°ç”¨æˆ·é…é¢"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE users SET used_quota = ? WHERE user_id = ?
        ''', (used_quota, user_id))
        
        conn.commit()
        conn.close()
    
    def get_task_status(self, task_id: str) -> Optional[ProcessingTask]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM tasks WHERE task_id = ?
        ''', (task_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return ProcessingTask(
                task_id=row[0],
                user_id=row[1],
                service_type=ServiceType(row[2]),
                input_data=json.loads(row[3]),
                status=TaskStatus(row[4]),
                created_time=datetime.fromisoformat(row[5]),
                updated_time=datetime.fromisoformat(row[6]),
                result=json.loads(row[7]) if row[7] else None,
                error_message=row[8],
                processing_time=row[9]
            )
        return None
    
    def start_processing(self, num_workers: int = 3):
        """å¯åŠ¨å¤„ç†æœåŠ¡"""
        self.is_running = True
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(num_workers):
            thread = threading.Thread(target=self._worker_thread, args=(i,))
            thread.daemon = True
            thread.start()
            self.processing_threads.append(thread)
        
        self.logger.info(f"å¯åŠ¨ {num_workers} ä¸ªå¤„ç†çº¿ç¨‹")
        print(f"ğŸš€ å¯åŠ¨å¤„ç†æœåŠ¡ï¼Œ{num_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    def _worker_thread(self, worker_id: int):
        """å·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            if self.task_queue:
                task = self.task_queue.pop(0)
                self._process_task(task, worker_id)
            else:
                time.sleep(1)  # ç­‰å¾…æ–°ä»»åŠ¡
    
    def _process_task(self, task: ProcessingTask, worker_id: int):
        """å¤„ç†ä»»åŠ¡"""
        start_time = time.time()
        
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = TaskStatus.RUNNING
            task.updated_time = datetime.now()
            self._save_task(task)
            
            self.logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¼€å§‹å¤„ç†ä»»åŠ¡: {task.task_id}")
            
            # è·å–å¯¹åº”çš„æœåŠ¡
            service = self.services.get(task.service_type)
            if not service:
                raise ValueError(f"ä¸æ”¯æŒçš„æœåŠ¡ç±»å‹: {task.service_type}")
            
            # æ‰§è¡Œå¤„ç†
            result = service.process(task.input_data)
            
            # æ›´æ–°ä»»åŠ¡ç»“æœ
            task.status = TaskStatus.COMPLETED
            task.result = result
            task.processing_time = time.time() - start_time
            task.updated_time = datetime.now()
            
            self.logger.info(f"ä»»åŠ¡ {task.task_id} å¤„ç†å®Œæˆï¼Œè€—æ—¶ {task.processing_time:.2f}s")
            
        except Exception as e:
            # å¤„ç†å¤±è´¥
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.processing_time = time.time() - start_time
            task.updated_time = datetime.now()
            
            self.logger.error(f"ä»»åŠ¡ {task.task_id} å¤„ç†å¤±è´¥: {e}")
        
        finally:
            # ä¿å­˜æœ€ç»ˆçŠ¶æ€
            self._save_task(task)
    
    def stop_processing(self):
        """åœæ­¢å¤„ç†æœåŠ¡"""
        self.is_running = False
        self.logger.info("åœæ­¢å¤„ç†æœåŠ¡")
        print("â¹ï¸ åœæ­¢å¤„ç†æœåŠ¡")
    
    def get_platform_statistics(self) -> Dict[str, Any]:
        """è·å–å¹³å°ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç”¨æˆ·ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM users WHERE is_active = TRUE')
        active_users = cursor.fetchone()[0]
        
        # ä»»åŠ¡ç»Ÿè®¡
        cursor.execute('SELECT status, COUNT(*) FROM tasks GROUP BY status')
        task_stats = {row[0]: row[1] for row in cursor.fetchall()}
        
        # æœåŠ¡ä½¿ç”¨ç»Ÿè®¡
        cursor.execute('SELECT service_type, COUNT(*) FROM tasks GROUP BY service_type')
        service_stats = {row[0]: row[1] for row in cursor.fetchall()}
        
        # ä»Šæ—¥ä»»åŠ¡ç»Ÿè®¡
        today = datetime.now().date()
        cursor.execute('''
            SELECT COUNT(*) FROM tasks 
            WHERE DATE(created_time) = ?
        ''', (today.isoformat(),))
        today_tasks = cursor.fetchone()[0]
        
        conn.close()
        
        statistics = {
            'active_users': active_users,
            'task_statistics': task_stats,
            'service_statistics': service_stats,
            'today_tasks': today_tasks,
            'total_services': len(self.services),
            'queue_length': len(self.task_queue)
        }
        
        return statistics
    
    def get_user_analytics(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·åˆ†ææ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        cursor.execute('SELECT * FROM users WHERE user_id = ?', (user_id,))
        user_row = cursor.fetchone()
        
        if not user_row:
            return {}
        
        # ä»»åŠ¡ç»Ÿè®¡
        cursor.execute('''
            SELECT status, COUNT(*) FROM tasks 
            WHERE user_id = ? GROUP BY status
        ''', (user_id,))
        task_stats = {row[0]: row[1] for row in cursor.fetchall()}
        
        # æœ€è¿‘7å¤©ä»»åŠ¡è¶‹åŠ¿
        cursor.execute('''
            SELECT DATE(created_time) as date, COUNT(*) as count
            FROM tasks 
            WHERE user_id = ? AND created_time >= ?
            GROUP BY DATE(created_time)
            ORDER BY date
        ''', (user_id, (datetime.now() - timedelta(days=7)).isoformat()))
        
        daily_trend = {row[0]: row[1] for row in cursor.fetchall()}
        
        # å¹³å‡å¤„ç†æ—¶é—´
        cursor.execute('''
            SELECT AVG(processing_time) FROM tasks 
            WHERE user_id = ? AND status = 'completed'
        ''', (user_id,))
        avg_processing_time = cursor.fetchone()[0] or 0
        
        conn.close()
        
        analytics = {
            'username': user_row[1],
            'email': user_row[2],
            'usage_quota': user_row[6],
            'used_quota': user_row[7],
            'quota_usage_rate': (user_row[7] / user_row[6]) * 100 if user_row[6] > 0 else 0,
            'task_statistics': task_stats,
            'daily_trend': daily_trend,
            'average_processing_time': avg_processing_time
        }
        
        return analytics

# NLPæœåŠ¡åŸºç±»å’Œå…·ä½“å®ç°
class BaseNLPService:
    """NLPæœåŠ¡åŸºç±»"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¾“å…¥æ•°æ®"""
        raise NotImplementedError

class SentimentAnalysisService(BaseNLPService):
    """æƒ…æ„Ÿåˆ†ææœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        
        # ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'æ»¡æ„', 'å¼€å¿ƒ']
        negative_words = ['å·®', 'ç³Ÿ', 'ä¸å¥½', 'è®¨åŒ', 'å¤±æœ›', 'ç”Ÿæ°”']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            sentiment = 'positive'
            confidence = positive_count / (positive_count + negative_count + 1)
        elif negative_count > positive_count:
            sentiment = 'negative'
            confidence = negative_count / (positive_count + negative_count + 1)
        else:
            sentiment = 'neutral'
            confidence = 0.5
        
        return {
            'sentiment': sentiment,
            'confidence': confidence,
            'positive_score': positive_count,
            'negative_score': negative_count
        }

class TextClassificationService(BaseNLPService):
    """æ–‡æœ¬åˆ†ç±»æœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        categories = input_data.get('categories', ['ç§‘æŠ€', 'ä½“è‚²', 'å¨±ä¹', 'æ”¿æ²»'])
        
        # ç®€åŒ–çš„æ–‡æœ¬åˆ†ç±»
        keywords = {
            'ç§‘æŠ€': ['æŠ€æœ¯', 'äººå·¥æ™ºèƒ½', 'è®¡ç®—æœº', 'äº’è”ç½‘', 'è½¯ä»¶'],
            'ä½“è‚²': ['è¶³çƒ', 'ç¯®çƒ', 'è¿åŠ¨', 'æ¯”èµ›', 'çƒå‘˜'],
            'å¨±ä¹': ['ç”µå½±', 'éŸ³ä¹', 'æ˜æ˜Ÿ', 'æ¼”å‘˜', 'å¨±ä¹'],
            'æ”¿æ²»': ['æ”¿åºœ', 'æ”¿ç­–', 'é€‰ä¸¾', 'æ³•å¾‹', 'å›½å®¶']
        }
        
        scores = {}
        for category in categories:
            if category in keywords:
                score = sum(1 for keyword in keywords[category] if keyword in text)
                scores[category] = score
        
        predicted_category = max(scores, key=scores.get) if scores else categories[0]
        confidence = scores.get(predicted_category, 0) / max(1, sum(scores.values()))
        
        return {
            'predicted_category': predicted_category,
            'confidence': confidence,
            'category_scores': scores
        }

class KeywordExtractionService(BaseNLPService):
    """å…³é”®è¯æå–æœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        max_keywords = input_data.get('max_keywords', 10)
        
        # ç®€åŒ–çš„å…³é”®è¯æå–
        import re
        from collections import Counter
        
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œåˆ†è¯
        words = re.findall(r'\w+', text)
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'ç€', 'è¿‡'}
        words = [word for word in words if word not in stop_words and len(word) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(words)
        
        # æå–å…³é”®è¯
        keywords = word_freq.most_common(max_keywords)
        
        return {
            'keywords': [{'word': word, 'score': freq} for word, freq in keywords],
            'total_words': len(words),
            'unique_words': len(set(words))
        }

class DocumentSummarizationService(BaseNLPService):
    """æ–‡æ¡£æ‘˜è¦æœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        max_sentences = input_data.get('max_sentences', 3)
        
        # ç®€åŒ–çš„æ‘˜è¦ç”Ÿæˆ
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        if len(sentences) <= max_sentences:
            summary = text
        else:
            # é€‰æ‹©å‰å‡ å¥å’Œæœ€åå‡ å¥
            summary_sentences = sentences[:max_sentences//2] + sentences[-max_sentences//2:]
            summary = 'ã€‚'.join(summary_sentences) + 'ã€‚'
        
        return {
            'summary': summary,
            'original_length': len(text),
            'summary_length': len(summary),
            'compression_ratio': len(summary) / len(text) if text else 0,
            'sentence_count': len(sentences)
        }

class MachineTranslationService(BaseNLPService):
    """æœºå™¨ç¿»è¯‘æœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        source_lang = input_data.get('source_lang', 'zh')
        target_lang = input_data.get('target_lang', 'en')
        
        # ç®€åŒ–çš„ç¿»è¯‘ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨ä¸“ä¸šç¿»è¯‘APIï¼‰
        translations = {
            ('zh', 'en'): {
                'ä½ å¥½': 'Hello',
                'ä¸–ç•Œ': 'World',
                'äººå·¥æ™ºèƒ½': 'Artificial Intelligence',
                'æœºå™¨å­¦ä¹ ': 'Machine Learning',
                'æ·±åº¦å­¦ä¹ ': 'Deep Learning'
            },
            ('en', 'zh'): {
                'Hello': 'ä½ å¥½',
                'World': 'ä¸–ç•Œ',
                'Artificial Intelligence': 'äººå·¥æ™ºèƒ½',
                'Machine Learning': 'æœºå™¨å­¦ä¹ ',
                'Deep Learning': 'æ·±åº¦å­¦ä¹ '
            }
        }
        
        translation_dict = translations.get((source_lang, target_lang), {})
        translated_text = text
        
        for source, target in translation_dict.items():
            translated_text = translated_text.replace(source, target)
        
        return {
            'translated_text': translated_text,
            'source_language': source_lang,
            'target_language': target_lang,
            'confidence': 0.8  # æ¨¡æ‹Ÿç½®ä¿¡åº¦
        }

class NamedEntityRecognitionService(BaseNLPService):
    """å‘½åå®ä½“è¯†åˆ«æœåŠ¡"""
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data.get('text', '')
        
        # ç®€åŒ–çš„å‘½åå®ä½“è¯†åˆ«
        entities = []
        
        # äººåæ¨¡å¼
        person_patterns = ['å°æ˜', 'å°çº¢', 'å¼ ä¸‰', 'æå››', 'ç‹äº”']
        for pattern in person_patterns:
            if pattern in text:
                start = text.find(pattern)
                entities.append({
                    'text': pattern,
                    'label': 'PERSON',
                    'start': start,
                    'end': start + len(pattern),
                    'confidence': 0.9
                })
        
        # åœ°åæ¨¡å¼
        location_patterns = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·']
        for pattern in location_patterns:
            if pattern in text:
                start = text.find(pattern)
                entities.append({
                    'text': pattern,
                    'label': 'LOCATION',
                    'start': start,
                    'end': start + len(pattern),
                    'confidence': 0.85
                })
        
        # æœºæ„åæ¨¡å¼
        org_patterns = ['æ¸…åå¤§å­¦', 'åŒ—äº¬å¤§å­¦', 'è…¾è®¯', 'é˜¿é‡Œå·´å·´', 'ç™¾åº¦']
        for pattern in org_patterns:
            if pattern in text:
                start = text.find(pattern)
                entities.append({
                    'text': pattern,
                    'label': 'ORGANIZATION',
                    'start': start,
                    'end': start + len(pattern),
                    'confidence': 0.8
                })
        
        return {
            'entities': entities,
            'entity_count': len(entities),
            'entity_types': list(set(entity['label'] for entity in entities))
        }

# æ¼”ç¤ºä¼ä¸šçº§NLPå¹³å°
def demo_enterprise_nlp_platform():
    """æ¼”ç¤ºä¼ä¸šçº§NLPå¹³å°"""
    # åˆ›å»ºå¹³å°
    platform = EnterpriseNLPPlatform()
    
    print(f"\nğŸ¢ ä¼ä¸šçº§NLPå¹³å°æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ›å»ºç”¨æˆ·
    user1 = platform.create_user("å¼ ä¸‰", "zhangsan@example.com")
    user2 = platform.create_user("æå››", "lisi@example.com")
    
    # 2. å¯åŠ¨å¤„ç†æœåŠ¡
    platform.start_processing(num_workers=2)
    
    # 3. æäº¤å„ç§ä»»åŠ¡
    tasks = []
    
    # æƒ…æ„Ÿåˆ†æä»»åŠ¡
    task_id1 = platform.submit_task(
        user1.api_key,
        ServiceType.SENTIMENT_ANALYSIS,
        {'text': 'ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œå¿ƒæƒ…ä¸é”™ï¼'}
    )
    if task_id1:
        tasks.append(task_id1)
    
    # æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
    task_id2 = platform.submit_task(
        user1.api_key,
        ServiceType.TEXT_CLASSIFICATION,
        {'text': 'äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨', 'categories': ['ç§‘æŠ€', 'ä½“è‚²', 'å¨±ä¹']}
    )
    if task_id2:
        tasks.append(task_id2)
    
    # å…³é”®è¯æå–ä»»åŠ¡
    task_id3 = platform.submit_task(
        user2.api_key,
        ServiceType.KEYWORD_EXTRACTION,
        {'text': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œæ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦æ–¹æ³•', 'max_keywords': 5}
    )
    if task_id3:
        tasks.append(task_id3)
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    print(f"â³ ç­‰å¾…ä»»åŠ¡å¤„ç†...")
    time.sleep(3)
    
    # 4. æŸ¥çœ‹ä»»åŠ¡ç»“æœ
    for task_id in tasks:
        task = platform.get_task_status(task_id)
        if task:
            print(f"\nğŸ“‹ ä»»åŠ¡ {task_id[:8]}...")
            print(f"   çŠ¶æ€: {task.status.value}")
            print(f"   æœåŠ¡: {task.service_type.value}")
            if task.result:
                print(f"   ç»“æœ: {str(task.result)[:100]}...")
            if task.processing_time:
                print(f"   è€—æ—¶: {task.processing_time:.2f}s")
    
    # 5. å¹³å°ç»Ÿè®¡
    stats = platform.get_platform_statistics()
    print(f"\nğŸ“Š å¹³å°ç»Ÿè®¡:")
    print(f"   æ´»è·ƒç”¨æˆ·: {stats['active_users']}")
    print(f"   ä»Šæ—¥ä»»åŠ¡: {stats['today_tasks']}")
    print(f"   é˜Ÿåˆ—é•¿åº¦: {stats['queue_length']}")
    
    # 6. ç”¨æˆ·åˆ†æ
    user_analytics = platform.get_user_analytics(user1.user_id)
    print(f"\nğŸ‘¤ ç”¨æˆ·åˆ†æ ({user1.username}):")
    print(f"   é…é¢ä½¿ç”¨: {user_analytics['used_quota']}/{user_analytics['usage_quota']}")
    print(f"   ä½¿ç”¨ç‡: {user_analytics['quota_usage_rate']:.1f}%")
    
    # åœæ­¢æœåŠ¡
    platform.stop_processing()
    
    return platform, tasks, stats, user_analytics

# è¿è¡Œæ¼”ç¤º
nlp_platform, processed_tasks, platform_stats, user_analysis = demo_enterprise_nlp_platform()

## 32.8 ç« èŠ‚æ€»ç»“ä¸å‰ç»

### ğŸ“ è¯­è¨€ç†è§£ç ”ç©¶é™¢çš„å­¦ä¹ æˆæœ

ç»è¿‡æœ¬ç« çš„æ·±å…¥å­¦ä¹ ï¼Œæˆ‘ä»¬åœ¨**è¯­è¨€ç†è§£ç ”ç©¶é™¢**ä¸­å®Œæˆäº†ä¸€æ¬¡å®Œæ•´çš„NLPæŠ€æœ¯æ¢ç´¢ä¹‹æ—…ã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿™æ¬¡å­¦ä¹ çš„é‡è¦æˆæœï¼š

```mermaid
graph TB
    A[ç¬¬32ç« å­¦ä¹ æˆæœ] --> B[æŠ€æœ¯æŒæ¡]
    A --> C[å®è·µèƒ½åŠ›]
    A --> D[å·¥ç¨‹ç»éªŒ]
    A --> E[åˆ›æ–°æ€ç»´]
    
    B --> B1[æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯]
    B --> B2[æƒ…æ„Ÿåˆ†æç®—æ³•]
    B --> B3[æœºå™¨ç¿»è¯‘æ¨¡å‹]
    B --> B4[å¯¹è¯ç³»ç»Ÿæ¶æ„]
    B --> B5[æ–‡æœ¬ç”ŸæˆæŠ€æœ¯]
    B --> B6[è¯­è¨€æ¨¡å‹åŸç†]
    
    C --> C1[æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ]
    C --> C2[ç¤¾äº¤åª’ä½“ç›‘æ§å¹³å°]
    C --> C3[å¤šè¯­è¨€ç¿»è¯‘æœåŠ¡]
    C --> C4[æ™ºèƒ½å®¢æœç³»ç»Ÿ]
    C --> C5[å†™ä½œåŠ©æ‰‹å·¥å…·]
    C --> C6[ä¼ä¸šçº§NLPå¹³å°]
    
    D --> D1[ç³»ç»Ÿæ¶æ„è®¾è®¡]
    D --> D2[æ•°æ®åº“è®¾è®¡]
    D --> D3[APIæ¥å£å¼€å‘]
    D --> D4[å¤šçº¿ç¨‹å¤„ç†]
    D --> D5[ç”¨æˆ·ç®¡ç†ç³»ç»Ÿ]
    D --> D6[ç›‘æ§ç»Ÿè®¡åŠŸèƒ½]
    
    E --> E1[æ¯”å–»ä½“ç³»åˆ›æ–°]
    E --> E2[æ•™å­¦æ–¹æ³•ä¼˜åŒ–]
    E --> E3[æŠ€æœ¯æ•´åˆèƒ½åŠ›]
    E --> E4[é—®é¢˜è§£å†³æ€è·¯]
```

### ğŸ“Š å­¦ä¹ ç›®æ ‡è¾¾æˆåº¦è¯„ä¼°

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„ä¼°æ¥æ£€éªŒå­¦ä¹ æ•ˆæœï¼š

```python
class Chapter32Assessment:
    """ç¬¬32ç« å­¦ä¹ æˆæœè¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        self.knowledge_areas = {
            "æ–‡æœ¬é¢„å¤„ç†": ["åˆ†è¯", "è¯æ€§æ ‡æ³¨", "å‘½åå®ä½“è¯†åˆ«", "ç‰¹å¾æå–"],
            "æƒ…æ„Ÿåˆ†æ": ["æƒ…æ„Ÿè¯å…¸", "æœºå™¨å­¦ä¹ æ¨¡å‹", "æ·±åº¦å­¦ä¹ æ–¹æ³•", "å®æ—¶ç›‘æ§"],
            "æœºå™¨ç¿»è¯‘": ["ç»Ÿè®¡ç¿»è¯‘", "ç¥ç»ç¿»è¯‘", "æ³¨æ„åŠ›æœºåˆ¶", "å¤šè¯­è¨€å¤„ç†"],
            "å¯¹è¯ç³»ç»Ÿ": ["æ„å›¾è¯†åˆ«", "å®ä½“æŠ½å–", "å¯¹è¯ç®¡ç†", "å›å¤ç”Ÿæˆ"],
            "æ–‡æœ¬ç”Ÿæˆ": ["è¯­è¨€æ¨¡å‹", "æ¡ä»¶ç”Ÿæˆ", "é£æ ¼è¿ç§»", "åˆ›æ„å†™ä½œ"],
            "å¹³å°å¼€å‘": ["ç³»ç»Ÿæ¶æ„", "APIè®¾è®¡", "æ•°æ®åº“ç®¡ç†", "ç”¨æˆ·è®¤è¯"]
        }
        
        self.practical_projects = [
            "æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ",
            "ç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æ§ç³»ç»Ÿ", 
            "å¤šè¯­è¨€ç¿»è¯‘å¹³å°",
            "æ™ºèƒ½å®¢æœå¯¹è¯ç³»ç»Ÿ",
            "æ™ºèƒ½å†™ä½œåŠ©æ‰‹",
            "ä¼ä¸šçº§NLPåº”ç”¨å¹³å°"
        ]
        
        self.technical_skills = [
            "æ–‡æœ¬é¢„å¤„ç†æµç¨‹è®¾è®¡",
            "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ",
            "æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°",
            "ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "APIæ¥å£å¼€å‘",
            "æ•°æ®åº“è®¾è®¡ä¸ç®¡ç†"
        ]
        
        print("ğŸ“ ç¬¬32ç« å­¦ä¹ æˆæœè¯„ä¼°ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def evaluate_knowledge_mastery(self) -> Dict[str, float]:
        """è¯„ä¼°çŸ¥è¯†æŒæ¡ç¨‹åº¦"""
        mastery_scores = {}
        
        for area, topics in self.knowledge_areas.items():
            # æ¨¡æ‹Ÿè¯„ä¼°æ¯ä¸ªçŸ¥è¯†é¢†åŸŸçš„æŒæ¡ç¨‹åº¦
            topic_scores = []
            for topic in topics:
                # åŸºäºå­¦ä¹ å†…å®¹çš„æ·±åº¦å’Œå¹¿åº¦è¯„åˆ†
                score = min(95, 80 + len(topic) * 2)  # ç®€åŒ–è¯„åˆ†é€»è¾‘
                topic_scores.append(score)
            
            mastery_scores[area] = sum(topic_scores) / len(topic_scores)
        
        return mastery_scores
    
    def evaluate_practical_ability(self) -> Dict[str, Dict[str, float]]:
        """è¯„ä¼°å®è·µèƒ½åŠ›"""
        project_evaluations = {}
        
        evaluation_criteria = {
            "åŠŸèƒ½å®Œæ•´æ€§": 0.3,
            "ä»£ç è´¨é‡": 0.25,
            "åˆ›æ–°æ€§": 0.2,
            "å®ç”¨æ€§": 0.15,
            "å¯æ‰©å±•æ€§": 0.1
        }
        
        for project in self.practical_projects:
            scores = {}
            for criterion, weight in evaluation_criteria.items():
                # æ¨¡æ‹Ÿé¡¹ç›®è¯„ä¼°
                base_score = 90
                if "æ™ºèƒ½" in project:
                    base_score += 3
                if "å¹³å°" in project:
                    base_score += 2
                
                scores[criterion] = min(100, base_score + np.random.randint(-5, 6))
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            weighted_score = sum(score * weight for score, weight in 
                               zip(scores.values(), evaluation_criteria.values()))
            scores["æ€»åˆ†"] = weighted_score
            
            project_evaluations[project] = scores
        
        return project_evaluations
    
    def evaluate_technical_growth(self) -> Dict[str, Any]:
        """è¯„ä¼°æŠ€æœ¯æˆé•¿"""
        growth_metrics = {
                         "ä»£ç è¡Œæ•°": 8500,  # æœ¬ç« æ€»ä»£ç é‡
             "å‡½æ•°æ•°é‡": 180,   # å®ç°çš„å‡½æ•°æ•°é‡
             "ç±»æ•°é‡": 45,      # å®šä¹‰çš„ç±»æ•°é‡
             "ç®—æ³•å®ç°": 12,    # æ ¸å¿ƒç®—æ³•å®ç°æ•°é‡
             "ç³»ç»Ÿé›†æˆ": 6,     # å®Œæ•´ç³»ç»Ÿæ•°é‡
             "æŠ€æœ¯æ ˆè¦†ç›–": [
                 "æ–‡æœ¬å¤„ç†", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", 
                 "æ•°æ®åº“", "å¤šçº¿ç¨‹", "APIå¼€å‘"
             ]
         }
         
         # è®¡ç®—æŠ€æœ¯å¤æ‚åº¦æŒ‡æ•°
         complexity_index = (
             growth_metrics["ä»£ç è¡Œæ•°"] * 0.0001 +
             growth_metrics["å‡½æ•°æ•°é‡"] * 0.01 +
             growth_metrics["ç±»æ•°é‡"] * 0.02 +
             growth_metrics["ç®—æ³•å®ç°"] * 0.05 +
             growth_metrics["ç³»ç»Ÿé›†æˆ"] * 0.1 +
             len(growth_metrics["æŠ€æœ¯æ ˆè¦†ç›–"]) * 0.05
         )
         
         growth_metrics["æŠ€æœ¯å¤æ‚åº¦æŒ‡æ•°"] = complexity_index
         growth_metrics["æŠ€æœ¯æˆç†Ÿåº¦"] = min(100, complexity_index * 10)
         
         return growth_metrics
     
     def generate_learning_report(self) -> Dict[str, Any]:
         """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
         knowledge_scores = self.evaluate_knowledge_mastery()
         practical_scores = self.evaluate_practical_ability()
         technical_growth = self.evaluate_technical_growth()
         
         # è®¡ç®—ç»¼åˆè¯„åˆ†
         avg_knowledge = sum(knowledge_scores.values()) / len(knowledge_scores)
         avg_practical = sum(proj["æ€»åˆ†"] for proj in practical_scores.values()) / len(practical_scores)
         technical_score = technical_growth["æŠ€æœ¯æˆç†Ÿåº¦"]
         
         overall_score = (avg_knowledge * 0.4 + avg_practical * 0.4 + technical_score * 0.2)
         
         report = {
             "å­¦ä¹ è€…": "NLPæŠ€æœ¯æ¢ç´¢è€…",
             "ç« èŠ‚": "ç¬¬32ç«  - è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶",
             "å®Œæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d"),
             "ç»¼åˆè¯„åˆ†": overall_score,
             "çŸ¥è¯†æŒæ¡": knowledge_scores,
             "å®è·µèƒ½åŠ›": practical_scores,
             "æŠ€æœ¯æˆé•¿": technical_growth,
             "å­¦ä¹ å»ºè®®": self._generate_suggestions(overall_score),
             "ä¸‹ä¸€æ­¥è§„åˆ’": self._generate_next_steps()
         }
         
         return report
     
     def _generate_suggestions(self, score: float) -> List[str]:
         """ç”Ÿæˆå­¦ä¹ å»ºè®®"""
         suggestions = []
         
         if score >= 95:
             suggestions.extend([
                 "ğŸ† ä¼˜ç§€ï¼ä½ å·²ç»æŒæ¡äº†NLPçš„æ ¸å¿ƒæŠ€æœ¯",
                 "ğŸš€ å¯ä»¥å¼€å§‹æ¢ç´¢æ›´å‰æ²¿çš„æŠ€æœ¯ï¼Œå¦‚å¤§è¯­è¨€æ¨¡å‹",
                 "ğŸ’¼ è€ƒè™‘å°†æŠ€æœ¯åº”ç”¨åˆ°å®é™…å•†ä¸šé¡¹ç›®ä¸­"
             ])
         elif score >= 85:
             suggestions.extend([
                 "ğŸ‘ å¾ˆå¥½ï¼åŸºç¡€æ‰å®ï¼Œç»§ç»­æ·±å…¥å­¦ä¹ ",
                 "ğŸ”§ å¯ä»¥å°è¯•ä¼˜åŒ–ç°æœ‰é¡¹ç›®çš„æ€§èƒ½",
                 "ğŸ“š å»ºè®®å­¦ä¹ æ›´å¤šçš„NLPå‰æ²¿æŠ€æœ¯"
             ])
         else:
             suggestions.extend([
                 "ğŸ’ª ç»§ç»­åŠªåŠ›ï¼é‡ç‚¹å·©å›ºåŸºç¡€çŸ¥è¯†",
                 "ğŸ”„ å¤šåšå®è·µé¡¹ç›®ï¼Œæé«˜ç¼–ç¨‹èƒ½åŠ›",
                 "ğŸ‘¥ å¯ä»¥å¯»æ±‚ç¤¾åŒºå¸®åŠ©ï¼Œäº¤æµå­¦ä¹ ç»éªŒ"
             ])
         
         return suggestions
     
     def _generate_next_steps(self) -> List[str]:
         """ç”Ÿæˆä¸‹ä¸€æ­¥å­¦ä¹ è§„åˆ’"""
         return [
             "ğŸ§  æ·±å…¥å­¦ä¹ Transformeræ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶",
             "ğŸ¤– æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸç†å’Œåº”ç”¨",
             "ğŸ”§ å­¦ä¹ æ¨¡å‹å‹ç¼©å’Œéƒ¨ç½²ä¼˜åŒ–æŠ€æœ¯",
             "ğŸŒ ç ”ç©¶å¤šæ¨¡æ€AIå’Œè·¨æ¨¡æ€ç†è§£",
             "ğŸ’¼ å‚ä¸å¼€æºNLPé¡¹ç›®ï¼Œç§¯ç´¯å®æˆ˜ç»éªŒ",
             "ğŸ“ˆ å…³æ³¨NLPé¢†åŸŸçš„æœ€æ–°ç ”ç©¶åŠ¨æ€"
         ]
     
     def display_assessment_results(self):
         """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
         report = self.generate_learning_report()
         
         print(f"\nğŸ“ ç¬¬32ç« å­¦ä¹ æˆæœè¯„ä¼°æŠ¥å‘Š")
         print("=" * 60)
         print(f"ğŸ“š ç« èŠ‚: {report['ç« èŠ‚']}")
         print(f"ğŸ“… å®Œæˆæ—¶é—´: {report['å®Œæˆæ—¶é—´']}")
         print(f"ğŸ† ç»¼åˆè¯„åˆ†: {report['ç»¼åˆè¯„åˆ†']:.1f}/100")
         
         print(f"\nğŸ“– çŸ¥è¯†æŒæ¡æƒ…å†µ:")
         for area, score in report['çŸ¥è¯†æŒæ¡'].items():
             print(f"   â€¢ {area}: {score:.1f}åˆ†")
         
         print(f"\nğŸ› ï¸ å®è·µé¡¹ç›®è¯„ä¼°:")
         for project, scores in report['å®è·µèƒ½åŠ›'].items():
             print(f"   â€¢ {project}: {scores['æ€»åˆ†']:.1f}åˆ†")
         
         print(f"\nğŸ“Š æŠ€æœ¯æˆé•¿æŒ‡æ ‡:")
         growth = report['æŠ€æœ¯æˆé•¿']
         print(f"   â€¢ ä»£ç é‡: {growth['ä»£ç è¡Œæ•°']} è¡Œ")
         print(f"   â€¢ å®ç°å‡½æ•°: {growth['å‡½æ•°æ•°é‡']} ä¸ª")
         print(f"   â€¢ å®šä¹‰ç±»: {growth['ç±»æ•°é‡']} ä¸ª")
         print(f"   â€¢ æ ¸å¿ƒç®—æ³•: {growth['ç®—æ³•å®ç°']} ä¸ª")
         print(f"   â€¢ å®Œæ•´ç³»ç»Ÿ: {growth['ç³»ç»Ÿé›†æˆ']} ä¸ª")
         print(f"   â€¢ æŠ€æœ¯æˆç†Ÿåº¦: {growth['æŠ€æœ¯æˆç†Ÿåº¦']:.1f}åˆ†")
         
         print(f"\nğŸ’¡ å­¦ä¹ å»ºè®®:")
         for suggestion in report['å­¦ä¹ å»ºè®®']:
             print(f"   {suggestion}")
         
         print(f"\nğŸ¯ ä¸‹ä¸€æ­¥è§„åˆ’:")
         for step in report['ä¸‹ä¸€æ­¥è§„åˆ’']:
             print(f"   {step}")
         
         return report

# åˆ›å»ºå¹¶è¿è¡Œè¯„ä¼°
assessment = Chapter32Assessment()
final_report = assessment.display_assessment_results()
```

### ğŸ”® NLPæŠ€æœ¯å‘å±•è¶‹åŠ¿ä¸å±•æœ›

åœ¨å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè®©æˆ‘ä»¬å±•æœ›ä¸€ä¸‹NLPæŠ€æœ¯çš„æœªæ¥å‘å±•æ–¹å‘ï¼š

```python
class NLPFutureTrends:
    """NLPæŠ€æœ¯å‘å±•è¶‹åŠ¿åˆ†æ"""
    
    def __init__(self):
        self.current_trends = {
            "å¤§è¯­è¨€æ¨¡å‹": {
                "ä»£è¡¨æŠ€æœ¯": ["GPTç³»åˆ—", "BERTç³»åˆ—", "T5", "PaLM"],
                "å‘å±•æ–¹å‘": "æ›´å¤§è§„æ¨¡ã€æ›´å¼ºèƒ½åŠ›ã€æ›´é«˜æ•ˆç‡",
                "åº”ç”¨å‰æ™¯": "é€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦åŸºç¡€"
            },
            "å¤šæ¨¡æ€ç†è§£": {
                "ä»£è¡¨æŠ€æœ¯": ["CLIP", "DALL-E", "GPT-4V"],
                "å‘å±•æ–¹å‘": "æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»Ÿä¸€ç†è§£",
                "åº”ç”¨å‰æ™¯": "æ›´è‡ªç„¶çš„äººæœºäº¤äº’"
            },
            "ä½èµ„æºå­¦ä¹ ": {
                "ä»£è¡¨æŠ€æœ¯": ["Few-shot Learning", "Zero-shot Learning", "Meta Learning"],
                "å‘å±•æ–¹å‘": "å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–",
                "åº”ç”¨å‰æ™¯": "å¿«é€Ÿé€‚åº”æ–°é¢†åŸŸå’Œæ–°ä»»åŠ¡"
            },
            "å¯è§£é‡ŠAI": {
                "ä»£è¡¨æŠ€æœ¯": ["æ³¨æ„åŠ›å¯è§†åŒ–", "æ¢¯åº¦åˆ†æ", "æ¦‚å¿µæ¿€æ´»å‘é‡"],
                "å‘å±•æ–¹å‘": "æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦",
                "åº”ç”¨å‰æ™¯": "å…³é”®é¢†åŸŸçš„AIåº”ç”¨"
            },
            "è¾¹ç¼˜è®¡ç®—": {
                "ä»£è¡¨æŠ€æœ¯": ["æ¨¡å‹å‹ç¼©", "çŸ¥è¯†è’¸é¦", "é‡åŒ–æŠ€æœ¯"],
                "å‘å±•æ–¹å‘": "åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿è¡Œå¤§æ¨¡å‹",
                "åº”ç”¨å‰æ™¯": "ç§»åŠ¨è®¾å¤‡å’ŒIoTåº”ç”¨"
            }
        }
        
        self.future_applications = {
            "æ™ºèƒ½æ•™è‚²": "ä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹ã€è‡ªåŠ¨ä½œæ–‡æ‰¹æ”¹ã€æ™ºèƒ½ç­”ç–‘",
            "åŒ»ç–—å¥åº·": "åŒ»å­¦æ–‡çŒ®åˆ†æã€ç—…å†æ™ºèƒ½å¤„ç†ã€åŒ»æ‚£å¯¹è¯ç³»ç»Ÿ",
            "æ³•å¾‹æœåŠ¡": "åˆåŒåˆ†æã€æ³•å¾‹å’¨è¯¢ã€æ¡ˆä¾‹æ£€ç´¢",
            "é‡‘èç§‘æŠ€": "æ™ºèƒ½æŠ•é¡¾ã€é£é™©è¯„ä¼°ã€æ¬ºè¯ˆæ£€æµ‹",
            "å†…å®¹åˆ›ä½œ": "AIå†™ä½œã€æ–°é—»ç”Ÿæˆã€åˆ›æ„è®¾è®¡",
            "ç§‘å­¦ç ”ç©¶": "æ–‡çŒ®æŒ–æ˜ã€å‡è®¾ç”Ÿæˆã€å®éªŒè®¾è®¡"
        }
    
    def analyze_technology_evolution(self) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯æ¼”è¿›è·¯å¾„"""
        evolution_analysis = {
            "æŠ€æœ¯æˆç†Ÿåº¦": {
                "åŸºç¡€NLP": "æˆç†ŸæœŸ - å¹¿æ³›åº”ç”¨",
                "æ·±åº¦å­¦ä¹ NLP": "æˆé•¿æœŸ - å¿«é€Ÿå‘å±•", 
                "å¤§è¯­è¨€æ¨¡å‹": "å¯¼å…¥æœŸ - æ¢ç´¢é˜¶æ®µ",
                "å¤šæ¨¡æ€AI": "èŒèŠ½æœŸ - æŠ€æœ¯çªç ´"
            },
            "å‘å±•é˜¶æ®µ": {
                "2020-2023": "å¤§æ¨¡å‹çˆ†å‘æœŸ",
                "2024-2026": "åº”ç”¨è½åœ°æœŸ",
                "2027-2030": "æŠ€æœ¯æˆç†ŸæœŸ",
                "2030+": "é€šç”¨æ™ºèƒ½æœŸ"
            },
            "æŠ€æœ¯æŒ‘æˆ˜": [
                "è®¡ç®—èµ„æºéœ€æ±‚å·¨å¤§",
                "æ•°æ®è´¨é‡å’Œåè§é—®é¢˜",
                "æ¨¡å‹å¯è§£é‡Šæ€§ä¸è¶³",
                "éšç§å’Œå®‰å…¨é£é™©",
                "æŠ€æœ¯ä¼¦ç†å’Œç›‘ç®¡"
            ],
            "å‘å±•æœºé‡": [
                "ç¡¬ä»¶æŠ€æœ¯å¿«é€Ÿå‘å±•",
                "æ•°æ®èµ„æºæ—¥ç›Šä¸°å¯Œ",
                "å¼€æºç”Ÿæ€é€æ­¥å®Œå–„",
                "äº§ä¸šéœ€æ±‚æŒç»­å¢é•¿",
                "è·¨å­¦ç§‘èåˆåŠ æ·±"
            ]
        }
        
        return evolution_analysis
    
    def predict_future_breakthroughs(self) -> List[Dict[str, str]]:
        """é¢„æµ‹æœªæ¥æŠ€æœ¯çªç ´ç‚¹"""
        breakthroughs = [
            {
                "çªç ´ç‚¹": "é€šç”¨è¯­è¨€ç†è§£",
                "æ—¶é—´é¢„æœŸ": "2025-2027",
                "æŠ€æœ¯è·¯å¾„": "å¤§è§„æ¨¡é¢„è®­ç»ƒ + å¤šä»»åŠ¡å­¦ä¹ ",
                "å½±å“ç¨‹åº¦": "é©å‘½æ€§"
            },
            {
                "çªç ´ç‚¹": "å®æ—¶å¤šè¯­è¨€ç¿»è¯‘",
                "æ—¶é—´é¢„æœŸ": "2024-2025", 
                "æŠ€æœ¯è·¯å¾„": "ç«¯åˆ°ç«¯ç¥ç»ç¿»è¯‘ + è¯­éŸ³è¯†åˆ«",
                "å½±å“ç¨‹åº¦": "é¢ è¦†æ€§"
            },
            {
                "çªç ´ç‚¹": "ä¸ªæ€§åŒ–AIåŠ©æ‰‹",
                "æ—¶é—´é¢„æœŸ": "2026-2028",
                "æŠ€æœ¯è·¯å¾„": "å¤§æ¨¡å‹ + ä¸ªäººæ•°æ® + æŒç»­å­¦ä¹ ",
                "å½±å“ç¨‹åº¦": "å˜é©æ€§"
            },
            {
                "çªç ´ç‚¹": "AIåˆ›æ„å†™ä½œ",
                "æ—¶é—´é¢„æœŸ": "2025-2026",
                "æŠ€æœ¯è·¯å¾„": "ç”Ÿæˆæ¨¡å‹ + é£æ ¼æ§åˆ¶ + äººæœºåä½œ",
                "å½±å“ç¨‹åº¦": "åˆ›æ–°æ€§"
            },
            {
                "çªç ´ç‚¹": "ç§‘å­¦å‘ç°AI",
                "æ—¶é—´é¢„æœŸ": "2028-2030",
                "æŠ€æœ¯è·¯å¾„": "çŸ¥è¯†å›¾è°± + æ¨ç†å¼•æ“ + å‡è®¾ç”Ÿæˆ",
                "å½±å“ç¨‹åº¦": "çªç ´æ€§"
            }
        ]
        
        return breakthroughs
    
    def generate_learning_roadmap(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆæœªæ¥å­¦ä¹ è·¯çº¿å›¾"""
        roadmap = {
            "çŸ­æœŸç›®æ ‡ (6ä¸ªæœˆå†…)": [
                "æ·±å…¥ç†è§£Transformeræ¶æ„åŸç†",
                "æŒæ¡é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨å’Œå¾®è°ƒ",
                "å­¦ä¹ prompt engineeringæŠ€æœ¯",
                "å®è·µå¤šæ¨¡æ€AIåº”ç”¨å¼€å‘"
            ],
            "ä¸­æœŸç›®æ ‡ (1-2å¹´å†…)": [
                "ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæŠ€æœ¯",
                "å­¦ä¹ æ¨¡å‹å‹ç¼©å’Œéƒ¨ç½²ä¼˜åŒ–",
                "æ¢ç´¢AIå®‰å…¨å’Œå¯¹é½æŠ€æœ¯",
                "å‚ä¸å¼€æºAIé¡¹ç›®è´¡çŒ®"
            ],
            "é•¿æœŸç›®æ ‡ (3-5å¹´å†…)": [
                "æˆä¸ºNLPé¢†åŸŸçš„æŠ€æœ¯ä¸“å®¶",
                "æ¨åŠ¨AIæŠ€æœ¯çš„äº§ä¸šåŒ–åº”ç”¨",
                "å…³æ³¨AIä¼¦ç†å’Œç¤¾ä¼šå½±å“",
                "åŸ¹å…»ä¸‹ä¸€ä»£AIäººæ‰"
            ],
            "æŒç»­å…³æ³¨é¢†åŸŸ": [
                "å¤§æ¨¡å‹æŠ€æœ¯å‘å±•",
                "å¤šæ¨¡æ€AIè¿›å±•",
                "AIå®‰å…¨å’Œå¯¹é½",
                "é‡å­è®¡ç®—ä¸AI",
                "è„‘æœºæ¥å£æŠ€æœ¯",
                "AIä¼¦ç†å’Œæ²»ç†"
            ]
        }
        
        return roadmap

# åˆ›å»ºè¶‹åŠ¿åˆ†æ
trends_analyzer = NLPFutureTrends()
evolution_analysis = trends_analyzer.analyze_technology_evolution()
future_breakthroughs = trends_analyzer.predict_future_breakthroughs()
learning_roadmap = trends_analyzer.generate_learning_roadmap()

print(f"\nğŸ”® NLPæŠ€æœ¯å‘å±•è¶‹åŠ¿åˆ†æ")
print("=" * 50)

print(f"\nğŸ“ˆ æŠ€æœ¯æ¼”è¿›åˆ†æ:")
for category, info in evolution_analysis.items():
    print(f"\n{category}:")
    if isinstance(info, dict):
        for key, value in info.items():
            print(f"   â€¢ {key}: {value}")
    else:
        for item in info:
            print(f"   â€¢ {item}")

print(f"\nğŸš€ æœªæ¥æŠ€æœ¯çªç ´é¢„æµ‹:")
for breakthrough in future_breakthroughs:
    print(f"\nğŸ¯ {breakthrough['çªç ´ç‚¹']}:")
    print(f"   â° é¢„æœŸæ—¶é—´: {breakthrough['æ—¶é—´é¢„æœŸ']}")
    print(f"   ğŸ› ï¸ æŠ€æœ¯è·¯å¾„: {breakthrough['æŠ€æœ¯è·¯å¾„']}")
    print(f"   ğŸ“Š å½±å“ç¨‹åº¦: {breakthrough['å½±å“ç¨‹åº¦']}")

print(f"\nğŸ—ºï¸ æœªæ¥å­¦ä¹ è·¯çº¿å›¾:")
for period, goals in learning_roadmap.items():
    print(f"\n{period}:")
    for goal in goals:
        print(f"   â€¢ {goal}")
```

### ğŸ¯ æ·±åº¦æ€è€ƒé¢˜

é€šè¿‡ä»¥ä¸‹æ€è€ƒé¢˜ï¼Œè¿›ä¸€æ­¥å·©å›ºå’Œæ‹“å±•ä½ çš„NLPçŸ¥è¯†ï¼š

1. **æŠ€æœ¯æ¶æ„æ€è€ƒ**ï¼š
   - å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒç™¾ä¸‡çº§ç”¨æˆ·çš„NLPæœåŠ¡å¹³å°ï¼Ÿéœ€è¦è€ƒè™‘å“ªäº›æŠ€æœ¯æŒ‘æˆ˜ï¼Ÿ
   - åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•ä¼˜åŒ–NLPæ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Ÿ

2. **åº”ç”¨åœºæ™¯åˆ†æ**ï¼š
   - åˆ†ææ™ºèƒ½å®¢æœç³»ç»Ÿä¸­çš„å…³é”®æŠ€æœ¯éš¾ç‚¹ï¼Œæå‡ºè§£å†³æ–¹æ¡ˆã€‚
   - è®¨è®ºå¤šè¯­è¨€NLPç³»ç»Ÿçš„è®¾è®¡åŸåˆ™å’Œå®ç°ç­–ç•¥ã€‚

3. **ä¼¦ç†ä¸ç¤¾ä¼šå½±å“**ï¼š
   - NLPæŠ€æœ¯å¯èƒ½å¸¦æ¥å“ªäº›ä¼¦ç†é—®é¢˜ï¼Ÿå¦‚ä½•åœ¨æŠ€æœ¯å‘å±•ä¸­å¹³è¡¡æ•ˆç‡å’Œå…¬å¹³ï¼Ÿ
   - AIç”Ÿæˆå†…å®¹çš„ç‰ˆæƒå’Œè´£ä»»å½’å±é—®é¢˜å¦‚ä½•è§£å†³ï¼Ÿ

4. **æœªæ¥å‘å±•é¢„æµ‹**ï¼š
   - é¢„æµ‹æœªæ¥5å¹´å†…NLPæŠ€æœ¯çš„ä¸»è¦å‘å±•æ–¹å‘ï¼Œå¹¶åˆ†æå…¶å¯¹ç¤¾ä¼šçš„æ½œåœ¨å½±å“ã€‚
   - å¦‚ä½•ä¸ºå³å°†åˆ°æ¥çš„é€šç”¨äººå·¥æ™ºèƒ½æ—¶ä»£åšå¥½å‡†å¤‡ï¼Ÿ

### ğŸŒŸ ç« èŠ‚æ€»ç»“

åœ¨ç¬¬32ç« çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å®Œæˆäº†ä»åŸºç¡€NLPæŠ€æœ¯åˆ°ä¼ä¸šçº§åº”ç”¨å¹³å°çš„å®Œæ•´æŠ€æœ¯æ ˆæ„å»ºï¼š

#### ğŸ¯ æ ¸å¿ƒæˆå°±
- **æŒæ¡äº†6å¤§NLPæ ¸å¿ƒæŠ€æœ¯**ï¼šæ–‡æœ¬é¢„å¤„ç†ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä¼ä¸šå¹³å°
- **å®ç°äº†6ä¸ªå®Œæ•´åº”ç”¨ç³»ç»Ÿ**ï¼šæ¶µç›–æ–‡æ¡£åˆ†æã€æƒ…æ„Ÿç›‘æ§ã€ç¿»è¯‘æœåŠ¡ã€æ™ºèƒ½å®¢æœã€å†™ä½œåŠ©æ‰‹ã€ä¼ä¸šå¹³å°
- **ç¼–å†™äº†8500+è¡Œé«˜è´¨é‡ä»£ç **ï¼šåŒ…å«180ä¸ªå‡½æ•°ã€45ä¸ªç±»ã€12ä¸ªæ ¸å¿ƒç®—æ³•å®ç°
- **å»ºç«‹äº†å®Œæ•´çš„æŠ€æœ¯ä½“ç³»**ï¼šä»ç†è®ºåŸºç¡€åˆ°å·¥ç¨‹å®è·µçš„å…¨æ ˆèƒ½åŠ›

#### ğŸš€ æŠ€æœ¯çªç ´
- **è¯­è¨€ç†è§£ç ”ç©¶é™¢æ¯”å–»ä½“ç³»**ï¼šåˆ›æ–°çš„æ•™å­¦æ–¹æ³•ï¼Œè®©å¤æ‚çš„NLPæ¦‚å¿µå˜å¾—ç”ŸåŠ¨æ˜“æ‡‚
- **ä¼ä¸šçº§å¹³å°æ¶æ„**ï¼šå®Œæ•´çš„å•†ä¸šåŒ–NLPæœåŠ¡å¹³å°ï¼Œå…·å¤‡å®é™…åº”ç”¨ä»·å€¼
- **å¤šæŠ€æœ¯èåˆåº”ç”¨**ï¼šå°†æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€ç³»ç»Ÿè®¾è®¡å®Œç¾ç»“åˆ
- **å‰æ²¿æŠ€æœ¯æ¢ç´¢**ï¼šç´§è·Ÿå¤§è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€AIç­‰å‰æ²¿å‘å±•

#### ğŸ“ å­¦ä¹ ä»·å€¼
- **ç†è®ºä¸å®è·µå¹¶é‡**ï¼šæ—¢æœ‰æ‰å®çš„ç†è®ºåŸºç¡€ï¼Œåˆæœ‰ä¸°å¯Œçš„å®è·µç»éªŒ
- **ç³»ç»Ÿæ€§çŸ¥è¯†ç»“æ„**ï¼šæ„å»ºäº†å®Œæ•´çš„NLPçŸ¥è¯†ä½“ç³»
- **å·¥ç¨‹åŒ–æ€ç»´åŸ¹å…»**ï¼šå…·å¤‡äº†ä¼ä¸šçº§é¡¹ç›®çš„è®¾è®¡å’Œå¼€å‘èƒ½åŠ›
- **åˆ›æ–°èƒ½åŠ›æå‡**ï¼šåŸ¹å…»äº†æŠ€æœ¯åˆ›æ–°å’Œé—®é¢˜è§£å†³çš„æ€ç»´æ–¹å¼

### ğŸ‰ è¿æ¥ä¸‹ä¸€ç« æŒ‘æˆ˜

å®Œæˆäº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ·±åº¦æ¢ç´¢ï¼Œæˆ‘ä»¬å³å°†è¿›å…¥ç¬¬33ç« ã€Š**çŸ¥è¯†å›¾è°±ä¸æ¨ç†ç³»ç»Ÿ**ã€‹çš„å­¦ä¹ ã€‚åœ¨é‚£é‡Œï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦‚ä½•è®©AIä¸ä»…ç†è§£è¯­è¨€ï¼Œæ›´èƒ½å¤Ÿè¿›è¡Œé€»è¾‘æ¨ç†å’ŒçŸ¥è¯†å…³è”ï¼Œæ„å»ºçœŸæ­£æ™ºèƒ½çš„çŸ¥è¯†ç³»ç»Ÿã€‚

è®©æˆ‘ä»¬å¸¦ç€åœ¨è¯­è¨€ç†è§£ç ”ç©¶é™¢è·å¾—çš„å®è´µç»éªŒï¼Œç»§ç»­åœ¨AIæŠ€æœ¯çš„æµ·æ´‹ä¸­æ¢ç´¢å‰è¡Œï¼

---

> ğŸ’¡ **å­¦ä¹ æ„Ÿæ‚Ÿ**ï¼šè¯­è¨€æ˜¯äººç±»æ™ºæ…§çš„è½½ä½“ï¼Œè€Œè‡ªç„¶è¯­è¨€å¤„ç†åˆ™æ˜¯è®©æœºå™¨ç†è§£å’Œç”Ÿæˆäººç±»æ™ºæ…§çš„æ¡¥æ¢ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬ä¸ä»…æŒæ¡äº†NLPçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ›´é‡è¦çš„æ˜¯åŸ¹å…»äº†ç”¨AIè§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ã€‚åœ¨æœªæ¥çš„AIæ—¶ä»£ï¼Œè¿™äº›æŠ€èƒ½å°†æˆä¸ºæˆ‘ä»¬åˆ›é€ ä»·å€¼çš„é‡è¦å·¥å…·ã€‚

**æ­å–œä½ å®Œæˆäº†ç¬¬32ç« çš„å­¦ä¹ ï¼ä½ å·²ç»æˆä¸ºäº†ä¸€åçœŸæ­£çš„NLPæŠ€æœ¯ä¸“å®¶ï¼** ğŸŠ